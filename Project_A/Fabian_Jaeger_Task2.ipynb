{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from Common import NeuralNet, MultiVariatePoly\n",
    "import time\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "torch.manual_seed(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pinns:\n",
    "    def __init__(self, n_int_, n_sb_, n_tb_):\n",
    "        self.n_int = n_int_\n",
    "        self.n_sb = n_sb_\n",
    "        self.n_tb = n_tb_\n",
    "\n",
    "        self.alpha_f = 0.005\n",
    "        self.alpha_s = 0.08\n",
    "        self.h_f = 5\n",
    "        self.h_s = 6\n",
    "        self.T_hot = 4\n",
    "        self.T_0 = 1\n",
    "        self.U_f = 1 \n",
    "\n",
    "        # Extrema of the solution domain (t,x) in [0,0.1]x[-1,1]\n",
    "        self.domain_extrema = torch.tensor([[0, 8],  # Time dimension\n",
    "                                            [0, 1]])  # Space dimension\n",
    "\n",
    "        # Number of space dimensions\n",
    "        self.space_dimensions = 1\n",
    "\n",
    "        # Parameter to balance role of data and PDE\n",
    "        self.lambda_u = 10\n",
    "\n",
    "        # F Dense NN to approximate the solution of the underlying heat equation\n",
    "        self.approximate_solution = NeuralNet(input_dimension=self.domain_extrema.shape[0], output_dimension=2,\n",
    "                                              n_hidden_layers=4,\n",
    "                                              neurons=20,\n",
    "                                              regularization_param=0.,\n",
    "                                              regularization_exp=2.,\n",
    "                                              retrain_seed=42)\n",
    "        '''self.approximate_solution = MultiVariatePoly(self.domain_extrema.shape[0], 3)'''\n",
    "\n",
    "        # Generator of Sobol sequences\n",
    "        self.soboleng = torch.quasirandom.SobolEngine(dimension=self.domain_extrema.shape[0])\n",
    "\n",
    "        # Training sets S_sb, S_tb, S_int as torch dataloader\n",
    "        self.training_set_sb, self.training_set_tb, self.training_set_int = self.assemble_datasets()\n",
    "\n",
    "    ################################################################################################\n",
    "    # Function to linearly transform a tensor whose value are between 0 and 1\n",
    "    # to a tensor whose values are between the domain extrema\n",
    "    def convert(self, tens):\n",
    "        assert (tens.shape[1] == self.domain_extrema.shape[0])\n",
    "        return tens * (self.domain_extrema[:, 1] - self.domain_extrema[:, 0]) + self.domain_extrema[:, 0]\n",
    "    \n",
    "    # Initial condition T_f(x,t = 0) = T_s(x,t=0) = T_0\n",
    "    # def initial_condition(self, x):\n",
    "    #     return torch.tensor(self.T_0)\n",
    "\n",
    "    ################################################################################################\n",
    "    # Function returning the input-output tensor required to assemble the training set S_tb corresponding to the temporal boundary\n",
    "\n",
    "    def fluid_velocity(self, inputs):\n",
    "        U_f = torch.full(inputs.shape, -1)\n",
    "    \n",
    "        for i, t in enumerate(inputs):\n",
    "            # Charging Phase\n",
    "            if (t <=1) or (t > 4 and t<=5): U_f[i] = 1\n",
    "            # Idle Phase\n",
    "            elif (t > 1 and t <= 2) or (t > 3 and t <= 4) or (t > 5 and t <= 6): U_f[i] = 0\n",
    "            # Discharging Phase\n",
    "            elif (t > 2 and t <= 3) or (t > 6 and t <= 7): U_f[i] = -1\n",
    "           \n",
    "        return U_f\n",
    "\n",
    "\n",
    "    def add_temporal_boundary_points(self):\n",
    "        # t0 = self.domain_extrema[0, 0]\n",
    "        # input_tb = self.soboleng.draw(self.n_tb)    # input_sb has two columns (t, x) both with random numbers in the two respective domains\n",
    "        # input_tb[:, 0] = torch.full(input_tb[:, 0].shape, t0)   # overwrite the entier column of time with t0\n",
    "        # output_tb = torch.full(input_tb.shape, self.T_0)\n",
    "        t0 = self.domain_extrema[0, 0]\n",
    "        input_tb = self.convert(self.soboleng.draw(self.n_tb))\n",
    "        input_tb[:, 0] = torch.full(input_tb[:, 0].shape, t0)\n",
    "        # print(\"Input temporal boundary shape: \", input_tb.shape)\n",
    "        # output_tb = self.initial_condition(input_tb[:, 1]).reshape(-1, 1)\n",
    "        output_tb = torch.randn(self.n_tb, 2).fill_(self.T_0)\n",
    "        # print( \"Output temporal boundary shape: \", output_tb.shape)\n",
    "        return input_tb, output_tb\n",
    "\n",
    "    # Function returning the input-output tensor required to assemble the training set S_sb corresponding to the spatial boundary\n",
    "    def add_spatial_boundary_points(self):\n",
    "        #! Should be fixed\n",
    "\n",
    "        #set first column of output to be T_f\n",
    "        #set second column of output to be T_s\n",
    "\n",
    "        # Get x-coordinates of the extrema of the domain\n",
    "        x0 = self.domain_extrema[1, 0]\n",
    "        xL = self.domain_extrema[1, 1]\n",
    "\n",
    "        input_sb = self.convert(self.soboleng.draw(self.n_sb))\n",
    "\n",
    "        # Corresponds to the dirichlet boundary condition u_b(t,-1) = 0\n",
    "        input_sb_0 = torch.clone(input_sb)\n",
    "        input_sb_0[:, 1] = torch.full(input_sb_0[:, 1].shape, x0)\n",
    "\n",
    "        # Corresponds to the dirichlet boundary condition u_b(t, 1) = 0\n",
    "        input_sb_L = torch.clone(input_sb)\n",
    "        input_sb_L[:, 1] = torch.full(input_sb_L[:, 1].shape, xL)\n",
    "\n",
    "        # These are the Dirichlet boundary conditions at the two extrema domain points -1 and 1\n",
    "        # output_sb_0 = torch.zeros((input_sb.shape[0], 1))\n",
    "        output_sb_L = torch.zeros((input_sb.shape[0], 2))\n",
    "        # output_sb_0 = ((self.T_hot-self.T_0)/(1+torch.exp(-200*(input_sb-0.25)))+self.T_0) \n",
    "\n",
    "        input_sb_t =  input_sb[:, 0]\n",
    "        # print(((self.T_hot-self.T_0)/(1+torch.exp(-200*(input_sb_t-0.25)))+self.T_0))\n",
    "        # print(input_sb_t)\n",
    "        # output_sb_0 = torch.full(((self.T_hot-self.T_0)/(1+torch.exp(-200*(input_sb_t-0.25)))+self.T_0), x0)\n",
    "\n",
    "        \n",
    "        output_sb_0 = ((self.T_hot-self.T_0)/(1+torch.exp(-200*(input_sb_t-0.25)))+self.T_0).reshape(-1,1)\n",
    "        new_tensor = torch.zeros_like(output_sb_0)\n",
    "\n",
    "        output_sb_0 = torch.cat((output_sb_0, new_tensor), dim=1)\n",
    "        # print(\"Output sb: {}\".format(output_sb_0))\n",
    "        # output_sb_0 = torch.full(output_sb_0[:, 1].shape, 0)\n",
    "\n",
    "        # conditions T_xL_dx\n",
    "\n",
    "        # print(\"Input sb 0: {}\".format(input_sb_0))\n",
    "        # print(\"Input sb L: {}\".format(input_sb_L))\n",
    "        # print(\"Output sb 0: {}\".format(output_sb_0))\n",
    "        # print(\"Output sb L: {}\".format(output_sb_L))\n",
    "\n",
    "        # print(\"Concatenated input: {}\".format(torch.cat([input_sb_0, input_sb_L], 0)))\n",
    "        # print(\"Concatenated output: {}\".format(torch.cat([output_sb_0, output_sb_L], 0)))\n",
    "    \n",
    "        # print(output_sb_0)\n",
    "\n",
    "        return torch.cat([input_sb_0, input_sb_L], 0), torch.cat([output_sb_0, output_sb_L], 0)\n",
    "\n",
    "    #  Function returning the input-output tensor required to assemble the training set S_int corresponding to the interior domain where the PDE is enforced\n",
    "    def add_interior_points(self):\n",
    "        input_int = self.convert(self.soboleng.draw(self.n_int))\n",
    "        # input_int = self.soboleng.draw(self.n_int)\n",
    "        output_int = torch.zeros((input_int.shape[0], 1))\n",
    "        return input_int, output_int\n",
    "\n",
    "    # Function returning the training sets S_sb, S_tb, S_int as dataloader\n",
    "    def assemble_datasets(self):\n",
    "        input_sb, output_sb = self.add_spatial_boundary_points()   # S_sb\n",
    "        input_tb, output_tb = self.add_temporal_boundary_points()  # S_tb\n",
    "        input_int, output_int = self.add_interior_points()         # S_int\n",
    "\n",
    "        training_set_sb = DataLoader(torch.utils.data.TensorDataset(input_sb, output_sb), batch_size=2*self.space_dimensions*self.n_sb, shuffle=False)\n",
    "        training_set_tb = DataLoader(torch.utils.data.TensorDataset(input_tb, output_tb), batch_size=self.n_tb, shuffle=False)\n",
    "        training_set_int = DataLoader(torch.utils.data.TensorDataset(input_int, output_int), batch_size=self.n_int, shuffle=False)\n",
    "\n",
    "        return training_set_sb, training_set_tb, training_set_int\n",
    "\n",
    "    ################################################################################################\n",
    "    # Function to compute the terms required in the definition of the TEMPORAL boundary residual\n",
    "    # Takes as input the temporal boundary points and returns the prediction of the neural network at the temporal boundary points\n",
    "    def apply_initial_condition(self, input_tb):\n",
    "        u_pred_tb = self.approximate_solution(input_tb)\n",
    "        #print(u_pred_tb)\n",
    "        return u_pred_tb\n",
    "\n",
    "    # Function to compute the terms required in the definition of the SPATIAL boundary residual\n",
    "    # Takes as input the spatial boundary points and returns the prediction of the neural network at the temporal boundary points\n",
    "    def apply_boundary_conditions(self, input_sb):\n",
    "        #! Error probably here\n",
    "        input_sb.requires_grad = True\n",
    "        # input_sb = input_sb.detach()\n",
    "        # input_sb.requires_grad = True\n",
    "\n",
    "\n",
    "        # u_pred_sb = self.approximate_solution(input_sb) \n",
    "        # print(u_pred_sb)\n",
    "\n",
    "        #CORRECT\n",
    "        input_sb_x0 = input_sb[:int(input_sb.shape[0]/2), :]\n",
    "        input_sb_xL = input_sb[int(input_sb.shape[0]/2):, :]\n",
    "\n",
    "\n",
    "        # print(\"Input sb x0: {}\".format(input_sb_x0))\n",
    "        # print(\"Input sb xL: {}\".format(input_sb_xL))\n",
    "       \n",
    "        #condtion on Tf_x0\n",
    "\n",
    "        #CORRECT\n",
    "\n",
    "        # pre\n",
    "\n",
    "        u_pred_Tf_x0 = self.approximate_solution(input_sb_x0)[:, 0]\n",
    "        u_pred_Ts_x0 = self.approximate_solution(input_sb_x0)[:, 1]\n",
    "\n",
    "        # print(\"T_f x0 pred: {}\".format(u_pred_Tf_x0))\n",
    "\n",
    "\n",
    "        grad_Ts_x0 = torch.autograd.grad(u_pred_Ts_x0.sum(), input_sb_x0, create_graph=True)[0][:, 1]\n",
    "\n",
    "        # u_pred_Ts_x0 = u_pred_Ts_x0.view(-1, 1)\n",
    "        # grad_Ts_x0 = grad_Ts_x0.view(-1, 1).squeeze()\n",
    "\n",
    "        # print(\"pred Grad T_s x0: {}\".format(grad_Ts_x0))\n",
    "\n",
    "\n",
    "\n",
    "        # u_pred_xL = self.approximate_solution(input_sb_xL)\n",
    "        u_pred_Tf_xL = self.approximate_solution(input_sb_xL)[:, 0]\n",
    "        u_pred_Ts_xL = self.approximate_solution(input_sb_xL)[:, 1]\n",
    "\n",
    "        # print(\"U_pred_Tf_x0: {}\".format(u_pred_Tf_x0))\n",
    "        # print(\"U_pred_Ts_x0: {}\".format(u_pred_Ts_x0))\n",
    "\n",
    "        #conditidion on grad_Ts_x0\n",
    "        \n",
    " \n",
    "\n",
    "        #condition on grad_Tf/s_xL\n",
    "      \n",
    "\n",
    "        grad_Tf_xL = torch.autograd.grad(u_pred_Tf_xL.sum(), input_sb_xL, create_graph=True)[0][:, 1]\n",
    "        grad_Ts_xL = torch.autograd.grad(u_pred_Ts_xL.sum(), input_sb_xL, create_graph=True)[0][:, 1]\n",
    "\n",
    "        # print(\"pred Grad T_f xL: {}\".format(grad_Tf_xL))\n",
    "        # print(\"pred Grad T_s xL: {}\".format(grad_Ts_xL))\n",
    "\n",
    "        # grad_Ts_xL = u_pred_Tf_x0.view(-1, 1).squeeze()\n",
    "        # grad_Tf_xL = grad_Tf_xL.view(-1, 1).squeeze()\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"grad_Tf_xL: \", grad_Tf_xL.shape)\n",
    "        # print(\"grad_Ts_xL: \", grad_Ts_xL.shape)\n",
    "\n",
    "        # print(\"u_pred_Tf_x0: \", u_pred_Tf_x0.shape)\n",
    "        # print(\"grad_Ts_x0: \", grad_Ts_x0.shape)\n",
    "\n",
    "        # print('u_pred: ', u_pred_sb.requires_grad)\n",
    "        # print('inp: ', input_sb.requires_grad)\n",
    "\n",
    "        u_pred_concat_1 = torch.cat((u_pred_Tf_x0.reshape(-1,1), grad_Ts_x0.reshape(-1,1)), dim =1)\n",
    "        # print(\"U_pred_concat_1: \", u_pred_concat_1)\n",
    "        u_pred_concat_2 = torch.cat((grad_Tf_xL.reshape(-1,1), grad_Ts_xL.reshape(-1,1)), dim =1)\n",
    "        # print(\"U_pred_concat_2: \", u_pred_concat_2)\n",
    "        total_concat = torch.cat((u_pred_concat_1, u_pred_concat_2), dim =0)\n",
    "        # print(\"Total_concat: \", total_concat.shape)\n",
    "\n",
    "\n",
    "        #obtain the prediction for (T_f, T_s) at the boundary points (x=0, t) for the first 64 and (x=1, t) for the second 64\n",
    "        # print(u_pred_sb.shape)\n",
    "        # print(input_sb.shape)\n",
    "\n",
    "        # print(u_pred_sb[:self.n_sb, 1].reshape(-1, 1).shape)\n",
    "        # print(input_sb[:self.n_sb, :].shape)\n",
    "        \n",
    "        # T_s_1= u_pred_sb[:self.n_sb, 1].reshape(-1, 1)\n",
    "        # T_s_2= u_pred_sb[:self.n_sb, 1].reshape(-1, 1) \n",
    "\n",
    "        # input_sb_1 = input_sb[:self.n_sb, :]\n",
    "        # input_sb_2 = input_sb[self.n_sb:-1, :]\n",
    "\n",
    "        # print(input_sb[:,0])\n",
    "        # # print('u_pred: ', T_s_1.requires_grad)\n",
    "        # # print('inp: ', input_sb_1.requires_grad)\n",
    "\n",
    "        # grad = torch.autograd.grad(u_pred_sb.sum(), input_sb, create_graph=True)[0]\n",
    "        # print(grad)\n",
    "\n",
    "        # grad_T_s_1 = grad[:self.n_sb, 1]\n",
    "        # grad_T_s_2 = grad[self.n_sb:, 1]\n",
    "        # print(\"Shape grad_T_s_1: \", grad_T_s_1.shape)\n",
    "        # print(\"Shape grad_T_s_2: \", grad_T_s_2.shape)\n",
    "        # pred_T_f_1 = (self.T_hot - self.T_0)/(1 + torch.exp(-200*(input_sb[:self.n_sb,0]))) + self.T_0\n",
    "        # grad_T_f_2 = grad[self.n_sb:, 0]\n",
    "        \n",
    "        # # pred_T_f_2 = (self.T_hot - self.T_0)/(1 + torch.exp(-200*(input_sb[self.n_sb:,0]))) + self.T_0\n",
    "        # print(\"Shape pred_T_f_1: \", pred_T_f_1.shape)\n",
    "        # # print(\"Shape pred_T_f_2: \", pred_T_f_2.shape)\n",
    "        # print(pred_T_f_1)\n",
    "        # print(pred_T_f_2)\n",
    "\n",
    "        # grad_T_s_1 = torch.autograd.grad(T_s_1.sum(), input_sb_1, create_graph=True)[0][:,1]\n",
    "        # grad_T_s_2 = torch.autograd.grad(T_s_2.sum(), input_sb_2, create_graph=True)[0][:,1]\n",
    "        # # grad_T_s = torch.clone(T_s_1)\n",
    "        # pred_T_f_1 = (self.T_hot - self.T_0)/(1 + torch.exp(-200*(u_pred_sb[:self.n_t, 0]))) + self.T_0\n",
    "        # pred_T_f_2 = (self.T_hot - self.T_0)/(1 + torch.exp(-200*(u_pred_sb[self.n_t:-1, 0]))) + self.T_0\n",
    "\n",
    "        # part_1 = torch.cat([pred_T_f_1, grad_T_s_1],1)\n",
    "        # part_2 = torch.cat([pred_T_f_2, grad_T_s_2],1)\n",
    "        # print(part_1)\n",
    "        # print(part_2)\n",
    "        # concat = torch.cat([part_1, part_2],0)\n",
    "        # print(pred_T_f)\n",
    "        # print(grad_T_s)\n",
    "\n",
    "        return torch.cat([u_pred_concat_1, u_pred_concat_2],0)\n",
    "\n",
    "    # Function to compute the PDE residuals\n",
    "    def compute_pde_residual(self, input_int):\n",
    "        #! Should be correct\n",
    "\n",
    "        input_int.requires_grad = True\n",
    "\n",
    "        # Obtain the prediction from the neural network\n",
    "        u = self.approximate_solution(input_int)\n",
    "        \n",
    "        # First column of prediction will be the temperature of the fluid\n",
    "        # Second column of prediction will be the temperature of the solid\n",
    "        T_f = u[:,0]\n",
    "        T_s = u[:,1]\n",
    "\n",
    "        # grad compute the gradient of a \"SCALAR\" function L with respect to some input nxm TENSOR Z=[[x1, y1],[x2,y2],[x3,y3],...,[xn,yn]], m=2\n",
    "        # it returns grad_L = [[dL/dx1, dL/dy1],[dL/dx2, dL/dy2],[dL/dx3, dL/dy3],...,[dL/dxn, dL/dyn]]\n",
    "        # Note: pytorch considers a tensor [u1, u2,u3, ... ,un] a vectorial function\n",
    "        # whereas sum_u = u1 + u2 + u3 + u4 + ... + un as a \"scalar\" one\n",
    "\n",
    "        # In our case ui = u(xi), therefore the line below returns:\n",
    "        # grad_u = [[dsum_u/dx1, dsum_u/dy1],[dsum_u/dx2, dsum_u/dy2],[dsum_u/dx3, dL/dy3],...,[dsum_u/dxm, dsum_u/dyn]]\n",
    "        # and dsum_u/dxi = d(u1 + u2 + u3 + u4 + ... + un)/dxi = d(u(x1) + u(x2) u3(x3) + u4(x4) + ... + u(xn))/dxi = dui/dxi\n",
    "        grad_tf = torch.autograd.grad(T_f.sum(), input_int, create_graph=True)[0]\n",
    "        grad_ts = torch.autograd.grad(T_s.sum(), input_int, create_graph=True)[0]\n",
    "        \n",
    "        grad_ts_t = grad_ts[:,0] \n",
    "        grad_ts_x = grad_ts[:,1]\n",
    "        grad_tf_t = grad_tf[:,0]\n",
    "        grad_tf_x = grad_tf[:,1 ]\n",
    "        \n",
    "        grad_tf_xx = torch.autograd.grad(grad_tf_x.sum(), input_int, create_graph=True)[0][:,1]\n",
    "        grad_ts_xx = torch.autograd.grad(grad_ts_x.sum(), input_int, create_graph=True)[0][:,1]\n",
    "\n",
    "\n",
    "        residual1 = (grad_tf_t) + (self.U_f*grad_tf_x) - (self.alpha_f*grad_tf_xx) + (self.h_f*(T_f - T_s))\n",
    "        residual2 = (grad_ts_t) - (self.alpha_s*grad_ts_xx) - self.h_s*(T_f - T_s)\n",
    "\n",
    "        # print(residual1, residual2)\n",
    "\n",
    "        # residual = residual1 + residual2\n",
    "        # grad_u_t = grad_u[:, 0]\n",
    "        # grad_u_x = grad_u[:, 1]\n",
    "        # grad_u_xx = torch.autograd.grad(grad_u_x.sum(), input_int, create_graph=True)[0][:, 1]\n",
    "\n",
    "        #grad_u_sq_x = torch.autograd.grad(u_sq.sum(), input_int, create_graph=True)[0][:,1]\n",
    "\n",
    "        # residual = grad_u_t - grad_u_xx\n",
    "        return residual1.reshape(-1, ), residual2.reshape(-1, )\n",
    "\n",
    "    # Function to compute the total loss (weighted sum of spatial boundary loss, temporal boundary loss and interior loss)\n",
    "    def compute_loss(self, inp_train_sb, u_train_sb, inp_train_tb, u_train_tb, inp_train_int, verbose=True):\n",
    "        u_pred_sb = self.apply_boundary_conditions(inp_train_sb)\n",
    "        u_pred_tb = self.apply_initial_condition(inp_train_tb)\n",
    "        # print(u_pred_sb.shape)\n",
    "\n",
    "\n",
    "        # print(\"U_pred_sb: {}\".format(u_pred_sb))\n",
    "        # Seems correct\n",
    "        # print(\"U_train_sb: {}\".format(u_train_sb))\n",
    "\n",
    "        assert (u_pred_sb.shape[1] == u_train_sb.shape[1])\n",
    "        assert (u_pred_tb.shape[1] == u_train_tb.shape[1])\n",
    "\n",
    "        res1, res2 = self.compute_pde_residual(inp_train_int)\n",
    "        # r_int = self.compute_pde_residual(inp_train_int)\n",
    "\n",
    "        # Initial Condition loss\n",
    "        res_tb = u_train_tb - u_pred_tb\n",
    "        \n",
    "        # This corresponds to the condition with the exponential in the boundary conditions\n",
    "        ##########################################\n",
    "\n",
    "        len_train_half = int(u_train_sb.shape[0]/2)\n",
    "        # print(\"len_train: {}\".format(len_train_half))\n",
    "        #! ERROR?\n",
    "        Tf_train_sb_0 = u_train_sb[:len_train_half, 0]\n",
    "        Tf_pred_sb_0 = u_pred_sb[:len_train_half, 0]\n",
    "        # print(\"Tf_train_sb_0: {}\".format(Tf_train_sb_0))\n",
    "        # print(\"Tf_pred_sb_0: {}\".format(Tf_pred_sb_0))\n",
    "        res_Tf_sb_0 = Tf_train_sb_0 - Tf_pred_sb_0\n",
    "\n",
    "        # Condition grad T_s/x at x = 0\n",
    "        Ts_train_sb_0 = u_train_sb[:len_train_half, 1]\n",
    "        Ts_pred_sb_0 = u_pred_sb[:len_train_half, 1]\n",
    "        # print(\"Ts_train_sb_0: {}\".format(Ts_train_sb_0))\n",
    "        # print(\"Ts_pred_sb_0: {}\".format(Ts_pred_sb_0))\n",
    "        res_Ts_sb_0 = Ts_train_sb_0 - Ts_pred_sb_0\n",
    "\n",
    "        # Condition grad T_s/x at x = 1\n",
    "        Ts_train_sb_L = u_train_sb[len_train_half:, 1]\n",
    "        Ts_pred_sb_L = u_pred_sb[len_train_half:, 1]\n",
    "        # print(\"Ts_train_sb_L: {}\".format(Ts_train_sb_L))\n",
    "        # print(\"Ts_pred_sb_L: {}\".format(Ts_pred_sb_L))\n",
    "        res_Ts_sb_L = Ts_train_sb_L - Ts_pred_sb_L\n",
    "\n",
    "        # Condition grad T_f/x at x = 1\n",
    "        Tf_train_sb_L = u_train_sb[len_train_half:, 0]\n",
    "        Tf_pred_sb_L = u_pred_sb[len_train_half:, 0]\n",
    "        # print(\"Tf_train_sb_L: {}\".format(Tf_train_sb_L))\n",
    "        # print(\"Tf_pred_sb_L: {}\".format(Tf_pred_sb_L))\n",
    "        res_Tf_sb_L = Tf_train_sb_L - Tf_pred_sb_L\n",
    "\n",
    "        # print(res_Tf_sb_0)\n",
    "        # print(res_Ts_sb_0)\n",
    "        # print(res_Ts_sb_L)\n",
    "        # print(res_Tf_sb_L)\n",
    "        ############################``\n",
    "\n",
    "        loss_sb = torch.mean(abs(res_Tf_sb_0) ** 2) + torch.mean(abs(res_Ts_sb_0) ** 2) + torch.mean(abs(res_Ts_sb_L) ** 2) + torch.mean(abs(res_Tf_sb_L) ** 2)\n",
    "        loss_tb = torch.mean(abs(res_tb) ** 2)\n",
    "        loss_int = torch.mean(abs(res1) ** 2) + torch.mean(abs(res2) ** 2)\n",
    "\n",
    "        # print(\"Loss int: {}\".format(loss_int))\n",
    "        # print(\"Loss sb: {}\".format(loss_sb))\n",
    "        # print(\"Loss tb: {}\".format(loss_tb))\n",
    "\n",
    "        loss_u = loss_sb + loss_tb\n",
    "\n",
    "        # loss = torch.log10(self.lambda_u * (loss_sb + loss_tb) + loss_int)\n",
    "        loss = torch.log10(self.lambda_u * (1*loss_sb + 1*loss_tb) + 1*loss_int)\n",
    "        if verbose: print(\"Total loss: \", round(loss.item(), 4), \"| PDE Loss: \", round(torch.log10(loss_u).item(), 4), \"| Function Loss: \", round(torch.log10(loss_int).item(), 4))\n",
    "\n",
    "        return loss\n",
    "\n",
    "    ################################################################################################\n",
    "    def fit(self, num_epochs, optimizer, verbose=True):\n",
    "        history = list()\n",
    "\n",
    "        # Loop over epochs\n",
    "        for epoch in range(num_epochs):\n",
    "            if verbose: print(\"################################ \", epoch, \" ################################\")\n",
    "\n",
    "            for j, ((inp_train_sb, u_train_sb), (inp_train_tb, u_train_tb), (inp_train_int, u_train_int)) in enumerate(zip(self.training_set_sb, self.training_set_tb, self.training_set_int)):\n",
    "                def closure():\n",
    "                    optimizer.zero_grad()\n",
    "                    loss = self.compute_loss(inp_train_sb, u_train_sb, inp_train_tb, u_train_tb, inp_train_int, verbose=verbose)\n",
    "                    loss.backward()\n",
    "\n",
    "                    history.append(loss.item())\n",
    "                    return loss\n",
    "\n",
    "                optimizer.step(closure=closure)\n",
    "\n",
    "        print('Final Loss: ', history[-1])\n",
    "\n",
    "        return history\n",
    "\n",
    "    ################################################################################################\n",
    "    def plotting(self):\n",
    "        inputs = self.soboleng.draw(100000)\n",
    "        inputs = self.convert(inputs)\n",
    "\n",
    "        output = self.approximate_solution(inputs).reshape(-1, )\n",
    "        # exact_output = self.exact_solution(inputs).reshape(-1, )\n",
    "\n",
    "        fig, axs = plt.subplots(1, 2, figsize=(16, 8), dpi=150)\n",
    "        # im1 = axs[0].scatter(inputs[:, 1].detach(), inputs[:, 0].detach(), c=exact_output.detach(), cmap=\"jet\")\n",
    "        axs[0].set_xlabel(\"x\")\n",
    "        axs[0].set_ylabel(\"t\")\n",
    "        # plt.colorbar(im1, ax=axs[0])\n",
    "        axs[0].grid(True, which=\"both\", ls=\":\")\n",
    "        im2 = axs[1].scatter(inputs[:, 1].detach(), inputs[:, 0].detach(), c=output.detach(), cmap=\"jet\")\n",
    "        axs[1].set_xlabel(\"x\")\n",
    "        axs[1].set_ylabel(\"t\")\n",
    "        plt.colorbar(im2, ax=axs[1])\n",
    "        axs[1].grid(True, which=\"both\", ls=\":\")\n",
    "        axs[0].set_title(\"Exact Solution\")\n",
    "        axs[1].set_title(\"Approximate Solution\")\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "        # err = (torch.mean((output - exact_output) ** 2) / torch.mean(exact_output ** 2)) ** 0.5 * 100\n",
    "        # print(\"L2 Relative Error Norm: \", err.item(), \"%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 1\n",
    "optimizer_LBFGS = optim.LBFGS(pinn.approximate_solution.parameters(),\n",
    "                              lr=float(0.5),\n",
    "                              max_iter=50000,\n",
    "                              max_eval=50000,\n",
    "                              history_size=150,\n",
    "                              line_search_fn=\"strong_wolfe\",\n",
    "                              tolerance_change=1.0 * np.finfo(float).eps)\n",
    "optimizer_ADAM = optim.Adam(pinn.approximate_solution.parameters(),\n",
    "                            lr=float(0.001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = pinn.fit(num_epochs=n_epochs,\n",
    "                optimizer=optimizer_LBFGS,\n",
    "                verbose=True)\n",
    "\n",
    "plt.figure(dpi=150)\n",
    "plt.grid(True, which=\"both\", ls=\":\")\n",
    "plt.plot(np.arange(1, len(hist) + 1), hist, label=\"Train Loss\")\n",
    "plt.xscale(\"log\")\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
