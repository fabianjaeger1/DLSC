{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "6u_j73SUfQxG",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "# Task 1: PINNs for solving PDEs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NOPdRWmOfQxI",
        "outputId": "7e48ccf5-8b35-43b2-fa3c-ebf973323f50",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x10f4de210>"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.optim as optim\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from Common import NeuralNet, MultiVariatePoly\n",
        "import pandas as pd\n",
        "import time\n",
        "torch.autograd.set_detect_anomaly(True)\n",
        "torch.manual_seed(128)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We consider the following PDE (corresponding to the evolution of the Temperature of the solid and liquid phase in the first cycle for the charging phase) governed by the following two equations $$\\begin{aligned} \\frac{\\partial \\bar{T}_{f}}{\\partial t}+U_{f} \\frac{\\partial \\bar{T}_{f}}{\\partial x}=\\alpha_{f} \\frac{\\partial^{2} \\bar{T}_{f}}{\\partial x^{2}}-h_{f}\\left(\\bar{T}_{f}-\\bar{T}_{s}\\right) & x \\in[0,1], t \\in[0,1] \\\\ \\frac{\\partial \\bar{T}_{s}}{\\partial t}=\\alpha_{s} \\frac{\\partial^{2} \\bar{T}_{s}}{\\partial x^{2}}+h_{s}\\left(\\bar{T}_{f}-\\bar{T}_{s}\\right) & x \\in[0,1], t \\in[0,1]\\end{aligned}$$ with initial conditions $$\\bar{T}_{f}(x, t=0)=\\bar{T}_{s}(x, t=0)=T_{0}, \\quad x \\in[0,1]$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "HgRzd8_wfQxK",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "class Pinns:\n",
        "    def __init__(self, n_int_, n_sb_, n_tb_):\n",
        "        self.n_int = n_int_\n",
        "        self.n_sb = n_sb_\n",
        "        self.n_tb = n_tb_\n",
        "\n",
        "        self.alpha_f = 0.05\n",
        "        self.alpha_s = 0.08\n",
        "        self.h_f = 5\n",
        "        self.h_s = 6\n",
        "        self.T_hot = 4\n",
        "        self.T_0 = 1\n",
        "        self.U_f = 1 \n",
        "\n",
        "        # Extrema of the solution domain (t,x) in [0,0.1]x[-1,1]\n",
        "        self.domain_extrema = torch.tensor([[0, 1],  # Time dimension\n",
        "                                            [0, 1]])  # Space dimension\n",
        "\n",
        "        # Number of space dimensions\n",
        "        self.space_dimensions = 1\n",
        "\n",
        "        # Parameter to balance role of data and PDE\n",
        "        self.lambda_u = 10\n",
        "\n",
        "        # F Dense NN to approximate the solution of the underlying heat equation\n",
        "        self.approximate_solution = NeuralNet(input_dimension=self.domain_extrema.shape[0], output_dimension=2,\n",
        "                                              n_hidden_layers=4,\n",
        "                                              neurons=20,\n",
        "                                              regularization_param=0.,\n",
        "                                              regularization_exp=2.,\n",
        "                                              retrain_seed=42)\n",
        "        '''self.approximate_solution = MultiVariatePoly(self.domain_extrema.shape[0], 3)'''\n",
        "\n",
        "        # Generator of Sobol sequences\n",
        "        self.soboleng = torch.quasirandom.SobolEngine(dimension=self.domain_extrema.shape[0])\n",
        "\n",
        "        # Training sets S_sb, S_tb, S_int as torch dataloader\n",
        "        self.training_set_sb, self.training_set_tb, self.training_set_int = self.assemble_datasets()\n",
        "\n",
        "    ################################################################################################\n",
        "    # Function to linearly transform a tensor whose value are between 0 and 1\n",
        "    # to a tensor whose values are between the domain extrema\n",
        "    def convert(self, tens):\n",
        "        assert (tens.shape[1] == self.domain_extrema.shape[0])\n",
        "        return tens * (self.domain_extrema[:, 1] - self.domain_extrema[:, 0]) + self.domain_extrema[:, 0]\n",
        "    \n",
        "    # Initial condition T_f(x,t = 0) = T_s(x,t=0) = T_0\n",
        "    # def initial_condition(self, x):\n",
        "    #     return torch.tensor(self.T_0)\n",
        "\n",
        "    ################################################################################################\n",
        "    # Function returning the input-output tensor required to assemble the training set S_tb corresponding to the temporal boundary\n",
        "    def add_temporal_boundary_points(self):\n",
        "        # t0 = self.domain_extrema[0, 0]\n",
        "        # input_tb = self.soboleng.draw(self.n_tb)    # input_sb has two columns (t, x) both with random numbers in the two respective domains\n",
        "        # input_tb[:, 0] = torch.full(input_tb[:, 0].shape, t0)   # overwrite the entier column of time with t0\n",
        "        # output_tb = torch.full(input_tb.shape, self.T_0)\n",
        "        t0 = self.domain_extrema[0, 0]\n",
        "        input_tb = self.convert(self.soboleng.draw(self.n_tb))\n",
        "        input_tb[:, 0] = torch.full(input_tb[:, 0].shape, t0)\n",
        "        # print(\"Input temporal boundary shape: \", input_tb.shape)\n",
        "        # output_tb = self.initial_condition(input_tb[:, 1]).reshape(-1, 1)\n",
        "        output_tb = torch.randn(self.n_tb, 2).fill_(self.T_0)\n",
        "        # print( \"Output temporal boundary shape: \", output_tb.shape)\n",
        "        return input_tb, output_tb\n",
        "\n",
        "    # Function returning the input-output tensor required to assemble the training set S_sb corresponding to the spatial boundary\n",
        "    def add_spatial_boundary_points(self):\n",
        "        #! Should be fixed\n",
        "\n",
        "        #set first column of output to be T_f\n",
        "        #set second column of output to be T_s\n",
        "\n",
        "        # Get x-coordinates of the extrema of the domain\n",
        "        x0 = self.domain_extrema[1, 0]\n",
        "        xL = self.domain_extrema[1, 1]\n",
        "\n",
        "        # input_sb = self.convert(self.soboleng.draw(self.n_sb))\n",
        "        input_sb = self.soboleng.draw(self.n_sb)\n",
        "\n",
        "        # Corresponds to the dirichlet boundary condition u_b(t,-1) = 0\n",
        "        input_sb_0 = torch.clone(input_sb)\n",
        "        input_sb_0[:, 1] = torch.full(input_sb_0[:, 1].shape, x0)\n",
        "\n",
        "        # Corresponds to the dirichlet boundary condition u_b(t, 1) = 0\n",
        "        input_sb_L = torch.clone(input_sb)\n",
        "        input_sb_L[:, 1] = torch.full(input_sb_L[:, 1].shape, xL)\n",
        "\n",
        "        # These are the Dirichlet boundary conditions at the two extrema domain points -1 and 1\n",
        "        # output_sb_0 = torch.zeros((input_sb.shape[0], 1))\n",
        "        output_sb_L = torch.zeros((input_sb.shape[0], 2))\n",
        "        # output_sb_0 = ((self.T_hot-self.T_0)/(1+torch.exp(-200*(input_sb-0.25)))+self.T_0) \n",
        "\n",
        "        input_sb_t =  input_sb[:, 0]\n",
        "        \n",
        "        output_sb_0 = ((self.T_hot-self.T_0)/(1+torch.exp(-200*(input_sb_t-0.25)))+self.T_0).reshape(-1,1)\n",
        "        new_tensor = torch.zeros_like(output_sb_0)\n",
        "\n",
        "        output_sb_0 = torch.cat((output_sb_0, new_tensor), dim=1)\n",
        "        # print(\"Output sb: {}\".format(output_sb_0))\n",
        "        # output_sb_0 = torch.full(output_sb_0[:, 1].shape, 0)\n",
        "\n",
        "        return torch.cat([input_sb_0, input_sb_L], 0), torch.cat([output_sb_0, output_sb_L], 0)\n",
        "\n",
        "    #  Function returning the input-output tensor required to assemble the training set S_int corresponding to the interior domain where the PDE is enforced\n",
        "    def add_interior_points(self):\n",
        "        # input_int = self.convert(self.soboleng.draw(self.n_int))\n",
        "        input_int = self.soboleng.draw(self.n_int)\n",
        "        output_int = torch.zeros((input_int.shape[0], 1))\n",
        "        return input_int, output_int\n",
        "\n",
        "    # Function returning the training sets S_sb, S_tb, S_int as dataloader\n",
        "    def assemble_datasets(self):\n",
        "        input_sb, output_sb = self.add_spatial_boundary_points()   # S_sb\n",
        "        input_tb, output_tb = self.add_temporal_boundary_points()  # S_tb\n",
        "        input_int, output_int = self.add_interior_points()         # S_int\n",
        "\n",
        "        training_set_sb = DataLoader(torch.utils.data.TensorDataset(input_sb, output_sb), batch_size=2*self.space_dimensions*self.n_sb, shuffle=False)\n",
        "        training_set_tb = DataLoader(torch.utils.data.TensorDataset(input_tb, output_tb), batch_size=self.n_tb, shuffle=False)\n",
        "        training_set_int = DataLoader(torch.utils.data.TensorDataset(input_int, output_int), batch_size=self.n_int, shuffle=False)\n",
        "\n",
        "        return training_set_sb, training_set_tb, training_set_int\n",
        "\n",
        "    ################################################################################################\n",
        "    # Function to compute the terms required in the definition of the TEMPORAL boundary residual\n",
        "    # Takes as input the temporal boundary points and returns the prediction of the neural network at the temporal boundary points\n",
        "    def apply_initial_condition(self, input_tb):\n",
        "        u_pred_tb = self.approximate_solution(input_tb)\n",
        "        #print(u_pred_tb)\n",
        "        return u_pred_tb\n",
        "\n",
        "    # Function to compute the terms required in the definition of the SPATIAL boundary residual\n",
        "    # Takes as input the spatial boundary points and returns the prediction of the neural network at the temporal boundary points\n",
        "    def apply_boundary_conditions(self, input_sb):\n",
        "        #! Error probably here\n",
        "        input_sb.requires_grad = True\n",
        "\n",
        "        #CORRECT\n",
        "        input_sb_x0 = input_sb[:int(input_sb.shape[0]/2), :]\n",
        "        input_sb_xL = input_sb[int(input_sb.shape[0]/2):, :]\n",
        "\n",
        "        u_pred_Tf_x0 = self.approximate_solution(input_sb_x0)[:, 0]\n",
        "        u_pred_Ts_x0 = self.approximate_solution(input_sb_x0)[:, 1]\n",
        "\n",
        "        # print(\"T_f x0 pred: {}\".format(u_pred_Tf_x0))\n",
        "\n",
        "\n",
        "        grad_Ts_x0 = torch.autograd.grad(u_pred_Ts_x0.sum(), input_sb_x0, create_graph=True)[0][:, 1]\n",
        "\n",
        "        # u_pred_xL = self.approximate_solution(input_sb_xL)\n",
        "        u_pred_Tf_xL = self.approximate_solution(input_sb_xL)[:, 0]\n",
        "        u_pred_Ts_xL = self.approximate_solution(input_sb_xL)[:, 1]\n",
        "\n",
        "        grad_Tf_xL = torch.autograd.grad(u_pred_Tf_xL.sum(), input_sb_xL, create_graph=True)[0][:, 1]\n",
        "        grad_Ts_xL = torch.autograd.grad(u_pred_Ts_xL.sum(), input_sb_xL, create_graph=True)[0][:, 1]\n",
        "\n",
        "        u_pred_concat_1 = torch.cat((u_pred_Tf_x0.reshape(-1,1), grad_Ts_x0.reshape(-1,1)), dim =1)\n",
        "        # print(\"U_pred_concat_1: \", u_pred_concat_1)\n",
        "        u_pred_concat_2 = torch.cat((grad_Tf_xL.reshape(-1,1), grad_Ts_xL.reshape(-1,1)), dim =1)\n",
        "        # print(\"U_pred_concat_2: \", u_pred_concat_2)\n",
        "        total_concat = torch.cat((u_pred_concat_1, u_pred_concat_2), dim =0)\n",
        "        # print(\"Total_concat: \", total_concat.shape)\n",
        "\n",
        "        return torch.cat([u_pred_concat_1, u_pred_concat_2],0)\n",
        "\n",
        "    # Function to compute the PDE residuals\n",
        "    def compute_pde_residual(self, input_int):\n",
        "        #! Should be correct\n",
        "\n",
        "        input_int.requires_grad = True\n",
        "\n",
        "        # Obtain the prediction from the neural network\n",
        "        u = self.approximate_solution(input_int)\n",
        "        \n",
        "        # First column of prediction will be the temperature of the fluid\n",
        "        # Second column of prediction will be the temperature of the solid\n",
        "        T_f = u[:,0]\n",
        "        T_s = u[:,1]\n",
        "\n",
        "        # grad compute the gradient of a \"SCALAR\" function L with respect to some input nxm TENSOR Z=[[x1, y1],[x2,y2],[x3,y3],...,[xn,yn]], m=2\n",
        "        # it returns grad_L = [[dL/dx1, dL/dy1],[dL/dx2, dL/dy2],[dL/dx3, dL/dy3],...,[dL/dxn, dL/dyn]]\n",
        "        # Note: pytorch considers a tensor [u1, u2,u3, ... ,un] a vectorial function\n",
        "        # whereas sum_u = u1 + u2 + u3 + u4 + ... + un as a \"scalar\" one\n",
        "\n",
        "        # In our case ui = u(xi), therefore the line below returns:\n",
        "        # grad_u = [[dsum_u/dx1, dsum_u/dy1],[dsum_u/dx2, dsum_u/dy2],[dsum_u/dx3, dL/dy3],...,[dsum_u/dxm, dsum_u/dyn]]\n",
        "        # and dsum_u/dxi = d(u1 + u2 + u3 + u4 + ... + un)/dxi = d(u(x1) + u(x2) u3(x3) + u4(x4) + ... + u(xn))/dxi = dui/dxi\n",
        "        grad_tf = torch.autograd.grad(T_f.sum(), input_int, create_graph=True)[0]\n",
        "        grad_ts = torch.autograd.grad(T_s.sum(), input_int, create_graph=True)[0]\n",
        "        \n",
        "        grad_ts_t = grad_ts[:,0] \n",
        "        grad_ts_x = grad_ts[:,1]\n",
        "        grad_tf_t = grad_tf[:,0]\n",
        "        grad_tf_x = grad_tf[:,1 ]\n",
        "        \n",
        "        grad_tf_xx = torch.autograd.grad(grad_tf_x.sum(), input_int, create_graph=True)[0][:,1]\n",
        "        grad_ts_xx = torch.autograd.grad(grad_ts_x.sum(), input_int, create_graph=True)[0][:,1]\n",
        "\n",
        "\n",
        "        residual1 = (grad_tf_t) + (self.U_f*grad_tf_x) - (self.alpha_f*grad_tf_xx) + (self.h_f*(T_f - T_s))\n",
        "        residual2 = (grad_ts_t) - (self.alpha_s*grad_ts_xx) - self.h_s*(T_f - T_s)\n",
        "\n",
        "        # print(residual1, residual2)\n",
        "\n",
        "        # residual = residual1 + residual2\n",
        "        # grad_u_t = grad_u[:, 0]\n",
        "        # grad_u_x = grad_u[:, 1]\n",
        "        # grad_u_xx = torch.autograd.grad(grad_u_x.sum(), input_int, create_graph=True)[0][:, 1]\n",
        "\n",
        "        #grad_u_sq_x = torch.autograd.grad(u_sq.sum(), input_int, create_graph=True)[0][:,1]\n",
        "\n",
        "        # residual = grad_u_t - grad_u_xx\n",
        "        return residual1.reshape(-1, ), residual2.reshape(-1, )\n",
        "\n",
        "    # Function to compute the total loss (weighted sum of spatial boundary loss, temporal boundary loss and interior loss)\n",
        "    def compute_loss(self, inp_train_sb, u_train_sb, inp_train_tb, u_train_tb, inp_train_int, verbose=True):\n",
        "        u_pred_sb = self.apply_boundary_conditions(inp_train_sb)\n",
        "        u_pred_tb = self.apply_initial_condition(inp_train_tb)\n",
        "        # print(u_pred_sb.shape)\n",
        "\n",
        "\n",
        "        # print(\"U_pred_sb: {}\".format(u_pred_sb))\n",
        "        # Seems correct\n",
        "        # print(\"U_train_sb: {}\".format(u_train_sb))\n",
        "\n",
        "        assert (u_pred_sb.shape[1] == u_train_sb.shape[1])\n",
        "        assert (u_pred_tb.shape[1] == u_train_tb.shape[1])\n",
        "\n",
        "        res1, res2 = self.compute_pde_residual(inp_train_int)\n",
        "        # r_int = self.compute_pde_residual(inp_train_int)\n",
        "\n",
        "        # Initial Condition loss\n",
        "        res_tb = u_train_tb - u_pred_tb\n",
        "        \n",
        "        # This corresponds to the condition with the exponential in the boundary conditions\n",
        "        ##########################################\n",
        "\n",
        "        len_train_half = int(u_train_sb.shape[0]/2)\n",
        "        # print(\"len_train: {}\".format(len_train_half))\n",
        "        #! ERROR?\n",
        "        Tf_train_sb_0 = u_train_sb[:len_train_half, 0]\n",
        "        Tf_pred_sb_0 = u_pred_sb[:len_train_half, 0]\n",
        "        # print(\"Tf_train_sb_0: {}\".format(Tf_train_sb_0))\n",
        "        # print(\"Tf_pred_sb_0: {}\".format(Tf_pred_sb_0))\n",
        "        res_Tf_sb_0 = Tf_train_sb_0 - Tf_pred_sb_0\n",
        "\n",
        "        # Condition grad T_s/x at x = 0\n",
        "        Ts_train_sb_0 = u_train_sb[:len_train_half, 1]\n",
        "        Ts_pred_sb_0 = u_pred_sb[:len_train_half, 1]\n",
        "        # print(\"Ts_train_sb_0: {}\".format(Ts_train_sb_0))\n",
        "        # print(\"Ts_pred_sb_0: {}\".format(Ts_pred_sb_0))\n",
        "        res_Ts_sb_0 = Ts_train_sb_0 - Ts_pred_sb_0\n",
        "\n",
        "        # Condition grad T_s/x at x = 1\n",
        "        Ts_train_sb_L = u_train_sb[len_train_half:, 1]\n",
        "        Ts_pred_sb_L = u_pred_sb[len_train_half:, 1]\n",
        "        # print(\"Ts_train_sb_L: {}\".format(Ts_train_sb_L))\n",
        "        # print(\"Ts_pred_sb_L: {}\".format(Ts_pred_sb_L))\n",
        "        res_Ts_sb_L = Ts_train_sb_L - Ts_pred_sb_L\n",
        "\n",
        "        # Condition grad T_f/x at x = 1\n",
        "        Tf_train_sb_L = u_train_sb[len_train_half:, 0]\n",
        "        Tf_pred_sb_L = u_pred_sb[len_train_half:, 0]\n",
        "        # print(\"Tf_train_sb_L: {}\".format(Tf_train_sb_L))\n",
        "        # print(\"Tf_pred_sb_L: {}\".format(Tf_pred_sb_L))\n",
        "        res_Tf_sb_L = Tf_train_sb_L - Tf_pred_sb_L\n",
        "\n",
        "        # print(res_Tf_sb_0)\n",
        "        # print(res_Ts_sb_0)\n",
        "        # print(res_Ts_sb_L)\n",
        "        # print(res_Tf_sb_L)\n",
        "        ############################``\n",
        "\n",
        "        loss_sb = torch.mean(abs(res_Tf_sb_0) ** 2) + torch.mean(abs(res_Ts_sb_0) ** 2) + torch.mean(abs(res_Ts_sb_L) ** 2) + torch.mean(abs(res_Tf_sb_L) ** 2)\n",
        "        loss_tb = torch.mean(abs(res_tb) ** 2)\n",
        "        loss_int = torch.mean(abs(res1) ** 2) + torch.mean(abs(res2) ** 2)\n",
        "\n",
        "        # print(\"Loss int: {}\".format(loss_int))\n",
        "        # print(\"Loss sb: {}\".format(loss_sb))\n",
        "        # print(\"Loss tb: {}\".format(loss_tb))\n",
        "\n",
        "        loss_u = loss_sb + loss_tb\n",
        "\n",
        "        # loss = torch.log10(self.lambda_u * (loss_sb + loss_tb) + loss_int)\n",
        "        loss = torch.log10(self.lambda_u * (1*loss_sb + 1*loss_tb) + 1*loss_int)\n",
        "        if verbose: print(\"Total loss: \", round(loss.item(), 4), \"| PDE Loss: \", round(torch.log10(loss_u).item(), 4), \"| Function Loss: \", round(torch.log10(loss_int).item(), 4))\n",
        "\n",
        "        return loss\n",
        "\n",
        "    ################################################################################################\n",
        "    def fit(self, num_epochs, optimizer, verbose=True):\n",
        "        history = list()\n",
        "\n",
        "        # Loop over epochs\n",
        "        for epoch in range(num_epochs):\n",
        "            if verbose: print(\"################################ \", epoch, \" ################################\")\n",
        "\n",
        "            for j, ((inp_train_sb, u_train_sb), (inp_train_tb, u_train_tb), (inp_train_int, u_train_int)) in enumerate(zip(self.training_set_sb, self.training_set_tb, self.training_set_int)):\n",
        "                def closure():\n",
        "                    optimizer.zero_grad()\n",
        "                    loss = self.compute_loss(inp_train_sb, u_train_sb, inp_train_tb, u_train_tb, inp_train_int, verbose=verbose)\n",
        "                    loss.backward()\n",
        "\n",
        "                    history.append(loss.item())\n",
        "                    return loss\n",
        "\n",
        "                optimizer.step(closure=closure)\n",
        "\n",
        "        print('Final Loss: ', history[-1])\n",
        "\n",
        "        return history\n",
        "\n",
        "    ################################################################################################\n",
        "    def plotting(self):\n",
        "        inputs = self.soboleng.draw(100000)\n",
        "        #inputs = self.convert(inputs)\n",
        "\n",
        "        output = self.approximate_solution(inputs).reshape\n",
        "        output_Tf = output[:, 0].reshape(-1, )\n",
        "        output_Ts = output[:, 1].reshape(-1, )\n",
        "\n",
        "        fig, axs = plt.subplots(1, 2, figsize=(16, 8), dpi=150)\n",
        "        im1 = axs[0].scatter(inputs[:, 1].detach(), inputs[:, 0].detach(), c=output_Tf.detach(), cmap=\"jet\")\n",
        "        axs[0].set_xlabel(\"x\")\n",
        "        axs[0].set_ylabel(\"t\")\n",
        "        # plt.colorbar(im1, ax=axs[0])\n",
        "        axs[0].grid(True, which=\"both\", ls=\":\")\n",
        "        im2 = axs[1].scatter(inputs[:, 1].detach(), inputs[:, 0].detach(), c=output_Ts.detach(), cmap=\"jet\")\n",
        "        axs[1].set_xlabel(\"x\")\n",
        "        axs[1].set_ylabel(\"t\")\n",
        "        plt.colorbar(im2, ax=axs[1])\n",
        "        axs[1].grid(True, which=\"both\", ls=\":\")\n",
        "        axs[0].set_title(\"Exact Solution $T_f$\")\n",
        "        axs[1].set_title(\"Approximate Solution $T_s$\")\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "        # err = (torch.mean((output - exact_output) ** 2) / torch.mean(exact_output ** 2)) ** 0.5 * 100\n",
        "        # print(\"L2 Relative Error Norm: \", err.item(), \"%\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "M3ug4ztBfQxM",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "n_int = 256\n",
        "n_sb = 64\n",
        "n_tb = 64\n",
        "\n",
        "pinn = Pinns(n_int, n_sb, n_tb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "0CPnHdKtfQxN",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "n_epochs = 1\n",
        "optimizer_LBFGS = optim.LBFGS(pinn.approximate_solution.parameters(),\n",
        "                              lr=float(0.5),\n",
        "                              max_iter=50000,\n",
        "                              max_eval=50000,\n",
        "                              history_size=150,\n",
        "                              line_search_fn=\"strong_wolfe\",\n",
        "                              tolerance_change=1.0 * np.finfo(float).eps)\n",
        "optimizer_ADAM = optim.Adam(pinn.approximate_solution.parameters(),\n",
        "                            lr=float(0.001))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "qub-M5jqfQxN",
        "outputId": "6cf2b62f-f8f2-4e80-b0c4-c3a736fa93eb",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "################################  0  ################################\n",
            "Total loss:  2.3624 | PDE Loss:  1.1527 | Function Loss:  1.9457\n",
            "Total loss:  2.3071 | PDE Loss:  1.1268 | Function Loss:  1.8382\n",
            "Total loss:  2.1081 | PDE Loss:  0.911 | Function Loss:  1.6701\n",
            "Total loss:  1.9951 | PDE Loss:  0.8484 | Function Loss:  1.4523\n",
            "Total loss:  1.8096 | PDE Loss:  0.7492 | Function Loss:  0.923\n",
            "Total loss:  1.7431 | PDE Loss:  0.7111 | Function Loss:  0.5944\n",
            "Total loss:  1.7065 | PDE Loss:  0.6671 | Function Loss:  0.6439\n",
            "Total loss:  1.5212 | PDE Loss:  0.2948 | Function Loss:  1.1301\n",
            "Total loss:  1.4731 | PDE Loss:  0.2881 | Function Loss:  1.0132\n",
            "Total loss:  1.4377 | PDE Loss:  0.2781 | Function Loss:  0.9256\n",
            "Total loss:  1.4281 | PDE Loss:  0.2721 | Function Loss:  0.9078\n",
            "Total loss:  1.414 | PDE Loss:  0.2566 | Function Loss:  0.8969\n",
            "Total loss:  1.385 | PDE Loss:  0.2167 | Function Loss:  0.8917\n",
            "Total loss:  1.3577 | PDE Loss:  0.1723 | Function Loss:  0.8986\n",
            "Total loss:  1.3273 | PDE Loss:  0.1241 | Function Loss:  0.8999\n",
            "Total loss:  1.299 | PDE Loss:  0.0842 | Function Loss:  0.8902\n",
            "Total loss:  1.2791 | PDE Loss:  0.0559 | Function Loss:  0.8831\n",
            "Total loss:  1.262 | PDE Loss:  0.0349 | Function Loss:  0.8718\n",
            "Total loss:  1.2422 | PDE Loss:  0.0227 | Function Loss:  0.8406\n",
            "Total loss:  1.2078 | PDE Loss:  0.0157 | Function Loss:  0.761\n",
            "Total loss:  1.1696 | PDE Loss:  0.0221 | Function Loss:  0.6289\n",
            "Total loss:  1.1309 | PDE Loss:  -0.0008 | Function Loss:  0.5484\n",
            "Total loss:  1.0733 | PDE Loss:  -0.0259 | Function Loss:  0.3831\n",
            "Total loss:  1.0477 | PDE Loss:  -0.0542 | Function Loss:  0.3682\n",
            "Total loss:  1.0327 | PDE Loss:  -0.0607 | Function Loss:  0.3197\n",
            "Total loss:  1.0194 | PDE Loss:  -0.0728 | Function Loss:  0.3012\n",
            "Total loss:  1.0042 | PDE Loss:  -0.0881 | Function Loss:  0.286\n",
            "Total loss:  0.9722 | PDE Loss:  -0.138 | Function Loss:  0.3227\n",
            "Total loss:  0.9346 | PDE Loss:  -0.1915 | Function Loss:  0.3358\n",
            "Total loss:  0.9026 | PDE Loss:  -0.2368 | Function Loss:  0.3412\n",
            "Total loss:  0.8773 | PDE Loss:  -0.26 | Function Loss:  0.3103\n",
            "Total loss:  0.8489 | PDE Loss:  -0.2717 | Function Loss:  0.2337\n",
            "Total loss:  0.8282 | PDE Loss:  -0.2955 | Function Loss:  0.2226\n",
            "Total loss:  0.8156 | PDE Loss:  -0.3033 | Function Loss:  0.195\n",
            "Total loss:  0.81 | PDE Loss:  -0.3123 | Function Loss:  0.1997\n",
            "Total loss:  0.8054 | PDE Loss:  -0.325 | Function Loss:  0.2191\n",
            "Total loss:  0.799 | PDE Loss:  -0.3371 | Function Loss:  0.2287\n",
            "Total loss:  0.7896 | PDE Loss:  -0.3541 | Function Loss:  0.2396\n",
            "Total loss:  0.7753 | PDE Loss:  -0.3772 | Function Loss:  0.2467\n",
            "Total loss:  0.7633 | PDE Loss:  -0.3927 | Function Loss:  0.2432\n",
            "Total loss:  0.7528 | PDE Loss:  -0.402 | Function Loss:  0.2299\n",
            "Total loss:  0.7408 | PDE Loss:  -0.4105 | Function Loss:  0.2092\n",
            "Total loss:  0.7299 | PDE Loss:  -0.4164 | Function Loss:  0.1863\n",
            "Total loss:  0.7175 | PDE Loss:  -0.4272 | Function Loss:  0.17\n",
            "Total loss:  0.7021 | PDE Loss:  -0.4366 | Function Loss:  0.139\n",
            "Total loss:  0.682 | PDE Loss:  -0.4521 | Function Loss:  0.1064\n",
            "Total loss:  0.6586 | PDE Loss:  -0.4609 | Function Loss:  0.0401\n",
            "Total loss:  0.6366 | PDE Loss:  -0.4799 | Function Loss:  0.0083\n",
            "Total loss:  0.628 | PDE Loss:  -0.4834 | Function Loss:  -0.0173\n",
            "Total loss:  0.6231 | PDE Loss:  -0.4882 | Function Loss:  -0.0228\n",
            "Total loss:  0.6164 | PDE Loss:  -0.4957 | Function Loss:  -0.0264\n",
            "Total loss:  0.6068 | PDE Loss:  -0.5038 | Function Loss:  -0.0415\n",
            "Total loss:  0.5978 | PDE Loss:  -0.5079 | Function Loss:  -0.068\n",
            "Total loss:  0.5899 | PDE Loss:  -0.5092 | Function Loss:  -0.1001\n",
            "Total loss:  0.5832 | PDE Loss:  -0.5122 | Function Loss:  -0.1218\n",
            "Total loss:  0.5745 | PDE Loss:  -0.5161 | Function Loss:  -0.1505\n",
            "Total loss:  0.5647 | PDE Loss:  -0.5232 | Function Loss:  -0.1726\n",
            "Total loss:  0.5541 | PDE Loss:  -0.5285 | Function Loss:  -0.2075\n",
            "Total loss:  0.5427 | PDE Loss:  -0.5345 | Function Loss:  -0.2454\n",
            "Total loss:  0.5267 | PDE Loss:  -0.5457 | Function Loss:  -0.287\n",
            "Total loss:  0.5156 | PDE Loss:  -0.5538 | Function Loss:  -0.3155\n",
            "Total loss:  0.5041 | PDE Loss:  -0.5683 | Function Loss:  -0.3098\n",
            "Total loss:  0.4916 | PDE Loss:  -0.5854 | Function Loss:  -0.298\n",
            "Total loss:  0.4808 | PDE Loss:  -0.6011 | Function Loss:  -0.2837\n",
            "Total loss:  0.4721 | PDE Loss:  -0.6115 | Function Loss:  -0.2846\n",
            "Total loss:  0.4602 | PDE Loss:  -0.6251 | Function Loss:  -0.2886\n",
            "Total loss:  0.4481 | PDE Loss:  -0.6324 | Function Loss:  -0.3236\n",
            "Total loss:  0.4378 | PDE Loss:  -0.6372 | Function Loss:  -0.3622\n",
            "Total loss:  0.4301 | PDE Loss:  -0.6406 | Function Loss:  -0.3931\n",
            "Total loss:  0.4259 | PDE Loss:  -0.6423 | Function Loss:  -0.4115\n",
            "Total loss:  0.4216 | PDE Loss:  -0.6453 | Function Loss:  -0.4234\n",
            "Total loss:  0.4157 | PDE Loss:  -0.6505 | Function Loss:  -0.434\n",
            "Total loss:  0.4078 | PDE Loss:  -0.6604 | Function Loss:  -0.4301\n",
            "Total loss:  0.3998 | PDE Loss:  -0.6708 | Function Loss:  -0.4236\n",
            "Total loss:  0.3921 | PDE Loss:  -0.6854 | Function Loss:  -0.3942\n",
            "Total loss:  0.3856 | PDE Loss:  -0.6962 | Function Loss:  -0.3801\n",
            "Total loss:  0.3773 | PDE Loss:  -0.7092 | Function Loss:  -0.3664\n",
            "Total loss:  0.371 | PDE Loss:  -0.7218 | Function Loss:  -0.3446\n",
            "Total loss:  0.3667 | PDE Loss:  -0.7286 | Function Loss:  -0.3391\n",
            "Total loss:  0.3644 | PDE Loss:  -0.7334 | Function Loss:  -0.3309\n",
            "Total loss:  0.3598 | PDE Loss:  -0.7408 | Function Loss:  -0.3249\n",
            "Total loss:  0.35 | PDE Loss:  -0.7588 | Function Loss:  -0.3042\n",
            "Total loss:  0.3389 | PDE Loss:  -0.7762 | Function Loss:  -0.294\n",
            "Total loss:  0.331 | PDE Loss:  -0.7881 | Function Loss:  -0.2892\n",
            "Total loss:  0.3249 | PDE Loss:  -0.793 | Function Loss:  -0.2991\n",
            "Total loss:  0.3212 | PDE Loss:  -0.795 | Function Loss:  -0.308\n",
            "Total loss:  0.3176 | PDE Loss:  -0.7961 | Function Loss:  -0.3197\n",
            "Total loss:  0.3152 | PDE Loss:  -0.7968 | Function Loss:  -0.3284\n",
            "Total loss:  0.3123 | PDE Loss:  -0.8001 | Function Loss:  -0.3295\n",
            "Total loss:  0.3072 | PDE Loss:  -0.804 | Function Loss:  -0.3391\n",
            "Total loss:  0.2958 | PDE Loss:  -0.8124 | Function Loss:  -0.3608\n",
            "Total loss:  0.2775 | PDE Loss:  -0.8253 | Function Loss:  -0.3989\n",
            "Total loss:  0.2642 | PDE Loss:  -0.8338 | Function Loss:  -0.4302\n",
            "Total loss:  0.2507 | PDE Loss:  -0.845 | Function Loss:  -0.4529\n",
            "Total loss:  0.2396 | PDE Loss:  -0.8548 | Function Loss:  -0.4699\n",
            "Total loss:  0.2213 | PDE Loss:  -0.8704 | Function Loss:  -0.4992\n",
            "Total loss:  0.2045 | PDE Loss:  -0.8848 | Function Loss:  -0.5263\n",
            "Total loss:  0.1919 | PDE Loss:  -0.9012 | Function Loss:  -0.5223\n",
            "Total loss:  0.1848 | PDE Loss:  -0.9103 | Function Loss:  -0.5217\n",
            "Total loss:  0.1814 | PDE Loss:  -0.9187 | Function Loss:  -0.5047\n",
            "Total loss:  0.1772 | PDE Loss:  -0.9227 | Function Loss:  -0.51\n",
            "Total loss:  0.1747 | PDE Loss:  -0.9247 | Function Loss:  -0.5148\n",
            "Total loss:  0.1715 | PDE Loss:  -0.9279 | Function Loss:  -0.5178\n",
            "Total loss:  0.1668 | PDE Loss:  -0.9315 | Function Loss:  -0.5269\n",
            "Total loss:  0.1582 | PDE Loss:  -0.9378 | Function Loss:  -0.5444\n",
            "Total loss:  0.1479 | PDE Loss:  -0.9467 | Function Loss:  -0.5604\n",
            "Total loss:  0.136 | PDE Loss:  -0.9565 | Function Loss:  -0.5812\n",
            "Total loss:  0.1216 | PDE Loss:  -0.9704 | Function Loss:  -0.5975\n",
            "Total loss:  0.1075 | PDE Loss:  -0.9888 | Function Loss:  -0.5937\n",
            "Total loss:  0.0948 | PDE Loss:  -1.01 | Function Loss:  -0.5741\n",
            "Total loss:  0.0882 | PDE Loss:  -1.0152 | Function Loss:  -0.586\n",
            "Total loss:  0.0818 | PDE Loss:  -1.0284 | Function Loss:  -0.5675\n",
            "Total loss:  0.0782 | PDE Loss:  -1.0425 | Function Loss:  -0.5368\n",
            "Total loss:  0.0736 | PDE Loss:  -1.0471 | Function Loss:  -0.5414\n",
            "Total loss:  0.071 | PDE Loss:  -1.0489 | Function Loss:  -0.5465\n",
            "Total loss:  0.0677 | PDE Loss:  -1.0509 | Function Loss:  -0.5541\n",
            "Total loss:  0.063 | PDE Loss:  -1.0584 | Function Loss:  -0.5501\n",
            "Total loss:  0.0572 | PDE Loss:  -1.066 | Function Loss:  -0.5504\n",
            "Total loss:  0.0506 | PDE Loss:  -1.078 | Function Loss:  -0.5406\n",
            "Total loss:  0.0424 | PDE Loss:  -1.0911 | Function Loss:  -0.5351\n",
            "Total loss:  0.0329 | PDE Loss:  -1.1078 | Function Loss:  -0.5253\n",
            "Total loss:  0.0246 | PDE Loss:  -1.1185 | Function Loss:  -0.527\n",
            "Total loss:  0.0197 | PDE Loss:  -1.1253 | Function Loss:  -0.5272\n",
            "Total loss:  0.0144 | PDE Loss:  -1.1303 | Function Loss:  -0.5333\n",
            "Total loss:  0.0093 | PDE Loss:  -1.1332 | Function Loss:  -0.5438\n",
            "Total loss:  0.0027 | PDE Loss:  -1.1393 | Function Loss:  -0.5518\n",
            "Total loss:  -0.0065 | PDE Loss:  -1.1437 | Function Loss:  -0.5738\n",
            "Total loss:  -0.0132 | PDE Loss:  -1.1502 | Function Loss:  -0.5809\n",
            "Total loss:  -0.0162 | PDE Loss:  -1.1519 | Function Loss:  -0.5873\n",
            "Total loss:  -0.0188 | PDE Loss:  -1.1555 | Function Loss:  -0.5872\n",
            "Total loss:  -0.021 | PDE Loss:  -1.1562 | Function Loss:  -0.5936\n",
            "Total loss:  -0.0246 | PDE Loss:  -1.1584 | Function Loss:  -0.6013\n",
            "Total loss:  -0.0305 | PDE Loss:  -1.1561 | Function Loss:  -0.6304\n",
            "Total loss:  -0.038 | PDE Loss:  -1.1584 | Function Loss:  -0.6538\n",
            "Total loss:  -0.0449 | PDE Loss:  -1.1613 | Function Loss:  -0.6738\n",
            "Total loss:  -0.0545 | PDE Loss:  -1.1677 | Function Loss:  -0.6937\n",
            "Total loss:  -0.0645 | PDE Loss:  -1.1802 | Function Loss:  -0.6957\n",
            "Total loss:  -0.0726 | PDE Loss:  -1.1953 | Function Loss:  -0.6814\n",
            "Total loss:  -0.0802 | PDE Loss:  -1.2101 | Function Loss:  -0.6675\n",
            "Total loss:  -0.0886 | PDE Loss:  -1.2311 | Function Loss:  -0.6417\n",
            "Total loss:  -0.0983 | PDE Loss:  -1.2516 | Function Loss:  -0.625\n",
            "Total loss:  -0.1055 | PDE Loss:  -1.272 | Function Loss:  -0.6025\n",
            "Total loss:  -0.1093 | PDE Loss:  -1.2809 | Function Loss:  -0.5955\n",
            "Total loss:  -0.1128 | PDE Loss:  -1.2867 | Function Loss:  -0.5945\n",
            "Total loss:  -0.1175 | PDE Loss:  -1.2914 | Function Loss:  -0.5988\n",
            "Total loss:  -0.1237 | PDE Loss:  -1.2942 | Function Loss:  -0.6121\n",
            "Total loss:  -0.1327 | PDE Loss:  -1.2993 | Function Loss:  -0.6296\n",
            "Total loss:  -0.139 | PDE Loss:  -1.2998 | Function Loss:  -0.6485\n",
            "Total loss:  -0.1427 | PDE Loss:  -1.3032 | Function Loss:  -0.6529\n",
            "Total loss:  -0.1459 | PDE Loss:  -1.3053 | Function Loss:  -0.6585\n",
            "Total loss:  -0.1495 | PDE Loss:  -1.31 | Function Loss:  -0.6595\n",
            "Total loss:  -0.1525 | PDE Loss:  -1.3091 | Function Loss:  -0.6713\n",
            "Total loss:  -0.1554 | PDE Loss:  -1.3112 | Function Loss:  -0.6761\n",
            "Total loss:  -0.1607 | PDE Loss:  -1.314 | Function Loss:  -0.6873\n",
            "Total loss:  -0.1671 | PDE Loss:  -1.3202 | Function Loss:  -0.6942\n",
            "Total loss:  -0.1753 | PDE Loss:  -1.3293 | Function Loss:  -0.7004\n",
            "Total loss:  -0.1841 | PDE Loss:  -1.341 | Function Loss:  -0.7022\n",
            "Total loss:  -0.1913 | PDE Loss:  -1.3515 | Function Loss:  -0.702\n",
            "Total loss:  -0.2017 | PDE Loss:  -1.3617 | Function Loss:  -0.7127\n",
            "Total loss:  -0.2177 | PDE Loss:  -1.3721 | Function Loss:  -0.7418\n",
            "Total loss:  -0.2317 | PDE Loss:  -1.3761 | Function Loss:  -0.7803\n",
            "Total loss:  -0.2442 | PDE Loss:  -1.3808 | Function Loss:  -0.8129\n",
            "Total loss:  -0.2585 | PDE Loss:  -1.3917 | Function Loss:  -0.8367\n",
            "Total loss:  -0.2696 | PDE Loss:  -1.3939 | Function Loss:  -0.8734\n",
            "Total loss:  -0.2757 | PDE Loss:  -1.3963 | Function Loss:  -0.8913\n",
            "Total loss:  -0.2807 | PDE Loss:  -1.4017 | Function Loss:  -0.8945\n",
            "Total loss:  -0.2847 | PDE Loss:  -1.4062 | Function Loss:  -0.8972\n",
            "Total loss:  -0.289 | PDE Loss:  -1.4076 | Function Loss:  -0.9108\n",
            "Total loss:  -0.2928 | PDE Loss:  -1.4131 | Function Loss:  -0.909\n",
            "Total loss:  -0.2956 | PDE Loss:  -1.4176 | Function Loss:  -0.9068\n",
            "Total loss:  -0.2991 | PDE Loss:  -1.4214 | Function Loss:  -0.909\n",
            "Total loss:  -0.3043 | PDE Loss:  -1.43 | Function Loss:  -0.9041\n",
            "Total loss:  -0.3091 | PDE Loss:  -1.4362 | Function Loss:  -0.9048\n",
            "Total loss:  -0.3178 | PDE Loss:  -1.445 | Function Loss:  -0.9133\n",
            "Total loss:  -0.3303 | PDE Loss:  -1.4581 | Function Loss:  -0.924\n",
            "Total loss:  -0.3424 | PDE Loss:  -1.4709 | Function Loss:  -0.934\n",
            "Total loss:  -0.3522 | PDE Loss:  -1.483 | Function Loss:  -0.9372\n",
            "Total loss:  -0.365 | PDE Loss:  -1.501 | Function Loss:  -0.9355\n",
            "Total loss:  -0.3752 | PDE Loss:  -1.5167 | Function Loss:  -0.9312\n",
            "Total loss:  -0.3839 | PDE Loss:  -1.5339 | Function Loss:  -0.9183\n",
            "Total loss:  -0.3908 | PDE Loss:  -1.5445 | Function Loss:  -0.9166\n",
            "Total loss:  -0.3991 | PDE Loss:  -1.5596 | Function Loss:  -0.9092\n",
            "Total loss:  -0.4087 | PDE Loss:  -1.5761 | Function Loss:  -0.9038\n",
            "Total loss:  -0.4186 | PDE Loss:  -1.5901 | Function Loss:  -0.9051\n",
            "Total loss:  -0.4301 | PDE Loss:  -1.599 | Function Loss:  -0.9219\n",
            "Total loss:  -0.439 | PDE Loss:  -1.6072 | Function Loss:  -0.9321\n",
            "Total loss:  -0.4436 | PDE Loss:  -1.6052 | Function Loss:  -0.951\n",
            "Total loss:  -0.4466 | PDE Loss:  -1.6089 | Function Loss:  -0.9525\n",
            "Total loss:  -0.4493 | PDE Loss:  -1.6094 | Function Loss:  -0.9605\n",
            "Total loss:  -0.4523 | PDE Loss:  -1.6108 | Function Loss:  -0.9668\n",
            "Total loss:  -0.4561 | PDE Loss:  -1.6138 | Function Loss:  -0.9724\n",
            "Total loss:  -0.4609 | PDE Loss:  -1.6188 | Function Loss:  -0.9767\n",
            "Total loss:  -0.4657 | PDE Loss:  -1.6258 | Function Loss:  -0.9768\n",
            "Total loss:  -0.4727 | PDE Loss:  -1.6391 | Function Loss:  -0.9699\n",
            "Total loss:  -0.4799 | PDE Loss:  -1.6527 | Function Loss:  -0.9636\n",
            "Total loss:  -0.4835 | PDE Loss:  -1.6609 | Function Loss:  -0.9578\n",
            "Total loss:  -0.487 | PDE Loss:  -1.6681 | Function Loss:  -0.9543\n",
            "Total loss:  -0.4905 | PDE Loss:  -1.676 | Function Loss:  -0.9495\n",
            "Total loss:  -0.4937 | PDE Loss:  -1.6824 | Function Loss:  -0.9467\n",
            "Total loss:  -0.4976 | PDE Loss:  -1.6895 | Function Loss:  -0.9449\n",
            "Total loss:  -0.5019 | PDE Loss:  -1.6976 | Function Loss:  -0.9422\n",
            "Total loss:  -0.5069 | PDE Loss:  -1.7061 | Function Loss:  -0.9411\n",
            "Total loss:  -0.5116 | PDE Loss:  -1.7173 | Function Loss:  -0.9348\n",
            "Total loss:  -0.515 | PDE Loss:  -1.7216 | Function Loss:  -0.9369\n",
            "Total loss:  -0.5178 | PDE Loss:  -1.7293 | Function Loss:  -0.9317\n",
            "Total loss:  -0.5204 | PDE Loss:  -1.7335 | Function Loss:  -0.9318\n",
            "Total loss:  -0.5231 | PDE Loss:  -1.7364 | Function Loss:  -0.9342\n",
            "Total loss:  -0.5267 | PDE Loss:  -1.7413 | Function Loss:  -0.9357\n",
            "Total loss:  -0.5315 | PDE Loss:  -1.7459 | Function Loss:  -0.9409\n",
            "Total loss:  -0.5394 | PDE Loss:  -1.7551 | Function Loss:  -0.9468\n",
            "Total loss:  -0.5508 | PDE Loss:  -1.7706 | Function Loss:  -0.9518\n",
            "Total loss:  -0.5619 | PDE Loss:  -1.7868 | Function Loss:  -0.9554\n",
            "Total loss:  -0.5709 | PDE Loss:  -1.8011 | Function Loss:  -0.9565\n",
            "Total loss:  -0.5778 | PDE Loss:  -1.8116 | Function Loss:  -0.9584\n",
            "Total loss:  -0.5826 | PDE Loss:  -1.8221 | Function Loss:  -0.9553\n",
            "Total loss:  -0.5861 | PDE Loss:  -1.8253 | Function Loss:  -0.9594\n",
            "Total loss:  -0.5886 | PDE Loss:  -1.8331 | Function Loss:  -0.9546\n",
            "Total loss:  -0.5908 | PDE Loss:  -1.836 | Function Loss:  -0.956\n",
            "Total loss:  -0.594 | PDE Loss:  -1.8426 | Function Loss:  -0.9547\n",
            "Total loss:  -0.5985 | PDE Loss:  -1.8493 | Function Loss:  -0.9563\n",
            "Total loss:  -0.6044 | PDE Loss:  -1.8568 | Function Loss:  -0.9601\n",
            "Total loss:  -0.6105 | PDE Loss:  -1.8633 | Function Loss:  -0.9657\n",
            "Total loss:  -0.6164 | PDE Loss:  -1.8675 | Function Loss:  -0.9738\n",
            "Total loss:  -0.6226 | PDE Loss:  -1.8688 | Function Loss:  -0.9864\n",
            "Total loss:  -0.6285 | PDE Loss:  -1.8773 | Function Loss:  -0.9889\n",
            "Total loss:  -0.6327 | PDE Loss:  -1.8786 | Function Loss:  -0.9969\n",
            "Total loss:  -0.6356 | PDE Loss:  -1.8819 | Function Loss:  -0.9992\n",
            "Total loss:  -0.6379 | PDE Loss:  -1.8864 | Function Loss:  -0.9987\n",
            "Total loss:  -0.6396 | PDE Loss:  -1.888 | Function Loss:  -1.0004\n",
            "Total loss:  -0.6417 | PDE Loss:  -1.8916 | Function Loss:  -1.0008\n",
            "Total loss:  -0.6469 | PDE Loss:  -1.8972 | Function Loss:  -1.0054\n",
            "Total loss:  -0.6537 | PDE Loss:  -1.9066 | Function Loss:  -1.0088\n",
            "Total loss:  -0.6603 | PDE Loss:  -1.9163 | Function Loss:  -1.0115\n",
            "Total loss:  -0.6671 | PDE Loss:  -1.9243 | Function Loss:  -1.0169\n",
            "Total loss:  -0.6757 | PDE Loss:  -1.9368 | Function Loss:  -1.0206\n",
            "Total loss:  -0.6829 | PDE Loss:  -1.9427 | Function Loss:  -1.0296\n",
            "Total loss:  -0.6888 | PDE Loss:  -1.9515 | Function Loss:  -1.0318\n",
            "Total loss:  -0.6918 | PDE Loss:  -1.9543 | Function Loss:  -1.0351\n",
            "Total loss:  -0.6948 | PDE Loss:  -1.9556 | Function Loss:  -1.0403\n",
            "Total loss:  -0.6986 | PDE Loss:  -1.9572 | Function Loss:  -1.0466\n",
            "Total loss:  -0.7036 | PDE Loss:  -1.9635 | Function Loss:  -1.0502\n",
            "Total loss:  -0.7094 | PDE Loss:  -1.9676 | Function Loss:  -1.0579\n",
            "Total loss:  -0.7146 | PDE Loss:  -1.9782 | Function Loss:  -1.0565\n",
            "Total loss:  -0.721 | PDE Loss:  -1.9882 | Function Loss:  -1.0587\n",
            "Total loss:  -0.7266 | PDE Loss:  -2.0042 | Function Loss:  -1.0525\n",
            "Total loss:  -0.7311 | PDE Loss:  -2.0143 | Function Loss:  -1.0507\n",
            "Total loss:  -0.7351 | PDE Loss:  -2.0283 | Function Loss:  -1.0441\n",
            "Total loss:  -0.7379 | PDE Loss:  -2.0351 | Function Loss:  -1.0428\n",
            "Total loss:  -0.7406 | PDE Loss:  -2.0438 | Function Loss:  -1.0394\n",
            "Total loss:  -0.7426 | PDE Loss:  -2.0462 | Function Loss:  -1.041\n",
            "Total loss:  -0.7451 | PDE Loss:  -2.0542 | Function Loss:  -1.0381\n",
            "Total loss:  -0.7473 | PDE Loss:  -2.0571 | Function Loss:  -1.0398\n",
            "Total loss:  -0.7525 | PDE Loss:  -2.0631 | Function Loss:  -1.0443\n",
            "Total loss:  -0.7566 | PDE Loss:  -2.0661 | Function Loss:  -1.0493\n",
            "Total loss:  -0.7595 | PDE Loss:  -2.0689 | Function Loss:  -1.0524\n",
            "Total loss:  -0.7625 | PDE Loss:  -2.0718 | Function Loss:  -1.0554\n",
            "Total loss:  -0.7649 | PDE Loss:  -2.0778 | Function Loss:  -1.0543\n",
            "Total loss:  -0.7676 | PDE Loss:  -2.0812 | Function Loss:  -1.0563\n",
            "Total loss:  -0.7713 | PDE Loss:  -2.0916 | Function Loss:  -1.0538\n",
            "Total loss:  -0.7755 | PDE Loss:  -2.0991 | Function Loss:  -1.0551\n",
            "Total loss:  -0.7809 | PDE Loss:  -2.1116 | Function Loss:  -1.0541\n",
            "Total loss:  -0.7874 | PDE Loss:  -2.1219 | Function Loss:  -1.0574\n",
            "Total loss:  -0.7937 | PDE Loss:  -2.1381 | Function Loss:  -1.0552\n",
            "Total loss:  -0.7986 | PDE Loss:  -2.1444 | Function Loss:  -1.0592\n",
            "Total loss:  -0.8019 | PDE Loss:  -2.1532 | Function Loss:  -1.0579\n",
            "Total loss:  -0.8054 | PDE Loss:  -2.1572 | Function Loss:  -1.061\n",
            "Total loss:  -0.8083 | PDE Loss:  -2.1626 | Function Loss:  -1.0619\n",
            "Total loss:  -0.8132 | PDE Loss:  -2.17 | Function Loss:  -1.0648\n",
            "Total loss:  -0.8176 | PDE Loss:  -2.1812 | Function Loss:  -1.0639\n",
            "Total loss:  -0.8215 | PDE Loss:  -2.1886 | Function Loss:  -1.0652\n",
            "Total loss:  -0.825 | PDE Loss:  -2.1977 | Function Loss:  -1.0646\n",
            "Total loss:  -0.8279 | PDE Loss:  -2.203 | Function Loss:  -1.0656\n",
            "Total loss:  -0.8299 | PDE Loss:  -2.2057 | Function Loss:  -1.0672\n",
            "Total loss:  -0.8315 | PDE Loss:  -2.2059 | Function Loss:  -1.0697\n",
            "Total loss:  -0.8343 | PDE Loss:  -2.2084 | Function Loss:  -1.0728\n",
            "Total loss:  -0.841 | PDE Loss:  -2.2172 | Function Loss:  -1.0781\n",
            "Total loss:  -0.8495 | PDE Loss:  -2.2211 | Function Loss:  -1.0898\n",
            "Total loss:  -0.8585 | PDE Loss:  -2.2336 | Function Loss:  -1.0963\n",
            "Total loss:  -0.8707 | PDE Loss:  -2.2518 | Function Loss:  -1.1041\n",
            "Total loss:  -0.8861 | PDE Loss:  -2.2617 | Function Loss:  -1.1236\n",
            "Total loss:  -0.8993 | PDE Loss:  -2.2724 | Function Loss:  -1.1385\n",
            "Total loss:  -0.9071 | PDE Loss:  -2.2925 | Function Loss:  -1.1376\n",
            "Total loss:  -0.9117 | PDE Loss:  -2.2938 | Function Loss:  -1.1444\n",
            "Total loss:  -0.9146 | PDE Loss:  -2.3036 | Function Loss:  -1.1424\n",
            "Total loss:  -0.9168 | PDE Loss:  -2.3102 | Function Loss:  -1.1417\n",
            "Total loss:  -0.9195 | PDE Loss:  -2.315 | Function Loss:  -1.143\n",
            "Total loss:  -0.923 | PDE Loss:  -2.3235 | Function Loss:  -1.1431\n",
            "Total loss:  -0.9258 | PDE Loss:  -2.3256 | Function Loss:  -1.1464\n",
            "Total loss:  -0.9287 | PDE Loss:  -2.3276 | Function Loss:  -1.1499\n",
            "Total loss:  -0.9342 | PDE Loss:  -2.3326 | Function Loss:  -1.1557\n",
            "Total loss:  -0.9426 | PDE Loss:  -2.3413 | Function Loss:  -1.1639\n",
            "Total loss:  -0.9541 | PDE Loss:  -2.3574 | Function Loss:  -1.1724\n",
            "Total loss:  -0.9669 | PDE Loss:  -2.3672 | Function Loss:  -1.1872\n",
            "Total loss:  -0.9747 | PDE Loss:  -2.3797 | Function Loss:  -1.1919\n",
            "Total loss:  -0.9828 | PDE Loss:  -2.4166 | Function Loss:  -1.1823\n",
            "Total loss:  -0.9911 | PDE Loss:  -2.4266 | Function Loss:  -1.1896\n",
            "Total loss:  -0.9977 | PDE Loss:  -2.4521 | Function Loss:  -1.1856\n",
            "Total loss:  -1.0025 | PDE Loss:  -2.4589 | Function Loss:  -1.1893\n",
            "Total loss:  -1.0051 | PDE Loss:  -2.4649 | Function Loss:  -1.1901\n",
            "Total loss:  -1.0071 | PDE Loss:  -2.4651 | Function Loss:  -1.193\n",
            "Total loss:  -1.0092 | PDE Loss:  -2.4631 | Function Loss:  -1.1974\n",
            "Total loss:  -1.0113 | PDE Loss:  -2.4621 | Function Loss:  -1.2012\n",
            "Total loss:  -1.014 | PDE Loss:  -2.4611 | Function Loss:  -1.2059\n",
            "Total loss:  -1.0177 | PDE Loss:  -2.458 | Function Loss:  -1.2135\n",
            "Total loss:  -1.0217 | PDE Loss:  -2.4556 | Function Loss:  -1.2212\n",
            "Total loss:  -1.0263 | PDE Loss:  -2.4595 | Function Loss:  -1.2261\n",
            "Total loss:  -1.0307 | PDE Loss:  -2.4636 | Function Loss:  -1.2308\n",
            "Total loss:  -1.0355 | PDE Loss:  -2.4748 | Function Loss:  -1.2318\n",
            "Total loss:  -1.0406 | PDE Loss:  -2.485 | Function Loss:  -1.234\n",
            "Total loss:  -1.0465 | PDE Loss:  -2.4996 | Function Loss:  -1.2351\n",
            "Total loss:  -1.0526 | PDE Loss:  -2.5109 | Function Loss:  -1.2384\n",
            "Total loss:  -1.0605 | PDE Loss:  -2.5277 | Function Loss:  -1.2417\n",
            "Total loss:  -1.0701 | PDE Loss:  -2.538 | Function Loss:  -1.2508\n",
            "Total loss:  -1.0785 | PDE Loss:  -2.5559 | Function Loss:  -1.2544\n",
            "Total loss:  -1.0837 | PDE Loss:  -2.5611 | Function Loss:  -1.2596\n",
            "Total loss:  -1.0876 | PDE Loss:  -2.5602 | Function Loss:  -1.2659\n",
            "Total loss:  -1.0905 | PDE Loss:  -2.5661 | Function Loss:  -1.2674\n",
            "Total loss:  -1.094 | PDE Loss:  -2.5606 | Function Loss:  -1.2754\n",
            "Total loss:  -1.0985 | PDE Loss:  -2.5679 | Function Loss:  -1.2785\n",
            "Total loss:  -1.1045 | PDE Loss:  -2.5639 | Function Loss:  -1.2897\n",
            "Total loss:  -1.1113 | PDE Loss:  -2.5677 | Function Loss:  -1.2982\n",
            "Total loss:  -1.1178 | PDE Loss:  -2.5749 | Function Loss:  -1.3043\n",
            "Total loss:  -1.123 | PDE Loss:  -2.5837 | Function Loss:  -1.3076\n",
            "Total loss:  -1.128 | PDE Loss:  -2.5923 | Function Loss:  -1.3107\n",
            "Total loss:  -1.1321 | PDE Loss:  -2.6054 | Function Loss:  -1.3101\n",
            "Total loss:  -1.1355 | PDE Loss:  -2.6101 | Function Loss:  -1.3128\n",
            "Total loss:  -1.1378 | PDE Loss:  -2.623 | Function Loss:  -1.3099\n",
            "Total loss:  -1.1402 | PDE Loss:  -2.6256 | Function Loss:  -1.3122\n",
            "Total loss:  -1.1423 | PDE Loss:  -2.6287 | Function Loss:  -1.3138\n",
            "Total loss:  -1.145 | PDE Loss:  -2.6312 | Function Loss:  -1.3166\n",
            "Total loss:  -1.1476 | PDE Loss:  -2.6368 | Function Loss:  -1.3177\n",
            "Total loss:  -1.1493 | PDE Loss:  -2.6423 | Function Loss:  -1.3176\n",
            "Total loss:  -1.1503 | PDE Loss:  -2.6446 | Function Loss:  -1.3181\n",
            "Total loss:  -1.1512 | PDE Loss:  -2.6471 | Function Loss:  -1.3182\n",
            "Total loss:  -1.1523 | PDE Loss:  -2.6492 | Function Loss:  -1.3188\n",
            "Total loss:  -1.1537 | PDE Loss:  -2.6513 | Function Loss:  -1.32\n",
            "Total loss:  -1.1559 | PDE Loss:  -2.6548 | Function Loss:  -1.3214\n",
            "Total loss:  -1.1589 | PDE Loss:  -2.657 | Function Loss:  -1.3248\n",
            "Total loss:  -1.1626 | PDE Loss:  -2.6622 | Function Loss:  -1.3279\n",
            "Total loss:  -1.167 | PDE Loss:  -2.665 | Function Loss:  -1.333\n",
            "Total loss:  -1.1724 | PDE Loss:  -2.6684 | Function Loss:  -1.3394\n",
            "Total loss:  -1.1787 | PDE Loss:  -2.6708 | Function Loss:  -1.3475\n",
            "Total loss:  -1.1834 | PDE Loss:  -2.6756 | Function Loss:  -1.3521\n",
            "Total loss:  -1.187 | PDE Loss:  -2.6733 | Function Loss:  -1.3585\n",
            "Total loss:  -1.1896 | PDE Loss:  -2.6785 | Function Loss:  -1.3598\n",
            "Total loss:  -1.1911 | PDE Loss:  -2.6765 | Function Loss:  -1.3632\n",
            "Total loss:  -1.1922 | PDE Loss:  -2.6764 | Function Loss:  -1.3649\n",
            "Total loss:  -1.1934 | PDE Loss:  -2.679 | Function Loss:  -1.3653\n",
            "Total loss:  -1.195 | PDE Loss:  -2.6804 | Function Loss:  -1.367\n",
            "Total loss:  -1.1969 | PDE Loss:  -2.6839 | Function Loss:  -1.3681\n",
            "Total loss:  -1.1991 | PDE Loss:  -2.6878 | Function Loss:  -1.3694\n",
            "Total loss:  -1.2021 | PDE Loss:  -2.6917 | Function Loss:  -1.372\n",
            "Total loss:  -1.2059 | PDE Loss:  -2.6984 | Function Loss:  -1.3745\n",
            "Total loss:  -1.2101 | PDE Loss:  -2.7009 | Function Loss:  -1.3795\n",
            "Total loss:  -1.2134 | PDE Loss:  -2.7066 | Function Loss:  -1.3816\n",
            "Total loss:  -1.2155 | PDE Loss:  -2.7034 | Function Loss:  -1.3863\n",
            "Total loss:  -1.2166 | PDE Loss:  -2.7046 | Function Loss:  -1.3873\n",
            "Total loss:  -1.2171 | PDE Loss:  -2.7026 | Function Loss:  -1.389\n",
            "Total loss:  -1.2175 | PDE Loss:  -2.7017 | Function Loss:  -1.39\n",
            "Total loss:  -1.2179 | PDE Loss:  -2.7008 | Function Loss:  -1.3911\n",
            "Total loss:  -1.2184 | PDE Loss:  -2.6994 | Function Loss:  -1.3926\n",
            "Total loss:  -1.2192 | PDE Loss:  -2.6987 | Function Loss:  -1.3941\n",
            "Total loss:  -1.2205 | PDE Loss:  -2.6986 | Function Loss:  -1.3961\n",
            "Total loss:  -1.2223 | PDE Loss:  -2.6981 | Function Loss:  -1.3991\n",
            "Total loss:  -1.2254 | PDE Loss:  -2.7028 | Function Loss:  -1.4013\n",
            "Total loss:  -1.2296 | PDE Loss:  -2.707 | Function Loss:  -1.4056\n",
            "Total loss:  -1.235 | PDE Loss:  -2.7197 | Function Loss:  -1.4074\n",
            "Total loss:  -1.2414 | PDE Loss:  -2.7346 | Function Loss:  -1.4096\n",
            "Total loss:  -1.2474 | PDE Loss:  -2.7497 | Function Loss:  -1.4114\n",
            "Total loss:  -1.252 | PDE Loss:  -2.7622 | Function Loss:  -1.4125\n",
            "Total loss:  -1.2548 | PDE Loss:  -2.7673 | Function Loss:  -1.4142\n",
            "Total loss:  -1.256 | PDE Loss:  -2.7692 | Function Loss:  -1.4152\n",
            "Total loss:  -1.2566 | PDE Loss:  -2.7698 | Function Loss:  -1.4158\n",
            "Total loss:  -1.257 | PDE Loss:  -2.7696 | Function Loss:  -1.4164\n",
            "Total loss:  -1.2574 | PDE Loss:  -2.7695 | Function Loss:  -1.417\n",
            "Total loss:  -1.2579 | PDE Loss:  -2.7696 | Function Loss:  -1.4177\n",
            "Total loss:  -1.2589 | PDE Loss:  -2.7694 | Function Loss:  -1.4192\n",
            "Total loss:  -1.2607 | PDE Loss:  -2.7715 | Function Loss:  -1.4209\n",
            "Total loss:  -1.2636 | PDE Loss:  -2.7717 | Function Loss:  -1.425\n",
            "Total loss:  -1.2681 | PDE Loss:  -2.7799 | Function Loss:  -1.4279\n",
            "Total loss:  -1.2737 | PDE Loss:  -2.7848 | Function Loss:  -1.4337\n",
            "Total loss:  -1.2788 | PDE Loss:  -2.7943 | Function Loss:  -1.4369\n",
            "Total loss:  -1.2836 | PDE Loss:  -2.8088 | Function Loss:  -1.4376\n",
            "Total loss:  -1.2876 | PDE Loss:  -2.8178 | Function Loss:  -1.4394\n",
            "Total loss:  -1.2896 | PDE Loss:  -2.8287 | Function Loss:  -1.4377\n",
            "Total loss:  -1.2902 | PDE Loss:  -2.8313 | Function Loss:  -1.4376\n",
            "Total loss:  -1.2906 | PDE Loss:  -2.8313 | Function Loss:  -1.4381\n",
            "Total loss:  -1.2909 | PDE Loss:  -2.8311 | Function Loss:  -1.4385\n",
            "Total loss:  -1.2913 | PDE Loss:  -2.8297 | Function Loss:  -1.4397\n",
            "Total loss:  -1.2921 | PDE Loss:  -2.8295 | Function Loss:  -1.4409\n",
            "Total loss:  -1.2932 | PDE Loss:  -2.8251 | Function Loss:  -1.4443\n",
            "Total loss:  -1.2949 | PDE Loss:  -2.8245 | Function Loss:  -1.447\n",
            "Total loss:  -1.2976 | PDE Loss:  -2.8217 | Function Loss:  -1.4519\n",
            "Total loss:  -1.3019 | PDE Loss:  -2.8195 | Function Loss:  -1.4591\n",
            "Total loss:  -1.3076 | PDE Loss:  -2.8197 | Function Loss:  -1.4671\n",
            "Total loss:  -1.3128 | PDE Loss:  -2.8161 | Function Loss:  -1.4763\n",
            "Total loss:  -1.3165 | PDE Loss:  -2.819 | Function Loss:  -1.4805\n",
            "Total loss:  -1.3201 | PDE Loss:  -2.8222 | Function Loss:  -1.4843\n",
            "Total loss:  -1.3229 | PDE Loss:  -2.8256 | Function Loss:  -1.4868\n",
            "Total loss:  -1.3246 | PDE Loss:  -2.8348 | Function Loss:  -1.4851\n",
            "Total loss:  -1.3259 | PDE Loss:  -2.838 | Function Loss:  -1.4855\n",
            "Total loss:  -1.3268 | PDE Loss:  -2.8453 | Function Loss:  -1.4836\n",
            "Total loss:  -1.328 | PDE Loss:  -2.8522 | Function Loss:  -1.4823\n",
            "Total loss:  -1.3296 | PDE Loss:  -2.8612 | Function Loss:  -1.4808\n",
            "Total loss:  -1.3316 | PDE Loss:  -2.8717 | Function Loss:  -1.4793\n",
            "Total loss:  -1.3342 | PDE Loss:  -2.8826 | Function Loss:  -1.4786\n",
            "Total loss:  -1.3377 | PDE Loss:  -2.8933 | Function Loss:  -1.4793\n",
            "Total loss:  -1.3423 | PDE Loss:  -2.9061 | Function Loss:  -1.4808\n",
            "Total loss:  -1.3479 | PDE Loss:  -2.9169 | Function Loss:  -1.4845\n",
            "Total loss:  -1.3536 | PDE Loss:  -2.9249 | Function Loss:  -1.4893\n",
            "Total loss:  -1.3587 | PDE Loss:  -2.9322 | Function Loss:  -1.4936\n",
            "Total loss:  -1.3617 | PDE Loss:  -2.9266 | Function Loss:  -1.4998\n",
            "Total loss:  -1.3637 | PDE Loss:  -2.9248 | Function Loss:  -1.5032\n",
            "Total loss:  -1.365 | PDE Loss:  -2.9232 | Function Loss:  -1.5056\n",
            "Total loss:  -1.3657 | PDE Loss:  -2.9215 | Function Loss:  -1.5073\n",
            "Total loss:  -1.3664 | PDE Loss:  -2.9215 | Function Loss:  -1.5082\n",
            "Total loss:  -1.3668 | PDE Loss:  -2.9214 | Function Loss:  -1.5087\n",
            "Total loss:  -1.367 | PDE Loss:  -2.9227 | Function Loss:  -1.5086\n",
            "Total loss:  -1.3673 | PDE Loss:  -2.9236 | Function Loss:  -1.5086\n",
            "Total loss:  -1.3676 | PDE Loss:  -2.9252 | Function Loss:  -1.5085\n",
            "Total loss:  -1.3681 | PDE Loss:  -2.9264 | Function Loss:  -1.5086\n",
            "Total loss:  -1.3688 | PDE Loss:  -2.9285 | Function Loss:  -1.5088\n",
            "Total loss:  -1.3698 | PDE Loss:  -2.9294 | Function Loss:  -1.5099\n",
            "Total loss:  -1.3714 | PDE Loss:  -2.9331 | Function Loss:  -1.5106\n",
            "Total loss:  -1.3739 | PDE Loss:  -2.9348 | Function Loss:  -1.5135\n",
            "Total loss:  -1.3776 | PDE Loss:  -2.9393 | Function Loss:  -1.5169\n",
            "Total loss:  -1.3829 | PDE Loss:  -2.945 | Function Loss:  -1.522\n",
            "Total loss:  -1.3892 | PDE Loss:  -2.9495 | Function Loss:  -1.529\n",
            "Total loss:  -1.3949 | PDE Loss:  -2.9544 | Function Loss:  -1.5351\n",
            "Total loss:  -1.3994 | PDE Loss:  -2.9576 | Function Loss:  -1.54\n",
            "Total loss:  -1.4027 | PDE Loss:  -2.9587 | Function Loss:  -1.5442\n",
            "Total loss:  -1.4045 | PDE Loss:  -2.9646 | Function Loss:  -1.5444\n",
            "Total loss:  -1.4061 | PDE Loss:  -2.9594 | Function Loss:  -1.5486\n",
            "Total loss:  -1.4073 | PDE Loss:  -2.9661 | Function Loss:  -1.5477\n",
            "Total loss:  -1.4082 | PDE Loss:  -2.9705 | Function Loss:  -1.5472\n",
            "Total loss:  -1.4092 | PDE Loss:  -2.9718 | Function Loss:  -1.5482\n",
            "Total loss:  -1.4101 | PDE Loss:  -2.974 | Function Loss:  -1.5485\n",
            "Total loss:  -1.4107 | PDE Loss:  -2.9742 | Function Loss:  -1.5493\n",
            "Total loss:  -1.4112 | PDE Loss:  -2.9748 | Function Loss:  -1.5497\n",
            "Total loss:  -1.4117 | PDE Loss:  -2.974 | Function Loss:  -1.5508\n",
            "Total loss:  -1.4125 | PDE Loss:  -2.9746 | Function Loss:  -1.5516\n",
            "Total loss:  -1.4135 | PDE Loss:  -2.9732 | Function Loss:  -1.5536\n",
            "Total loss:  -1.415 | PDE Loss:  -2.9753 | Function Loss:  -1.5547\n",
            "Total loss:  -1.4168 | PDE Loss:  -2.9758 | Function Loss:  -1.5571\n",
            "Total loss:  -1.4191 | PDE Loss:  -2.9801 | Function Loss:  -1.5587\n",
            "Total loss:  -1.4223 | PDE Loss:  -2.9933 | Function Loss:  -1.5581\n",
            "Total loss:  -1.4256 | PDE Loss:  -2.9972 | Function Loss:  -1.5612\n",
            "Total loss:  -1.4282 | PDE Loss:  -3.0073 | Function Loss:  -1.5611\n",
            "Total loss:  -1.4323 | PDE Loss:  -3.0146 | Function Loss:  -1.564\n",
            "Total loss:  -1.4358 | PDE Loss:  -3.0422 | Function Loss:  -1.5593\n",
            "Total loss:  -1.4377 | PDE Loss:  -3.0456 | Function Loss:  -1.5606\n",
            "Total loss:  -1.4386 | PDE Loss:  -3.0502 | Function Loss:  -1.5604\n",
            "Total loss:  -1.4394 | PDE Loss:  -3.0554 | Function Loss:  -1.5598\n",
            "Total loss:  -1.4398 | PDE Loss:  -3.0572 | Function Loss:  -1.5598\n",
            "Total loss:  -1.4403 | PDE Loss:  -3.0597 | Function Loss:  -1.5596\n",
            "Total loss:  -1.4409 | PDE Loss:  -3.0609 | Function Loss:  -1.56\n",
            "Total loss:  -1.4416 | PDE Loss:  -3.0623 | Function Loss:  -1.5605\n",
            "Total loss:  -1.4424 | PDE Loss:  -3.0626 | Function Loss:  -1.5615\n",
            "Total loss:  -1.4433 | PDE Loss:  -3.0634 | Function Loss:  -1.5624\n",
            "Total loss:  -1.4446 | PDE Loss:  -3.0651 | Function Loss:  -1.5636\n",
            "Total loss:  -1.4466 | PDE Loss:  -3.0667 | Function Loss:  -1.5657\n",
            "Total loss:  -1.4489 | PDE Loss:  -3.073 | Function Loss:  -1.5667\n",
            "Total loss:  -1.4508 | PDE Loss:  -3.0797 | Function Loss:  -1.5671\n",
            "Total loss:  -1.4524 | PDE Loss:  -3.084 | Function Loss:  -1.5679\n",
            "Total loss:  -1.4542 | PDE Loss:  -3.0915 | Function Loss:  -1.568\n",
            "Total loss:  -1.4562 | PDE Loss:  -3.0972 | Function Loss:  -1.5689\n",
            "Total loss:  -1.4582 | PDE Loss:  -3.103 | Function Loss:  -1.5698\n",
            "Total loss:  -1.46 | PDE Loss:  -3.107 | Function Loss:  -1.571\n",
            "Total loss:  -1.4617 | PDE Loss:  -3.1106 | Function Loss:  -1.5721\n",
            "Total loss:  -1.4628 | PDE Loss:  -3.1087 | Function Loss:  -1.574\n",
            "Total loss:  -1.4634 | PDE Loss:  -3.1108 | Function Loss:  -1.5743\n",
            "Total loss:  -1.4638 | PDE Loss:  -3.1099 | Function Loss:  -1.575\n",
            "Total loss:  -1.4641 | PDE Loss:  -3.11 | Function Loss:  -1.5754\n",
            "Total loss:  -1.4644 | PDE Loss:  -3.1101 | Function Loss:  -1.5757\n",
            "Total loss:  -1.4646 | PDE Loss:  -3.1107 | Function Loss:  -1.5758\n",
            "Total loss:  -1.4649 | PDE Loss:  -3.111 | Function Loss:  -1.5761\n",
            "Total loss:  -1.4654 | PDE Loss:  -3.1128 | Function Loss:  -1.5762\n",
            "Total loss:  -1.4661 | PDE Loss:  -3.1152 | Function Loss:  -1.5765\n",
            "Total loss:  -1.4669 | PDE Loss:  -3.1178 | Function Loss:  -1.5768\n",
            "Total loss:  -1.4684 | PDE Loss:  -3.1214 | Function Loss:  -1.5776\n",
            "Total loss:  -1.4701 | PDE Loss:  -3.1317 | Function Loss:  -1.5769\n",
            "Total loss:  -1.472 | PDE Loss:  -3.1292 | Function Loss:  -1.58\n",
            "Total loss:  -1.4747 | PDE Loss:  -3.1294 | Function Loss:  -1.5834\n",
            "Total loss:  -1.4781 | PDE Loss:  -3.1246 | Function Loss:  -1.5892\n",
            "Total loss:  -1.481 | PDE Loss:  -3.1181 | Function Loss:  -1.5949\n",
            "Total loss:  -1.4838 | PDE Loss:  -3.12 | Function Loss:  -1.5979\n",
            "Total loss:  -1.4862 | PDE Loss:  -3.1212 | Function Loss:  -1.6006\n",
            "Total loss:  -1.4887 | PDE Loss:  -3.1276 | Function Loss:  -1.6021\n",
            "Total loss:  -1.4906 | PDE Loss:  -3.136 | Function Loss:  -1.602\n",
            "Total loss:  -1.4917 | PDE Loss:  -3.1408 | Function Loss:  -1.6021\n",
            "Total loss:  -1.4924 | PDE Loss:  -3.1474 | Function Loss:  -1.601\n",
            "Total loss:  -1.4928 | PDE Loss:  -3.1507 | Function Loss:  -1.6006\n",
            "Total loss:  -1.4931 | PDE Loss:  -3.1553 | Function Loss:  -1.5997\n",
            "Total loss:  -1.4935 | PDE Loss:  -3.1581 | Function Loss:  -1.5994\n",
            "Total loss:  -1.4938 | PDE Loss:  -3.1619 | Function Loss:  -1.5987\n",
            "Total loss:  -1.4941 | PDE Loss:  -3.1636 | Function Loss:  -1.5987\n",
            "Total loss:  -1.4945 | PDE Loss:  -3.1663 | Function Loss:  -1.5984\n",
            "Total loss:  -1.495 | PDE Loss:  -3.1677 | Function Loss:  -1.5987\n",
            "Total loss:  -1.4956 | PDE Loss:  -3.1702 | Function Loss:  -1.5989\n",
            "Total loss:  -1.4966 | PDE Loss:  -3.1724 | Function Loss:  -1.5994\n",
            "Total loss:  -1.4978 | PDE Loss:  -3.1787 | Function Loss:  -1.5994\n",
            "Total loss:  -1.4994 | PDE Loss:  -3.1846 | Function Loss:  -1.5998\n",
            "Total loss:  -1.5009 | PDE Loss:  -3.1925 | Function Loss:  -1.5997\n",
            "Total loss:  -1.503 | PDE Loss:  -3.2041 | Function Loss:  -1.5994\n",
            "Total loss:  -1.5053 | PDE Loss:  -3.2195 | Function Loss:  -1.5985\n",
            "Total loss:  -1.507 | PDE Loss:  -3.2298 | Function Loss:  -1.5982\n",
            "Total loss:  -1.509 | PDE Loss:  -3.245 | Function Loss:  -1.5971\n",
            "Total loss:  -1.5107 | PDE Loss:  -3.2574 | Function Loss:  -1.5964\n",
            "Total loss:  -1.512 | PDE Loss:  -3.2619 | Function Loss:  -1.5971\n",
            "Total loss:  -1.5133 | PDE Loss:  -3.2712 | Function Loss:  -1.5966\n",
            "Total loss:  -1.5144 | PDE Loss:  -3.2691 | Function Loss:  -1.5985\n",
            "Total loss:  -1.5158 | PDE Loss:  -3.2672 | Function Loss:  -1.6005\n",
            "Total loss:  -1.5173 | PDE Loss:  -3.261 | Function Loss:  -1.6037\n",
            "Total loss:  -1.5188 | PDE Loss:  -3.2568 | Function Loss:  -1.6065\n",
            "Total loss:  -1.5202 | PDE Loss:  -3.2514 | Function Loss:  -1.6094\n",
            "Total loss:  -1.5215 | PDE Loss:  -3.2514 | Function Loss:  -1.611\n",
            "Total loss:  -1.5224 | PDE Loss:  -3.25 | Function Loss:  -1.6124\n",
            "Total loss:  -1.523 | PDE Loss:  -3.2584 | Function Loss:  -1.6113\n",
            "Total loss:  -1.5236 | PDE Loss:  -3.2578 | Function Loss:  -1.6122\n",
            "Total loss:  -1.524 | PDE Loss:  -3.2607 | Function Loss:  -1.612\n",
            "Total loss:  -1.5246 | PDE Loss:  -3.2651 | Function Loss:  -1.6117\n",
            "Total loss:  -1.5253 | PDE Loss:  -3.2733 | Function Loss:  -1.6108\n",
            "Total loss:  -1.5261 | PDE Loss:  -3.2809 | Function Loss:  -1.6102\n",
            "Total loss:  -1.5272 | PDE Loss:  -3.2923 | Function Loss:  -1.6091\n",
            "Total loss:  -1.5285 | PDE Loss:  -3.3017 | Function Loss:  -1.6086\n",
            "Total loss:  -1.5298 | PDE Loss:  -3.3118 | Function Loss:  -1.6082\n",
            "Total loss:  -1.5313 | PDE Loss:  -3.319 | Function Loss:  -1.6086\n",
            "Total loss:  -1.5332 | PDE Loss:  -3.3237 | Function Loss:  -1.6099\n",
            "Total loss:  -1.5353 | PDE Loss:  -3.3308 | Function Loss:  -1.6111\n",
            "Total loss:  -1.5376 | PDE Loss:  -3.3271 | Function Loss:  -1.6145\n",
            "Total loss:  -1.5397 | PDE Loss:  -3.3261 | Function Loss:  -1.6172\n",
            "Total loss:  -1.5423 | PDE Loss:  -3.3237 | Function Loss:  -1.6208\n",
            "Total loss:  -1.545 | PDE Loss:  -3.3237 | Function Loss:  -1.6241\n",
            "Total loss:  -1.5475 | PDE Loss:  -3.3288 | Function Loss:  -1.6261\n",
            "Total loss:  -1.5496 | PDE Loss:  -3.3346 | Function Loss:  -1.6275\n",
            "Total loss:  -1.5513 | PDE Loss:  -3.343 | Function Loss:  -1.6278\n",
            "Total loss:  -1.5524 | PDE Loss:  -3.3569 | Function Loss:  -1.6265\n",
            "Total loss:  -1.5533 | PDE Loss:  -3.3616 | Function Loss:  -1.6267\n",
            "Total loss:  -1.5543 | PDE Loss:  -3.3811 | Function Loss:  -1.6243\n",
            "Total loss:  -1.5551 | PDE Loss:  -3.3847 | Function Loss:  -1.6247\n",
            "Total loss:  -1.5559 | PDE Loss:  -3.3897 | Function Loss:  -1.6247\n",
            "Total loss:  -1.5566 | PDE Loss:  -3.3918 | Function Loss:  -1.6253\n",
            "Total loss:  -1.5573 | PDE Loss:  -3.387 | Function Loss:  -1.6269\n",
            "Total loss:  -1.5581 | PDE Loss:  -3.3808 | Function Loss:  -1.6289\n",
            "Total loss:  -1.5589 | PDE Loss:  -3.3698 | Function Loss:  -1.6318\n",
            "Total loss:  -1.5595 | PDE Loss:  -3.3629 | Function Loss:  -1.6338\n",
            "Total loss:  -1.5603 | PDE Loss:  -3.3573 | Function Loss:  -1.6358\n",
            "Total loss:  -1.5614 | PDE Loss:  -3.3518 | Function Loss:  -1.6382\n",
            "Total loss:  -1.5629 | PDE Loss:  -3.3493 | Function Loss:  -1.6405\n",
            "Total loss:  -1.5649 | PDE Loss:  -3.3488 | Function Loss:  -1.6429\n",
            "Total loss:  -1.5672 | PDE Loss:  -3.351 | Function Loss:  -1.6453\n",
            "Total loss:  -1.5701 | PDE Loss:  -3.3539 | Function Loss:  -1.6482\n",
            "Total loss:  -1.5726 | PDE Loss:  -3.3647 | Function Loss:  -1.6491\n",
            "Total loss:  -1.576 | PDE Loss:  -3.3703 | Function Loss:  -1.652\n",
            "Total loss:  -1.5791 | PDE Loss:  -3.3823 | Function Loss:  -1.6534\n",
            "Total loss:  -1.5815 | PDE Loss:  -3.3826 | Function Loss:  -1.6562\n",
            "Total loss:  -1.5832 | PDE Loss:  -3.3867 | Function Loss:  -1.6575\n",
            "Total loss:  -1.5843 | PDE Loss:  -3.3847 | Function Loss:  -1.6592\n",
            "Total loss:  -1.5852 | PDE Loss:  -3.3779 | Function Loss:  -1.6616\n",
            "Total loss:  -1.5862 | PDE Loss:  -3.3736 | Function Loss:  -1.6635\n",
            "Total loss:  -1.587 | PDE Loss:  -3.3648 | Function Loss:  -1.6663\n",
            "Total loss:  -1.5877 | PDE Loss:  -3.3557 | Function Loss:  -1.6689\n",
            "Total loss:  -1.5884 | PDE Loss:  -3.3507 | Function Loss:  -1.6708\n",
            "Total loss:  -1.5888 | PDE Loss:  -3.3438 | Function Loss:  -1.6728\n",
            "Total loss:  -1.5892 | PDE Loss:  -3.3422 | Function Loss:  -1.6736\n",
            "Total loss:  -1.5896 | PDE Loss:  -3.3428 | Function Loss:  -1.6739\n",
            "Total loss:  -1.5899 | PDE Loss:  -3.346 | Function Loss:  -1.6737\n",
            "Total loss:  -1.5902 | PDE Loss:  -3.3502 | Function Loss:  -1.6732\n",
            "Total loss:  -1.5905 | PDE Loss:  -3.3542 | Function Loss:  -1.6726\n",
            "Total loss:  -1.5909 | PDE Loss:  -3.3591 | Function Loss:  -1.6721\n",
            "Total loss:  -1.5914 | PDE Loss:  -3.3648 | Function Loss:  -1.6715\n",
            "Total loss:  -1.592 | PDE Loss:  -3.3695 | Function Loss:  -1.6713\n",
            "Total loss:  -1.5926 | PDE Loss:  -3.3731 | Function Loss:  -1.6713\n",
            "Total loss:  -1.5932 | PDE Loss:  -3.3725 | Function Loss:  -1.6722\n",
            "Total loss:  -1.594 | PDE Loss:  -3.373 | Function Loss:  -1.673\n",
            "Total loss:  -1.595 | PDE Loss:  -3.3672 | Function Loss:  -1.6754\n",
            "Total loss:  -1.5962 | PDE Loss:  -3.3646 | Function Loss:  -1.6773\n",
            "Total loss:  -1.5977 | PDE Loss:  -3.3585 | Function Loss:  -1.6804\n",
            "Total loss:  -1.5994 | PDE Loss:  -3.3556 | Function Loss:  -1.6831\n",
            "Total loss:  -1.6014 | PDE Loss:  -3.3535 | Function Loss:  -1.686\n",
            "Total loss:  -1.6036 | PDE Loss:  -3.3536 | Function Loss:  -1.6886\n",
            "Total loss:  -1.6055 | PDE Loss:  -3.3583 | Function Loss:  -1.6899\n",
            "Total loss:  -1.6071 | PDE Loss:  -3.3617 | Function Loss:  -1.6911\n",
            "Total loss:  -1.6084 | PDE Loss:  -3.3686 | Function Loss:  -1.6912\n",
            "Total loss:  -1.6094 | PDE Loss:  -3.3747 | Function Loss:  -1.6912\n",
            "Total loss:  -1.6101 | PDE Loss:  -3.3795 | Function Loss:  -1.6911\n",
            "Total loss:  -1.6106 | PDE Loss:  -3.3824 | Function Loss:  -1.6911\n",
            "Total loss:  -1.6111 | PDE Loss:  -3.3837 | Function Loss:  -1.6915\n",
            "Total loss:  -1.6116 | PDE Loss:  -3.3822 | Function Loss:  -1.6923\n",
            "Total loss:  -1.6121 | PDE Loss:  -3.3805 | Function Loss:  -1.6933\n",
            "Total loss:  -1.6126 | PDE Loss:  -3.3754 | Function Loss:  -1.6949\n",
            "Total loss:  -1.6131 | PDE Loss:  -3.3722 | Function Loss:  -1.6962\n",
            "Total loss:  -1.6135 | PDE Loss:  -3.3683 | Function Loss:  -1.6976\n",
            "Total loss:  -1.6141 | PDE Loss:  -3.3643 | Function Loss:  -1.6991\n",
            "Total loss:  -1.6147 | PDE Loss:  -3.36 | Function Loss:  -1.7008\n",
            "Total loss:  -1.6155 | PDE Loss:  -3.3601 | Function Loss:  -1.7017\n",
            "Total loss:  -1.6165 | PDE Loss:  -3.3578 | Function Loss:  -1.7034\n",
            "Total loss:  -1.6179 | PDE Loss:  -3.3581 | Function Loss:  -1.705\n",
            "Total loss:  -1.6193 | PDE Loss:  -3.3653 | Function Loss:  -1.7052\n",
            "Total loss:  -1.6213 | PDE Loss:  -3.3728 | Function Loss:  -1.7061\n",
            "Total loss:  -1.6232 | PDE Loss:  -3.3791 | Function Loss:  -1.7069\n",
            "Total loss:  -1.6259 | PDE Loss:  -3.3952 | Function Loss:  -1.7069\n",
            "Total loss:  -1.6292 | PDE Loss:  -3.4014 | Function Loss:  -1.7096\n",
            "Total loss:  -1.629 | PDE Loss:  -3.4134 | Function Loss:  -1.7069\n",
            "Total loss:  -1.6307 | PDE Loss:  -3.4135 | Function Loss:  -1.7089\n",
            "Total loss:  -1.6325 | PDE Loss:  -3.4309 | Function Loss:  -1.7078\n",
            "Total loss:  -1.6349 | PDE Loss:  -3.4307 | Function Loss:  -1.7107\n",
            "Total loss:  -1.637 | PDE Loss:  -3.4269 | Function Loss:  -1.7138\n",
            "Total loss:  -1.6385 | PDE Loss:  -3.4312 | Function Loss:  -1.7148\n",
            "Total loss:  -1.6396 | PDE Loss:  -3.4267 | Function Loss:  -1.717\n",
            "Total loss:  -1.6406 | PDE Loss:  -3.4333 | Function Loss:  -1.717\n",
            "Total loss:  -1.6415 | PDE Loss:  -3.4356 | Function Loss:  -1.7176\n",
            "Total loss:  -1.6423 | PDE Loss:  -3.4331 | Function Loss:  -1.7191\n",
            "Total loss:  -1.6432 | PDE Loss:  -3.4379 | Function Loss:  -1.7191\n",
            "Total loss:  -1.6438 | PDE Loss:  -3.44 | Function Loss:  -1.7194\n",
            "Total loss:  -1.6441 | PDE Loss:  -3.4435 | Function Loss:  -1.7191\n",
            "Total loss:  -1.6445 | PDE Loss:  -3.446 | Function Loss:  -1.7191\n",
            "Total loss:  -1.6449 | PDE Loss:  -3.4478 | Function Loss:  -1.7193\n",
            "Total loss:  -1.6454 | PDE Loss:  -3.4512 | Function Loss:  -1.7192\n",
            "Total loss:  -1.6457 | PDE Loss:  -3.4511 | Function Loss:  -1.7196\n",
            "Total loss:  -1.6461 | PDE Loss:  -3.451 | Function Loss:  -1.7201\n",
            "Total loss:  -1.6465 | PDE Loss:  -3.4499 | Function Loss:  -1.7208\n",
            "Total loss:  -1.6471 | PDE Loss:  -3.4496 | Function Loss:  -1.7216\n",
            "Total loss:  -1.6482 | PDE Loss:  -3.4465 | Function Loss:  -1.7235\n",
            "Total loss:  -1.6497 | PDE Loss:  -3.4442 | Function Loss:  -1.7256\n",
            "Total loss:  -1.6516 | PDE Loss:  -3.4384 | Function Loss:  -1.729\n",
            "Total loss:  -1.6539 | PDE Loss:  -3.4356 | Function Loss:  -1.7323\n",
            "Total loss:  -1.6562 | PDE Loss:  -3.4313 | Function Loss:  -1.736\n",
            "Total loss:  -1.6585 | PDE Loss:  -3.4381 | Function Loss:  -1.7374\n",
            "Total loss:  -1.6603 | PDE Loss:  -3.4394 | Function Loss:  -1.7393\n",
            "Total loss:  -1.6614 | PDE Loss:  -3.4467 | Function Loss:  -1.7392\n",
            "Total loss:  -1.6622 | PDE Loss:  -3.4532 | Function Loss:  -1.7389\n",
            "Total loss:  -1.6628 | PDE Loss:  -3.4581 | Function Loss:  -1.7386\n",
            "Total loss:  -1.6633 | PDE Loss:  -3.4611 | Function Loss:  -1.7386\n",
            "Total loss:  -1.6637 | PDE Loss:  -3.4628 | Function Loss:  -1.7388\n",
            "Total loss:  -1.664 | PDE Loss:  -3.4631 | Function Loss:  -1.7392\n",
            "Total loss:  -1.6645 | PDE Loss:  -3.4632 | Function Loss:  -1.7397\n",
            "Total loss:  -1.665 | PDE Loss:  -3.4617 | Function Loss:  -1.7405\n",
            "Total loss:  -1.6656 | PDE Loss:  -3.4603 | Function Loss:  -1.7416\n",
            "Total loss:  -1.6665 | PDE Loss:  -3.4571 | Function Loss:  -1.7432\n",
            "Total loss:  -1.6675 | PDE Loss:  -3.4556 | Function Loss:  -1.7447\n",
            "Total loss:  -1.6686 | PDE Loss:  -3.4535 | Function Loss:  -1.7465\n",
            "Total loss:  -1.6704 | PDE Loss:  -3.4516 | Function Loss:  -1.749\n",
            "Total loss:  -1.6727 | PDE Loss:  -3.4519 | Function Loss:  -1.7516\n",
            "Total loss:  -1.675 | PDE Loss:  -3.45 | Function Loss:  -1.7548\n",
            "Total loss:  -1.6768 | PDE Loss:  -3.4578 | Function Loss:  -1.7554\n",
            "Total loss:  -1.6784 | PDE Loss:  -3.4617 | Function Loss:  -1.7566\n",
            "Total loss:  -1.6795 | PDE Loss:  -3.4621 | Function Loss:  -1.7578\n",
            "Total loss:  -1.6805 | PDE Loss:  -3.4689 | Function Loss:  -1.7577\n",
            "Total loss:  -1.6814 | PDE Loss:  -3.4748 | Function Loss:  -1.7576\n",
            "Total loss:  -1.6821 | PDE Loss:  -3.4725 | Function Loss:  -1.7589\n",
            "Total loss:  -1.6827 | PDE Loss:  -3.4759 | Function Loss:  -1.7589\n",
            "Total loss:  -1.6832 | PDE Loss:  -3.4729 | Function Loss:  -1.7601\n",
            "Total loss:  -1.6837 | PDE Loss:  -3.4712 | Function Loss:  -1.761\n",
            "Total loss:  -1.6842 | PDE Loss:  -3.4689 | Function Loss:  -1.7621\n",
            "Total loss:  -1.6848 | PDE Loss:  -3.4684 | Function Loss:  -1.7629\n",
            "Total loss:  -1.6855 | PDE Loss:  -3.4657 | Function Loss:  -1.7643\n",
            "Total loss:  -1.6861 | PDE Loss:  -3.4667 | Function Loss:  -1.7647\n",
            "Total loss:  -1.6866 | PDE Loss:  -3.4684 | Function Loss:  -1.7651\n",
            "Total loss:  -1.6871 | PDE Loss:  -3.4676 | Function Loss:  -1.7658\n",
            "Total loss:  -1.6877 | PDE Loss:  -3.4718 | Function Loss:  -1.7657\n",
            "Total loss:  -1.6884 | PDE Loss:  -3.4744 | Function Loss:  -1.766\n",
            "Total loss:  -1.689 | PDE Loss:  -3.4785 | Function Loss:  -1.766\n",
            "Total loss:  -1.6897 | PDE Loss:  -3.4805 | Function Loss:  -1.7664\n",
            "Total loss:  -1.6903 | PDE Loss:  -3.4818 | Function Loss:  -1.7668\n",
            "Total loss:  -1.6908 | PDE Loss:  -3.4837 | Function Loss:  -1.7671\n",
            "Total loss:  -1.6912 | PDE Loss:  -3.4806 | Function Loss:  -1.7682\n",
            "Total loss:  -1.6916 | PDE Loss:  -3.4814 | Function Loss:  -1.7685\n",
            "Total loss:  -1.6922 | PDE Loss:  -3.4828 | Function Loss:  -1.7689\n",
            "Total loss:  -1.6929 | PDE Loss:  -3.4816 | Function Loss:  -1.77\n",
            "Total loss:  -1.6937 | PDE Loss:  -3.4847 | Function Loss:  -1.7704\n",
            "Total loss:  -1.6948 | PDE Loss:  -3.4832 | Function Loss:  -1.7719\n",
            "Total loss:  -1.6958 | PDE Loss:  -3.4863 | Function Loss:  -1.7725\n",
            "Total loss:  -1.6969 | PDE Loss:  -3.4898 | Function Loss:  -1.7732\n",
            "Total loss:  -1.6982 | PDE Loss:  -3.4942 | Function Loss:  -1.7739\n",
            "Total loss:  -1.6993 | PDE Loss:  -3.5001 | Function Loss:  -1.7741\n",
            "Total loss:  -1.7003 | PDE Loss:  -3.5064 | Function Loss:  -1.7742\n",
            "Total loss:  -1.7018 | PDE Loss:  -3.5196 | Function Loss:  -1.7735\n",
            "Total loss:  -1.7032 | PDE Loss:  -3.5273 | Function Loss:  -1.7737\n",
            "Total loss:  -1.7043 | PDE Loss:  -3.5363 | Function Loss:  -1.7735\n",
            "Total loss:  -1.7056 | PDE Loss:  -3.5424 | Function Loss:  -1.774\n",
            "Total loss:  -1.7074 | PDE Loss:  -3.5514 | Function Loss:  -1.7745\n",
            "Total loss:  -1.7092 | PDE Loss:  -3.5521 | Function Loss:  -1.7765\n",
            "Total loss:  -1.7106 | PDE Loss:  -3.5586 | Function Loss:  -1.777\n",
            "Total loss:  -1.7116 | PDE Loss:  -3.5592 | Function Loss:  -1.7782\n",
            "Total loss:  -1.7127 | PDE Loss:  -3.5579 | Function Loss:  -1.7796\n",
            "Total loss:  -1.7137 | PDE Loss:  -3.5612 | Function Loss:  -1.7803\n",
            "Total loss:  -1.7145 | PDE Loss:  -3.5559 | Function Loss:  -1.7821\n",
            "Total loss:  -1.7151 | PDE Loss:  -3.5571 | Function Loss:  -1.7826\n",
            "Total loss:  -1.7156 | PDE Loss:  -3.5568 | Function Loss:  -1.7833\n",
            "Total loss:  -1.7162 | PDE Loss:  -3.5567 | Function Loss:  -1.7839\n",
            "Total loss:  -1.7168 | PDE Loss:  -3.5594 | Function Loss:  -1.7842\n",
            "Total loss:  -1.7174 | PDE Loss:  -3.5595 | Function Loss:  -1.7849\n",
            "Total loss:  -1.718 | PDE Loss:  -3.5665 | Function Loss:  -1.7844\n",
            "Total loss:  -1.7189 | PDE Loss:  -3.5659 | Function Loss:  -1.7855\n",
            "Total loss:  -1.7199 | PDE Loss:  -3.5743 | Function Loss:  -1.7854\n",
            "Total loss:  -1.7214 | PDE Loss:  -3.5759 | Function Loss:  -1.7868\n",
            "Total loss:  -1.7228 | PDE Loss:  -3.5867 | Function Loss:  -1.7867\n",
            "Total loss:  -1.7243 | PDE Loss:  -3.5957 | Function Loss:  -1.787\n",
            "Total loss:  -1.7258 | PDE Loss:  -3.6049 | Function Loss:  -1.7873\n",
            "Total loss:  -1.7269 | PDE Loss:  -3.6169 | Function Loss:  -1.7868\n",
            "Total loss:  -1.7277 | PDE Loss:  -3.6167 | Function Loss:  -1.7878\n",
            "Total loss:  -1.7284 | PDE Loss:  -3.6234 | Function Loss:  -1.7875\n",
            "Total loss:  -1.7288 | PDE Loss:  -3.6219 | Function Loss:  -1.7883\n",
            "Total loss:  -1.7291 | PDE Loss:  -3.623 | Function Loss:  -1.7885\n",
            "Total loss:  -1.7295 | PDE Loss:  -3.6229 | Function Loss:  -1.7889\n",
            "Total loss:  -1.73 | PDE Loss:  -3.6238 | Function Loss:  -1.7894\n",
            "Total loss:  -1.7307 | PDE Loss:  -3.622 | Function Loss:  -1.7904\n",
            "Total loss:  -1.7312 | PDE Loss:  -3.6243 | Function Loss:  -1.7907\n",
            "Total loss:  -1.7318 | PDE Loss:  -3.6226 | Function Loss:  -1.7916\n",
            "Total loss:  -1.7326 | PDE Loss:  -3.6224 | Function Loss:  -1.7926\n",
            "Total loss:  -1.7335 | PDE Loss:  -3.6222 | Function Loss:  -1.7935\n",
            "Total loss:  -1.7347 | PDE Loss:  -3.6214 | Function Loss:  -1.795\n",
            "Total loss:  -1.7362 | PDE Loss:  -3.6223 | Function Loss:  -1.7966\n",
            "Total loss:  -1.7379 | PDE Loss:  -3.6255 | Function Loss:  -1.7981\n",
            "Total loss:  -1.7395 | PDE Loss:  -3.615 | Function Loss:  -1.8016\n",
            "Total loss:  -1.7408 | PDE Loss:  -3.622 | Function Loss:  -1.802\n",
            "Total loss:  -1.7418 | PDE Loss:  -3.6296 | Function Loss:  -1.802\n",
            "Total loss:  -1.7426 | PDE Loss:  -3.6335 | Function Loss:  -1.8023\n",
            "Total loss:  -1.7431 | PDE Loss:  -3.6408 | Function Loss:  -1.8019\n",
            "Total loss:  -1.7435 | PDE Loss:  -3.6446 | Function Loss:  -1.8018\n",
            "Total loss:  -1.744 | PDE Loss:  -3.6483 | Function Loss:  -1.8018\n",
            "Total loss:  -1.7446 | PDE Loss:  -3.6563 | Function Loss:  -1.8014\n",
            "Total loss:  -1.7454 | PDE Loss:  -3.6588 | Function Loss:  -1.802\n",
            "Total loss:  -1.7463 | PDE Loss:  -3.6667 | Function Loss:  -1.8018\n",
            "Total loss:  -1.747 | PDE Loss:  -3.67 | Function Loss:  -1.8022\n",
            "Total loss:  -1.7476 | PDE Loss:  -3.6704 | Function Loss:  -1.8028\n",
            "Total loss:  -1.748 | PDE Loss:  -3.6747 | Function Loss:  -1.8027\n",
            "Total loss:  -1.7485 | PDE Loss:  -3.6733 | Function Loss:  -1.8034\n",
            "Total loss:  -1.749 | PDE Loss:  -3.674 | Function Loss:  -1.8039\n",
            "Total loss:  -1.7495 | PDE Loss:  -3.6737 | Function Loss:  -1.8045\n",
            "Total loss:  -1.75 | PDE Loss:  -3.6715 | Function Loss:  -1.8054\n",
            "Total loss:  -1.7505 | PDE Loss:  -3.6732 | Function Loss:  -1.8058\n",
            "Total loss:  -1.7511 | PDE Loss:  -3.6704 | Function Loss:  -1.8068\n",
            "Total loss:  -1.7516 | PDE Loss:  -3.669 | Function Loss:  -1.8076\n",
            "Total loss:  -1.7524 | PDE Loss:  -3.6682 | Function Loss:  -1.8086\n",
            "Total loss:  -1.7532 | PDE Loss:  -3.6673 | Function Loss:  -1.8097\n",
            "Total loss:  -1.7541 | PDE Loss:  -3.6646 | Function Loss:  -1.811\n",
            "Total loss:  -1.755 | PDE Loss:  -3.6637 | Function Loss:  -1.8122\n",
            "Total loss:  -1.7557 | PDE Loss:  -3.664 | Function Loss:  -1.8129\n",
            "Total loss:  -1.7566 | PDE Loss:  -3.6643 | Function Loss:  -1.814\n",
            "Total loss:  -1.7578 | PDE Loss:  -3.6667 | Function Loss:  -1.815\n",
            "Total loss:  -1.759 | PDE Loss:  -3.666 | Function Loss:  -1.8165\n",
            "Total loss:  -1.7601 | PDE Loss:  -3.6634 | Function Loss:  -1.818\n",
            "Total loss:  -1.761 | PDE Loss:  -3.6667 | Function Loss:  -1.8186\n",
            "Total loss:  -1.7617 | PDE Loss:  -3.6625 | Function Loss:  -1.82\n",
            "Total loss:  -1.7624 | PDE Loss:  -3.6646 | Function Loss:  -1.8206\n",
            "Total loss:  -1.7632 | PDE Loss:  -3.6636 | Function Loss:  -1.8216\n",
            "Total loss:  -1.764 | PDE Loss:  -3.6698 | Function Loss:  -1.8216\n",
            "Total loss:  -1.7648 | PDE Loss:  -3.6697 | Function Loss:  -1.8225\n",
            "Total loss:  -1.7657 | PDE Loss:  -3.6762 | Function Loss:  -1.8227\n",
            "Total loss:  -1.7671 | PDE Loss:  -3.6807 | Function Loss:  -1.8236\n",
            "Total loss:  -1.7685 | PDE Loss:  -3.6872 | Function Loss:  -1.8243\n",
            "Total loss:  -1.7697 | PDE Loss:  -3.6904 | Function Loss:  -1.8252\n",
            "Total loss:  -1.7705 | PDE Loss:  -3.6935 | Function Loss:  -1.8257\n",
            "Total loss:  -1.7711 | PDE Loss:  -3.697 | Function Loss:  -1.8259\n",
            "Total loss:  -1.77 | PDE Loss:  -3.6758 | Function Loss:  -1.8276\n",
            "Total loss:  -1.7713 | PDE Loss:  -3.693 | Function Loss:  -1.8267\n",
            "Total loss:  -1.7719 | PDE Loss:  -3.6959 | Function Loss:  -1.827\n",
            "Total loss:  -1.7725 | PDE Loss:  -3.6986 | Function Loss:  -1.8273\n",
            "Total loss:  -1.7732 | PDE Loss:  -3.7012 | Function Loss:  -1.8277\n",
            "Total loss:  -1.7739 | PDE Loss:  -3.7034 | Function Loss:  -1.8283\n",
            "Total loss:  -1.7746 | PDE Loss:  -3.7067 | Function Loss:  -1.8286\n",
            "Total loss:  -1.7754 | PDE Loss:  -3.7106 | Function Loss:  -1.829\n",
            "Total loss:  -1.7762 | PDE Loss:  -3.7133 | Function Loss:  -1.8295\n",
            "Total loss:  -1.7769 | PDE Loss:  -3.7183 | Function Loss:  -1.8297\n",
            "Total loss:  -1.7776 | PDE Loss:  -3.7204 | Function Loss:  -1.8302\n",
            "Total loss:  -1.7783 | PDE Loss:  -3.7229 | Function Loss:  -1.8306\n",
            "Total loss:  -1.779 | PDE Loss:  -3.7267 | Function Loss:  -1.8309\n",
            "Total loss:  -1.7796 | PDE Loss:  -3.7247 | Function Loss:  -1.8319\n",
            "Total loss:  -1.7801 | PDE Loss:  -3.726 | Function Loss:  -1.8323\n",
            "Total loss:  -1.7808 | PDE Loss:  -3.7305 | Function Loss:  -1.8325\n",
            "Total loss:  -1.7814 | PDE Loss:  -3.7248 | Function Loss:  -1.8339\n",
            "Total loss:  -1.782 | PDE Loss:  -3.7313 | Function Loss:  -1.8337\n",
            "Total loss:  -1.7824 | PDE Loss:  -3.7325 | Function Loss:  -1.8341\n",
            "Total loss:  -1.7831 | PDE Loss:  -3.735 | Function Loss:  -1.8346\n",
            "Total loss:  -1.7838 | PDE Loss:  -3.7337 | Function Loss:  -1.8355\n",
            "Total loss:  -1.7844 | PDE Loss:  -3.7359 | Function Loss:  -1.8359\n",
            "Total loss:  -1.7848 | PDE Loss:  -3.738 | Function Loss:  -1.8361\n",
            "Total loss:  -1.7854 | PDE Loss:  -3.7397 | Function Loss:  -1.8365\n",
            "Total loss:  -1.786 | PDE Loss:  -3.7431 | Function Loss:  -1.8368\n",
            "Total loss:  -1.7866 | PDE Loss:  -3.745 | Function Loss:  -1.8373\n",
            "Total loss:  -1.7872 | PDE Loss:  -3.7484 | Function Loss:  -1.8375\n",
            "Total loss:  -1.7877 | PDE Loss:  -3.7498 | Function Loss:  -1.8379\n",
            "Total loss:  -1.7881 | PDE Loss:  -3.7516 | Function Loss:  -1.8382\n",
            "Total loss:  -1.7886 | PDE Loss:  -3.7555 | Function Loss:  -1.8382\n",
            "Total loss:  -1.7891 | PDE Loss:  -3.7545 | Function Loss:  -1.8389\n",
            "Total loss:  -1.7896 | PDE Loss:  -3.7599 | Function Loss:  -1.8388\n",
            "Total loss:  -1.7903 | PDE Loss:  -3.7593 | Function Loss:  -1.8397\n",
            "Total loss:  -1.791 | PDE Loss:  -3.7617 | Function Loss:  -1.8401\n",
            "Total loss:  -1.7918 | PDE Loss:  -3.7632 | Function Loss:  -1.8408\n",
            "Total loss:  -1.7925 | PDE Loss:  -3.7602 | Function Loss:  -1.8419\n",
            "Total loss:  -1.793 | PDE Loss:  -3.7664 | Function Loss:  -1.8418\n",
            "Total loss:  -1.7934 | PDE Loss:  -3.76 | Function Loss:  -1.843\n",
            "Total loss:  -1.7938 | PDE Loss:  -3.7637 | Function Loss:  -1.843\n",
            "Total loss:  -1.7941 | PDE Loss:  -3.7632 | Function Loss:  -1.8434\n",
            "Total loss:  -1.7945 | PDE Loss:  -3.7632 | Function Loss:  -1.8439\n",
            "Total loss:  -1.7949 | PDE Loss:  -3.7616 | Function Loss:  -1.8445\n",
            "Total loss:  -1.7952 | PDE Loss:  -3.7639 | Function Loss:  -1.8446\n",
            "Total loss:  -1.7954 | PDE Loss:  -3.7636 | Function Loss:  -1.8449\n",
            "Total loss:  -1.7958 | PDE Loss:  -3.7645 | Function Loss:  -1.8451\n",
            "Total loss:  -1.7961 | PDE Loss:  -3.7653 | Function Loss:  -1.8454\n",
            "Total loss:  -1.7964 | PDE Loss:  -3.7653 | Function Loss:  -1.8458\n",
            "Total loss:  -1.7967 | PDE Loss:  -3.7679 | Function Loss:  -1.8458\n",
            "Total loss:  -1.7972 | PDE Loss:  -3.7687 | Function Loss:  -1.8462\n",
            "Total loss:  -1.7976 | PDE Loss:  -3.7718 | Function Loss:  -1.8463\n",
            "Total loss:  -1.7982 | PDE Loss:  -3.7782 | Function Loss:  -1.8462\n",
            "Total loss:  -1.7989 | PDE Loss:  -3.7853 | Function Loss:  -1.8462\n",
            "Total loss:  -1.7996 | PDE Loss:  -3.7914 | Function Loss:  -1.8462\n",
            "Total loss:  -1.8002 | PDE Loss:  -3.7983 | Function Loss:  -1.8461\n",
            "Total loss:  -1.8009 | PDE Loss:  -3.8019 | Function Loss:  -1.8465\n",
            "Total loss:  -1.8017 | PDE Loss:  -3.8075 | Function Loss:  -1.8469\n",
            "Total loss:  -1.8028 | PDE Loss:  -3.8091 | Function Loss:  -1.8479\n",
            "Total loss:  -1.8043 | PDE Loss:  -3.81 | Function Loss:  -1.8494\n",
            "Total loss:  -1.8059 | PDE Loss:  -3.8038 | Function Loss:  -1.8518\n",
            "Total loss:  -1.8071 | PDE Loss:  -3.7977 | Function Loss:  -1.8539\n",
            "Total loss:  -1.8079 | PDE Loss:  -3.7935 | Function Loss:  -1.8553\n",
            "Total loss:  -1.8089 | PDE Loss:  -3.7914 | Function Loss:  -1.8566\n",
            "Total loss:  -1.8097 | PDE Loss:  -3.7829 | Function Loss:  -1.8585\n",
            "Total loss:  -1.8102 | PDE Loss:  -3.7823 | Function Loss:  -1.8592\n",
            "Total loss:  -1.8108 | PDE Loss:  -3.7834 | Function Loss:  -1.8597\n",
            "Total loss:  -1.8114 | PDE Loss:  -3.7736 | Function Loss:  -1.8615\n",
            "Total loss:  -1.8118 | PDE Loss:  -3.7803 | Function Loss:  -1.8613\n",
            "Total loss:  -1.8122 | PDE Loss:  -3.7848 | Function Loss:  -1.8611\n",
            "Total loss:  -1.8127 | PDE Loss:  -3.789 | Function Loss:  -1.8612\n",
            "Total loss:  -1.8132 | PDE Loss:  -3.7939 | Function Loss:  -1.8611\n",
            "Total loss:  -1.8137 | PDE Loss:  -3.7976 | Function Loss:  -1.8612\n",
            "Total loss:  -1.8144 | PDE Loss:  -3.8024 | Function Loss:  -1.8615\n",
            "Total loss:  -1.8153 | PDE Loss:  -3.806 | Function Loss:  -1.8621\n",
            "Total loss:  -1.8162 | PDE Loss:  -3.8122 | Function Loss:  -1.8624\n",
            "Total loss:  -1.8172 | PDE Loss:  -3.8119 | Function Loss:  -1.8636\n",
            "Total loss:  -1.8183 | PDE Loss:  -3.8157 | Function Loss:  -1.8644\n",
            "Total loss:  -1.8193 | PDE Loss:  -3.8057 | Function Loss:  -1.8666\n",
            "Total loss:  -1.82 | PDE Loss:  -3.8032 | Function Loss:  -1.8677\n",
            "Total loss:  -1.8206 | PDE Loss:  -3.7963 | Function Loss:  -1.8692\n",
            "Total loss:  -1.8213 | PDE Loss:  -3.7883 | Function Loss:  -1.8709\n",
            "Total loss:  -1.822 | PDE Loss:  -3.7878 | Function Loss:  -1.8717\n",
            "Total loss:  -1.8227 | PDE Loss:  -3.7762 | Function Loss:  -1.874\n",
            "Total loss:  -1.8232 | PDE Loss:  -3.7742 | Function Loss:  -1.8747\n",
            "Total loss:  -1.8236 | PDE Loss:  -3.7728 | Function Loss:  -1.8754\n",
            "Total loss:  -1.8239 | PDE Loss:  -3.7771 | Function Loss:  -1.8752\n",
            "Total loss:  -1.8241 | PDE Loss:  -3.7786 | Function Loss:  -1.8753\n",
            "Total loss:  -1.8244 | PDE Loss:  -3.7825 | Function Loss:  -1.875\n",
            "Total loss:  -1.8246 | PDE Loss:  -3.7847 | Function Loss:  -1.875\n",
            "Total loss:  -1.825 | PDE Loss:  -3.7887 | Function Loss:  -1.875\n",
            "Total loss:  -1.8254 | PDE Loss:  -3.7898 | Function Loss:  -1.8753\n",
            "Total loss:  -1.8258 | PDE Loss:  -3.7869 | Function Loss:  -1.8761\n",
            "Total loss:  -1.8261 | PDE Loss:  -3.7856 | Function Loss:  -1.8766\n",
            "Total loss:  -1.8264 | PDE Loss:  -3.7786 | Function Loss:  -1.8778\n",
            "Total loss:  -1.8266 | PDE Loss:  -3.7757 | Function Loss:  -1.8785\n",
            "Total loss:  -1.8269 | PDE Loss:  -3.7707 | Function Loss:  -1.8794\n",
            "Total loss:  -1.8271 | PDE Loss:  -3.7675 | Function Loss:  -1.8801\n",
            "Total loss:  -1.8274 | PDE Loss:  -3.7639 | Function Loss:  -1.8809\n",
            "Total loss:  -1.8277 | PDE Loss:  -3.763 | Function Loss:  -1.8813\n",
            "Total loss:  -1.8279 | PDE Loss:  -3.7624 | Function Loss:  -1.8816\n",
            "Total loss:  -1.8281 | PDE Loss:  -3.7638 | Function Loss:  -1.8817\n",
            "Total loss:  -1.8283 | PDE Loss:  -3.7656 | Function Loss:  -1.8816\n",
            "Total loss:  -1.8285 | PDE Loss:  -3.769 | Function Loss:  -1.8814\n",
            "Total loss:  -1.8288 | PDE Loss:  -3.7711 | Function Loss:  -1.8814\n",
            "Total loss:  -1.829 | PDE Loss:  -3.7746 | Function Loss:  -1.8813\n",
            "Total loss:  -1.8294 | PDE Loss:  -3.7763 | Function Loss:  -1.8814\n",
            "Total loss:  -1.8301 | PDE Loss:  -3.7844 | Function Loss:  -1.8812\n",
            "Total loss:  -1.8308 | PDE Loss:  -3.7873 | Function Loss:  -1.8817\n",
            "Total loss:  -1.8321 | PDE Loss:  -3.7915 | Function Loss:  -1.8826\n",
            "Total loss:  -1.8334 | PDE Loss:  -3.7921 | Function Loss:  -1.884\n",
            "Total loss:  -1.8345 | PDE Loss:  -3.7969 | Function Loss:  -1.8847\n",
            "Total loss:  -1.8356 | PDE Loss:  -3.7971 | Function Loss:  -1.8858\n",
            "Total loss:  -1.8367 | PDE Loss:  -3.8002 | Function Loss:  -1.8867\n",
            "Total loss:  -1.8375 | PDE Loss:  -3.7947 | Function Loss:  -1.8882\n",
            "Total loss:  -1.8381 | PDE Loss:  -3.795 | Function Loss:  -1.8889\n",
            "Total loss:  -1.8385 | PDE Loss:  -3.7891 | Function Loss:  -1.8901\n",
            "Total loss:  -1.8389 | PDE Loss:  -3.7919 | Function Loss:  -1.8902\n",
            "Total loss:  -1.8393 | PDE Loss:  -3.7856 | Function Loss:  -1.8914\n",
            "Total loss:  -1.8395 | PDE Loss:  -3.786 | Function Loss:  -1.8916\n",
            "Total loss:  -1.8399 | PDE Loss:  -3.7872 | Function Loss:  -1.892\n",
            "Total loss:  -1.8405 | PDE Loss:  -3.7929 | Function Loss:  -1.8919\n",
            "Total loss:  -1.841 | PDE Loss:  -3.7921 | Function Loss:  -1.8926\n",
            "Total loss:  -1.8415 | PDE Loss:  -3.7941 | Function Loss:  -1.8928\n",
            "Total loss:  -1.8418 | PDE Loss:  -3.7935 | Function Loss:  -1.8933\n",
            "Total loss:  -1.8421 | PDE Loss:  -3.7932 | Function Loss:  -1.8937\n",
            "Total loss:  -1.8423 | PDE Loss:  -3.793 | Function Loss:  -1.8939\n",
            "Total loss:  -1.8425 | PDE Loss:  -3.7932 | Function Loss:  -1.8941\n",
            "Total loss:  -1.8427 | PDE Loss:  -3.7921 | Function Loss:  -1.8945\n",
            "Total loss:  -1.8429 | PDE Loss:  -3.792 | Function Loss:  -1.8947\n",
            "Total loss:  -1.8432 | PDE Loss:  -3.7903 | Function Loss:  -1.8952\n",
            "Total loss:  -1.8435 | PDE Loss:  -3.7901 | Function Loss:  -1.8956\n",
            "Total loss:  -1.8437 | PDE Loss:  -3.7891 | Function Loss:  -1.896\n",
            "Total loss:  -1.844 | PDE Loss:  -3.7888 | Function Loss:  -1.8963\n",
            "Total loss:  -1.8443 | PDE Loss:  -3.7882 | Function Loss:  -1.8968\n",
            "Total loss:  -1.8446 | PDE Loss:  -3.7888 | Function Loss:  -1.8971\n",
            "Total loss:  -1.8449 | PDE Loss:  -3.7905 | Function Loss:  -1.8972\n",
            "Total loss:  -1.8453 | PDE Loss:  -3.7929 | Function Loss:  -1.8973\n",
            "Total loss:  -1.8457 | PDE Loss:  -3.7977 | Function Loss:  -1.8972\n",
            "Total loss:  -1.8462 | PDE Loss:  -3.8005 | Function Loss:  -1.8973\n",
            "Total loss:  -1.8465 | PDE Loss:  -3.8036 | Function Loss:  -1.8973\n",
            "Total loss:  -1.8469 | PDE Loss:  -3.8053 | Function Loss:  -1.8975\n",
            "Total loss:  -1.8472 | PDE Loss:  -3.8073 | Function Loss:  -1.8976\n",
            "Total loss:  -1.8474 | PDE Loss:  -3.8064 | Function Loss:  -1.898\n",
            "Total loss:  -1.8477 | PDE Loss:  -3.8053 | Function Loss:  -1.8984\n",
            "Total loss:  -1.8479 | PDE Loss:  -3.8037 | Function Loss:  -1.8989\n",
            "Total loss:  -1.8482 | PDE Loss:  -3.8016 | Function Loss:  -1.8995\n",
            "Total loss:  -1.8485 | PDE Loss:  -3.8018 | Function Loss:  -1.8997\n",
            "Total loss:  -1.8487 | PDE Loss:  -3.8024 | Function Loss:  -1.8999\n",
            "Total loss:  -1.849 | PDE Loss:  -3.8047 | Function Loss:  -1.9\n",
            "Total loss:  -1.8494 | PDE Loss:  -3.8087 | Function Loss:  -1.8999\n",
            "Total loss:  -1.8497 | PDE Loss:  -3.8131 | Function Loss:  -1.8998\n",
            "Total loss:  -1.85 | PDE Loss:  -3.8168 | Function Loss:  -1.8997\n",
            "Total loss:  -1.8503 | PDE Loss:  -3.8198 | Function Loss:  -1.8996\n",
            "Total loss:  -1.8506 | PDE Loss:  -3.8204 | Function Loss:  -1.8999\n",
            "Total loss:  -1.8511 | PDE Loss:  -3.8197 | Function Loss:  -1.9004\n",
            "Total loss:  -1.8516 | PDE Loss:  -3.8165 | Function Loss:  -1.9014\n",
            "Total loss:  -1.8521 | PDE Loss:  -3.8109 | Function Loss:  -1.9027\n",
            "Total loss:  -1.8528 | PDE Loss:  -3.807 | Function Loss:  -1.904\n",
            "Total loss:  -1.8535 | PDE Loss:  -3.7969 | Function Loss:  -1.9061\n",
            "Total loss:  -1.8542 | PDE Loss:  -3.795 | Function Loss:  -1.907\n",
            "Total loss:  -1.8548 | PDE Loss:  -3.7899 | Function Loss:  -1.9084\n",
            "Total loss:  -1.8553 | PDE Loss:  -3.7925 | Function Loss:  -1.9086\n",
            "Total loss:  -1.8557 | PDE Loss:  -3.7917 | Function Loss:  -1.9091\n",
            "Total loss:  -1.856 | PDE Loss:  -3.797 | Function Loss:  -1.9088\n",
            "Total loss:  -1.8562 | PDE Loss:  -3.8011 | Function Loss:  -1.9086\n",
            "Total loss:  -1.8565 | PDE Loss:  -3.8055 | Function Loss:  -1.9083\n",
            "Total loss:  -1.8569 | PDE Loss:  -3.8106 | Function Loss:  -1.9081\n",
            "Total loss:  -1.8575 | PDE Loss:  -3.8195 | Function Loss:  -1.9076\n",
            "Total loss:  -1.858 | PDE Loss:  -3.8281 | Function Loss:  -1.9072\n",
            "Total loss:  -1.8587 | PDE Loss:  -3.8398 | Function Loss:  -1.9066\n",
            "Total loss:  -1.8593 | PDE Loss:  -3.8497 | Function Loss:  -1.9062\n",
            "Total loss:  -1.8599 | PDE Loss:  -3.8539 | Function Loss:  -1.9063\n",
            "Total loss:  -1.8603 | PDE Loss:  -3.8605 | Function Loss:  -1.906\n",
            "Total loss:  -1.8606 | PDE Loss:  -3.8561 | Function Loss:  -1.9069\n",
            "Total loss:  -1.8609 | PDE Loss:  -3.8587 | Function Loss:  -1.9069\n",
            "Total loss:  -1.8611 | PDE Loss:  -3.8573 | Function Loss:  -1.9073\n",
            "Total loss:  -1.8614 | PDE Loss:  -3.8578 | Function Loss:  -1.9075\n",
            "Total loss:  -1.8617 | PDE Loss:  -3.8578 | Function Loss:  -1.9078\n",
            "Total loss:  -1.8619 | PDE Loss:  -3.8599 | Function Loss:  -1.9079\n",
            "Total loss:  -1.8621 | PDE Loss:  -3.862 | Function Loss:  -1.9079\n",
            "Total loss:  -1.8623 | PDE Loss:  -3.8644 | Function Loss:  -1.9078\n",
            "Total loss:  -1.8625 | PDE Loss:  -3.8671 | Function Loss:  -1.9077\n",
            "Total loss:  -1.8627 | PDE Loss:  -3.8694 | Function Loss:  -1.9078\n",
            "Total loss:  -1.863 | PDE Loss:  -3.8719 | Function Loss:  -1.9078\n",
            "Total loss:  -1.8633 | PDE Loss:  -3.874 | Function Loss:  -1.9078\n",
            "Total loss:  -1.8635 | PDE Loss:  -3.8759 | Function Loss:  -1.9079\n",
            "Total loss:  -1.8638 | PDE Loss:  -3.8774 | Function Loss:  -1.9081\n",
            "Total loss:  -1.8641 | PDE Loss:  -3.8813 | Function Loss:  -1.908\n",
            "Total loss:  -1.8644 | PDE Loss:  -3.8794 | Function Loss:  -1.9085\n",
            "Total loss:  -1.8646 | PDE Loss:  -3.884 | Function Loss:  -1.9082\n",
            "Total loss:  -1.8648 | PDE Loss:  -3.8888 | Function Loss:  -1.908\n",
            "Total loss:  -1.8652 | PDE Loss:  -3.8925 | Function Loss:  -1.908\n",
            "Total loss:  -1.8655 | PDE Loss:  -3.8988 | Function Loss:  -1.9077\n",
            "Total loss:  -1.8659 | PDE Loss:  -3.8998 | Function Loss:  -1.9081\n",
            "Total loss:  -1.8664 | PDE Loss:  -3.903 | Function Loss:  -1.9083\n",
            "Total loss:  -1.8672 | PDE Loss:  -3.907 | Function Loss:  -1.9087\n",
            "Total loss:  -1.8679 | PDE Loss:  -3.9039 | Function Loss:  -1.9099\n",
            "Total loss:  -1.8686 | PDE Loss:  -3.9132 | Function Loss:  -1.9097\n",
            "Total loss:  -1.8694 | PDE Loss:  -3.903 | Function Loss:  -1.9116\n",
            "Total loss:  -1.87 | PDE Loss:  -3.9077 | Function Loss:  -1.9118\n",
            "Total loss:  -1.8707 | PDE Loss:  -3.9005 | Function Loss:  -1.9133\n",
            "Total loss:  -1.8714 | PDE Loss:  -3.9026 | Function Loss:  -1.9138\n",
            "Total loss:  -1.8721 | PDE Loss:  -3.9018 | Function Loss:  -1.9147\n",
            "Total loss:  -1.8729 | PDE Loss:  -3.9015 | Function Loss:  -1.9156\n",
            "Total loss:  -1.874 | PDE Loss:  -3.9076 | Function Loss:  -1.9162\n",
            "Total loss:  -1.875 | PDE Loss:  -3.9076 | Function Loss:  -1.9173\n",
            "Total loss:  -1.8757 | PDE Loss:  -3.9125 | Function Loss:  -1.9175\n",
            "Total loss:  -1.8762 | PDE Loss:  -3.92 | Function Loss:  -1.9174\n",
            "Total loss:  -1.8766 | PDE Loss:  -3.9214 | Function Loss:  -1.9177\n",
            "Total loss:  -1.8771 | PDE Loss:  -3.9245 | Function Loss:  -1.9179\n",
            "Total loss:  -1.8775 | PDE Loss:  -3.9289 | Function Loss:  -1.9179\n",
            "Total loss:  -1.8779 | PDE Loss:  -3.9289 | Function Loss:  -1.9183\n",
            "Total loss:  -1.8782 | PDE Loss:  -3.9319 | Function Loss:  -1.9183\n",
            "Total loss:  -1.8785 | PDE Loss:  -3.9317 | Function Loss:  -1.9187\n",
            "Total loss:  -1.8788 | PDE Loss:  -3.9339 | Function Loss:  -1.9189\n",
            "Total loss:  -1.8792 | PDE Loss:  -3.9361 | Function Loss:  -1.919\n",
            "Total loss:  -1.8796 | PDE Loss:  -3.943 | Function Loss:  -1.9188\n",
            "Total loss:  -1.88 | PDE Loss:  -3.9442 | Function Loss:  -1.9192\n",
            "Total loss:  -1.8805 | PDE Loss:  -3.9519 | Function Loss:  -1.919\n",
            "Total loss:  -1.8811 | PDE Loss:  -3.9586 | Function Loss:  -1.919\n",
            "Total loss:  -1.8817 | PDE Loss:  -3.9713 | Function Loss:  -1.9185\n",
            "Total loss:  -1.8822 | PDE Loss:  -3.9799 | Function Loss:  -1.9183\n",
            "Total loss:  -1.8828 | PDE Loss:  -3.9911 | Function Loss:  -1.9181\n",
            "Total loss:  -1.8837 | PDE Loss:  -4.0021 | Function Loss:  -1.9181\n",
            "Total loss:  -1.8843 | PDE Loss:  -4.0103 | Function Loss:  -1.9181\n",
            "Total loss:  -1.8851 | PDE Loss:  -4.0169 | Function Loss:  -1.9184\n",
            "Total loss:  -1.8858 | PDE Loss:  -4.0173 | Function Loss:  -1.9192\n",
            "Total loss:  -1.8865 | PDE Loss:  -4.0177 | Function Loss:  -1.9199\n",
            "Total loss:  -1.8871 | PDE Loss:  -4.0098 | Function Loss:  -1.9211\n",
            "Total loss:  -1.8874 | PDE Loss:  -4.0083 | Function Loss:  -1.9216\n",
            "Total loss:  -1.8877 | PDE Loss:  -3.9975 | Function Loss:  -1.9229\n",
            "Total loss:  -1.8881 | PDE Loss:  -3.9964 | Function Loss:  -1.9233\n",
            "Total loss:  -1.8884 | PDE Loss:  -3.9913 | Function Loss:  -1.9241\n",
            "Total loss:  -1.8889 | PDE Loss:  -3.989 | Function Loss:  -1.9248\n",
            "Total loss:  -1.8893 | PDE Loss:  -3.9849 | Function Loss:  -1.9256\n",
            "Total loss:  -1.8898 | PDE Loss:  -3.9838 | Function Loss:  -1.9263\n",
            "Total loss:  -1.8903 | PDE Loss:  -3.9816 | Function Loss:  -1.927\n",
            "Total loss:  -1.8908 | PDE Loss:  -3.9797 | Function Loss:  -1.9277\n",
            "Total loss:  -1.8915 | PDE Loss:  -3.9781 | Function Loss:  -1.9286\n",
            "Total loss:  -1.8921 | PDE Loss:  -3.975 | Function Loss:  -1.9296\n",
            "Total loss:  -1.893 | PDE Loss:  -3.9711 | Function Loss:  -1.9308\n",
            "Total loss:  -1.8939 | PDE Loss:  -3.9714 | Function Loss:  -1.9319\n",
            "Total loss:  -1.8947 | PDE Loss:  -3.9724 | Function Loss:  -1.9327\n",
            "Total loss:  -1.8953 | PDE Loss:  -3.9481 | Function Loss:  -1.9356\n",
            "Total loss:  -1.8959 | PDE Loss:  -3.9602 | Function Loss:  -1.9351\n",
            "Total loss:  -1.8963 | PDE Loss:  -3.9662 | Function Loss:  -1.9349\n",
            "Total loss:  -1.8967 | PDE Loss:  -3.9635 | Function Loss:  -1.9357\n",
            "Total loss:  -1.8971 | PDE Loss:  -3.96 | Function Loss:  -1.9364\n",
            "Total loss:  -1.8974 | PDE Loss:  -3.9537 | Function Loss:  -1.9374\n",
            "Total loss:  -1.8978 | PDE Loss:  -3.9489 | Function Loss:  -1.9382\n",
            "Total loss:  -1.8981 | PDE Loss:  -3.9429 | Function Loss:  -1.9392\n",
            "Total loss:  -1.8985 | PDE Loss:  -3.9377 | Function Loss:  -1.9402\n",
            "Total loss:  -1.899 | PDE Loss:  -3.932 | Function Loss:  -1.9412\n",
            "Total loss:  -1.8994 | PDE Loss:  -3.9274 | Function Loss:  -1.9422\n",
            "Total loss:  -1.8997 | PDE Loss:  -3.9263 | Function Loss:  -1.9426\n",
            "Total loss:  -1.9 | PDE Loss:  -3.9201 | Function Loss:  -1.9436\n",
            "Total loss:  -1.9002 | PDE Loss:  -3.9207 | Function Loss:  -1.9438\n",
            "Total loss:  -1.9005 | PDE Loss:  -3.9204 | Function Loss:  -1.9441\n",
            "Total loss:  -1.9008 | PDE Loss:  -3.9212 | Function Loss:  -1.9444\n",
            "Total loss:  -1.9012 | PDE Loss:  -3.9214 | Function Loss:  -1.9448\n",
            "Total loss:  -1.9018 | PDE Loss:  -3.922 | Function Loss:  -1.9454\n",
            "Total loss:  -1.9023 | PDE Loss:  -3.9212 | Function Loss:  -1.946\n",
            "Total loss:  -1.903 | PDE Loss:  -3.9233 | Function Loss:  -1.9465\n",
            "Total loss:  -1.9037 | PDE Loss:  -3.9222 | Function Loss:  -1.9474\n",
            "Total loss:  -1.9043 | PDE Loss:  -3.9246 | Function Loss:  -1.9478\n",
            "Total loss:  -1.9048 | PDE Loss:  -3.9264 | Function Loss:  -1.9482\n",
            "Total loss:  -1.9053 | PDE Loss:  -3.9274 | Function Loss:  -1.9487\n",
            "Total loss:  -1.9058 | PDE Loss:  -3.9261 | Function Loss:  -1.9494\n",
            "Total loss:  -1.9062 | PDE Loss:  -3.93 | Function Loss:  -1.9494\n",
            "Total loss:  -1.9066 | PDE Loss:  -3.9234 | Function Loss:  -1.9506\n",
            "Total loss:  -1.9071 | PDE Loss:  -3.926 | Function Loss:  -1.9508\n",
            "Total loss:  -1.9076 | PDE Loss:  -3.9253 | Function Loss:  -1.9514\n",
            "Total loss:  -1.9082 | PDE Loss:  -3.9236 | Function Loss:  -1.9523\n",
            "Total loss:  -1.9089 | PDE Loss:  -3.9149 | Function Loss:  -1.954\n",
            "Total loss:  -1.9095 | PDE Loss:  -3.9131 | Function Loss:  -1.9548\n",
            "Total loss:  -1.9099 | PDE Loss:  -3.9092 | Function Loss:  -1.9558\n",
            "Total loss:  -1.9105 | PDE Loss:  -3.9073 | Function Loss:  -1.9567\n",
            "Total loss:  -1.9113 | PDE Loss:  -3.9111 | Function Loss:  -1.9571\n",
            "Total loss:  -1.912 | PDE Loss:  -3.9139 | Function Loss:  -1.9575\n",
            "Total loss:  -1.9125 | PDE Loss:  -3.9246 | Function Loss:  -1.957\n",
            "Total loss:  -1.913 | PDE Loss:  -3.9312 | Function Loss:  -1.9568\n",
            "Total loss:  -1.9135 | PDE Loss:  -3.942 | Function Loss:  -1.9562\n",
            "Total loss:  -1.9141 | PDE Loss:  -3.9527 | Function Loss:  -1.9558\n",
            "Total loss:  -1.9147 | PDE Loss:  -3.9649 | Function Loss:  -1.9552\n",
            "Total loss:  -1.9153 | PDE Loss:  -3.9745 | Function Loss:  -1.955\n",
            "Total loss:  -1.9163 | PDE Loss:  -3.9839 | Function Loss:  -1.9552\n",
            "Total loss:  -1.9177 | PDE Loss:  -3.993 | Function Loss:  -1.9558\n",
            "Total loss:  -1.9189 | PDE Loss:  -3.9935 | Function Loss:  -1.9571\n",
            "Total loss:  -1.9198 | PDE Loss:  -3.9866 | Function Loss:  -1.9587\n",
            "Total loss:  -1.9207 | PDE Loss:  -3.9787 | Function Loss:  -1.9605\n",
            "Total loss:  -1.9214 | PDE Loss:  -3.968 | Function Loss:  -1.9623\n",
            "Total loss:  -1.922 | PDE Loss:  -3.9618 | Function Loss:  -1.9635\n",
            "Total loss:  -1.9228 | PDE Loss:  -3.9504 | Function Loss:  -1.9655\n",
            "Total loss:  -1.9235 | PDE Loss:  -3.9492 | Function Loss:  -1.9665\n",
            "Total loss:  -1.9241 | PDE Loss:  -3.9491 | Function Loss:  -1.9671\n",
            "Total loss:  -1.9247 | PDE Loss:  -3.9494 | Function Loss:  -1.9678\n",
            "Total loss:  -1.925 | PDE Loss:  -3.9528 | Function Loss:  -1.9677\n",
            "Total loss:  -1.9253 | PDE Loss:  -3.9567 | Function Loss:  -1.9677\n",
            "Total loss:  -1.9257 | PDE Loss:  -3.9611 | Function Loss:  -1.9677\n",
            "Total loss:  -1.926 | PDE Loss:  -3.9644 | Function Loss:  -1.9677\n",
            "Total loss:  -1.9263 | PDE Loss:  -3.9666 | Function Loss:  -1.9678\n",
            "Total loss:  -1.9268 | PDE Loss:  -3.969 | Function Loss:  -1.9682\n",
            "Total loss:  -1.9274 | PDE Loss:  -3.9683 | Function Loss:  -1.9688\n",
            "Total loss:  -1.9279 | PDE Loss:  -3.9673 | Function Loss:  -1.9695\n",
            "Total loss:  -1.9285 | PDE Loss:  -3.9622 | Function Loss:  -1.9707\n",
            "Total loss:  -1.9289 | PDE Loss:  -3.9559 | Function Loss:  -1.9718\n",
            "Total loss:  -1.9295 | PDE Loss:  -3.9482 | Function Loss:  -1.9732\n",
            "Total loss:  -1.9299 | PDE Loss:  -3.9406 | Function Loss:  -1.9745\n",
            "Total loss:  -1.9303 | PDE Loss:  -3.9359 | Function Loss:  -1.9754\n",
            "Total loss:  -1.9306 | PDE Loss:  -3.93 | Function Loss:  -1.9764\n",
            "Total loss:  -1.9307 | PDE Loss:  -3.9286 | Function Loss:  -1.9767\n",
            "Total loss:  -1.9309 | PDE Loss:  -3.9281 | Function Loss:  -1.977\n",
            "Total loss:  -1.9311 | PDE Loss:  -3.9281 | Function Loss:  -1.9772\n",
            "Total loss:  -1.9312 | PDE Loss:  -3.9288 | Function Loss:  -1.9773\n",
            "Total loss:  -1.9314 | PDE Loss:  -3.9288 | Function Loss:  -1.9775\n",
            "Total loss:  -1.9316 | PDE Loss:  -3.9292 | Function Loss:  -1.9776\n",
            "Total loss:  -1.9318 | PDE Loss:  -3.9295 | Function Loss:  -1.9778\n",
            "Total loss:  -1.9319 | PDE Loss:  -3.9269 | Function Loss:  -1.9782\n",
            "Total loss:  -1.932 | PDE Loss:  -3.9261 | Function Loss:  -1.9785\n",
            "Total loss:  -1.9322 | PDE Loss:  -3.9238 | Function Loss:  -1.9789\n",
            "Total loss:  -1.9324 | PDE Loss:  -3.9243 | Function Loss:  -1.979\n",
            "Total loss:  -1.9325 | PDE Loss:  -3.9246 | Function Loss:  -1.9792\n",
            "Total loss:  -1.9327 | PDE Loss:  -3.9272 | Function Loss:  -1.979\n",
            "Total loss:  -1.9328 | PDE Loss:  -3.9302 | Function Loss:  -1.9789\n",
            "Total loss:  -1.933 | PDE Loss:  -3.9319 | Function Loss:  -1.9789\n",
            "Total loss:  -1.9332 | PDE Loss:  -3.9346 | Function Loss:  -1.9788\n",
            "Total loss:  -1.9336 | PDE Loss:  -3.9384 | Function Loss:  -1.9788\n",
            "Total loss:  -1.934 | PDE Loss:  -3.9416 | Function Loss:  -1.979\n",
            "Total loss:  -1.9346 | PDE Loss:  -3.9457 | Function Loss:  -1.9792\n",
            "Total loss:  -1.9352 | PDE Loss:  -3.9468 | Function Loss:  -1.9797\n",
            "Total loss:  -1.9359 | PDE Loss:  -3.9462 | Function Loss:  -1.9805\n",
            "Total loss:  -1.9365 | PDE Loss:  -3.9451 | Function Loss:  -1.9813\n",
            "Total loss:  -1.9372 | PDE Loss:  -3.9423 | Function Loss:  -1.9824\n",
            "Total loss:  -1.938 | PDE Loss:  -3.94 | Function Loss:  -1.9835\n",
            "Total loss:  -1.9385 | PDE Loss:  -3.9413 | Function Loss:  -1.984\n",
            "Total loss:  -1.9391 | PDE Loss:  -3.9435 | Function Loss:  -1.9844\n",
            "Total loss:  -1.9397 | PDE Loss:  -3.9482 | Function Loss:  -1.9846\n",
            "Total loss:  -1.9404 | PDE Loss:  -3.9524 | Function Loss:  -1.9848\n",
            "Total loss:  -1.9411 | PDE Loss:  -3.9572 | Function Loss:  -1.9851\n",
            "Total loss:  -1.9419 | PDE Loss:  -3.9596 | Function Loss:  -1.9857\n",
            "Total loss:  -1.9427 | PDE Loss:  -3.9609 | Function Loss:  -1.9865\n",
            "Total loss:  -1.9435 | PDE Loss:  -3.9587 | Function Loss:  -1.9876\n",
            "Total loss:  -1.9443 | PDE Loss:  -3.9551 | Function Loss:  -1.9888\n",
            "Total loss:  -1.9449 | PDE Loss:  -3.9521 | Function Loss:  -1.9899\n",
            "Total loss:  -1.9456 | PDE Loss:  -3.9479 | Function Loss:  -1.9911\n",
            "Total loss:  -1.9462 | PDE Loss:  -3.947 | Function Loss:  -1.9919\n",
            "Total loss:  -1.9469 | PDE Loss:  -3.9451 | Function Loss:  -1.9929\n",
            "Total loss:  -1.9476 | PDE Loss:  -3.9464 | Function Loss:  -1.9935\n",
            "Total loss:  -1.9482 | PDE Loss:  -3.9493 | Function Loss:  -1.9938\n",
            "Total loss:  -1.9487 | PDE Loss:  -3.9497 | Function Loss:  -1.9943\n",
            "Total loss:  -1.9491 | PDE Loss:  -3.9531 | Function Loss:  -1.9944\n",
            "Total loss:  -1.9494 | PDE Loss:  -3.9545 | Function Loss:  -1.9946\n",
            "Total loss:  -1.9497 | PDE Loss:  -3.9575 | Function Loss:  -1.9946\n",
            "Total loss:  -1.9499 | PDE Loss:  -3.9588 | Function Loss:  -1.9947\n",
            "Total loss:  -1.9501 | PDE Loss:  -3.961 | Function Loss:  -1.9947\n",
            "Total loss:  -1.9504 | PDE Loss:  -3.9542 | Function Loss:  -1.9957\n",
            "Total loss:  -1.9506 | PDE Loss:  -3.9558 | Function Loss:  -1.9957\n",
            "Total loss:  -1.9507 | PDE Loss:  -3.9533 | Function Loss:  -1.9962\n",
            "Total loss:  -1.9509 | PDE Loss:  -3.9509 | Function Loss:  -1.9966\n",
            "Total loss:  -1.951 | PDE Loss:  -3.9463 | Function Loss:  -1.9972\n",
            "Total loss:  -1.951 | PDE Loss:  -3.9431 | Function Loss:  -1.9977\n",
            "Total loss:  -1.9511 | PDE Loss:  -3.94 | Function Loss:  -1.9981\n",
            "Total loss:  -1.9513 | PDE Loss:  -3.9361 | Function Loss:  -1.9987\n",
            "Total loss:  -1.9514 | PDE Loss:  -3.9305 | Function Loss:  -1.9996\n",
            "Total loss:  -1.9517 | PDE Loss:  -3.9282 | Function Loss:  -2.0001\n",
            "Total loss:  -1.9519 | PDE Loss:  -3.9224 | Function Loss:  -2.0011\n",
            "Total loss:  -1.9522 | PDE Loss:  -3.9227 | Function Loss:  -2.0014\n",
            "Total loss:  -1.9526 | PDE Loss:  -3.9206 | Function Loss:  -2.0021\n",
            "Total loss:  -1.953 | PDE Loss:  -3.9251 | Function Loss:  -2.002\n",
            "Total loss:  -1.9534 | PDE Loss:  -3.9272 | Function Loss:  -2.0022\n",
            "Total loss:  -1.954 | PDE Loss:  -3.9328 | Function Loss:  -2.0022\n",
            "Total loss:  -1.955 | PDE Loss:  -3.9316 | Function Loss:  -2.0034\n",
            "Total loss:  -1.9561 | PDE Loss:  -3.9352 | Function Loss:  -2.0043\n",
            "Total loss:  -1.9578 | PDE Loss:  -3.9389 | Function Loss:  -2.0057\n",
            "Total loss:  -1.9597 | PDE Loss:  -3.9463 | Function Loss:  -2.007\n",
            "Total loss:  -1.962 | PDE Loss:  -3.9427 | Function Loss:  -2.0099\n",
            "Total loss:  -1.9635 | PDE Loss:  -3.9498 | Function Loss:  -2.0108\n",
            "Total loss:  -1.9649 | PDE Loss:  -3.9507 | Function Loss:  -2.0123\n",
            "Total loss:  -1.9664 | PDE Loss:  -3.9393 | Function Loss:  -2.0153\n",
            "Total loss:  -1.9673 | PDE Loss:  -3.938 | Function Loss:  -2.0164\n",
            "Total loss:  -1.9678 | PDE Loss:  -3.9359 | Function Loss:  -2.0172\n",
            "Total loss:  -1.9683 | PDE Loss:  -3.9373 | Function Loss:  -2.0176\n",
            "Total loss:  -1.9689 | PDE Loss:  -3.9406 | Function Loss:  -2.0179\n",
            "Total loss:  -1.97 | PDE Loss:  -3.9492 | Function Loss:  -2.0181\n",
            "Total loss:  -1.9714 | PDE Loss:  -3.9622 | Function Loss:  -2.0182\n",
            "Total loss:  -1.9727 | PDE Loss:  -3.9739 | Function Loss:  -2.0183\n",
            "Total loss:  -1.974 | PDE Loss:  -3.9902 | Function Loss:  -2.018\n",
            "Total loss:  -1.9755 | PDE Loss:  -4.004 | Function Loss:  -2.0182\n",
            "Total loss:  -1.9766 | PDE Loss:  -4.0029 | Function Loss:  -2.0195\n",
            "Total loss:  -1.9778 | PDE Loss:  -4.0033 | Function Loss:  -2.0209\n",
            "Total loss:  -1.9792 | PDE Loss:  -3.991 | Function Loss:  -2.0237\n",
            "Total loss:  -1.9806 | PDE Loss:  -3.9844 | Function Loss:  -2.0259\n",
            "Total loss:  -1.9816 | PDE Loss:  -3.9657 | Function Loss:  -2.0292\n",
            "Total loss:  -1.9824 | PDE Loss:  -3.9583 | Function Loss:  -2.031\n",
            "Total loss:  -1.9831 | PDE Loss:  -3.9392 | Function Loss:  -2.034\n",
            "Total loss:  -1.9836 | PDE Loss:  -3.9403 | Function Loss:  -2.0345\n",
            "Total loss:  -1.9841 | PDE Loss:  -3.9483 | Function Loss:  -2.0341\n",
            "Total loss:  -1.9846 | PDE Loss:  -3.9539 | Function Loss:  -2.034\n",
            "Total loss:  -1.9851 | PDE Loss:  -3.958 | Function Loss:  -2.034\n",
            "Total loss:  -1.9856 | PDE Loss:  -3.9617 | Function Loss:  -2.0341\n",
            "Total loss:  -1.986 | PDE Loss:  -3.9625 | Function Loss:  -2.0345\n",
            "Total loss:  -1.9864 | PDE Loss:  -3.9622 | Function Loss:  -2.0349\n",
            "Total loss:  -1.9868 | PDE Loss:  -3.9607 | Function Loss:  -2.0356\n",
            "Total loss:  -1.9872 | PDE Loss:  -3.9593 | Function Loss:  -2.0362\n",
            "Total loss:  -1.9877 | PDE Loss:  -3.9576 | Function Loss:  -2.037\n",
            "Total loss:  -1.988 | PDE Loss:  -3.9507 | Function Loss:  -2.0381\n",
            "Total loss:  -1.9886 | PDE Loss:  -3.9479 | Function Loss:  -2.0391\n",
            "Total loss:  -1.9891 | PDE Loss:  -3.953 | Function Loss:  -2.0391\n",
            "Total loss:  -1.9896 | PDE Loss:  -3.9529 | Function Loss:  -2.0396\n",
            "Total loss:  -1.9902 | PDE Loss:  -3.9535 | Function Loss:  -2.0402\n",
            "Total loss:  -1.9906 | PDE Loss:  -3.9555 | Function Loss:  -2.0404\n",
            "Total loss:  -1.991 | PDE Loss:  -3.9562 | Function Loss:  -2.0408\n",
            "Total loss:  -1.9914 | PDE Loss:  -3.9628 | Function Loss:  -2.0404\n",
            "Total loss:  -1.9917 | PDE Loss:  -3.9639 | Function Loss:  -2.0406\n",
            "Total loss:  -1.9919 | PDE Loss:  -3.9686 | Function Loss:  -2.0404\n",
            "Total loss:  -1.9921 | PDE Loss:  -3.9703 | Function Loss:  -2.0404\n",
            "Total loss:  -1.9923 | PDE Loss:  -3.973 | Function Loss:  -2.0402\n",
            "Total loss:  -1.9924 | PDE Loss:  -3.9744 | Function Loss:  -2.0402\n",
            "Total loss:  -1.9926 | PDE Loss:  -3.9756 | Function Loss:  -2.0402\n",
            "Total loss:  -1.9927 | PDE Loss:  -3.9766 | Function Loss:  -2.0403\n",
            "Total loss:  -1.9929 | PDE Loss:  -3.9787 | Function Loss:  -2.0403\n",
            "Total loss:  -1.9931 | PDE Loss:  -3.9802 | Function Loss:  -2.0403\n",
            "Total loss:  -1.9934 | PDE Loss:  -3.9834 | Function Loss:  -2.0402\n",
            "Total loss:  -1.9937 | PDE Loss:  -3.9803 | Function Loss:  -2.041\n",
            "Total loss:  -1.9939 | PDE Loss:  -3.9823 | Function Loss:  -2.041\n",
            "Total loss:  -1.9943 | PDE Loss:  -3.9843 | Function Loss:  -2.0411\n",
            "Total loss:  -1.9946 | PDE Loss:  -3.9856 | Function Loss:  -2.0413\n",
            "Total loss:  -1.9948 | PDE Loss:  -3.9871 | Function Loss:  -2.0415\n",
            "Total loss:  -1.9951 | PDE Loss:  -3.9879 | Function Loss:  -2.0416\n",
            "Total loss:  -1.9954 | PDE Loss:  -3.9888 | Function Loss:  -2.0419\n",
            "Total loss:  -1.9957 | PDE Loss:  -3.9897 | Function Loss:  -2.0422\n",
            "Total loss:  -1.9961 | PDE Loss:  -3.9891 | Function Loss:  -2.0426\n",
            "Total loss:  -1.9964 | PDE Loss:  -3.9894 | Function Loss:  -2.043\n",
            "Total loss:  -1.9968 | PDE Loss:  -3.9871 | Function Loss:  -2.0436\n",
            "Total loss:  -1.9971 | PDE Loss:  -3.9852 | Function Loss:  -2.0442\n",
            "Total loss:  -1.9976 | PDE Loss:  -3.9814 | Function Loss:  -2.0452\n",
            "Total loss:  -1.9982 | PDE Loss:  -3.9765 | Function Loss:  -2.0464\n",
            "Total loss:  -1.9989 | PDE Loss:  -3.9634 | Function Loss:  -2.0488\n",
            "Total loss:  -1.9995 | PDE Loss:  -3.9555 | Function Loss:  -2.0505\n",
            "Total loss:  -2.0004 | PDE Loss:  -3.9482 | Function Loss:  -2.0524\n",
            "Total loss:  -2.0014 | PDE Loss:  -3.9362 | Function Loss:  -2.0551\n",
            "Total loss:  -2.0023 | PDE Loss:  -3.9294 | Function Loss:  -2.0569\n",
            "Total loss:  -2.0031 | PDE Loss:  -3.9277 | Function Loss:  -2.0581\n",
            "Total loss:  -2.004 | PDE Loss:  -3.923 | Function Loss:  -2.0598\n",
            "Total loss:  -2.0046 | PDE Loss:  -3.9238 | Function Loss:  -2.0603\n",
            "Total loss:  -2.0054 | PDE Loss:  -3.9234 | Function Loss:  -2.0613\n",
            "Total loss:  -2.0067 | PDE Loss:  -3.9242 | Function Loss:  -2.0627\n",
            "Total loss:  -2.0082 | PDE Loss:  -3.9217 | Function Loss:  -2.0647\n",
            "Total loss:  -2.0092 | PDE Loss:  -3.9218 | Function Loss:  -2.0659\n",
            "Total loss:  -2.0103 | PDE Loss:  -3.9234 | Function Loss:  -2.0668\n",
            "Total loss:  -2.0112 | PDE Loss:  -3.9224 | Function Loss:  -2.0681\n",
            "Total loss:  -2.0119 | PDE Loss:  -3.9219 | Function Loss:  -2.0689\n",
            "Total loss:  -2.0125 | PDE Loss:  -3.9287 | Function Loss:  -2.0686\n",
            "Total loss:  -2.013 | PDE Loss:  -3.9283 | Function Loss:  -2.0693\n",
            "Total loss:  -2.0133 | PDE Loss:  -3.9246 | Function Loss:  -2.0701\n",
            "Total loss:  -2.0137 | PDE Loss:  -3.9295 | Function Loss:  -2.0699\n",
            "Total loss:  -2.0143 | PDE Loss:  -3.9344 | Function Loss:  -2.0699\n",
            "Total loss:  -2.0148 | PDE Loss:  -3.9414 | Function Loss:  -2.0696\n",
            "Total loss:  -2.0154 | PDE Loss:  -3.944 | Function Loss:  -2.0698\n",
            "Total loss:  -2.016 | PDE Loss:  -3.947 | Function Loss:  -2.0701\n",
            "Total loss:  -2.0165 | PDE Loss:  -3.9491 | Function Loss:  -2.0705\n",
            "Total loss:  -2.017 | PDE Loss:  -3.9467 | Function Loss:  -2.0713\n",
            "Total loss:  -2.0174 | PDE Loss:  -3.9454 | Function Loss:  -2.0719\n",
            "Total loss:  -2.0179 | PDE Loss:  -3.9401 | Function Loss:  -2.0732\n",
            "Total loss:  -2.0186 | PDE Loss:  -3.9395 | Function Loss:  -2.0741\n",
            "Total loss:  -2.0193 | PDE Loss:  -3.9365 | Function Loss:  -2.0753\n",
            "Total loss:  -2.0201 | PDE Loss:  -3.9359 | Function Loss:  -2.0763\n",
            "Total loss:  -2.0211 | PDE Loss:  -3.9392 | Function Loss:  -2.077\n",
            "Total loss:  -2.022 | PDE Loss:  -3.9419 | Function Loss:  -2.0777\n",
            "Total loss:  -2.0227 | PDE Loss:  -3.948 | Function Loss:  -2.0776\n",
            "Total loss:  -2.0232 | PDE Loss:  -3.9494 | Function Loss:  -2.078\n",
            "Total loss:  -2.0235 | PDE Loss:  -3.9543 | Function Loss:  -2.0777\n",
            "Total loss:  -2.0238 | PDE Loss:  -3.9536 | Function Loss:  -2.0781\n",
            "Total loss:  -2.024 | PDE Loss:  -3.9549 | Function Loss:  -2.0781\n",
            "Total loss:  -2.0242 | PDE Loss:  -3.9562 | Function Loss:  -2.0782\n",
            "Total loss:  -2.0244 | PDE Loss:  -3.9561 | Function Loss:  -2.0784\n",
            "Total loss:  -2.0246 | PDE Loss:  -3.9582 | Function Loss:  -2.0785\n",
            "Total loss:  -2.0248 | PDE Loss:  -3.9589 | Function Loss:  -2.0786\n",
            "Total loss:  -2.0251 | PDE Loss:  -3.9597 | Function Loss:  -2.0787\n",
            "Total loss:  -2.0254 | PDE Loss:  -3.9613 | Function Loss:  -2.0789\n",
            "Total loss:  -2.0257 | PDE Loss:  -3.9617 | Function Loss:  -2.0792\n",
            "Total loss:  -2.0261 | PDE Loss:  -3.9638 | Function Loss:  -2.0793\n",
            "Total loss:  -2.0264 | PDE Loss:  -3.9637 | Function Loss:  -2.0797\n",
            "Total loss:  -2.0268 | PDE Loss:  -3.9651 | Function Loss:  -2.08\n",
            "Total loss:  -2.0272 | PDE Loss:  -3.9619 | Function Loss:  -2.0809\n",
            "Total loss:  -2.0276 | PDE Loss:  -3.9623 | Function Loss:  -2.0813\n",
            "Total loss:  -2.028 | PDE Loss:  -3.9596 | Function Loss:  -2.082\n",
            "Total loss:  -2.0283 | PDE Loss:  -3.9594 | Function Loss:  -2.0824\n",
            "Total loss:  -2.0286 | PDE Loss:  -3.9573 | Function Loss:  -2.083\n",
            "Total loss:  -2.0289 | PDE Loss:  -3.9569 | Function Loss:  -2.0834\n",
            "Total loss:  -2.0292 | PDE Loss:  -3.9557 | Function Loss:  -2.084\n",
            "Total loss:  -2.0295 | PDE Loss:  -3.9554 | Function Loss:  -2.0844\n",
            "Total loss:  -2.0299 | PDE Loss:  -3.9558 | Function Loss:  -2.0847\n",
            "Total loss:  -2.0303 | PDE Loss:  -3.9568 | Function Loss:  -2.085\n",
            "Total loss:  -2.0308 | PDE Loss:  -3.9591 | Function Loss:  -2.0853\n",
            "Total loss:  -2.0313 | PDE Loss:  -3.9628 | Function Loss:  -2.0854\n",
            "Total loss:  -2.0318 | PDE Loss:  -3.9654 | Function Loss:  -2.0856\n",
            "Total loss:  -2.0322 | PDE Loss:  -3.9689 | Function Loss:  -2.0856\n",
            "Total loss:  -2.0328 | PDE Loss:  -3.9679 | Function Loss:  -2.0864\n",
            "Total loss:  -2.0333 | PDE Loss:  -3.9696 | Function Loss:  -2.0868\n",
            "Total loss:  -2.034 | PDE Loss:  -3.9747 | Function Loss:  -2.0869\n",
            "Total loss:  -2.0347 | PDE Loss:  -3.9751 | Function Loss:  -2.0876\n",
            "Total loss:  -2.0353 | PDE Loss:  -3.9782 | Function Loss:  -2.0879\n",
            "Total loss:  -2.036 | PDE Loss:  -3.9782 | Function Loss:  -2.0886\n",
            "Total loss:  -2.0367 | PDE Loss:  -3.9803 | Function Loss:  -2.0893\n",
            "Total loss:  -2.0376 | PDE Loss:  -3.982 | Function Loss:  -2.09\n",
            "Total loss:  -2.0387 | PDE Loss:  -3.9841 | Function Loss:  -2.091\n",
            "Total loss:  -2.0399 | PDE Loss:  -3.9896 | Function Loss:  -2.0916\n",
            "Total loss:  -2.0408 | PDE Loss:  -3.9889 | Function Loss:  -2.0927\n",
            "Total loss:  -2.0416 | PDE Loss:  -3.9905 | Function Loss:  -2.0934\n",
            "Total loss:  -2.0422 | PDE Loss:  -3.9894 | Function Loss:  -2.0943\n",
            "Total loss:  -2.0429 | PDE Loss:  -3.9896 | Function Loss:  -2.095\n",
            "Total loss:  -2.0436 | PDE Loss:  -3.99 | Function Loss:  -2.0958\n",
            "Total loss:  -2.0443 | PDE Loss:  -3.9899 | Function Loss:  -2.0965\n",
            "Total loss:  -2.0449 | PDE Loss:  -3.9893 | Function Loss:  -2.0973\n",
            "Total loss:  -2.0456 | PDE Loss:  -3.9907 | Function Loss:  -2.0979\n",
            "Total loss:  -2.0462 | PDE Loss:  -3.9904 | Function Loss:  -2.0986\n",
            "Total loss:  -2.0467 | PDE Loss:  -3.9899 | Function Loss:  -2.0993\n",
            "Total loss:  -2.0475 | PDE Loss:  -3.9906 | Function Loss:  -2.1\n",
            "Total loss:  -2.0482 | PDE Loss:  -3.9887 | Function Loss:  -2.1011\n",
            "Total loss:  -2.0488 | PDE Loss:  -3.9924 | Function Loss:  -2.1014\n",
            "Total loss:  -2.0493 | PDE Loss:  -3.9924 | Function Loss:  -2.1019\n",
            "Total loss:  -2.0498 | PDE Loss:  -3.9945 | Function Loss:  -2.1021\n",
            "Total loss:  -2.0501 | PDE Loss:  -3.9959 | Function Loss:  -2.1023\n",
            "Total loss:  -2.0504 | PDE Loss:  -3.9962 | Function Loss:  -2.1027\n",
            "Total loss:  -2.0508 | PDE Loss:  -3.997 | Function Loss:  -2.103\n",
            "Total loss:  -2.0511 | PDE Loss:  -3.9965 | Function Loss:  -2.1034\n",
            "Total loss:  -2.0514 | PDE Loss:  -3.995 | Function Loss:  -2.1039\n",
            "Total loss:  -2.0517 | PDE Loss:  -3.9931 | Function Loss:  -2.1045\n",
            "Total loss:  -2.0519 | PDE Loss:  -3.9914 | Function Loss:  -2.105\n",
            "Total loss:  -2.0522 | PDE Loss:  -3.9955 | Function Loss:  -2.1047\n",
            "Total loss:  -2.0525 | PDE Loss:  -3.9936 | Function Loss:  -2.1053\n",
            "Total loss:  -2.0527 | PDE Loss:  -3.9929 | Function Loss:  -2.1056\n",
            "Total loss:  -2.0529 | PDE Loss:  -3.9918 | Function Loss:  -2.1061\n",
            "Total loss:  -2.0531 | PDE Loss:  -3.9919 | Function Loss:  -2.1063\n",
            "Total loss:  -2.0534 | PDE Loss:  -3.9932 | Function Loss:  -2.1064\n",
            "Total loss:  -2.0537 | PDE Loss:  -3.9949 | Function Loss:  -2.1065\n",
            "Total loss:  -2.054 | PDE Loss:  -3.9967 | Function Loss:  -2.1066\n",
            "Total loss:  -2.0543 | PDE Loss:  -3.9978 | Function Loss:  -2.1068\n",
            "Total loss:  -2.0546 | PDE Loss:  -4.0007 | Function Loss:  -2.1068\n",
            "Total loss:  -2.0549 | PDE Loss:  -4.0023 | Function Loss:  -2.1069\n",
            "Total loss:  -2.0552 | PDE Loss:  -4.0052 | Function Loss:  -2.1069\n",
            "Total loss:  -2.0554 | PDE Loss:  -4.0063 | Function Loss:  -2.107\n",
            "Total loss:  -2.0557 | PDE Loss:  -4.0076 | Function Loss:  -2.1071\n",
            "Total loss:  -2.056 | PDE Loss:  -4.0089 | Function Loss:  -2.1073\n",
            "Total loss:  -2.0563 | PDE Loss:  -4.0113 | Function Loss:  -2.1074\n",
            "Total loss:  -2.0566 | PDE Loss:  -4.0131 | Function Loss:  -2.1075\n",
            "Total loss:  -2.0569 | PDE Loss:  -4.0143 | Function Loss:  -2.1077\n",
            "Total loss:  -2.0572 | PDE Loss:  -4.0159 | Function Loss:  -2.1078\n",
            "Total loss:  -2.0575 | PDE Loss:  -4.0153 | Function Loss:  -2.1082\n",
            "Total loss:  -2.0578 | PDE Loss:  -4.016 | Function Loss:  -2.1084\n",
            "Total loss:  -2.058 | PDE Loss:  -4.0148 | Function Loss:  -2.1089\n",
            "Total loss:  -2.0583 | PDE Loss:  -4.0134 | Function Loss:  -2.1093\n",
            "Total loss:  -2.0586 | PDE Loss:  -4.015 | Function Loss:  -2.1094\n",
            "Total loss:  -2.0589 | PDE Loss:  -4.0153 | Function Loss:  -2.1098\n",
            "Total loss:  -2.0591 | PDE Loss:  -4.0144 | Function Loss:  -2.1102\n",
            "Total loss:  -2.0594 | PDE Loss:  -4.0149 | Function Loss:  -2.1103\n",
            "Total loss:  -2.0595 | PDE Loss:  -4.015 | Function Loss:  -2.1105\n",
            "Total loss:  -2.0596 | PDE Loss:  -4.0155 | Function Loss:  -2.1106\n",
            "Total loss:  -2.0598 | PDE Loss:  -4.0158 | Function Loss:  -2.1108\n",
            "Total loss:  -2.0601 | PDE Loss:  -4.0154 | Function Loss:  -2.1111\n",
            "Total loss:  -2.0605 | PDE Loss:  -4.0147 | Function Loss:  -2.1116\n",
            "Total loss:  -2.0609 | PDE Loss:  -4.0131 | Function Loss:  -2.1123\n",
            "Total loss:  -2.0613 | PDE Loss:  -4.0129 | Function Loss:  -2.1128\n",
            "Total loss:  -2.0619 | PDE Loss:  -4.0094 | Function Loss:  -2.1139\n",
            "Total loss:  -2.0626 | PDE Loss:  -4.0097 | Function Loss:  -2.1147\n",
            "Total loss:  -2.0632 | PDE Loss:  -4.0069 | Function Loss:  -2.1157\n",
            "Total loss:  -2.0638 | PDE Loss:  -4.0076 | Function Loss:  -2.1162\n",
            "Total loss:  -2.0644 | PDE Loss:  -4.0052 | Function Loss:  -2.1172\n",
            "Total loss:  -2.0649 | PDE Loss:  -4.0055 | Function Loss:  -2.1178\n",
            "Total loss:  -2.0655 | PDE Loss:  -4.0035 | Function Loss:  -2.1188\n",
            "Total loss:  -2.0663 | PDE Loss:  -4.0051 | Function Loss:  -2.1194\n",
            "Total loss:  -2.0672 | PDE Loss:  -4.0034 | Function Loss:  -2.1206\n",
            "Total loss:  -2.0681 | PDE Loss:  -4.0035 | Function Loss:  -2.1217\n",
            "Total loss:  -2.0692 | PDE Loss:  -4.001 | Function Loss:  -2.1232\n",
            "Total loss:  -2.0702 | PDE Loss:  -3.9991 | Function Loss:  -2.1246\n",
            "Total loss:  -2.0714 | PDE Loss:  -3.9952 | Function Loss:  -2.1265\n",
            "Total loss:  -2.0725 | PDE Loss:  -3.9884 | Function Loss:  -2.1287\n",
            "Total loss:  -2.0736 | PDE Loss:  -3.9834 | Function Loss:  -2.1306\n",
            "Total loss:  -2.0743 | PDE Loss:  -3.9789 | Function Loss:  -2.1321\n",
            "Total loss:  -2.0749 | PDE Loss:  -3.9764 | Function Loss:  -2.1332\n",
            "Total loss:  -2.0754 | PDE Loss:  -3.9725 | Function Loss:  -2.1343\n",
            "Total loss:  -2.0758 | PDE Loss:  -3.9734 | Function Loss:  -2.1346\n",
            "Total loss:  -2.0761 | PDE Loss:  -3.971 | Function Loss:  -2.1353\n",
            "Total loss:  -2.0766 | PDE Loss:  -3.9743 | Function Loss:  -2.1353\n",
            "Total loss:  -2.077 | PDE Loss:  -3.9735 | Function Loss:  -2.136\n",
            "Total loss:  -2.0776 | PDE Loss:  -3.9763 | Function Loss:  -2.1362\n",
            "Total loss:  -2.0782 | PDE Loss:  -3.9813 | Function Loss:  -2.1362\n",
            "Total loss:  -2.079 | PDE Loss:  -3.9855 | Function Loss:  -2.1365\n",
            "Total loss:  -2.0797 | PDE Loss:  -3.9922 | Function Loss:  -2.1364\n",
            "Total loss:  -2.0804 | PDE Loss:  -3.9962 | Function Loss:  -2.1366\n",
            "Total loss:  -2.0809 | PDE Loss:  -3.996 | Function Loss:  -2.1372\n",
            "Total loss:  -2.0814 | PDE Loss:  -3.9994 | Function Loss:  -2.1373\n",
            "Total loss:  -2.0818 | PDE Loss:  -3.9996 | Function Loss:  -2.1377\n",
            "Total loss:  -2.0822 | PDE Loss:  -4.0021 | Function Loss:  -2.1379\n",
            "Total loss:  -2.0828 | PDE Loss:  -4.0005 | Function Loss:  -2.1388\n",
            "Total loss:  -2.0835 | PDE Loss:  -4.0042 | Function Loss:  -2.139\n",
            "Total loss:  -2.0841 | PDE Loss:  -3.9978 | Function Loss:  -2.1406\n",
            "Total loss:  -2.0847 | PDE Loss:  -4.0022 | Function Loss:  -2.1407\n",
            "Total loss:  -2.0852 | PDE Loss:  -4.0028 | Function Loss:  -2.1412\n",
            "Total loss:  -2.0858 | PDE Loss:  -4.0058 | Function Loss:  -2.1414\n",
            "Total loss:  -2.0864 | PDE Loss:  -4.0098 | Function Loss:  -2.1416\n",
            "Total loss:  -2.0872 | PDE Loss:  -4.0151 | Function Loss:  -2.1417\n",
            "Total loss:  -2.0878 | PDE Loss:  -4.022 | Function Loss:  -2.1416\n",
            "Total loss:  -2.0884 | PDE Loss:  -4.0242 | Function Loss:  -2.1419\n",
            "Total loss:  -2.089 | PDE Loss:  -4.0307 | Function Loss:  -2.1417\n",
            "Total loss:  -2.0894 | PDE Loss:  -4.0278 | Function Loss:  -2.1426\n",
            "Total loss:  -2.0897 | PDE Loss:  -4.0301 | Function Loss:  -2.1426\n",
            "Total loss:  -2.09 | PDE Loss:  -4.0296 | Function Loss:  -2.143\n",
            "Total loss:  -2.0904 | PDE Loss:  -4.0308 | Function Loss:  -2.1433\n",
            "Total loss:  -2.0908 | PDE Loss:  -4.0313 | Function Loss:  -2.1437\n",
            "Total loss:  -2.0911 | PDE Loss:  -4.035 | Function Loss:  -2.1436\n",
            "Total loss:  -2.0914 | PDE Loss:  -4.035 | Function Loss:  -2.1439\n",
            "Total loss:  -2.0918 | PDE Loss:  -4.0381 | Function Loss:  -2.144\n",
            "Total loss:  -2.0922 | PDE Loss:  -4.0402 | Function Loss:  -2.1442\n",
            "Total loss:  -2.0926 | PDE Loss:  -4.0457 | Function Loss:  -2.1439\n",
            "Total loss:  -2.093 | PDE Loss:  -4.0474 | Function Loss:  -2.1441\n",
            "Total loss:  -2.0934 | PDE Loss:  -4.0518 | Function Loss:  -2.144\n",
            "Total loss:  -2.0937 | PDE Loss:  -4.0533 | Function Loss:  -2.1442\n",
            "Total loss:  -2.0939 | PDE Loss:  -4.0556 | Function Loss:  -2.1442\n",
            "Total loss:  -2.0941 | PDE Loss:  -4.0559 | Function Loss:  -2.1444\n",
            "Total loss:  -2.0943 | PDE Loss:  -4.0577 | Function Loss:  -2.1443\n",
            "Total loss:  -2.0944 | PDE Loss:  -4.0567 | Function Loss:  -2.1446\n",
            "Total loss:  -2.0946 | PDE Loss:  -4.0573 | Function Loss:  -2.1447\n",
            "Total loss:  -2.0948 | PDE Loss:  -4.0564 | Function Loss:  -2.145\n",
            "Total loss:  -2.095 | PDE Loss:  -4.0559 | Function Loss:  -2.1454\n",
            "Total loss:  -2.0953 | PDE Loss:  -4.0549 | Function Loss:  -2.1458\n",
            "Total loss:  -2.0954 | PDE Loss:  -4.0593 | Function Loss:  -2.1453\n",
            "Total loss:  -2.0958 | PDE Loss:  -4.0593 | Function Loss:  -2.1458\n",
            "Total loss:  -2.0961 | PDE Loss:  -4.0584 | Function Loss:  -2.1462\n",
            "Total loss:  -2.0963 | PDE Loss:  -4.0592 | Function Loss:  -2.1464\n",
            "Total loss:  -2.0967 | PDE Loss:  -4.0585 | Function Loss:  -2.1469\n",
            "Total loss:  -2.0971 | PDE Loss:  -4.0603 | Function Loss:  -2.1471\n",
            "Total loss:  -2.0975 | PDE Loss:  -4.0595 | Function Loss:  -2.1477\n",
            "Total loss:  -2.098 | PDE Loss:  -4.0586 | Function Loss:  -2.1483\n",
            "Total loss:  -2.0984 | PDE Loss:  -4.0562 | Function Loss:  -2.1491\n",
            "Total loss:  -2.0988 | PDE Loss:  -4.0536 | Function Loss:  -2.1499\n",
            "Total loss:  -2.0991 | PDE Loss:  -4.0515 | Function Loss:  -2.1505\n",
            "Total loss:  -2.0993 | PDE Loss:  -4.0504 | Function Loss:  -2.1509\n",
            "Total loss:  -2.0995 | PDE Loss:  -4.049 | Function Loss:  -2.1513\n",
            "Total loss:  -2.0997 | PDE Loss:  -4.0495 | Function Loss:  -2.1514\n",
            "Total loss:  -2.0998 | PDE Loss:  -4.0493 | Function Loss:  -2.1515\n",
            "Total loss:  -2.0999 | PDE Loss:  -4.0513 | Function Loss:  -2.1515\n",
            "Total loss:  -2.1001 | PDE Loss:  -4.0506 | Function Loss:  -2.1518\n",
            "Total loss:  -2.1003 | PDE Loss:  -4.0539 | Function Loss:  -2.1515\n",
            "Total loss:  -2.1004 | PDE Loss:  -4.054 | Function Loss:  -2.1517\n",
            "Total loss:  -2.1006 | PDE Loss:  -4.0545 | Function Loss:  -2.1518\n",
            "Total loss:  -2.1007 | PDE Loss:  -4.0554 | Function Loss:  -2.1518\n",
            "Total loss:  -2.1009 | PDE Loss:  -4.0568 | Function Loss:  -2.1519\n",
            "Total loss:  -2.1012 | PDE Loss:  -4.0577 | Function Loss:  -2.152\n",
            "Total loss:  -2.1014 | PDE Loss:  -4.0594 | Function Loss:  -2.1521\n",
            "Total loss:  -2.1016 | PDE Loss:  -4.0606 | Function Loss:  -2.1522\n",
            "Total loss:  -2.1019 | PDE Loss:  -4.0634 | Function Loss:  -2.1521\n",
            "Total loss:  -2.1021 | PDE Loss:  -4.0631 | Function Loss:  -2.1524\n",
            "Total loss:  -2.1022 | PDE Loss:  -4.0656 | Function Loss:  -2.1523\n",
            "Total loss:  -2.1024 | PDE Loss:  -4.0677 | Function Loss:  -2.1522\n",
            "Total loss:  -2.1026 | PDE Loss:  -4.0697 | Function Loss:  -2.1522\n",
            "Total loss:  -2.1028 | PDE Loss:  -4.0716 | Function Loss:  -2.1522\n",
            "Total loss:  -2.1031 | PDE Loss:  -4.0731 | Function Loss:  -2.1523\n",
            "Total loss:  -2.1032 | PDE Loss:  -4.0743 | Function Loss:  -2.1523\n",
            "Total loss:  -2.1034 | PDE Loss:  -4.0747 | Function Loss:  -2.1525\n",
            "Total loss:  -2.1036 | PDE Loss:  -4.075 | Function Loss:  -2.1527\n",
            "Total loss:  -2.1038 | PDE Loss:  -4.0749 | Function Loss:  -2.1529\n",
            "Total loss:  -2.104 | PDE Loss:  -4.074 | Function Loss:  -2.1532\n",
            "Total loss:  -2.1042 | PDE Loss:  -4.0744 | Function Loss:  -2.1534\n",
            "Total loss:  -2.1044 | PDE Loss:  -4.0715 | Function Loss:  -2.154\n",
            "Total loss:  -2.1047 | PDE Loss:  -4.0725 | Function Loss:  -2.1542\n",
            "Total loss:  -2.1052 | PDE Loss:  -4.0745 | Function Loss:  -2.1545\n",
            "Total loss:  -2.1058 | PDE Loss:  -4.0775 | Function Loss:  -2.1548\n",
            "Total loss:  -2.1064 | PDE Loss:  -4.0807 | Function Loss:  -2.1551\n",
            "Total loss:  -2.1071 | PDE Loss:  -4.0844 | Function Loss:  -2.1554\n",
            "Total loss:  -2.1079 | PDE Loss:  -4.0893 | Function Loss:  -2.1557\n",
            "Total loss:  -2.1087 | PDE Loss:  -4.0929 | Function Loss:  -2.1562\n",
            "Total loss:  -2.1094 | PDE Loss:  -4.0956 | Function Loss:  -2.1567\n",
            "Total loss:  -2.1101 | PDE Loss:  -4.0986 | Function Loss:  -2.1572\n",
            "Total loss:  -2.1108 | PDE Loss:  -4.0951 | Function Loss:  -2.1584\n",
            "Total loss:  -2.1114 | PDE Loss:  -4.1006 | Function Loss:  -2.1583\n",
            "Total loss:  -2.1117 | PDE Loss:  -4.0976 | Function Loss:  -2.159\n",
            "Total loss:  -2.1119 | PDE Loss:  -4.096 | Function Loss:  -2.1594\n",
            "Total loss:  -2.1121 | PDE Loss:  -4.0961 | Function Loss:  -2.1597\n",
            "Total loss:  -2.1124 | PDE Loss:  -4.0963 | Function Loss:  -2.16\n",
            "Total loss:  -2.1128 | PDE Loss:  -4.0974 | Function Loss:  -2.1603\n",
            "Total loss:  -2.1133 | PDE Loss:  -4.0966 | Function Loss:  -2.1609\n",
            "Total loss:  -2.1136 | PDE Loss:  -4.1044 | Function Loss:  -2.1604\n",
            "Total loss:  -2.114 | PDE Loss:  -4.1017 | Function Loss:  -2.1611\n",
            "Total loss:  -2.1143 | PDE Loss:  -4.1013 | Function Loss:  -2.1615\n",
            "Total loss:  -2.1148 | PDE Loss:  -4.1009 | Function Loss:  -2.1621\n",
            "Total loss:  -2.1153 | PDE Loss:  -4.1049 | Function Loss:  -2.1622\n",
            "Total loss:  -2.1157 | PDE Loss:  -4.1053 | Function Loss:  -2.1626\n",
            "Total loss:  -2.116 | PDE Loss:  -4.1069 | Function Loss:  -2.1627\n",
            "Total loss:  -2.1163 | PDE Loss:  -4.1086 | Function Loss:  -2.1629\n",
            "Total loss:  -2.1165 | PDE Loss:  -4.1094 | Function Loss:  -2.1631\n",
            "Total loss:  -2.1168 | PDE Loss:  -4.1102 | Function Loss:  -2.1632\n",
            "Total loss:  -2.117 | PDE Loss:  -4.1105 | Function Loss:  -2.1635\n",
            "Total loss:  -2.1173 | PDE Loss:  -4.1098 | Function Loss:  -2.1639\n",
            "Total loss:  -2.1176 | PDE Loss:  -4.1133 | Function Loss:  -2.1639\n",
            "Total loss:  -2.118 | PDE Loss:  -4.1107 | Function Loss:  -2.1645\n",
            "Total loss:  -2.1184 | PDE Loss:  -4.1099 | Function Loss:  -2.1651\n",
            "Total loss:  -2.1189 | PDE Loss:  -4.1064 | Function Loss:  -2.166\n",
            "Total loss:  -2.1193 | PDE Loss:  -4.1063 | Function Loss:  -2.1665\n",
            "Total loss:  -2.1196 | PDE Loss:  -4.105 | Function Loss:  -2.167\n",
            "Total loss:  -2.1198 | PDE Loss:  -4.1051 | Function Loss:  -2.1672\n",
            "Total loss:  -2.12 | PDE Loss:  -4.1055 | Function Loss:  -2.1674\n",
            "Total loss:  -2.1203 | PDE Loss:  -4.1057 | Function Loss:  -2.1677\n",
            "Total loss:  -2.1205 | PDE Loss:  -4.1068 | Function Loss:  -2.1678\n",
            "Total loss:  -2.1206 | PDE Loss:  -4.1075 | Function Loss:  -2.1679\n",
            "Total loss:  -2.1209 | PDE Loss:  -4.1084 | Function Loss:  -2.1681\n",
            "Total loss:  -2.1212 | PDE Loss:  -4.1081 | Function Loss:  -2.1685\n",
            "Total loss:  -2.1215 | PDE Loss:  -4.109 | Function Loss:  -2.1687\n",
            "Total loss:  -2.1218 | PDE Loss:  -4.1086 | Function Loss:  -2.1691\n",
            "Total loss:  -2.1221 | PDE Loss:  -4.1103 | Function Loss:  -2.1692\n",
            "Total loss:  -2.1223 | PDE Loss:  -4.1094 | Function Loss:  -2.1696\n",
            "Total loss:  -2.1225 | PDE Loss:  -4.1102 | Function Loss:  -2.1697\n",
            "Total loss:  -2.1227 | PDE Loss:  -4.1097 | Function Loss:  -2.1699\n",
            "Total loss:  -2.1228 | PDE Loss:  -4.1102 | Function Loss:  -2.17\n",
            "Total loss:  -2.123 | PDE Loss:  -4.11 | Function Loss:  -2.1702\n",
            "Total loss:  -2.1232 | PDE Loss:  -4.1109 | Function Loss:  -2.1703\n",
            "Total loss:  -2.1233 | PDE Loss:  -4.111 | Function Loss:  -2.1705\n",
            "Total loss:  -2.1235 | PDE Loss:  -4.1127 | Function Loss:  -2.1705\n",
            "Total loss:  -2.1237 | PDE Loss:  -4.1126 | Function Loss:  -2.1708\n",
            "Total loss:  -2.1239 | PDE Loss:  -4.1137 | Function Loss:  -2.1708\n",
            "Total loss:  -2.1242 | PDE Loss:  -4.1139 | Function Loss:  -2.1711\n",
            "Total loss:  -2.1244 | PDE Loss:  -4.1153 | Function Loss:  -2.1712\n",
            "Total loss:  -2.1247 | PDE Loss:  -4.1149 | Function Loss:  -2.1716\n",
            "Total loss:  -2.125 | PDE Loss:  -4.1169 | Function Loss:  -2.1717\n",
            "Total loss:  -2.1252 | PDE Loss:  -4.1157 | Function Loss:  -2.1721\n",
            "Total loss:  -2.1255 | PDE Loss:  -4.1168 | Function Loss:  -2.1722\n",
            "Total loss:  -2.1257 | PDE Loss:  -4.1171 | Function Loss:  -2.1724\n",
            "Total loss:  -2.126 | PDE Loss:  -4.1182 | Function Loss:  -2.1727\n",
            "Total loss:  -2.1264 | PDE Loss:  -4.1202 | Function Loss:  -2.1729\n",
            "Total loss:  -2.1269 | PDE Loss:  -4.1245 | Function Loss:  -2.1729\n",
            "Total loss:  -2.1274 | PDE Loss:  -4.13 | Function Loss:  -2.1729\n",
            "Total loss:  -2.1279 | PDE Loss:  -4.1338 | Function Loss:  -2.173\n",
            "Total loss:  -2.1285 | PDE Loss:  -4.1376 | Function Loss:  -2.1732\n",
            "Total loss:  -2.129 | PDE Loss:  -4.1421 | Function Loss:  -2.1733\n",
            "Total loss:  -2.1296 | PDE Loss:  -4.1445 | Function Loss:  -2.1737\n",
            "Total loss:  -2.1301 | PDE Loss:  -4.146 | Function Loss:  -2.1741\n",
            "Total loss:  -2.1306 | PDE Loss:  -4.1461 | Function Loss:  -2.1747\n",
            "Total loss:  -2.1311 | PDE Loss:  -4.1442 | Function Loss:  -2.1754\n",
            "Total loss:  -2.1316 | PDE Loss:  -4.1444 | Function Loss:  -2.176\n",
            "Total loss:  -2.1321 | PDE Loss:  -4.1414 | Function Loss:  -2.1768\n",
            "Total loss:  -2.1324 | PDE Loss:  -4.142 | Function Loss:  -2.1772\n",
            "Total loss:  -2.1328 | PDE Loss:  -4.1424 | Function Loss:  -2.1775\n",
            "Total loss:  -2.1332 | PDE Loss:  -4.1427 | Function Loss:  -2.1779\n",
            "Total loss:  -2.1336 | PDE Loss:  -4.1456 | Function Loss:  -2.178\n",
            "Total loss:  -2.134 | PDE Loss:  -4.1466 | Function Loss:  -2.1784\n",
            "Total loss:  -2.1345 | PDE Loss:  -4.15 | Function Loss:  -2.1786\n",
            "Total loss:  -2.135 | PDE Loss:  -4.1529 | Function Loss:  -2.1788\n",
            "Total loss:  -2.1354 | PDE Loss:  -4.1524 | Function Loss:  -2.1793\n",
            "Total loss:  -2.1358 | PDE Loss:  -4.1559 | Function Loss:  -2.1793\n",
            "Total loss:  -2.136 | PDE Loss:  -4.1553 | Function Loss:  -2.1797\n",
            "Total loss:  -2.1362 | PDE Loss:  -4.1543 | Function Loss:  -2.18\n",
            "Total loss:  -2.1364 | PDE Loss:  -4.1518 | Function Loss:  -2.1805\n",
            "Total loss:  -2.1366 | PDE Loss:  -4.1495 | Function Loss:  -2.181\n",
            "Total loss:  -2.1368 | PDE Loss:  -4.1471 | Function Loss:  -2.1814\n",
            "Total loss:  -2.137 | PDE Loss:  -4.145 | Function Loss:  -2.1818\n",
            "Total loss:  -2.1372 | PDE Loss:  -4.1431 | Function Loss:  -2.1823\n",
            "Total loss:  -2.1374 | PDE Loss:  -4.1424 | Function Loss:  -2.1826\n",
            "Total loss:  -2.1377 | PDE Loss:  -4.1412 | Function Loss:  -2.183\n",
            "Total loss:  -2.1379 | PDE Loss:  -4.1462 | Function Loss:  -2.1827\n",
            "Total loss:  -2.1382 | PDE Loss:  -4.1449 | Function Loss:  -2.1832\n",
            "Total loss:  -2.1384 | PDE Loss:  -4.1448 | Function Loss:  -2.1834\n",
            "Total loss:  -2.1386 | PDE Loss:  -4.1462 | Function Loss:  -2.1835\n",
            "Total loss:  -2.1388 | PDE Loss:  -4.1476 | Function Loss:  -2.1836\n",
            "Total loss:  -2.139 | PDE Loss:  -4.15 | Function Loss:  -2.1836\n",
            "Total loss:  -2.1392 | PDE Loss:  -4.151 | Function Loss:  -2.1836\n",
            "Total loss:  -2.1394 | PDE Loss:  -4.1543 | Function Loss:  -2.1835\n",
            "Total loss:  -2.1396 | PDE Loss:  -4.1548 | Function Loss:  -2.1837\n",
            "Total loss:  -2.1398 | PDE Loss:  -4.1568 | Function Loss:  -2.1837\n",
            "Total loss:  -2.14 | PDE Loss:  -4.1569 | Function Loss:  -2.1839\n",
            "Total loss:  -2.1403 | PDE Loss:  -4.1576 | Function Loss:  -2.1842\n",
            "Total loss:  -2.1407 | PDE Loss:  -4.1604 | Function Loss:  -2.1844\n",
            "Total loss:  -2.1413 | PDE Loss:  -4.1617 | Function Loss:  -2.1848\n",
            "Total loss:  -2.1419 | PDE Loss:  -4.1639 | Function Loss:  -2.1852\n",
            "Total loss:  -2.1424 | PDE Loss:  -4.1638 | Function Loss:  -2.1859\n",
            "Total loss:  -2.1429 | PDE Loss:  -4.1633 | Function Loss:  -2.1865\n",
            "Total loss:  -2.1433 | PDE Loss:  -4.1644 | Function Loss:  -2.1867\n",
            "Total loss:  -2.1436 | PDE Loss:  -4.1668 | Function Loss:  -2.1868\n",
            "Total loss:  -2.1439 | PDE Loss:  -4.1661 | Function Loss:  -2.1873\n",
            "Total loss:  -2.1443 | PDE Loss:  -4.173 | Function Loss:  -2.187\n",
            "Total loss:  -2.1445 | PDE Loss:  -4.1731 | Function Loss:  -2.1872\n",
            "Total loss:  -2.1448 | PDE Loss:  -4.1739 | Function Loss:  -2.1874\n",
            "Total loss:  -2.1453 | PDE Loss:  -4.1744 | Function Loss:  -2.1879\n",
            "Total loss:  -2.1458 | PDE Loss:  -4.1737 | Function Loss:  -2.1886\n",
            "Total loss:  -2.1463 | PDE Loss:  -4.1749 | Function Loss:  -2.189\n",
            "Total loss:  -2.1467 | PDE Loss:  -4.1687 | Function Loss:  -2.1901\n",
            "Total loss:  -2.1471 | PDE Loss:  -4.1713 | Function Loss:  -2.1902\n",
            "Total loss:  -2.1474 | PDE Loss:  -4.1738 | Function Loss:  -2.1903\n",
            "Total loss:  -2.1477 | PDE Loss:  -4.1754 | Function Loss:  -2.1905\n",
            "Total loss:  -2.1481 | PDE Loss:  -4.1777 | Function Loss:  -2.1906\n",
            "Total loss:  -2.1484 | PDE Loss:  -4.1785 | Function Loss:  -2.191\n",
            "Total loss:  -2.1488 | PDE Loss:  -4.1812 | Function Loss:  -2.1911\n",
            "Total loss:  -2.1493 | PDE Loss:  -4.1825 | Function Loss:  -2.1915\n",
            "Total loss:  -2.1498 | PDE Loss:  -4.1863 | Function Loss:  -2.1917\n",
            "Total loss:  -2.1503 | PDE Loss:  -4.1871 | Function Loss:  -2.1921\n",
            "Total loss:  -2.1507 | PDE Loss:  -4.1907 | Function Loss:  -2.1922\n",
            "Total loss:  -2.151 | PDE Loss:  -4.1912 | Function Loss:  -2.1925\n",
            "Total loss:  -2.1514 | PDE Loss:  -4.1919 | Function Loss:  -2.1928\n",
            "Total loss:  -2.1518 | PDE Loss:  -4.1938 | Function Loss:  -2.1931\n",
            "Total loss:  -2.1522 | PDE Loss:  -4.1926 | Function Loss:  -2.1937\n",
            "Total loss:  -2.1527 | PDE Loss:  -4.1945 | Function Loss:  -2.194\n",
            "Total loss:  -2.153 | PDE Loss:  -4.1946 | Function Loss:  -2.1944\n",
            "Total loss:  -2.1534 | PDE Loss:  -4.1941 | Function Loss:  -2.1948\n",
            "Total loss:  -2.1538 | PDE Loss:  -4.1931 | Function Loss:  -2.1954\n",
            "Total loss:  -2.1541 | PDE Loss:  -4.1903 | Function Loss:  -2.196\n",
            "Total loss:  -2.1544 | PDE Loss:  -4.1912 | Function Loss:  -2.1963\n",
            "Total loss:  -2.1548 | PDE Loss:  -4.1874 | Function Loss:  -2.1971\n",
            "Total loss:  -2.1552 | PDE Loss:  -4.1881 | Function Loss:  -2.1974\n",
            "Total loss:  -2.1556 | PDE Loss:  -4.1857 | Function Loss:  -2.1981\n",
            "Total loss:  -2.156 | PDE Loss:  -4.1865 | Function Loss:  -2.1985\n",
            "Total loss:  -2.1564 | PDE Loss:  -4.1844 | Function Loss:  -2.1992\n",
            "Total loss:  -2.1568 | PDE Loss:  -4.1862 | Function Loss:  -2.1994\n",
            "Total loss:  -2.1572 | PDE Loss:  -4.1858 | Function Loss:  -2.1999\n",
            "Total loss:  -2.1576 | PDE Loss:  -4.1885 | Function Loss:  -2.2\n",
            "Total loss:  -2.1581 | PDE Loss:  -4.189 | Function Loss:  -2.2005\n",
            "Total loss:  -2.1586 | PDE Loss:  -4.1916 | Function Loss:  -2.2008\n",
            "Total loss:  -2.1592 | PDE Loss:  -4.1926 | Function Loss:  -2.2014\n",
            "Total loss:  -2.1597 | PDE Loss:  -4.195 | Function Loss:  -2.2017\n",
            "Total loss:  -2.1602 | PDE Loss:  -4.1964 | Function Loss:  -2.2022\n",
            "Total loss:  -2.1607 | PDE Loss:  -4.1968 | Function Loss:  -2.2026\n",
            "Total loss:  -2.1612 | PDE Loss:  -4.1975 | Function Loss:  -2.2031\n",
            "Total loss:  -2.1618 | PDE Loss:  -4.1969 | Function Loss:  -2.2039\n",
            "Total loss:  -2.1625 | PDE Loss:  -4.1929 | Function Loss:  -2.205\n",
            "Total loss:  -2.163 | PDE Loss:  -4.1924 | Function Loss:  -2.2056\n",
            "Total loss:  -2.1636 | PDE Loss:  -4.1888 | Function Loss:  -2.2066\n",
            "Total loss:  -2.1641 | PDE Loss:  -4.1877 | Function Loss:  -2.2073\n",
            "Total loss:  -2.1647 | PDE Loss:  -4.1845 | Function Loss:  -2.2084\n",
            "Total loss:  -2.1654 | PDE Loss:  -4.1801 | Function Loss:  -2.2095\n",
            "Total loss:  -2.1659 | PDE Loss:  -4.1796 | Function Loss:  -2.2102\n",
            "Total loss:  -2.1664 | PDE Loss:  -4.18 | Function Loss:  -2.2107\n",
            "Total loss:  -2.167 | PDE Loss:  -4.1774 | Function Loss:  -2.2116\n",
            "Total loss:  -2.1675 | PDE Loss:  -4.1816 | Function Loss:  -2.2117\n",
            "Total loss:  -2.1678 | PDE Loss:  -4.1847 | Function Loss:  -2.2118\n",
            "Total loss:  -2.1681 | PDE Loss:  -4.1826 | Function Loss:  -2.2123\n",
            "Total loss:  -2.1685 | PDE Loss:  -4.1839 | Function Loss:  -2.2126\n",
            "Total loss:  -2.1689 | PDE Loss:  -4.1788 | Function Loss:  -2.2136\n",
            "Total loss:  -2.1694 | PDE Loss:  -4.1765 | Function Loss:  -2.2144\n",
            "Total loss:  -2.17 | PDE Loss:  -4.1637 | Function Loss:  -2.2164\n",
            "Total loss:  -2.1703 | PDE Loss:  -4.1567 | Function Loss:  -2.2176\n",
            "Total loss:  -2.1707 | PDE Loss:  -4.1504 | Function Loss:  -2.2188\n",
            "Total loss:  -2.1711 | PDE Loss:  -4.146 | Function Loss:  -2.2197\n",
            "Total loss:  -2.1715 | PDE Loss:  -4.1419 | Function Loss:  -2.2206\n",
            "Total loss:  -2.1719 | PDE Loss:  -4.1387 | Function Loss:  -2.2215\n",
            "Total loss:  -2.1724 | PDE Loss:  -4.135 | Function Loss:  -2.2225\n",
            "Total loss:  -2.1728 | PDE Loss:  -4.1273 | Function Loss:  -2.224\n",
            "Total loss:  -2.1733 | PDE Loss:  -4.1305 | Function Loss:  -2.224\n",
            "Total loss:  -2.1736 | PDE Loss:  -4.1335 | Function Loss:  -2.2241\n",
            "Total loss:  -2.174 | PDE Loss:  -4.1359 | Function Loss:  -2.2242\n",
            "Total loss:  -2.1742 | PDE Loss:  -4.1385 | Function Loss:  -2.2241\n",
            "Total loss:  -2.1744 | PDE Loss:  -4.139 | Function Loss:  -2.2243\n",
            "Total loss:  -2.1745 | PDE Loss:  -4.1392 | Function Loss:  -2.2244\n",
            "Total loss:  -2.1747 | PDE Loss:  -4.1393 | Function Loss:  -2.2245\n",
            "Total loss:  -2.1749 | PDE Loss:  -4.138 | Function Loss:  -2.2249\n",
            "Total loss:  -2.1751 | PDE Loss:  -4.1374 | Function Loss:  -2.2253\n",
            "Total loss:  -2.1754 | PDE Loss:  -4.1384 | Function Loss:  -2.2254\n",
            "Total loss:  -2.1757 | PDE Loss:  -4.1375 | Function Loss:  -2.2259\n",
            "Total loss:  -2.1759 | PDE Loss:  -4.1398 | Function Loss:  -2.2259\n",
            "Total loss:  -2.1762 | PDE Loss:  -4.1421 | Function Loss:  -2.2259\n",
            "Total loss:  -2.1764 | PDE Loss:  -4.146 | Function Loss:  -2.2256\n",
            "Total loss:  -2.1765 | PDE Loss:  -4.1485 | Function Loss:  -2.2255\n",
            "Total loss:  -2.1766 | PDE Loss:  -4.152 | Function Loss:  -2.2252\n",
            "Total loss:  -2.1767 | PDE Loss:  -4.1542 | Function Loss:  -2.2251\n",
            "Total loss:  -2.1769 | PDE Loss:  -4.157 | Function Loss:  -2.2249\n",
            "Total loss:  -2.1771 | PDE Loss:  -4.1605 | Function Loss:  -2.2247\n",
            "Total loss:  -2.1771 | PDE Loss:  -4.1623 | Function Loss:  -2.2245\n",
            "Total loss:  -2.1773 | PDE Loss:  -4.1656 | Function Loss:  -2.2244\n",
            "Total loss:  -2.1775 | PDE Loss:  -4.1673 | Function Loss:  -2.2245\n",
            "Total loss:  -2.1778 | PDE Loss:  -4.1697 | Function Loss:  -2.2245\n",
            "Total loss:  -2.1781 | PDE Loss:  -4.1724 | Function Loss:  -2.2245\n",
            "Total loss:  -2.1784 | PDE Loss:  -4.1758 | Function Loss:  -2.2245\n",
            "Total loss:  -2.1788 | PDE Loss:  -4.1798 | Function Loss:  -2.2244\n",
            "Total loss:  -2.1791 | PDE Loss:  -4.1839 | Function Loss:  -2.2243\n",
            "Total loss:  -2.1794 | PDE Loss:  -4.1873 | Function Loss:  -2.2243\n",
            "Total loss:  -2.1796 | PDE Loss:  -4.1892 | Function Loss:  -2.2243\n",
            "Total loss:  -2.1797 | PDE Loss:  -4.1899 | Function Loss:  -2.2244\n",
            "Total loss:  -2.1798 | PDE Loss:  -4.1898 | Function Loss:  -2.2245\n",
            "Total loss:  -2.1799 | PDE Loss:  -4.1889 | Function Loss:  -2.2247\n",
            "Total loss:  -2.18 | PDE Loss:  -4.1884 | Function Loss:  -2.2248\n",
            "Total loss:  -2.1801 | PDE Loss:  -4.1872 | Function Loss:  -2.225\n",
            "Total loss:  -2.1801 | PDE Loss:  -4.1857 | Function Loss:  -2.2253\n",
            "Total loss:  -2.1803 | PDE Loss:  -4.1855 | Function Loss:  -2.2255\n",
            "Total loss:  -2.1804 | PDE Loss:  -4.1856 | Function Loss:  -2.2256\n",
            "Total loss:  -2.1806 | PDE Loss:  -4.1868 | Function Loss:  -2.2257\n",
            "Total loss:  -2.1808 | PDE Loss:  -4.1886 | Function Loss:  -2.2257\n",
            "Total loss:  -2.181 | PDE Loss:  -4.1905 | Function Loss:  -2.2257\n",
            "Total loss:  -2.1812 | PDE Loss:  -4.1937 | Function Loss:  -2.2256\n",
            "Total loss:  -2.1814 | PDE Loss:  -4.1969 | Function Loss:  -2.2255\n",
            "Total loss:  -2.1817 | PDE Loss:  -4.2015 | Function Loss:  -2.2253\n",
            "Total loss:  -2.1821 | PDE Loss:  -4.2066 | Function Loss:  -2.2253\n",
            "Total loss:  -2.1826 | PDE Loss:  -4.2126 | Function Loss:  -2.2252\n",
            "Total loss:  -2.1831 | PDE Loss:  -4.2185 | Function Loss:  -2.2251\n",
            "Total loss:  -2.1836 | PDE Loss:  -4.2214 | Function Loss:  -2.2253\n",
            "Total loss:  -2.1841 | PDE Loss:  -4.2219 | Function Loss:  -2.2259\n",
            "Total loss:  -2.1845 | PDE Loss:  -4.2218 | Function Loss:  -2.2263\n",
            "Total loss:  -2.1849 | PDE Loss:  -4.2192 | Function Loss:  -2.227\n",
            "Total loss:  -2.1852 | PDE Loss:  -4.2147 | Function Loss:  -2.2279\n",
            "Total loss:  -2.1857 | PDE Loss:  -4.2119 | Function Loss:  -2.2286\n",
            "Total loss:  -2.1862 | PDE Loss:  -4.2028 | Function Loss:  -2.2301\n",
            "Total loss:  -2.1867 | PDE Loss:  -4.2014 | Function Loss:  -2.2308\n",
            "Total loss:  -2.1872 | PDE Loss:  -4.1925 | Function Loss:  -2.2323\n",
            "Total loss:  -2.1875 | PDE Loss:  -4.1915 | Function Loss:  -2.2329\n",
            "Total loss:  -2.1879 | PDE Loss:  -4.1913 | Function Loss:  -2.2333\n",
            "Total loss:  -2.1885 | PDE Loss:  -4.1924 | Function Loss:  -2.2338\n",
            "Total loss:  -2.1892 | PDE Loss:  -4.1929 | Function Loss:  -2.2346\n",
            "Total loss:  -2.1899 | PDE Loss:  -4.1927 | Function Loss:  -2.2354\n",
            "Total loss:  -2.1906 | PDE Loss:  -4.1913 | Function Loss:  -2.2363\n",
            "Total loss:  -2.1913 | PDE Loss:  -4.1878 | Function Loss:  -2.2375\n",
            "Total loss:  -2.192 | PDE Loss:  -4.1854 | Function Loss:  -2.2385\n",
            "Total loss:  -2.1928 | PDE Loss:  -4.1862 | Function Loss:  -2.2393\n",
            "Total loss:  -2.1935 | PDE Loss:  -4.1863 | Function Loss:  -2.2401\n",
            "Total loss:  -2.1946 | PDE Loss:  -4.187 | Function Loss:  -2.2412\n",
            "Total loss:  -2.1955 | PDE Loss:  -4.1903 | Function Loss:  -2.2418\n",
            "Total loss:  -2.1962 | PDE Loss:  -4.1905 | Function Loss:  -2.2426\n",
            "Total loss:  -2.1967 | PDE Loss:  -4.197 | Function Loss:  -2.2425\n",
            "Total loss:  -2.1971 | PDE Loss:  -4.1958 | Function Loss:  -2.243\n",
            "Total loss:  -2.1973 | PDE Loss:  -4.1987 | Function Loss:  -2.2429\n",
            "Total loss:  -2.1975 | PDE Loss:  -4.1987 | Function Loss:  -2.2432\n",
            "Total loss:  -2.1977 | PDE Loss:  -4.2002 | Function Loss:  -2.2432\n",
            "Total loss:  -2.1979 | PDE Loss:  -4.2015 | Function Loss:  -2.2433\n",
            "Total loss:  -2.1983 | PDE Loss:  -4.2011 | Function Loss:  -2.2437\n",
            "Total loss:  -2.1987 | PDE Loss:  -4.2024 | Function Loss:  -2.244\n",
            "Total loss:  -2.199 | PDE Loss:  -4.1998 | Function Loss:  -2.2447\n",
            "Total loss:  -2.1993 | PDE Loss:  -4.198 | Function Loss:  -2.2452\n",
            "Total loss:  -2.1997 | PDE Loss:  -4.1951 | Function Loss:  -2.246\n",
            "Total loss:  -2.2 | PDE Loss:  -4.1922 | Function Loss:  -2.2467\n",
            "Total loss:  -2.2005 | PDE Loss:  -4.1901 | Function Loss:  -2.2474\n",
            "Total loss:  -2.201 | PDE Loss:  -4.1861 | Function Loss:  -2.2485\n",
            "Total loss:  -2.2011 | PDE Loss:  -4.1817 | Function Loss:  -2.249\n",
            "Total loss:  -2.2014 | PDE Loss:  -4.1851 | Function Loss:  -2.249\n",
            "Total loss:  -2.2018 | PDE Loss:  -4.1839 | Function Loss:  -2.2496\n",
            "Total loss:  -2.2022 | PDE Loss:  -4.1813 | Function Loss:  -2.2503\n",
            "Total loss:  -2.2024 | PDE Loss:  -4.1816 | Function Loss:  -2.2506\n",
            "Total loss:  -2.2027 | PDE Loss:  -4.182 | Function Loss:  -2.2508\n",
            "Total loss:  -2.2029 | PDE Loss:  -4.183 | Function Loss:  -2.2509\n",
            "Total loss:  -2.2032 | PDE Loss:  -4.1847 | Function Loss:  -2.251\n",
            "Total loss:  -2.2035 | PDE Loss:  -4.187 | Function Loss:  -2.2511\n",
            "Total loss:  -2.2038 | PDE Loss:  -4.1881 | Function Loss:  -2.2513\n",
            "Total loss:  -2.2041 | PDE Loss:  -4.19 | Function Loss:  -2.2514\n",
            "Total loss:  -2.2044 | PDE Loss:  -4.1898 | Function Loss:  -2.2518\n",
            "Total loss:  -2.2047 | PDE Loss:  -4.1902 | Function Loss:  -2.2521\n",
            "Total loss:  -2.2051 | PDE Loss:  -4.1869 | Function Loss:  -2.2529\n",
            "Total loss:  -2.2054 | PDE Loss:  -4.1844 | Function Loss:  -2.2535\n",
            "Total loss:  -2.2057 | PDE Loss:  -4.1803 | Function Loss:  -2.2544\n",
            "Total loss:  -2.206 | PDE Loss:  -4.1769 | Function Loss:  -2.2551\n",
            "Total loss:  -2.2062 | PDE Loss:  -4.1746 | Function Loss:  -2.2556\n",
            "Total loss:  -2.2065 | PDE Loss:  -4.1725 | Function Loss:  -2.2562\n",
            "Total loss:  -2.2067 | PDE Loss:  -4.1706 | Function Loss:  -2.2567\n",
            "Total loss:  -2.207 | PDE Loss:  -4.1698 | Function Loss:  -2.2571\n",
            "Total loss:  -2.2072 | PDE Loss:  -4.1696 | Function Loss:  -2.2574\n",
            "Total loss:  -2.2075 | PDE Loss:  -4.1694 | Function Loss:  -2.2578\n",
            "Total loss:  -2.2079 | PDE Loss:  -4.1707 | Function Loss:  -2.258\n",
            "Total loss:  -2.2084 | PDE Loss:  -4.1691 | Function Loss:  -2.2587\n",
            "Total loss:  -2.2088 | PDE Loss:  -4.1719 | Function Loss:  -2.2589\n",
            "Total loss:  -2.2093 | PDE Loss:  -4.1724 | Function Loss:  -2.2593\n",
            "Total loss:  -2.2097 | PDE Loss:  -4.1757 | Function Loss:  -2.2594\n",
            "Total loss:  -2.2102 | PDE Loss:  -4.1769 | Function Loss:  -2.2599\n",
            "Total loss:  -2.2107 | PDE Loss:  -4.18 | Function Loss:  -2.26\n",
            "Total loss:  -2.2113 | PDE Loss:  -4.183 | Function Loss:  -2.2603\n",
            "Total loss:  -2.2119 | PDE Loss:  -4.186 | Function Loss:  -2.2606\n",
            "Total loss:  -2.2125 | PDE Loss:  -4.1882 | Function Loss:  -2.261\n",
            "Total loss:  -2.2129 | PDE Loss:  -4.1907 | Function Loss:  -2.2612\n",
            "Total loss:  -2.2133 | PDE Loss:  -4.1898 | Function Loss:  -2.2617\n",
            "Total loss:  -2.2135 | PDE Loss:  -4.1905 | Function Loss:  -2.2619\n",
            "Total loss:  -2.2137 | PDE Loss:  -4.1898 | Function Loss:  -2.2622\n",
            "Total loss:  -2.214 | PDE Loss:  -4.1885 | Function Loss:  -2.2627\n",
            "Total loss:  -2.2146 | PDE Loss:  -4.1861 | Function Loss:  -2.2636\n",
            "Total loss:  -2.2151 | PDE Loss:  -4.1833 | Function Loss:  -2.2645\n",
            "Total loss:  -2.2154 | PDE Loss:  -4.1796 | Function Loss:  -2.2654\n",
            "Total loss:  -2.2158 | PDE Loss:  -4.1781 | Function Loss:  -2.266\n",
            "Total loss:  -2.2162 | PDE Loss:  -4.1741 | Function Loss:  -2.2669\n",
            "Total loss:  -2.2165 | PDE Loss:  -4.1727 | Function Loss:  -2.2674\n",
            "Total loss:  -2.2169 | PDE Loss:  -4.1716 | Function Loss:  -2.2679\n",
            "Total loss:  -2.2174 | PDE Loss:  -4.1708 | Function Loss:  -2.2687\n",
            "Total loss:  -2.2181 | PDE Loss:  -4.1707 | Function Loss:  -2.2695\n",
            "Total loss:  -2.2192 | PDE Loss:  -4.1727 | Function Loss:  -2.2704\n",
            "Total loss:  -2.2205 | PDE Loss:  -4.173 | Function Loss:  -2.2718\n",
            "Total loss:  -2.2218 | PDE Loss:  -4.1783 | Function Loss:  -2.2727\n",
            "Total loss:  -2.2229 | PDE Loss:  -4.1792 | Function Loss:  -2.2738\n",
            "Total loss:  -2.2238 | PDE Loss:  -4.1842 | Function Loss:  -2.2742\n",
            "Total loss:  -2.2246 | PDE Loss:  -4.1845 | Function Loss:  -2.275\n",
            "Total loss:  -2.2253 | PDE Loss:  -4.1917 | Function Loss:  -2.2749\n",
            "Total loss:  -2.2258 | PDE Loss:  -4.1901 | Function Loss:  -2.2757\n",
            "Total loss:  -2.2263 | PDE Loss:  -4.1917 | Function Loss:  -2.276\n",
            "Total loss:  -2.2269 | PDE Loss:  -4.1932 | Function Loss:  -2.2765\n",
            "Total loss:  -2.2276 | PDE Loss:  -4.1944 | Function Loss:  -2.2772\n",
            "Total loss:  -2.2282 | PDE Loss:  -4.1982 | Function Loss:  -2.2775\n",
            "Total loss:  -2.2288 | PDE Loss:  -4.1991 | Function Loss:  -2.278\n",
            "Total loss:  -2.2293 | PDE Loss:  -4.1998 | Function Loss:  -2.2785\n",
            "Total loss:  -2.2298 | PDE Loss:  -4.2023 | Function Loss:  -2.2787\n",
            "Total loss:  -2.2302 | PDE Loss:  -4.2021 | Function Loss:  -2.2792\n",
            "Total loss:  -2.2307 | PDE Loss:  -4.2031 | Function Loss:  -2.2796\n",
            "Total loss:  -2.2312 | PDE Loss:  -4.2028 | Function Loss:  -2.2803\n",
            "Total loss:  -2.2318 | PDE Loss:  -4.2009 | Function Loss:  -2.2811\n",
            "Total loss:  -2.2322 | PDE Loss:  -4.2007 | Function Loss:  -2.2816\n",
            "Total loss:  -2.2327 | PDE Loss:  -4.1999 | Function Loss:  -2.2823\n",
            "Total loss:  -2.2333 | PDE Loss:  -4.2 | Function Loss:  -2.2829\n",
            "Total loss:  -2.2338 | PDE Loss:  -4.1989 | Function Loss:  -2.2837\n",
            "Total loss:  -2.2344 | PDE Loss:  -4.1954 | Function Loss:  -2.2847\n",
            "Total loss:  -2.2349 | PDE Loss:  -4.1955 | Function Loss:  -2.2852\n",
            "Total loss:  -2.2354 | PDE Loss:  -4.1942 | Function Loss:  -2.286\n",
            "Total loss:  -2.2361 | PDE Loss:  -4.1919 | Function Loss:  -2.287\n",
            "Total loss:  -2.2366 | PDE Loss:  -4.1901 | Function Loss:  -2.2879\n",
            "Total loss:  -2.2371 | PDE Loss:  -4.1868 | Function Loss:  -2.2888\n",
            "Total loss:  -2.2375 | PDE Loss:  -4.1849 | Function Loss:  -2.2895\n",
            "Total loss:  -2.2379 | PDE Loss:  -4.1812 | Function Loss:  -2.2904\n",
            "Total loss:  -2.2383 | PDE Loss:  -4.1811 | Function Loss:  -2.2909\n",
            "Total loss:  -2.2387 | PDE Loss:  -4.1782 | Function Loss:  -2.2917\n",
            "Total loss:  -2.239 | PDE Loss:  -4.1786 | Function Loss:  -2.2921\n",
            "Total loss:  -2.2394 | PDE Loss:  -4.1784 | Function Loss:  -2.2924\n",
            "Total loss:  -2.2397 | PDE Loss:  -4.1786 | Function Loss:  -2.2928\n",
            "Total loss:  -2.24 | PDE Loss:  -4.1788 | Function Loss:  -2.2931\n",
            "Total loss:  -2.2402 | PDE Loss:  -4.1789 | Function Loss:  -2.2934\n",
            "Total loss:  -2.2405 | PDE Loss:  -4.1783 | Function Loss:  -2.2937\n",
            "Total loss:  -2.2407 | PDE Loss:  -4.179 | Function Loss:  -2.2939\n",
            "Total loss:  -2.241 | PDE Loss:  -4.1768 | Function Loss:  -2.2945\n",
            "Total loss:  -2.2413 | PDE Loss:  -4.178 | Function Loss:  -2.2947\n",
            "Total loss:  -2.2415 | PDE Loss:  -4.1758 | Function Loss:  -2.2952\n",
            "Total loss:  -2.2417 | PDE Loss:  -4.1775 | Function Loss:  -2.2952\n",
            "Total loss:  -2.2419 | PDE Loss:  -4.1773 | Function Loss:  -2.2954\n",
            "Total loss:  -2.242 | PDE Loss:  -4.1788 | Function Loss:  -2.2954\n",
            "Total loss:  -2.2421 | PDE Loss:  -4.1792 | Function Loss:  -2.2955\n",
            "Total loss:  -2.2422 | PDE Loss:  -4.1812 | Function Loss:  -2.2954\n",
            "Total loss:  -2.2424 | PDE Loss:  -4.182 | Function Loss:  -2.2954\n",
            "Total loss:  -2.2425 | PDE Loss:  -4.1836 | Function Loss:  -2.2953\n",
            "Total loss:  -2.2426 | PDE Loss:  -4.1849 | Function Loss:  -2.2953\n",
            "Total loss:  -2.2428 | PDE Loss:  -4.1858 | Function Loss:  -2.2954\n",
            "Total loss:  -2.243 | PDE Loss:  -4.1866 | Function Loss:  -2.2955\n",
            "Total loss:  -2.2432 | PDE Loss:  -4.1861 | Function Loss:  -2.2958\n",
            "Total loss:  -2.2435 | PDE Loss:  -4.1854 | Function Loss:  -2.2962\n",
            "Total loss:  -2.2439 | PDE Loss:  -4.1828 | Function Loss:  -2.297\n",
            "Total loss:  -2.2444 | PDE Loss:  -4.1788 | Function Loss:  -2.2981\n",
            "Total loss:  -2.2448 | PDE Loss:  -4.1745 | Function Loss:  -2.2991\n",
            "Total loss:  -2.2453 | PDE Loss:  -4.171 | Function Loss:  -2.3002\n",
            "Total loss:  -2.2458 | PDE Loss:  -4.1667 | Function Loss:  -2.3013\n",
            "Total loss:  -2.2463 | PDE Loss:  -4.1672 | Function Loss:  -2.3018\n",
            "Total loss:  -2.2467 | PDE Loss:  -4.1614 | Function Loss:  -2.3031\n",
            "Total loss:  -2.2471 | PDE Loss:  -4.1657 | Function Loss:  -2.3029\n",
            "Total loss:  -2.2474 | PDE Loss:  -4.1672 | Function Loss:  -2.3031\n",
            "Total loss:  -2.248 | PDE Loss:  -4.1717 | Function Loss:  -2.3031\n",
            "Total loss:  -2.2488 | PDE Loss:  -4.1767 | Function Loss:  -2.3033\n",
            "Total loss:  -2.2493 | PDE Loss:  -4.1798 | Function Loss:  -2.3036\n",
            "Total loss:  -2.25 | PDE Loss:  -4.1812 | Function Loss:  -2.3041\n",
            "Total loss:  -2.2506 | PDE Loss:  -4.18 | Function Loss:  -2.305\n",
            "Total loss:  -2.2511 | PDE Loss:  -4.1788 | Function Loss:  -2.3057\n",
            "Total loss:  -2.2515 | PDE Loss:  -4.1739 | Function Loss:  -2.3068\n",
            "Total loss:  -2.2518 | PDE Loss:  -4.172 | Function Loss:  -2.3074\n",
            "Total loss:  -2.2521 | PDE Loss:  -4.1722 | Function Loss:  -2.3077\n",
            "Total loss:  -2.2523 | PDE Loss:  -4.1697 | Function Loss:  -2.3083\n",
            "Total loss:  -2.2525 | PDE Loss:  -4.1708 | Function Loss:  -2.3084\n",
            "Total loss:  -2.2527 | PDE Loss:  -4.1713 | Function Loss:  -2.3085\n",
            "Total loss:  -2.253 | PDE Loss:  -4.1739 | Function Loss:  -2.3085\n",
            "Total loss:  -2.2533 | PDE Loss:  -4.1777 | Function Loss:  -2.3084\n",
            "Total loss:  -2.2537 | PDE Loss:  -4.1806 | Function Loss:  -2.3084\n",
            "Total loss:  -2.2541 | PDE Loss:  -4.1846 | Function Loss:  -2.3083\n",
            "Total loss:  -2.2546 | PDE Loss:  -4.1882 | Function Loss:  -2.3084\n",
            "Total loss:  -2.2548 | PDE Loss:  -4.19 | Function Loss:  -2.3084\n",
            "Total loss:  -2.2553 | PDE Loss:  -4.191 | Function Loss:  -2.3088\n",
            "Total loss:  -2.2557 | PDE Loss:  -4.1909 | Function Loss:  -2.3093\n",
            "Total loss:  -2.2561 | PDE Loss:  -4.19 | Function Loss:  -2.3099\n",
            "Total loss:  -2.2565 | PDE Loss:  -4.1891 | Function Loss:  -2.3104\n",
            "Total loss:  -2.257 | PDE Loss:  -4.1874 | Function Loss:  -2.3113\n",
            "Total loss:  -2.2576 | PDE Loss:  -4.1889 | Function Loss:  -2.3117\n",
            "Total loss:  -2.2583 | PDE Loss:  -4.1848 | Function Loss:  -2.313\n",
            "Total loss:  -2.2589 | PDE Loss:  -4.1897 | Function Loss:  -2.3131\n",
            "Total loss:  -2.2595 | PDE Loss:  -4.1889 | Function Loss:  -2.3138\n",
            "Total loss:  -2.2601 | PDE Loss:  -4.1942 | Function Loss:  -2.3138\n",
            "Total loss:  -2.2606 | PDE Loss:  -4.1929 | Function Loss:  -2.3146\n",
            "Total loss:  -2.261 | PDE Loss:  -4.1956 | Function Loss:  -2.3146\n",
            "Total loss:  -2.2613 | PDE Loss:  -4.197 | Function Loss:  -2.3148\n",
            "Total loss:  -2.2616 | PDE Loss:  -4.1948 | Function Loss:  -2.3155\n",
            "Total loss:  -2.2621 | PDE Loss:  -4.1926 | Function Loss:  -2.3163\n",
            "Total loss:  -2.2623 | PDE Loss:  -4.193 | Function Loss:  -2.3165\n",
            "Total loss:  -2.2628 | PDE Loss:  -4.1885 | Function Loss:  -2.3176\n",
            "Total loss:  -2.2631 | PDE Loss:  -4.1874 | Function Loss:  -2.3182\n",
            "Total loss:  -2.2633 | PDE Loss:  -4.1841 | Function Loss:  -2.3189\n",
            "Total loss:  -2.2636 | PDE Loss:  -4.1822 | Function Loss:  -2.3194\n",
            "Total loss:  -2.2639 | PDE Loss:  -4.18 | Function Loss:  -2.32\n",
            "Total loss:  -2.2643 | PDE Loss:  -4.177 | Function Loss:  -2.3209\n",
            "Total loss:  -2.2647 | PDE Loss:  -4.1763 | Function Loss:  -2.3215\n",
            "Total loss:  -2.265 | PDE Loss:  -4.1749 | Function Loss:  -2.3221\n",
            "Total loss:  -2.2655 | PDE Loss:  -4.1749 | Function Loss:  -2.3226\n",
            "Total loss:  -2.2659 | PDE Loss:  -4.1756 | Function Loss:  -2.3229\n",
            "Total loss:  -2.2661 | PDE Loss:  -4.1753 | Function Loss:  -2.3233\n",
            "Total loss:  -2.2664 | PDE Loss:  -4.177 | Function Loss:  -2.3233\n",
            "Total loss:  -2.2666 | PDE Loss:  -4.1777 | Function Loss:  -2.3235\n",
            "Total loss:  -2.2669 | PDE Loss:  -4.1788 | Function Loss:  -2.3236\n",
            "Total loss:  -2.2672 | PDE Loss:  -4.18 | Function Loss:  -2.3238\n",
            "Total loss:  -2.2675 | PDE Loss:  -4.1778 | Function Loss:  -2.3245\n",
            "Total loss:  -2.2678 | PDE Loss:  -4.1785 | Function Loss:  -2.3248\n",
            "Total loss:  -2.2682 | PDE Loss:  -4.1791 | Function Loss:  -2.3251\n",
            "Total loss:  -2.2686 | PDE Loss:  -4.1748 | Function Loss:  -2.3262\n",
            "Total loss:  -2.269 | PDE Loss:  -4.1761 | Function Loss:  -2.3264\n",
            "Total loss:  -2.2693 | PDE Loss:  -4.1737 | Function Loss:  -2.3271\n",
            "Total loss:  -2.2696 | PDE Loss:  -4.1744 | Function Loss:  -2.3274\n",
            "Total loss:  -2.2699 | PDE Loss:  -4.1722 | Function Loss:  -2.328\n",
            "Total loss:  -2.27 | PDE Loss:  -4.1711 | Function Loss:  -2.3283\n",
            "Total loss:  -2.2703 | PDE Loss:  -4.1703 | Function Loss:  -2.3287\n",
            "Total loss:  -2.2706 | PDE Loss:  -4.1685 | Function Loss:  -2.3293\n",
            "Total loss:  -2.271 | PDE Loss:  -4.1668 | Function Loss:  -2.33\n",
            "Total loss:  -2.2714 | PDE Loss:  -4.1633 | Function Loss:  -2.331\n",
            "Total loss:  -2.2718 | PDE Loss:  -4.1603 | Function Loss:  -2.332\n",
            "Total loss:  -2.2721 | PDE Loss:  -4.1588 | Function Loss:  -2.3325\n",
            "Total loss:  -2.2726 | PDE Loss:  -4.158 | Function Loss:  -2.3331\n",
            "Total loss:  -2.273 | PDE Loss:  -4.1565 | Function Loss:  -2.3338\n",
            "Total loss:  -2.2732 | PDE Loss:  -4.1578 | Function Loss:  -2.3339\n",
            "Total loss:  -2.2735 | PDE Loss:  -4.1568 | Function Loss:  -2.3344\n",
            "Total loss:  -2.2737 | PDE Loss:  -4.1612 | Function Loss:  -2.334\n",
            "Total loss:  -2.2739 | PDE Loss:  -4.161 | Function Loss:  -2.3343\n",
            "Total loss:  -2.2741 | PDE Loss:  -4.1626 | Function Loss:  -2.3343\n",
            "Total loss:  -2.2744 | PDE Loss:  -4.1638 | Function Loss:  -2.3344\n",
            "Total loss:  -2.2748 | PDE Loss:  -4.1639 | Function Loss:  -2.3348\n",
            "Total loss:  -2.2752 | PDE Loss:  -4.1642 | Function Loss:  -2.3353\n",
            "Total loss:  -2.2756 | PDE Loss:  -4.1621 | Function Loss:  -2.336\n",
            "Total loss:  -2.276 | PDE Loss:  -4.1606 | Function Loss:  -2.3367\n",
            "Total loss:  -2.2764 | PDE Loss:  -4.1577 | Function Loss:  -2.3376\n",
            "Total loss:  -2.2768 | PDE Loss:  -4.156 | Function Loss:  -2.3383\n",
            "Total loss:  -2.277 | PDE Loss:  -4.1543 | Function Loss:  -2.3388\n",
            "Total loss:  -2.2772 | PDE Loss:  -4.1532 | Function Loss:  -2.3392\n",
            "Total loss:  -2.2773 | PDE Loss:  -4.1533 | Function Loss:  -2.3394\n",
            "Total loss:  -2.2775 | PDE Loss:  -4.153 | Function Loss:  -2.3396\n",
            "Total loss:  -2.2777 | PDE Loss:  -4.1541 | Function Loss:  -2.3396\n",
            "Total loss:  -2.2778 | PDE Loss:  -4.1543 | Function Loss:  -2.3397\n",
            "Total loss:  -2.2779 | PDE Loss:  -4.156 | Function Loss:  -2.3396\n",
            "Total loss:  -2.278 | PDE Loss:  -4.1566 | Function Loss:  -2.3396\n",
            "Total loss:  -2.2782 | PDE Loss:  -4.1579 | Function Loss:  -2.3396\n",
            "Total loss:  -2.2783 | PDE Loss:  -4.1587 | Function Loss:  -2.3397\n",
            "Total loss:  -2.2786 | PDE Loss:  -4.1594 | Function Loss:  -2.3398\n",
            "Total loss:  -2.2788 | PDE Loss:  -4.1586 | Function Loss:  -2.3402\n",
            "Total loss:  -2.2791 | PDE Loss:  -4.1601 | Function Loss:  -2.3403\n",
            "Total loss:  -2.2793 | PDE Loss:  -4.16 | Function Loss:  -2.3406\n",
            "Total loss:  -2.2796 | PDE Loss:  -4.1608 | Function Loss:  -2.3408\n",
            "Total loss:  -2.2799 | PDE Loss:  -4.1618 | Function Loss:  -2.341\n",
            "Total loss:  -2.2802 | PDE Loss:  -4.1634 | Function Loss:  -2.3411\n",
            "Total loss:  -2.2804 | PDE Loss:  -4.1653 | Function Loss:  -2.3411\n",
            "Total loss:  -2.2807 | PDE Loss:  -4.1682 | Function Loss:  -2.341\n",
            "Total loss:  -2.2811 | PDE Loss:  -4.1711 | Function Loss:  -2.3409\n",
            "Total loss:  -2.2814 | PDE Loss:  -4.1752 | Function Loss:  -2.3408\n",
            "Total loss:  -2.2818 | PDE Loss:  -4.1799 | Function Loss:  -2.3405\n",
            "Total loss:  -2.282 | PDE Loss:  -4.181 | Function Loss:  -2.3406\n",
            "Total loss:  -2.2823 | PDE Loss:  -4.1832 | Function Loss:  -2.3407\n",
            "Total loss:  -2.2826 | PDE Loss:  -4.184 | Function Loss:  -2.3408\n",
            "Total loss:  -2.2828 | PDE Loss:  -4.1831 | Function Loss:  -2.3412\n",
            "Total loss:  -2.2831 | PDE Loss:  -4.1852 | Function Loss:  -2.3412\n",
            "Total loss:  -2.2832 | PDE Loss:  -4.184 | Function Loss:  -2.3415\n",
            "Total loss:  -2.2834 | PDE Loss:  -4.1838 | Function Loss:  -2.3418\n",
            "Total loss:  -2.2837 | PDE Loss:  -4.1832 | Function Loss:  -2.3422\n",
            "Total loss:  -2.284 | PDE Loss:  -4.1842 | Function Loss:  -2.3424\n",
            "Total loss:  -2.2843 | PDE Loss:  -4.1856 | Function Loss:  -2.3426\n",
            "Total loss:  -2.2847 | PDE Loss:  -4.1882 | Function Loss:  -2.3426\n",
            "Total loss:  -2.285 | PDE Loss:  -4.1916 | Function Loss:  -2.3425\n",
            "Total loss:  -2.2853 | PDE Loss:  -4.1949 | Function Loss:  -2.3424\n",
            "Total loss:  -2.2857 | PDE Loss:  -4.1992 | Function Loss:  -2.3422\n",
            "Total loss:  -2.2861 | PDE Loss:  -4.2025 | Function Loss:  -2.3422\n",
            "Total loss:  -2.2866 | PDE Loss:  -4.2088 | Function Loss:  -2.342\n",
            "Total loss:  -2.2872 | PDE Loss:  -4.212 | Function Loss:  -2.3422\n",
            "Total loss:  -2.2878 | PDE Loss:  -4.2151 | Function Loss:  -2.3425\n",
            "Total loss:  -2.2885 | PDE Loss:  -4.2215 | Function Loss:  -2.3423\n",
            "Total loss:  -2.2892 | PDE Loss:  -4.2227 | Function Loss:  -2.343\n",
            "Total loss:  -2.2901 | PDE Loss:  -4.2241 | Function Loss:  -2.3438\n",
            "Total loss:  -2.2909 | PDE Loss:  -4.2232 | Function Loss:  -2.3449\n",
            "Total loss:  -2.2916 | PDE Loss:  -4.2265 | Function Loss:  -2.3452\n",
            "Total loss:  -2.2922 | PDE Loss:  -4.2271 | Function Loss:  -2.3459\n",
            "Total loss:  -2.2933 | PDE Loss:  -4.2313 | Function Loss:  -2.3465\n",
            "Total loss:  -2.2947 | PDE Loss:  -4.2332 | Function Loss:  -2.3479\n",
            "Total loss:  -2.2958 | PDE Loss:  -4.236 | Function Loss:  -2.3488\n",
            "Total loss:  -2.2969 | PDE Loss:  -4.2378 | Function Loss:  -2.3497\n",
            "Total loss:  -2.2976 | PDE Loss:  -4.2383 | Function Loss:  -2.3505\n",
            "Total loss:  -2.298 | PDE Loss:  -4.2384 | Function Loss:  -2.3509\n",
            "Total loss:  -2.2984 | PDE Loss:  -4.2386 | Function Loss:  -2.3514\n",
            "Total loss:  -2.299 | PDE Loss:  -4.2395 | Function Loss:  -2.3518\n",
            "Total loss:  -2.2995 | PDE Loss:  -4.2399 | Function Loss:  -2.3524\n",
            "Total loss:  -2.3001 | PDE Loss:  -4.2407 | Function Loss:  -2.353\n",
            "Total loss:  -2.3011 | PDE Loss:  -4.243 | Function Loss:  -2.3538\n",
            "Total loss:  -2.3021 | PDE Loss:  -4.2411 | Function Loss:  -2.3552\n",
            "Total loss:  -2.3027 | PDE Loss:  -4.2458 | Function Loss:  -2.3553\n",
            "Total loss:  -2.3031 | PDE Loss:  -4.2446 | Function Loss:  -2.3559\n",
            "Total loss:  -2.3035 | PDE Loss:  -4.2433 | Function Loss:  -2.3565\n",
            "Total loss:  -2.3038 | PDE Loss:  -4.2429 | Function Loss:  -2.3569\n",
            "Total loss:  -2.3042 | PDE Loss:  -4.2423 | Function Loss:  -2.3574\n",
            "Total loss:  -2.3046 | PDE Loss:  -4.2435 | Function Loss:  -2.3577\n",
            "Total loss:  -2.305 | PDE Loss:  -4.2435 | Function Loss:  -2.3581\n",
            "Total loss:  -2.3053 | PDE Loss:  -4.2449 | Function Loss:  -2.3583\n",
            "Total loss:  -2.3057 | PDE Loss:  -4.2462 | Function Loss:  -2.3586\n",
            "Total loss:  -2.306 | PDE Loss:  -4.2472 | Function Loss:  -2.3588\n",
            "Total loss:  -2.3065 | PDE Loss:  -4.2451 | Function Loss:  -2.3597\n",
            "Total loss:  -2.3072 | PDE Loss:  -4.2459 | Function Loss:  -2.3603\n",
            "Total loss:  -2.3078 | PDE Loss:  -4.2443 | Function Loss:  -2.3612\n",
            "Total loss:  -2.3087 | PDE Loss:  -4.2421 | Function Loss:  -2.3625\n",
            "Total loss:  -2.3098 | PDE Loss:  -4.2407 | Function Loss:  -2.364\n",
            "Total loss:  -2.3108 | PDE Loss:  -4.2423 | Function Loss:  -2.3649\n",
            "Total loss:  -2.3114 | PDE Loss:  -4.2446 | Function Loss:  -2.3652\n",
            "Total loss:  -2.3118 | PDE Loss:  -4.2453 | Function Loss:  -2.3657\n",
            "Total loss:  -2.3121 | PDE Loss:  -4.2462 | Function Loss:  -2.3659\n",
            "Total loss:  -2.3124 | PDE Loss:  -4.2486 | Function Loss:  -2.3659\n",
            "Total loss:  -2.3128 | PDE Loss:  -4.2468 | Function Loss:  -2.3665\n",
            "Total loss:  -2.3132 | PDE Loss:  -4.2508 | Function Loss:  -2.3664\n",
            "Total loss:  -2.3136 | PDE Loss:  -4.2524 | Function Loss:  -2.3667\n",
            "Total loss:  -2.3141 | PDE Loss:  -4.2543 | Function Loss:  -2.367\n",
            "Total loss:  -2.3146 | PDE Loss:  -4.2564 | Function Loss:  -2.3673\n",
            "Total loss:  -2.3151 | PDE Loss:  -4.2583 | Function Loss:  -2.3677\n",
            "Total loss:  -2.3156 | PDE Loss:  -4.2584 | Function Loss:  -2.3682\n",
            "Total loss:  -2.316 | PDE Loss:  -4.2595 | Function Loss:  -2.3685\n",
            "Total loss:  -2.3164 | PDE Loss:  -4.2598 | Function Loss:  -2.3689\n",
            "Total loss:  -2.3168 | PDE Loss:  -4.2599 | Function Loss:  -2.3694\n",
            "Total loss:  -2.3172 | PDE Loss:  -4.261 | Function Loss:  -2.3697\n",
            "Total loss:  -2.3175 | PDE Loss:  -4.2627 | Function Loss:  -2.3698\n",
            "Total loss:  -2.3179 | PDE Loss:  -4.2632 | Function Loss:  -2.3702\n",
            "Total loss:  -2.3182 | PDE Loss:  -4.2668 | Function Loss:  -2.37\n",
            "Total loss:  -2.3185 | PDE Loss:  -4.2693 | Function Loss:  -2.3701\n",
            "Total loss:  -2.319 | PDE Loss:  -4.2716 | Function Loss:  -2.3703\n",
            "Total loss:  -2.3195 | PDE Loss:  -4.2747 | Function Loss:  -2.3706\n",
            "Total loss:  -2.3201 | PDE Loss:  -4.2742 | Function Loss:  -2.3713\n",
            "Total loss:  -2.3208 | PDE Loss:  -4.2777 | Function Loss:  -2.3716\n",
            "Total loss:  -2.3213 | PDE Loss:  -4.2766 | Function Loss:  -2.3724\n",
            "Total loss:  -2.3221 | PDE Loss:  -4.2765 | Function Loss:  -2.3732\n",
            "Total loss:  -2.3229 | PDE Loss:  -4.2728 | Function Loss:  -2.3746\n",
            "Total loss:  -2.3235 | PDE Loss:  -4.2742 | Function Loss:  -2.3751\n",
            "Total loss:  -2.3239 | PDE Loss:  -4.2738 | Function Loss:  -2.3756\n",
            "Total loss:  -2.3245 | PDE Loss:  -4.2791 | Function Loss:  -2.3756\n",
            "Total loss:  -2.3249 | PDE Loss:  -4.2799 | Function Loss:  -2.376\n",
            "Total loss:  -2.3253 | PDE Loss:  -4.2811 | Function Loss:  -2.3763\n",
            "Total loss:  -2.3257 | PDE Loss:  -4.2837 | Function Loss:  -2.3764\n",
            "Total loss:  -2.3261 | PDE Loss:  -4.2847 | Function Loss:  -2.3767\n",
            "Total loss:  -2.3265 | PDE Loss:  -4.2858 | Function Loss:  -2.377\n",
            "Total loss:  -2.3269 | PDE Loss:  -4.2872 | Function Loss:  -2.3773\n",
            "Total loss:  -2.3273 | PDE Loss:  -4.2875 | Function Loss:  -2.3777\n",
            "Total loss:  -2.3278 | PDE Loss:  -4.2885 | Function Loss:  -2.3781\n",
            "Total loss:  -2.3283 | PDE Loss:  -4.2869 | Function Loss:  -2.3789\n",
            "Total loss:  -2.3286 | PDE Loss:  -4.2882 | Function Loss:  -2.3791\n",
            "Total loss:  -2.3289 | PDE Loss:  -4.2878 | Function Loss:  -2.3795\n",
            "Total loss:  -2.3292 | PDE Loss:  -4.2896 | Function Loss:  -2.3796\n",
            "Total loss:  -2.3294 | PDE Loss:  -4.2901 | Function Loss:  -2.3797\n",
            "Total loss:  -2.3296 | PDE Loss:  -4.2926 | Function Loss:  -2.3797\n",
            "Total loss:  -2.3299 | PDE Loss:  -4.2943 | Function Loss:  -2.3798\n",
            "Total loss:  -2.3303 | PDE Loss:  -4.297 | Function Loss:  -2.3799\n",
            "Total loss:  -2.3309 | PDE Loss:  -4.299 | Function Loss:  -2.3804\n",
            "Total loss:  -2.3314 | PDE Loss:  -4.3073 | Function Loss:  -2.3799\n",
            "Total loss:  -2.3318 | PDE Loss:  -4.3082 | Function Loss:  -2.3803\n",
            "Total loss:  -2.3323 | PDE Loss:  -4.3067 | Function Loss:  -2.381\n",
            "Total loss:  -2.3327 | PDE Loss:  -4.3082 | Function Loss:  -2.3813\n",
            "Total loss:  -2.333 | PDE Loss:  -4.3067 | Function Loss:  -2.3818\n",
            "Total loss:  -2.3333 | PDE Loss:  -4.3062 | Function Loss:  -2.3822\n",
            "Total loss:  -2.3335 | PDE Loss:  -4.3103 | Function Loss:  -2.3819\n",
            "Total loss:  -2.3339 | PDE Loss:  -4.3096 | Function Loss:  -2.3825\n",
            "Total loss:  -2.3343 | PDE Loss:  -4.3101 | Function Loss:  -2.3829\n",
            "Total loss:  -2.3346 | PDE Loss:  -4.3079 | Function Loss:  -2.3834\n",
            "Total loss:  -2.3348 | PDE Loss:  -4.308 | Function Loss:  -2.3837\n",
            "Total loss:  -2.335 | PDE Loss:  -4.306 | Function Loss:  -2.3842\n",
            "Total loss:  -2.3353 | PDE Loss:  -4.3051 | Function Loss:  -2.3845\n",
            "Total loss:  -2.3355 | PDE Loss:  -4.3031 | Function Loss:  -2.385\n",
            "Total loss:  -2.3358 | PDE Loss:  -4.3017 | Function Loss:  -2.3855\n",
            "Total loss:  -2.3361 | PDE Loss:  -4.2997 | Function Loss:  -2.3861\n",
            "Total loss:  -2.3364 | PDE Loss:  -4.2994 | Function Loss:  -2.3865\n",
            "Total loss:  -2.3368 | PDE Loss:  -4.2983 | Function Loss:  -2.387\n",
            "Total loss:  -2.3372 | PDE Loss:  -4.3002 | Function Loss:  -2.3872\n",
            "Total loss:  -2.3375 | PDE Loss:  -4.3002 | Function Loss:  -2.3876\n",
            "Total loss:  -2.3378 | PDE Loss:  -4.3033 | Function Loss:  -2.3876\n",
            "Total loss:  -2.3381 | PDE Loss:  -4.304 | Function Loss:  -2.3878\n",
            "Total loss:  -2.3383 | PDE Loss:  -4.3069 | Function Loss:  -2.3877\n",
            "Total loss:  -2.3386 | PDE Loss:  -4.3078 | Function Loss:  -2.3879\n",
            "Total loss:  -2.339 | PDE Loss:  -4.3098 | Function Loss:  -2.3881\n",
            "Total loss:  -2.3394 | PDE Loss:  -4.312 | Function Loss:  -2.3883\n",
            "Total loss:  -2.3399 | PDE Loss:  -4.3109 | Function Loss:  -2.389\n",
            "Total loss:  -2.3403 | PDE Loss:  -4.3139 | Function Loss:  -2.3891\n",
            "Total loss:  -2.3406 | PDE Loss:  -4.3123 | Function Loss:  -2.3897\n",
            "Total loss:  -2.3409 | PDE Loss:  -4.3081 | Function Loss:  -2.3905\n",
            "Total loss:  -2.3413 | PDE Loss:  -4.3067 | Function Loss:  -2.3911\n",
            "Total loss:  -2.3415 | PDE Loss:  -4.3036 | Function Loss:  -2.3917\n",
            "Total loss:  -2.3417 | PDE Loss:  -4.3004 | Function Loss:  -2.3924\n",
            "Total loss:  -2.3421 | PDE Loss:  -4.2993 | Function Loss:  -2.3929\n",
            "Total loss:  -2.3424 | PDE Loss:  -4.2981 | Function Loss:  -2.3934\n",
            "Total loss:  -2.3428 | PDE Loss:  -4.2993 | Function Loss:  -2.3937\n",
            "Total loss:  -2.3432 | PDE Loss:  -4.3013 | Function Loss:  -2.3939\n",
            "Total loss:  -2.3437 | PDE Loss:  -4.305 | Function Loss:  -2.394\n",
            "Total loss:  -2.3442 | PDE Loss:  -4.3097 | Function Loss:  -2.394\n",
            "Total loss:  -2.3446 | PDE Loss:  -4.3124 | Function Loss:  -2.394\n",
            "Total loss:  -2.3453 | PDE Loss:  -4.3173 | Function Loss:  -2.3943\n",
            "Total loss:  -2.3459 | PDE Loss:  -4.3207 | Function Loss:  -2.3945\n",
            "Total loss:  -2.3464 | PDE Loss:  -4.3218 | Function Loss:  -2.3949\n",
            "Total loss:  -2.3468 | PDE Loss:  -4.3239 | Function Loss:  -2.3952\n",
            "Total loss:  -2.3471 | PDE Loss:  -4.3237 | Function Loss:  -2.3956\n",
            "Total loss:  -2.3475 | PDE Loss:  -4.3238 | Function Loss:  -2.3959\n",
            "Total loss:  -2.3479 | PDE Loss:  -4.3253 | Function Loss:  -2.3962\n",
            "Total loss:  -2.3482 | PDE Loss:  -4.3219 | Function Loss:  -2.397\n",
            "Total loss:  -2.3484 | PDE Loss:  -4.3262 | Function Loss:  -2.3967\n",
            "Total loss:  -2.3485 | PDE Loss:  -4.3268 | Function Loss:  -2.3968\n",
            "Total loss:  -2.3488 | PDE Loss:  -4.3292 | Function Loss:  -2.3967\n",
            "Total loss:  -2.349 | PDE Loss:  -4.3329 | Function Loss:  -2.3966\n",
            "Total loss:  -2.3494 | PDE Loss:  -4.3396 | Function Loss:  -2.3962\n",
            "Total loss:  -2.3496 | PDE Loss:  -4.3429 | Function Loss:  -2.3961\n",
            "Total loss:  -2.35 | PDE Loss:  -4.3471 | Function Loss:  -2.3961\n",
            "Total loss:  -2.3505 | PDE Loss:  -4.3497 | Function Loss:  -2.3963\n",
            "Total loss:  -2.3509 | PDE Loss:  -4.3521 | Function Loss:  -2.3965\n",
            "Total loss:  -2.3512 | PDE Loss:  -4.3525 | Function Loss:  -2.3968\n",
            "Total loss:  -2.3515 | PDE Loss:  -4.3532 | Function Loss:  -2.3971\n",
            "Total loss:  -2.3518 | PDE Loss:  -4.3514 | Function Loss:  -2.3976\n",
            "Total loss:  -2.352 | PDE Loss:  -4.3511 | Function Loss:  -2.3978\n",
            "Total loss:  -2.3523 | PDE Loss:  -4.35 | Function Loss:  -2.3984\n",
            "Total loss:  -2.3527 | PDE Loss:  -4.3493 | Function Loss:  -2.3989\n",
            "Total loss:  -2.3531 | PDE Loss:  -4.3488 | Function Loss:  -2.3993\n",
            "Total loss:  -2.3534 | PDE Loss:  -4.3492 | Function Loss:  -2.3996\n",
            "Total loss:  -2.3537 | PDE Loss:  -4.3496 | Function Loss:  -2.3999\n",
            "Total loss:  -2.354 | PDE Loss:  -4.3516 | Function Loss:  -2.4\n",
            "Total loss:  -2.3542 | PDE Loss:  -4.3523 | Function Loss:  -2.4002\n",
            "Total loss:  -2.3544 | PDE Loss:  -4.3544 | Function Loss:  -2.4001\n",
            "Total loss:  -2.3546 | PDE Loss:  -4.3549 | Function Loss:  -2.4003\n",
            "Total loss:  -2.3548 | PDE Loss:  -4.3562 | Function Loss:  -2.4004\n",
            "Total loss:  -2.3551 | PDE Loss:  -4.3567 | Function Loss:  -2.4007\n",
            "Total loss:  -2.3555 | PDE Loss:  -4.3552 | Function Loss:  -2.4013\n",
            "Total loss:  -2.3557 | PDE Loss:  -4.3548 | Function Loss:  -2.4016\n",
            "Total loss:  -2.3561 | PDE Loss:  -4.3529 | Function Loss:  -2.4022\n",
            "Total loss:  -2.3565 | PDE Loss:  -4.3513 | Function Loss:  -2.4029\n",
            "Total loss:  -2.3569 | PDE Loss:  -4.349 | Function Loss:  -2.4035\n",
            "Total loss:  -2.3571 | PDE Loss:  -4.3473 | Function Loss:  -2.4039\n",
            "Total loss:  -2.3573 | PDE Loss:  -4.3462 | Function Loss:  -2.4043\n",
            "Total loss:  -2.3576 | PDE Loss:  -4.3456 | Function Loss:  -2.4047\n",
            "Total loss:  -2.3579 | PDE Loss:  -4.345 | Function Loss:  -2.4051\n",
            "Total loss:  -2.3581 | PDE Loss:  -4.3456 | Function Loss:  -2.4053\n",
            "Total loss:  -2.3583 | PDE Loss:  -4.3456 | Function Loss:  -2.4055\n",
            "Total loss:  -2.3585 | PDE Loss:  -4.3467 | Function Loss:  -2.4056\n",
            "Total loss:  -2.3587 | PDE Loss:  -4.3477 | Function Loss:  -2.4057\n",
            "Total loss:  -2.3588 | PDE Loss:  -4.3472 | Function Loss:  -2.4058\n",
            "Total loss:  -2.3589 | PDE Loss:  -4.348 | Function Loss:  -2.4059\n",
            "Total loss:  -2.359 | PDE Loss:  -4.3465 | Function Loss:  -2.4061\n",
            "Total loss:  -2.359 | PDE Loss:  -4.3461 | Function Loss:  -2.4063\n",
            "Total loss:  -2.3592 | PDE Loss:  -4.3445 | Function Loss:  -2.4066\n",
            "Total loss:  -2.3593 | PDE Loss:  -4.343 | Function Loss:  -2.4069\n",
            "Total loss:  -2.3595 | PDE Loss:  -4.3399 | Function Loss:  -2.4075\n",
            "Total loss:  -2.3596 | PDE Loss:  -4.3391 | Function Loss:  -2.4077\n",
            "Total loss:  -2.3598 | PDE Loss:  -4.3384 | Function Loss:  -2.408\n",
            "Total loss:  -2.36 | PDE Loss:  -4.3385 | Function Loss:  -2.4082\n",
            "Total loss:  -2.3601 | PDE Loss:  -4.3386 | Function Loss:  -2.4084\n",
            "Total loss:  -2.3603 | PDE Loss:  -4.3394 | Function Loss:  -2.4084\n",
            "Total loss:  -2.3605 | PDE Loss:  -4.3395 | Function Loss:  -2.4086\n",
            "Total loss:  -2.3606 | PDE Loss:  -4.3401 | Function Loss:  -2.4087\n",
            "Total loss:  -2.3608 | PDE Loss:  -4.3404 | Function Loss:  -2.4089\n",
            "Total loss:  -2.3609 | PDE Loss:  -4.3408 | Function Loss:  -2.409\n",
            "Total loss:  -2.3611 | PDE Loss:  -4.3418 | Function Loss:  -2.409\n",
            "Total loss:  -2.3612 | PDE Loss:  -4.3426 | Function Loss:  -2.4091\n",
            "Total loss:  -2.3614 | PDE Loss:  -4.3437 | Function Loss:  -2.4092\n",
            "Total loss:  -2.3616 | PDE Loss:  -4.3449 | Function Loss:  -2.4093\n",
            "Total loss:  -2.3619 | PDE Loss:  -4.3465 | Function Loss:  -2.4094\n",
            "Total loss:  -2.3623 | PDE Loss:  -4.3487 | Function Loss:  -2.4096\n",
            "Total loss:  -2.3626 | PDE Loss:  -4.3492 | Function Loss:  -2.4099\n",
            "Total loss:  -2.363 | PDE Loss:  -4.3503 | Function Loss:  -2.4101\n",
            "Total loss:  -2.3637 | PDE Loss:  -4.3528 | Function Loss:  -2.4107\n",
            "Total loss:  -2.3646 | PDE Loss:  -4.355 | Function Loss:  -2.4115\n",
            "Total loss:  -2.3655 | PDE Loss:  -4.3566 | Function Loss:  -2.4123\n",
            "Total loss:  -2.3665 | PDE Loss:  -4.3578 | Function Loss:  -2.4133\n",
            "Total loss:  -2.3677 | PDE Loss:  -4.3605 | Function Loss:  -2.4142\n",
            "Total loss:  -2.3688 | PDE Loss:  -4.3595 | Function Loss:  -2.4156\n",
            "Total loss:  -2.3696 | PDE Loss:  -4.3623 | Function Loss:  -2.4162\n",
            "Total loss:  -2.3703 | PDE Loss:  -4.3641 | Function Loss:  -2.4168\n",
            "Total loss:  -2.371 | PDE Loss:  -4.3697 | Function Loss:  -2.4169\n",
            "Total loss:  -2.3713 | PDE Loss:  -4.3713 | Function Loss:  -2.4171\n",
            "Total loss:  -2.3716 | PDE Loss:  -4.3735 | Function Loss:  -2.4172\n",
            "Total loss:  -2.3719 | PDE Loss:  -4.3754 | Function Loss:  -2.4173\n",
            "Total loss:  -2.3724 | PDE Loss:  -4.3784 | Function Loss:  -2.4175\n",
            "Total loss:  -2.3729 | PDE Loss:  -4.3815 | Function Loss:  -2.4178\n",
            "Total loss:  -2.3734 | PDE Loss:  -4.3857 | Function Loss:  -2.4178\n",
            "Total loss:  -2.3739 | PDE Loss:  -4.391 | Function Loss:  -2.4178\n",
            "Total loss:  -2.3743 | PDE Loss:  -4.3947 | Function Loss:  -2.4178\n",
            "Total loss:  -2.3748 | PDE Loss:  -4.3959 | Function Loss:  -2.4183\n",
            "Total loss:  -2.3752 | PDE Loss:  -4.4007 | Function Loss:  -2.4183\n",
            "Total loss:  -2.3757 | PDE Loss:  -4.4001 | Function Loss:  -2.4188\n",
            "Total loss:  -2.3762 | PDE Loss:  -4.4005 | Function Loss:  -2.4194\n",
            "Total loss:  -2.3769 | PDE Loss:  -4.3995 | Function Loss:  -2.4202\n",
            "Total loss:  -2.3775 | PDE Loss:  -4.3989 | Function Loss:  -2.4209\n",
            "Total loss:  -2.378 | PDE Loss:  -4.3982 | Function Loss:  -2.4215\n",
            "Total loss:  -2.3785 | PDE Loss:  -4.398 | Function Loss:  -2.4221\n",
            "Total loss:  -2.3791 | PDE Loss:  -4.3991 | Function Loss:  -2.4227\n",
            "Total loss:  -2.3797 | PDE Loss:  -4.3989 | Function Loss:  -2.4234\n",
            "Total loss:  -2.3803 | PDE Loss:  -4.4001 | Function Loss:  -2.424\n",
            "Total loss:  -2.3809 | PDE Loss:  -4.4006 | Function Loss:  -2.4245\n",
            "Total loss:  -2.3817 | PDE Loss:  -4.4018 | Function Loss:  -2.4253\n",
            "Total loss:  -2.3827 | PDE Loss:  -4.4015 | Function Loss:  -2.4264\n",
            "Total loss:  -2.3837 | PDE Loss:  -4.4047 | Function Loss:  -2.4271\n",
            "Total loss:  -2.3845 | PDE Loss:  -4.4024 | Function Loss:  -2.4283\n",
            "Total loss:  -2.3852 | PDE Loss:  -4.4022 | Function Loss:  -2.4291\n",
            "Total loss:  -2.3857 | PDE Loss:  -4.401 | Function Loss:  -2.4298\n",
            "Total loss:  -2.3864 | PDE Loss:  -4.3995 | Function Loss:  -2.4307\n",
            "Total loss:  -2.3869 | PDE Loss:  -4.3984 | Function Loss:  -2.4314\n",
            "Total loss:  -2.3875 | PDE Loss:  -4.3978 | Function Loss:  -2.4321\n",
            "Total loss:  -2.388 | PDE Loss:  -4.3964 | Function Loss:  -2.4328\n",
            "Total loss:  -2.3884 | PDE Loss:  -4.3971 | Function Loss:  -2.4332\n",
            "Total loss:  -2.3888 | PDE Loss:  -4.3974 | Function Loss:  -2.4336\n",
            "Total loss:  -2.3891 | PDE Loss:  -4.399 | Function Loss:  -2.4338\n",
            "Total loss:  -2.3896 | PDE Loss:  -4.4016 | Function Loss:  -2.434\n",
            "Total loss:  -2.3901 | PDE Loss:  -4.4033 | Function Loss:  -2.4344\n",
            "Total loss:  -2.3905 | PDE Loss:  -4.4058 | Function Loss:  -2.4346\n",
            "Total loss:  -2.3909 | PDE Loss:  -4.4093 | Function Loss:  -2.4347\n",
            "Total loss:  -2.3915 | PDE Loss:  -4.412 | Function Loss:  -2.435\n",
            "Total loss:  -2.3918 | PDE Loss:  -4.414 | Function Loss:  -2.4352\n",
            "Total loss:  -2.3921 | PDE Loss:  -4.4154 | Function Loss:  -2.4353\n",
            "Total loss:  -2.3923 | PDE Loss:  -4.4149 | Function Loss:  -2.4357\n",
            "Total loss:  -2.3926 | PDE Loss:  -4.4161 | Function Loss:  -2.4359\n",
            "Total loss:  -2.393 | PDE Loss:  -4.4106 | Function Loss:  -2.4368\n",
            "Total loss:  -2.3932 | PDE Loss:  -4.4117 | Function Loss:  -2.437\n",
            "Total loss:  -2.3936 | PDE Loss:  -4.4126 | Function Loss:  -2.4373\n",
            "Total loss:  -2.394 | PDE Loss:  -4.4132 | Function Loss:  -2.4377\n",
            "Total loss:  -2.3944 | PDE Loss:  -4.4133 | Function Loss:  -2.4381\n",
            "Total loss:  -2.3948 | PDE Loss:  -4.4128 | Function Loss:  -2.4386\n",
            "Total loss:  -2.3952 | PDE Loss:  -4.412 | Function Loss:  -2.4391\n",
            "Total loss:  -2.3957 | PDE Loss:  -4.4098 | Function Loss:  -2.4399\n",
            "Total loss:  -2.396 | PDE Loss:  -4.4087 | Function Loss:  -2.4404\n",
            "Total loss:  -2.3963 | PDE Loss:  -4.4074 | Function Loss:  -2.4408\n",
            "Total loss:  -2.3966 | PDE Loss:  -4.4062 | Function Loss:  -2.4413\n",
            "Total loss:  -2.3969 | PDE Loss:  -4.4069 | Function Loss:  -2.4416\n",
            "Total loss:  -2.3974 | PDE Loss:  -4.4067 | Function Loss:  -2.4421\n",
            "Total loss:  -2.3978 | PDE Loss:  -4.4081 | Function Loss:  -2.4424\n",
            "Total loss:  -2.3982 | PDE Loss:  -4.4103 | Function Loss:  -2.4426\n",
            "Total loss:  -2.3987 | PDE Loss:  -4.411 | Function Loss:  -2.4431\n",
            "Total loss:  -2.399 | PDE Loss:  -4.4123 | Function Loss:  -2.4433\n",
            "Total loss:  -2.3992 | PDE Loss:  -4.4114 | Function Loss:  -2.4437\n",
            "Total loss:  -2.3994 | PDE Loss:  -4.4107 | Function Loss:  -2.444\n",
            "Total loss:  -2.3997 | PDE Loss:  -4.4095 | Function Loss:  -2.4444\n",
            "Total loss:  -2.4 | PDE Loss:  -4.4068 | Function Loss:  -2.445\n",
            "Total loss:  -2.4003 | PDE Loss:  -4.4053 | Function Loss:  -2.4455\n",
            "Total loss:  -2.4006 | PDE Loss:  -4.4025 | Function Loss:  -2.4461\n",
            "Total loss:  -2.4009 | PDE Loss:  -4.4008 | Function Loss:  -2.4467\n",
            "Total loss:  -2.4013 | PDE Loss:  -4.3992 | Function Loss:  -2.4473\n",
            "Total loss:  -2.4016 | PDE Loss:  -4.3954 | Function Loss:  -2.4481\n",
            "Total loss:  -2.402 | PDE Loss:  -4.3943 | Function Loss:  -2.4486\n",
            "Total loss:  -2.4022 | PDE Loss:  -4.3942 | Function Loss:  -2.4489\n",
            "Total loss:  -2.4027 | PDE Loss:  -4.3942 | Function Loss:  -2.4494\n",
            "Total loss:  -2.4031 | PDE Loss:  -4.3942 | Function Loss:  -2.4499\n",
            "Total loss:  -2.4034 | PDE Loss:  -4.3934 | Function Loss:  -2.4503\n",
            "Total loss:  -2.4036 | PDE Loss:  -4.3935 | Function Loss:  -2.4505\n",
            "Total loss:  -2.4038 | PDE Loss:  -4.3931 | Function Loss:  -2.4507\n",
            "Total loss:  -2.404 | PDE Loss:  -4.3928 | Function Loss:  -2.451\n",
            "Total loss:  -2.4043 | PDE Loss:  -4.3925 | Function Loss:  -2.4514\n",
            "Total loss:  -2.4048 | PDE Loss:  -4.3925 | Function Loss:  -2.4519\n",
            "Total loss:  -2.4054 | PDE Loss:  -4.3954 | Function Loss:  -2.4522\n",
            "Total loss:  -2.4058 | PDE Loss:  -4.3942 | Function Loss:  -2.4529\n",
            "Total loss:  -2.4063 | PDE Loss:  -4.3973 | Function Loss:  -2.453\n",
            "Total loss:  -2.4066 | PDE Loss:  -4.3958 | Function Loss:  -2.4536\n",
            "Total loss:  -2.4069 | PDE Loss:  -4.395 | Function Loss:  -2.454\n",
            "Total loss:  -2.4071 | PDE Loss:  -4.3954 | Function Loss:  -2.4542\n",
            "Total loss:  -2.4073 | PDE Loss:  -4.3939 | Function Loss:  -2.4546\n",
            "Total loss:  -2.4077 | PDE Loss:  -4.3924 | Function Loss:  -2.4552\n",
            "Total loss:  -2.408 | PDE Loss:  -4.3907 | Function Loss:  -2.4558\n",
            "Total loss:  -2.4083 | PDE Loss:  -4.3849 | Function Loss:  -2.4567\n",
            "Total loss:  -2.4086 | PDE Loss:  -4.3855 | Function Loss:  -2.457\n",
            "Total loss:  -2.4089 | PDE Loss:  -4.3874 | Function Loss:  -2.4571\n",
            "Total loss:  -2.4092 | PDE Loss:  -4.3901 | Function Loss:  -2.4572\n",
            "Total loss:  -2.4096 | PDE Loss:  -4.3932 | Function Loss:  -2.4572\n",
            "Total loss:  -2.4099 | PDE Loss:  -4.3966 | Function Loss:  -2.4572\n",
            "Total loss:  -2.4102 | PDE Loss:  -4.3997 | Function Loss:  -2.4571\n",
            "Total loss:  -2.4106 | PDE Loss:  -4.4044 | Function Loss:  -2.4571\n",
            "Total loss:  -2.411 | PDE Loss:  -4.4077 | Function Loss:  -2.4571\n",
            "Total loss:  -2.4113 | PDE Loss:  -4.4115 | Function Loss:  -2.4571\n",
            "Total loss:  -2.4117 | PDE Loss:  -4.4144 | Function Loss:  -2.4571\n",
            "Total loss:  -2.412 | PDE Loss:  -4.4167 | Function Loss:  -2.4573\n",
            "Total loss:  -2.4125 | PDE Loss:  -4.4234 | Function Loss:  -2.457\n",
            "Total loss:  -2.4127 | PDE Loss:  -4.4236 | Function Loss:  -2.4573\n",
            "Total loss:  -2.4133 | PDE Loss:  -4.4234 | Function Loss:  -2.4579\n",
            "Total loss:  -2.4138 | PDE Loss:  -4.424 | Function Loss:  -2.4584\n",
            "Total loss:  -2.4142 | PDE Loss:  -4.4239 | Function Loss:  -2.4589\n",
            "Total loss:  -2.4146 | PDE Loss:  -4.4255 | Function Loss:  -2.4591\n",
            "Total loss:  -2.4149 | PDE Loss:  -4.4253 | Function Loss:  -2.4595\n",
            "Total loss:  -2.4152 | PDE Loss:  -4.425 | Function Loss:  -2.4599\n",
            "Total loss:  -2.4155 | PDE Loss:  -4.4241 | Function Loss:  -2.4603\n",
            "Total loss:  -2.4157 | PDE Loss:  -4.4221 | Function Loss:  -2.4608\n",
            "Total loss:  -2.416 | PDE Loss:  -4.4201 | Function Loss:  -2.4613\n",
            "Total loss:  -2.4163 | PDE Loss:  -4.4168 | Function Loss:  -2.462\n",
            "Total loss:  -2.4167 | PDE Loss:  -4.4132 | Function Loss:  -2.4629\n",
            "Total loss:  -2.4169 | PDE Loss:  -4.4006 | Function Loss:  -2.4645\n",
            "Total loss:  -2.4174 | PDE Loss:  -4.4007 | Function Loss:  -2.465\n",
            "Total loss:  -2.4178 | PDE Loss:  -4.4014 | Function Loss:  -2.4654\n",
            "Total loss:  -2.4181 | PDE Loss:  -4.401 | Function Loss:  -2.4658\n",
            "Total loss:  -2.4184 | PDE Loss:  -4.4001 | Function Loss:  -2.4662\n",
            "Total loss:  -2.4187 | PDE Loss:  -4.3984 | Function Loss:  -2.4668\n",
            "Total loss:  -2.4189 | PDE Loss:  -4.3923 | Function Loss:  -2.4677\n",
            "Total loss:  -2.4192 | PDE Loss:  -4.393 | Function Loss:  -2.468\n",
            "Total loss:  -2.4195 | PDE Loss:  -4.3928 | Function Loss:  -2.4684\n",
            "Total loss:  -2.4198 | PDE Loss:  -4.3913 | Function Loss:  -2.4689\n",
            "Total loss:  -2.4201 | PDE Loss:  -4.3902 | Function Loss:  -2.4693\n",
            "Total loss:  -2.4203 | PDE Loss:  -4.3885 | Function Loss:  -2.4697\n",
            "Total loss:  -2.4205 | PDE Loss:  -4.3867 | Function Loss:  -2.4702\n",
            "Total loss:  -2.4207 | PDE Loss:  -4.3855 | Function Loss:  -2.4706\n",
            "Total loss:  -2.4209 | PDE Loss:  -4.3836 | Function Loss:  -2.4711\n",
            "Total loss:  -2.4212 | PDE Loss:  -4.383 | Function Loss:  -2.4715\n",
            "Total loss:  -2.4216 | PDE Loss:  -4.3825 | Function Loss:  -2.472\n",
            "Total loss:  -2.4222 | PDE Loss:  -4.3815 | Function Loss:  -2.4727\n",
            "Total loss:  -2.4226 | PDE Loss:  -4.3814 | Function Loss:  -2.4732\n",
            "Total loss:  -2.4231 | PDE Loss:  -4.3808 | Function Loss:  -2.4738\n",
            "Total loss:  -2.4237 | PDE Loss:  -4.3805 | Function Loss:  -2.4745\n",
            "Total loss:  -2.4245 | PDE Loss:  -4.38 | Function Loss:  -2.4755\n",
            "Total loss:  -2.4255 | PDE Loss:  -4.3781 | Function Loss:  -2.4768\n",
            "Total loss:  -2.4265 | PDE Loss:  -4.3759 | Function Loss:  -2.4782\n",
            "Total loss:  -2.4274 | PDE Loss:  -4.3705 | Function Loss:  -2.4799\n",
            "Total loss:  -2.4282 | PDE Loss:  -4.3681 | Function Loss:  -2.4812\n",
            "Total loss:  -2.4286 | PDE Loss:  -4.3645 | Function Loss:  -2.4821\n",
            "Total loss:  -2.4291 | PDE Loss:  -4.3648 | Function Loss:  -2.4827\n",
            "Total loss:  -2.4297 | PDE Loss:  -4.363 | Function Loss:  -2.4835\n",
            "Total loss:  -2.4303 | PDE Loss:  -4.3658 | Function Loss:  -2.4839\n",
            "Total loss:  -2.4311 | PDE Loss:  -4.3626 | Function Loss:  -2.4851\n",
            "Total loss:  -2.4318 | PDE Loss:  -4.3682 | Function Loss:  -2.4852\n",
            "Total loss:  -2.4326 | PDE Loss:  -4.374 | Function Loss:  -2.4853\n",
            "Total loss:  -2.4336 | PDE Loss:  -4.3843 | Function Loss:  -2.4852\n",
            "Total loss:  -2.4344 | PDE Loss:  -4.3899 | Function Loss:  -2.4854\n",
            "Total loss:  -2.4351 | PDE Loss:  -4.3962 | Function Loss:  -2.4854\n",
            "Total loss:  -2.436 | PDE Loss:  -4.4003 | Function Loss:  -2.486\n",
            "Total loss:  -2.4371 | PDE Loss:  -4.4021 | Function Loss:  -2.4869\n",
            "Total loss:  -2.4379 | PDE Loss:  -4.4024 | Function Loss:  -2.4878\n",
            "Total loss:  -2.4387 | PDE Loss:  -4.4012 | Function Loss:  -2.4888\n",
            "Total loss:  -2.4393 | PDE Loss:  -4.3965 | Function Loss:  -2.49\n",
            "Total loss:  -2.4399 | PDE Loss:  -4.3927 | Function Loss:  -2.4912\n",
            "Total loss:  -2.4407 | PDE Loss:  -4.3893 | Function Loss:  -2.4925\n",
            "Total loss:  -2.4399 | PDE Loss:  -4.3624 | Function Loss:  -2.4952\n",
            "Total loss:  -2.4412 | PDE Loss:  -4.3808 | Function Loss:  -2.4942\n",
            "Total loss:  -2.4421 | PDE Loss:  -4.3813 | Function Loss:  -2.4952\n",
            "Total loss:  -2.443 | PDE Loss:  -4.3814 | Function Loss:  -2.4961\n",
            "Total loss:  -2.4437 | PDE Loss:  -4.3854 | Function Loss:  -2.4964\n",
            "Total loss:  -2.4442 | PDE Loss:  -4.3871 | Function Loss:  -2.4968\n",
            "Total loss:  -2.4445 | PDE Loss:  -4.3887 | Function Loss:  -2.4969\n",
            "Total loss:  -2.4448 | PDE Loss:  -4.3919 | Function Loss:  -2.4969\n",
            "Total loss:  -2.4452 | PDE Loss:  -4.3924 | Function Loss:  -2.4973\n",
            "Total loss:  -2.4456 | PDE Loss:  -4.3941 | Function Loss:  -2.4974\n",
            "Total loss:  -2.4459 | PDE Loss:  -4.3945 | Function Loss:  -2.4977\n",
            "Total loss:  -2.4461 | PDE Loss:  -4.3925 | Function Loss:  -2.4982\n",
            "Total loss:  -2.4463 | PDE Loss:  -4.3916 | Function Loss:  -2.4985\n",
            "Total loss:  -2.4464 | PDE Loss:  -4.3902 | Function Loss:  -2.4989\n",
            "Total loss:  -2.4466 | PDE Loss:  -4.3896 | Function Loss:  -2.4992\n",
            "Total loss:  -2.4469 | PDE Loss:  -4.3881 | Function Loss:  -2.4997\n",
            "Total loss:  -2.4472 | PDE Loss:  -4.3879 | Function Loss:  -2.5001\n",
            "Total loss:  -2.4476 | PDE Loss:  -4.387 | Function Loss:  -2.5006\n",
            "Total loss:  -2.4479 | PDE Loss:  -4.3879 | Function Loss:  -2.5008\n",
            "Total loss:  -2.4482 | PDE Loss:  -4.3891 | Function Loss:  -2.501\n",
            "Total loss:  -2.4486 | PDE Loss:  -4.3904 | Function Loss:  -2.5013\n",
            "Total loss:  -2.449 | PDE Loss:  -4.3933 | Function Loss:  -2.5015\n",
            "Total loss:  -2.4495 | PDE Loss:  -4.398 | Function Loss:  -2.5013\n",
            "Total loss:  -2.4499 | PDE Loss:  -4.401 | Function Loss:  -2.5015\n",
            "Total loss:  -2.4504 | PDE Loss:  -4.405 | Function Loss:  -2.5016\n",
            "Total loss:  -2.451 | PDE Loss:  -4.4085 | Function Loss:  -2.5018\n",
            "Total loss:  -2.4514 | PDE Loss:  -4.4106 | Function Loss:  -2.502\n",
            "Total loss:  -2.4518 | PDE Loss:  -4.4107 | Function Loss:  -2.5024\n",
            "Total loss:  -2.4522 | PDE Loss:  -4.4125 | Function Loss:  -2.5026\n",
            "Total loss:  -2.4525 | PDE Loss:  -4.4118 | Function Loss:  -2.503\n",
            "Total loss:  -2.4529 | PDE Loss:  -4.4133 | Function Loss:  -2.5033\n",
            "Total loss:  -2.4532 | PDE Loss:  -4.4132 | Function Loss:  -2.5036\n",
            "Total loss:  -2.4534 | PDE Loss:  -4.4146 | Function Loss:  -2.5037\n",
            "Total loss:  -2.4536 | PDE Loss:  -4.415 | Function Loss:  -2.5038\n",
            "Total loss:  -2.4538 | PDE Loss:  -4.4159 | Function Loss:  -2.504\n",
            "Total loss:  -2.4542 | PDE Loss:  -4.4162 | Function Loss:  -2.5043\n",
            "Total loss:  -2.4545 | PDE Loss:  -4.415 | Function Loss:  -2.5049\n",
            "Total loss:  -2.4549 | PDE Loss:  -4.4135 | Function Loss:  -2.5055\n",
            "Total loss:  -2.4552 | PDE Loss:  -4.4103 | Function Loss:  -2.5063\n",
            "Total loss:  -2.4555 | PDE Loss:  -4.4082 | Function Loss:  -2.5068\n",
            "Total loss:  -2.4557 | PDE Loss:  -4.4046 | Function Loss:  -2.5075\n",
            "Total loss:  -2.4559 | PDE Loss:  -4.403 | Function Loss:  -2.5079\n",
            "Total loss:  -2.456 | PDE Loss:  -4.4006 | Function Loss:  -2.5084\n",
            "Total loss:  -2.4562 | PDE Loss:  -4.3995 | Function Loss:  -2.5087\n",
            "Total loss:  -2.4563 | PDE Loss:  -4.3981 | Function Loss:  -2.5091\n",
            "Total loss:  -2.4565 | PDE Loss:  -4.3973 | Function Loss:  -2.5094\n",
            "Total loss:  -2.4567 | PDE Loss:  -4.3965 | Function Loss:  -2.5097\n",
            "Total loss:  -2.4569 | PDE Loss:  -4.3964 | Function Loss:  -2.5099\n",
            "Total loss:  -2.4571 | PDE Loss:  -4.3973 | Function Loss:  -2.5101\n",
            "Total loss:  -2.4574 | PDE Loss:  -4.3987 | Function Loss:  -2.5102\n",
            "Total loss:  -2.4577 | PDE Loss:  -4.3992 | Function Loss:  -2.5105\n",
            "Total loss:  -2.4579 | PDE Loss:  -4.3996 | Function Loss:  -2.5107\n",
            "Total loss:  -2.4583 | PDE Loss:  -4.4008 | Function Loss:  -2.5109\n",
            "Total loss:  -2.4587 | PDE Loss:  -4.3976 | Function Loss:  -2.5118\n",
            "Total loss:  -2.4591 | PDE Loss:  -4.3971 | Function Loss:  -2.5123\n",
            "Total loss:  -2.4595 | PDE Loss:  -4.3942 | Function Loss:  -2.5131\n",
            "Total loss:  -2.4598 | PDE Loss:  -4.3923 | Function Loss:  -2.5138\n",
            "Total loss:  -2.4601 | PDE Loss:  -4.3896 | Function Loss:  -2.5145\n",
            "Total loss:  -2.4603 | PDE Loss:  -4.3884 | Function Loss:  -2.5149\n",
            "Total loss:  -2.4605 | PDE Loss:  -4.3878 | Function Loss:  -2.5152\n",
            "Total loss:  -2.4607 | PDE Loss:  -4.3878 | Function Loss:  -2.5153\n",
            "Total loss:  -2.4609 | PDE Loss:  -4.3886 | Function Loss:  -2.5155\n",
            "Total loss:  -2.4611 | PDE Loss:  -4.389 | Function Loss:  -2.5157\n",
            "Total loss:  -2.4614 | PDE Loss:  -4.3896 | Function Loss:  -2.5159\n",
            "Total loss:  -2.4617 | PDE Loss:  -4.3899 | Function Loss:  -2.5162\n",
            "Total loss:  -2.462 | PDE Loss:  -4.3893 | Function Loss:  -2.5167\n",
            "Total loss:  -2.4625 | PDE Loss:  -4.388 | Function Loss:  -2.5173\n",
            "Total loss:  -2.4629 | PDE Loss:  -4.386 | Function Loss:  -2.5181\n",
            "Total loss:  -2.4632 | PDE Loss:  -4.3837 | Function Loss:  -2.5188\n",
            "Total loss:  -2.4635 | PDE Loss:  -4.3817 | Function Loss:  -2.5194\n",
            "Total loss:  -2.4638 | PDE Loss:  -4.379 | Function Loss:  -2.52\n",
            "Total loss:  -2.464 | PDE Loss:  -4.3774 | Function Loss:  -2.5205\n",
            "Total loss:  -2.4641 | PDE Loss:  -4.3764 | Function Loss:  -2.5208\n",
            "Total loss:  -2.4643 | PDE Loss:  -4.3751 | Function Loss:  -2.5212\n",
            "Total loss:  -2.4644 | PDE Loss:  -4.3758 | Function Loss:  -2.5213\n",
            "Total loss:  -2.4646 | PDE Loss:  -4.3757 | Function Loss:  -2.5214\n",
            "Total loss:  -2.4647 | PDE Loss:  -4.3765 | Function Loss:  -2.5215\n",
            "Total loss:  -2.4649 | PDE Loss:  -4.3775 | Function Loss:  -2.5215\n",
            "Total loss:  -2.4651 | PDE Loss:  -4.3787 | Function Loss:  -2.5217\n",
            "Total loss:  -2.4653 | PDE Loss:  -4.3777 | Function Loss:  -2.5219\n",
            "Total loss:  -2.4657 | PDE Loss:  -4.3797 | Function Loss:  -2.5221\n",
            "Total loss:  -2.4661 | PDE Loss:  -4.3817 | Function Loss:  -2.5223\n",
            "Total loss:  -2.4664 | PDE Loss:  -4.3834 | Function Loss:  -2.5225\n",
            "Total loss:  -2.4668 | PDE Loss:  -4.3842 | Function Loss:  -2.5228\n",
            "Total loss:  -2.4673 | PDE Loss:  -4.3864 | Function Loss:  -2.5231\n",
            "Total loss:  -2.4679 | PDE Loss:  -4.3863 | Function Loss:  -2.5238\n",
            "Total loss:  -2.4686 | PDE Loss:  -4.3892 | Function Loss:  -2.5242\n",
            "Total loss:  -2.4693 | PDE Loss:  -4.3884 | Function Loss:  -2.5251\n",
            "Total loss:  -2.4699 | PDE Loss:  -4.3874 | Function Loss:  -2.5259\n",
            "Total loss:  -2.4708 | PDE Loss:  -4.3908 | Function Loss:  -2.5264\n",
            "Total loss:  -2.4713 | PDE Loss:  -4.3933 | Function Loss:  -2.5266\n",
            "Total loss:  -2.4719 | PDE Loss:  -4.3941 | Function Loss:  -2.5273\n",
            "Total loss:  -2.4724 | PDE Loss:  -4.396 | Function Loss:  -2.5276\n",
            "Total loss:  -2.4729 | PDE Loss:  -4.3971 | Function Loss:  -2.528\n",
            "Total loss:  -2.4733 | PDE Loss:  -4.3979 | Function Loss:  -2.5283\n",
            "Total loss:  -2.4738 | PDE Loss:  -4.4021 | Function Loss:  -2.5282\n",
            "Total loss:  -2.4743 | PDE Loss:  -4.4044 | Function Loss:  -2.5285\n",
            "Total loss:  -2.4746 | PDE Loss:  -4.4068 | Function Loss:  -2.5286\n",
            "Total loss:  -2.4751 | PDE Loss:  -4.41 | Function Loss:  -2.5287\n",
            "Total loss:  -2.4756 | PDE Loss:  -4.4136 | Function Loss:  -2.5288\n",
            "Total loss:  -2.476 | PDE Loss:  -4.4174 | Function Loss:  -2.5288\n",
            "Total loss:  -2.4765 | PDE Loss:  -4.4212 | Function Loss:  -2.5288\n",
            "Total loss:  -2.4769 | PDE Loss:  -4.4255 | Function Loss:  -2.5287\n",
            "Total loss:  -2.4772 | PDE Loss:  -4.4281 | Function Loss:  -2.5287\n",
            "Total loss:  -2.4775 | PDE Loss:  -4.4295 | Function Loss:  -2.5289\n",
            "Total loss:  -2.4778 | PDE Loss:  -4.4307 | Function Loss:  -2.5291\n",
            "Total loss:  -2.4781 | PDE Loss:  -4.431 | Function Loss:  -2.5294\n",
            "Total loss:  -2.4783 | PDE Loss:  -4.4314 | Function Loss:  -2.5296\n",
            "Total loss:  -2.4787 | PDE Loss:  -4.4317 | Function Loss:  -2.53\n",
            "Total loss:  -2.4791 | PDE Loss:  -4.4319 | Function Loss:  -2.5304\n",
            "Total loss:  -2.4795 | PDE Loss:  -4.4329 | Function Loss:  -2.5308\n",
            "Total loss:  -2.48 | PDE Loss:  -4.432 | Function Loss:  -2.5315\n",
            "Total loss:  -2.4805 | PDE Loss:  -4.4319 | Function Loss:  -2.5321\n",
            "Total loss:  -2.481 | PDE Loss:  -4.4305 | Function Loss:  -2.5327\n",
            "Total loss:  -2.4814 | PDE Loss:  -4.4291 | Function Loss:  -2.5333\n",
            "Total loss:  -2.4817 | PDE Loss:  -4.4283 | Function Loss:  -2.5338\n",
            "Total loss:  -2.4821 | PDE Loss:  -4.4275 | Function Loss:  -2.5344\n",
            "Total loss:  -2.4825 | PDE Loss:  -4.4271 | Function Loss:  -2.5349\n",
            "Total loss:  -2.4829 | PDE Loss:  -4.4259 | Function Loss:  -2.5355\n",
            "Total loss:  -2.4833 | PDE Loss:  -4.4267 | Function Loss:  -2.5358\n",
            "Total loss:  -2.4836 | PDE Loss:  -4.4264 | Function Loss:  -2.5362\n",
            "Total loss:  -2.4838 | PDE Loss:  -4.427 | Function Loss:  -2.5364\n",
            "Total loss:  -2.4841 | PDE Loss:  -4.4271 | Function Loss:  -2.5366\n",
            "Total loss:  -2.4844 | PDE Loss:  -4.4274 | Function Loss:  -2.537\n",
            "Total loss:  -2.4847 | PDE Loss:  -4.428 | Function Loss:  -2.5372\n",
            "Total loss:  -2.485 | PDE Loss:  -4.4278 | Function Loss:  -2.5376\n",
            "Total loss:  -2.4854 | PDE Loss:  -4.4281 | Function Loss:  -2.538\n",
            "Total loss:  -2.4858 | PDE Loss:  -4.4275 | Function Loss:  -2.5385\n",
            "Total loss:  -2.4864 | PDE Loss:  -4.4279 | Function Loss:  -2.5391\n",
            "Total loss:  -2.4869 | PDE Loss:  -4.4266 | Function Loss:  -2.5399\n",
            "Total loss:  -2.4874 | PDE Loss:  -4.4298 | Function Loss:  -2.5401\n",
            "Total loss:  -2.4878 | PDE Loss:  -4.4281 | Function Loss:  -2.5407\n",
            "Total loss:  -2.4882 | PDE Loss:  -4.4267 | Function Loss:  -2.5413\n",
            "Total loss:  -2.4885 | PDE Loss:  -4.4246 | Function Loss:  -2.542\n",
            "Total loss:  -2.4889 | PDE Loss:  -4.4242 | Function Loss:  -2.5424\n",
            "Total loss:  -2.4891 | PDE Loss:  -4.4207 | Function Loss:  -2.5432\n",
            "Total loss:  -2.4894 | PDE Loss:  -4.4224 | Function Loss:  -2.5432\n",
            "Total loss:  -2.4896 | PDE Loss:  -4.424 | Function Loss:  -2.5433\n",
            "Total loss:  -2.49 | PDE Loss:  -4.4282 | Function Loss:  -2.5432\n",
            "Total loss:  -2.4904 | PDE Loss:  -4.431 | Function Loss:  -2.5433\n",
            "Total loss:  -2.4908 | PDE Loss:  -4.4342 | Function Loss:  -2.5433\n",
            "Total loss:  -2.4911 | PDE Loss:  -4.4365 | Function Loss:  -2.5434\n",
            "Total loss:  -2.4914 | PDE Loss:  -4.4391 | Function Loss:  -2.5434\n",
            "Total loss:  -2.4918 | PDE Loss:  -4.4415 | Function Loss:  -2.5435\n",
            "Total loss:  -2.4921 | PDE Loss:  -4.4436 | Function Loss:  -2.5436\n",
            "Total loss:  -2.4925 | PDE Loss:  -4.4462 | Function Loss:  -2.5437\n",
            "Total loss:  -2.4929 | PDE Loss:  -4.4479 | Function Loss:  -2.544\n",
            "Total loss:  -2.4933 | PDE Loss:  -4.4487 | Function Loss:  -2.5443\n",
            "Total loss:  -2.4937 | PDE Loss:  -4.4499 | Function Loss:  -2.5446\n",
            "Total loss:  -2.4942 | PDE Loss:  -4.4504 | Function Loss:  -2.5451\n",
            "Total loss:  -2.4948 | PDE Loss:  -4.4519 | Function Loss:  -2.5456\n",
            "Total loss:  -2.4953 | PDE Loss:  -4.4516 | Function Loss:  -2.5462\n",
            "Total loss:  -2.4957 | PDE Loss:  -4.4532 | Function Loss:  -2.5465\n",
            "Total loss:  -2.4961 | PDE Loss:  -4.4547 | Function Loss:  -2.5467\n",
            "Total loss:  -2.4965 | PDE Loss:  -4.456 | Function Loss:  -2.547\n",
            "Total loss:  -2.4968 | PDE Loss:  -4.4571 | Function Loss:  -2.5472\n",
            "Total loss:  -2.497 | PDE Loss:  -4.4577 | Function Loss:  -2.5474\n",
            "Total loss:  -2.4973 | PDE Loss:  -4.4581 | Function Loss:  -2.5477\n",
            "Total loss:  -2.4976 | PDE Loss:  -4.4574 | Function Loss:  -2.5481\n",
            "Total loss:  -2.4979 | PDE Loss:  -4.4575 | Function Loss:  -2.5484\n",
            "Total loss:  -2.4982 | PDE Loss:  -4.4569 | Function Loss:  -2.5488\n",
            "Total loss:  -2.4984 | PDE Loss:  -4.4553 | Function Loss:  -2.5493\n",
            "Total loss:  -2.4988 | PDE Loss:  -4.4556 | Function Loss:  -2.5497\n",
            "Total loss:  -2.4992 | PDE Loss:  -4.456 | Function Loss:  -2.55\n",
            "Total loss:  -2.4995 | PDE Loss:  -4.4549 | Function Loss:  -2.5505\n",
            "Total loss:  -2.4997 | PDE Loss:  -4.4545 | Function Loss:  -2.5508\n",
            "Total loss:  -2.5001 | PDE Loss:  -4.4536 | Function Loss:  -2.5514\n",
            "Total loss:  -2.5007 | PDE Loss:  -4.4526 | Function Loss:  -2.5522\n",
            "Total loss:  -2.5014 | PDE Loss:  -4.4532 | Function Loss:  -2.5529\n",
            "Total loss:  -2.5021 | PDE Loss:  -4.4542 | Function Loss:  -2.5535\n",
            "Total loss:  -2.5027 | PDE Loss:  -4.4574 | Function Loss:  -2.5538\n",
            "Total loss:  -2.5034 | PDE Loss:  -4.4603 | Function Loss:  -2.5542\n",
            "Total loss:  -2.5039 | PDE Loss:  -4.4632 | Function Loss:  -2.5544\n",
            "Total loss:  -2.5043 | PDE Loss:  -4.4668 | Function Loss:  -2.5544\n",
            "Total loss:  -2.5047 | PDE Loss:  -4.4678 | Function Loss:  -2.5547\n",
            "Total loss:  -2.5053 | PDE Loss:  -4.4706 | Function Loss:  -2.5551\n",
            "Total loss:  -2.506 | PDE Loss:  -4.4729 | Function Loss:  -2.5556\n",
            "Total loss:  -2.5065 | PDE Loss:  -4.4771 | Function Loss:  -2.5556\n",
            "Total loss:  -2.507 | PDE Loss:  -4.477 | Function Loss:  -2.5562\n",
            "Total loss:  -2.5074 | PDE Loss:  -4.4782 | Function Loss:  -2.5565\n",
            "Total loss:  -2.5079 | PDE Loss:  -4.4772 | Function Loss:  -2.5572\n",
            "Total loss:  -2.5084 | PDE Loss:  -4.4805 | Function Loss:  -2.5574\n",
            "Total loss:  -2.5088 | PDE Loss:  -4.4803 | Function Loss:  -2.5578\n",
            "Total loss:  -2.5092 | PDE Loss:  -4.4809 | Function Loss:  -2.5582\n",
            "Total loss:  -2.5096 | PDE Loss:  -4.4807 | Function Loss:  -2.5587\n",
            "Total loss:  -2.51 | PDE Loss:  -4.4808 | Function Loss:  -2.5591\n",
            "Total loss:  -2.5105 | PDE Loss:  -4.481 | Function Loss:  -2.5596\n",
            "Total loss:  -2.5109 | PDE Loss:  -4.4812 | Function Loss:  -2.5601\n",
            "Total loss:  -2.5112 | PDE Loss:  -4.4819 | Function Loss:  -2.5603\n",
            "Total loss:  -2.5115 | PDE Loss:  -4.482 | Function Loss:  -2.5606\n",
            "Total loss:  -2.5117 | PDE Loss:  -4.4828 | Function Loss:  -2.5608\n",
            "Total loss:  -2.5119 | PDE Loss:  -4.4831 | Function Loss:  -2.561\n",
            "Total loss:  -2.5122 | PDE Loss:  -4.4832 | Function Loss:  -2.5613\n",
            "Total loss:  -2.5126 | PDE Loss:  -4.4845 | Function Loss:  -2.5616\n",
            "Total loss:  -2.513 | PDE Loss:  -4.4839 | Function Loss:  -2.5622\n",
            "Total loss:  -2.5134 | PDE Loss:  -4.4849 | Function Loss:  -2.5625\n",
            "Total loss:  -2.514 | PDE Loss:  -4.4864 | Function Loss:  -2.563\n",
            "Total loss:  -2.5148 | PDE Loss:  -4.489 | Function Loss:  -2.5635\n",
            "Total loss:  -2.5153 | PDE Loss:  -4.4904 | Function Loss:  -2.5639\n",
            "Total loss:  -2.5157 | PDE Loss:  -4.4909 | Function Loss:  -2.5643\n",
            "Total loss:  -2.5159 | PDE Loss:  -4.4905 | Function Loss:  -2.5646\n",
            "Total loss:  -2.5162 | PDE Loss:  -4.4908 | Function Loss:  -2.5649\n",
            "Total loss:  -2.5165 | PDE Loss:  -4.4908 | Function Loss:  -2.5652\n",
            "Total loss:  -2.5168 | PDE Loss:  -4.4896 | Function Loss:  -2.5657\n",
            "Total loss:  -2.5171 | PDE Loss:  -4.4902 | Function Loss:  -2.566\n",
            "Total loss:  -2.5174 | PDE Loss:  -4.4879 | Function Loss:  -2.5666\n",
            "Total loss:  -2.5177 | PDE Loss:  -4.488 | Function Loss:  -2.5668\n",
            "Total loss:  -2.5179 | PDE Loss:  -4.4838 | Function Loss:  -2.5676\n",
            "Total loss:  -2.5181 | PDE Loss:  -4.4822 | Function Loss:  -2.5681\n",
            "Total loss:  -2.5183 | PDE Loss:  -4.4814 | Function Loss:  -2.5684\n",
            "Total loss:  -2.5185 | PDE Loss:  -4.4802 | Function Loss:  -2.5687\n",
            "Total loss:  -2.5187 | PDE Loss:  -4.4802 | Function Loss:  -2.569\n",
            "Total loss:  -2.5189 | PDE Loss:  -4.4814 | Function Loss:  -2.5691\n",
            "Total loss:  -2.5192 | PDE Loss:  -4.4826 | Function Loss:  -2.5692\n",
            "Total loss:  -2.5193 | PDE Loss:  -4.485 | Function Loss:  -2.5691\n",
            "Total loss:  -2.5195 | PDE Loss:  -4.488 | Function Loss:  -2.5689\n",
            "Total loss:  -2.5198 | PDE Loss:  -4.4904 | Function Loss:  -2.5689\n",
            "Total loss:  -2.5199 | PDE Loss:  -4.4994 | Function Loss:  -2.5679\n",
            "Total loss:  -2.5201 | PDE Loss:  -4.4994 | Function Loss:  -2.5682\n",
            "Total loss:  -2.5203 | PDE Loss:  -4.4969 | Function Loss:  -2.5687\n",
            "Total loss:  -2.5204 | PDE Loss:  -4.4967 | Function Loss:  -2.5689\n",
            "Total loss:  -2.5205 | PDE Loss:  -4.4953 | Function Loss:  -2.5691\n",
            "Total loss:  -2.5206 | PDE Loss:  -4.4931 | Function Loss:  -2.5696\n",
            "Total loss:  -2.5208 | PDE Loss:  -4.4913 | Function Loss:  -2.57\n",
            "Total loss:  -2.521 | PDE Loss:  -4.4887 | Function Loss:  -2.5705\n",
            "Total loss:  -2.5212 | PDE Loss:  -4.487 | Function Loss:  -2.5709\n",
            "Total loss:  -2.5214 | PDE Loss:  -4.4853 | Function Loss:  -2.5714\n",
            "Total loss:  -2.5217 | PDE Loss:  -4.4849 | Function Loss:  -2.5718\n",
            "Total loss:  -2.5218 | PDE Loss:  -4.4733 | Function Loss:  -2.5733\n",
            "Total loss:  -2.5222 | PDE Loss:  -4.477 | Function Loss:  -2.5733\n",
            "Total loss:  -2.5226 | PDE Loss:  -4.4815 | Function Loss:  -2.5731\n",
            "Total loss:  -2.523 | PDE Loss:  -4.484 | Function Loss:  -2.5733\n",
            "Total loss:  -2.5233 | PDE Loss:  -4.4855 | Function Loss:  -2.5735\n",
            "Total loss:  -2.5238 | PDE Loss:  -4.487 | Function Loss:  -2.5738\n",
            "Total loss:  -2.5237 | PDE Loss:  -4.4746 | Function Loss:  -2.5752\n",
            "Total loss:  -2.524 | PDE Loss:  -4.4826 | Function Loss:  -2.5746\n",
            "Total loss:  -2.5244 | PDE Loss:  -4.4843 | Function Loss:  -2.5748\n",
            "Total loss:  -2.5247 | PDE Loss:  -4.4826 | Function Loss:  -2.5754\n",
            "Total loss:  -2.525 | PDE Loss:  -4.4828 | Function Loss:  -2.5757\n",
            "Total loss:  -2.5253 | PDE Loss:  -4.48 | Function Loss:  -2.5763\n",
            "Total loss:  -2.5255 | PDE Loss:  -4.476 | Function Loss:  -2.5772\n",
            "Total loss:  -2.5258 | PDE Loss:  -4.4746 | Function Loss:  -2.5776\n",
            "Total loss:  -2.5261 | PDE Loss:  -4.4724 | Function Loss:  -2.5783\n",
            "Total loss:  -2.5262 | PDE Loss:  -4.4656 | Function Loss:  -2.5792\n",
            "Total loss:  -2.5263 | PDE Loss:  -4.4694 | Function Loss:  -2.5789\n",
            "Total loss:  -2.5266 | PDE Loss:  -4.4705 | Function Loss:  -2.5791\n",
            "Total loss:  -2.527 | PDE Loss:  -4.4732 | Function Loss:  -2.5791\n",
            "Total loss:  -2.5273 | PDE Loss:  -4.4758 | Function Loss:  -2.5792\n",
            "Total loss:  -2.5278 | PDE Loss:  -4.48 | Function Loss:  -2.5792\n",
            "Total loss:  -2.5282 | PDE Loss:  -4.482 | Function Loss:  -2.5794\n",
            "Total loss:  -2.5286 | PDE Loss:  -4.4868 | Function Loss:  -2.5793\n",
            "Total loss:  -2.5289 | PDE Loss:  -4.4876 | Function Loss:  -2.5795\n",
            "Total loss:  -2.5292 | PDE Loss:  -4.4884 | Function Loss:  -2.5797\n",
            "Total loss:  -2.5296 | PDE Loss:  -4.4885 | Function Loss:  -2.5802\n",
            "Total loss:  -2.5301 | PDE Loss:  -4.4888 | Function Loss:  -2.5807\n",
            "Total loss:  -2.5307 | PDE Loss:  -4.489 | Function Loss:  -2.5813\n",
            "Total loss:  -2.5311 | PDE Loss:  -4.4895 | Function Loss:  -2.5818\n",
            "Total loss:  -2.5315 | PDE Loss:  -4.4912 | Function Loss:  -2.582\n",
            "Total loss:  -2.5319 | PDE Loss:  -4.4893 | Function Loss:  -2.5826\n",
            "Total loss:  -2.5322 | PDE Loss:  -4.4914 | Function Loss:  -2.5827\n",
            "Total loss:  -2.5324 | PDE Loss:  -4.4948 | Function Loss:  -2.5825\n",
            "Total loss:  -2.5327 | PDE Loss:  -4.4956 | Function Loss:  -2.5827\n",
            "Total loss:  -2.533 | PDE Loss:  -4.4951 | Function Loss:  -2.5831\n",
            "Total loss:  -2.5332 | PDE Loss:  -4.4957 | Function Loss:  -2.5834\n",
            "Total loss:  -2.5336 | PDE Loss:  -4.4941 | Function Loss:  -2.584\n",
            "Total loss:  -2.5339 | PDE Loss:  -4.4933 | Function Loss:  -2.5845\n",
            "Total loss:  -2.5346 | PDE Loss:  -4.4913 | Function Loss:  -2.5855\n",
            "Total loss:  -2.5353 | PDE Loss:  -4.4887 | Function Loss:  -2.5866\n",
            "Total loss:  -2.5358 | PDE Loss:  -4.4889 | Function Loss:  -2.5871\n",
            "Total loss:  -2.5362 | PDE Loss:  -4.4902 | Function Loss:  -2.5874\n",
            "Total loss:  -2.5367 | PDE Loss:  -4.4919 | Function Loss:  -2.5877\n",
            "Total loss:  -2.5373 | PDE Loss:  -4.4965 | Function Loss:  -2.5878\n",
            "Total loss:  -2.5378 | PDE Loss:  -4.5007 | Function Loss:  -2.5879\n",
            "Total loss:  -2.5383 | PDE Loss:  -4.5055 | Function Loss:  -2.5878\n",
            "Total loss:  -2.5386 | PDE Loss:  -4.5077 | Function Loss:  -2.5879\n",
            "Total loss:  -2.5389 | PDE Loss:  -4.5099 | Function Loss:  -2.588\n",
            "Total loss:  -2.5391 | PDE Loss:  -4.5118 | Function Loss:  -2.588\n",
            "Total loss:  -2.5393 | PDE Loss:  -4.5124 | Function Loss:  -2.5882\n",
            "Total loss:  -2.5394 | PDE Loss:  -4.5137 | Function Loss:  -2.5881\n",
            "Total loss:  -2.5395 | PDE Loss:  -4.5141 | Function Loss:  -2.5882\n",
            "Total loss:  -2.5398 | PDE Loss:  -4.5147 | Function Loss:  -2.5885\n",
            "Total loss:  -2.5401 | PDE Loss:  -4.5164 | Function Loss:  -2.5886\n",
            "Total loss:  -2.5405 | PDE Loss:  -4.5192 | Function Loss:  -2.5887\n",
            "Total loss:  -2.5408 | PDE Loss:  -4.5294 | Function Loss:  -2.5878\n",
            "Total loss:  -2.5411 | PDE Loss:  -4.5291 | Function Loss:  -2.5883\n",
            "Total loss:  -2.5415 | PDE Loss:  -4.5296 | Function Loss:  -2.5886\n",
            "Total loss:  -2.5418 | PDE Loss:  -4.5293 | Function Loss:  -2.589\n",
            "Total loss:  -2.542 | PDE Loss:  -4.5295 | Function Loss:  -2.5892\n",
            "Total loss:  -2.5423 | PDE Loss:  -4.5298 | Function Loss:  -2.5894\n",
            "Total loss:  -2.5426 | PDE Loss:  -4.53 | Function Loss:  -2.5897\n",
            "Total loss:  -2.5429 | PDE Loss:  -4.5302 | Function Loss:  -2.5901\n",
            "Total loss:  -2.5433 | PDE Loss:  -4.5301 | Function Loss:  -2.5905\n",
            "Total loss:  -2.5437 | PDE Loss:  -4.5293 | Function Loss:  -2.591\n",
            "Total loss:  -2.5441 | PDE Loss:  -4.5278 | Function Loss:  -2.5917\n",
            "Total loss:  -2.5445 | PDE Loss:  -4.5256 | Function Loss:  -2.5924\n",
            "Total loss:  -2.5449 | PDE Loss:  -4.522 | Function Loss:  -2.5932\n",
            "Total loss:  -2.5452 | PDE Loss:  -4.5195 | Function Loss:  -2.594\n",
            "Total loss:  -2.5457 | PDE Loss:  -4.516 | Function Loss:  -2.5949\n",
            "Total loss:  -2.5461 | PDE Loss:  -4.5133 | Function Loss:  -2.5956\n",
            "Total loss:  -2.5463 | PDE Loss:  -4.5133 | Function Loss:  -2.5959\n",
            "Total loss:  -2.5467 | PDE Loss:  -4.5133 | Function Loss:  -2.5963\n",
            "Total loss:  -2.5472 | PDE Loss:  -4.5146 | Function Loss:  -2.5967\n",
            "Total loss:  -2.5476 | PDE Loss:  -4.5136 | Function Loss:  -2.5973\n",
            "Total loss:  -2.548 | PDE Loss:  -4.5141 | Function Loss:  -2.5977\n",
            "Total loss:  -2.5484 | PDE Loss:  -4.5137 | Function Loss:  -2.5982\n",
            "Total loss:  -2.5487 | PDE Loss:  -4.5134 | Function Loss:  -2.5986\n",
            "Total loss:  -2.5491 | PDE Loss:  -4.5142 | Function Loss:  -2.5989\n",
            "Total loss:  -2.5494 | PDE Loss:  -4.5132 | Function Loss:  -2.5994\n",
            "Total loss:  -2.5498 | PDE Loss:  -4.5146 | Function Loss:  -2.5996\n",
            "Total loss:  -2.55 | PDE Loss:  -4.5143 | Function Loss:  -2.5999\n",
            "Total loss:  -2.5502 | PDE Loss:  -4.5141 | Function Loss:  -2.6001\n",
            "Total loss:  -2.5504 | PDE Loss:  -4.5155 | Function Loss:  -2.6002\n",
            "Total loss:  -2.5507 | PDE Loss:  -4.516 | Function Loss:  -2.6005\n",
            "Total loss:  -2.551 | PDE Loss:  -4.5183 | Function Loss:  -2.6005\n",
            "Total loss:  -2.5513 | PDE Loss:  -4.5202 | Function Loss:  -2.6007\n",
            "Total loss:  -2.5516 | PDE Loss:  -4.5223 | Function Loss:  -2.6008\n",
            "Total loss:  -2.552 | PDE Loss:  -4.5261 | Function Loss:  -2.6007\n",
            "Total loss:  -2.5523 | PDE Loss:  -4.5278 | Function Loss:  -2.6009\n",
            "Total loss:  -2.5526 | PDE Loss:  -4.5316 | Function Loss:  -2.6008\n",
            "Total loss:  -2.553 | PDE Loss:  -4.5348 | Function Loss:  -2.6009\n",
            "Total loss:  -2.5535 | PDE Loss:  -4.539 | Function Loss:  -2.6009\n",
            "Total loss:  -2.5539 | PDE Loss:  -4.5436 | Function Loss:  -2.6008\n",
            "Total loss:  -2.5544 | PDE Loss:  -4.5456 | Function Loss:  -2.6011\n",
            "Total loss:  -2.5548 | PDE Loss:  -4.5496 | Function Loss:  -2.6011\n",
            "Total loss:  -2.5551 | PDE Loss:  -4.5499 | Function Loss:  -2.6014\n",
            "Total loss:  -2.5553 | PDE Loss:  -4.5504 | Function Loss:  -2.6016\n",
            "Total loss:  -2.5556 | PDE Loss:  -4.5501 | Function Loss:  -2.6019\n",
            "Total loss:  -2.5558 | PDE Loss:  -4.5491 | Function Loss:  -2.6023\n",
            "Total loss:  -2.5562 | PDE Loss:  -4.5474 | Function Loss:  -2.6029\n",
            "Total loss:  -2.5565 | PDE Loss:  -4.5468 | Function Loss:  -2.6033\n",
            "Total loss:  -2.5567 | PDE Loss:  -4.5471 | Function Loss:  -2.6035\n",
            "Total loss:  -2.557 | PDE Loss:  -4.5482 | Function Loss:  -2.6038\n",
            "Total loss:  -2.5573 | PDE Loss:  -4.549 | Function Loss:  -2.604\n",
            "Total loss:  -2.5575 | PDE Loss:  -4.5501 | Function Loss:  -2.6041\n",
            "Total loss:  -2.5576 | PDE Loss:  -4.5506 | Function Loss:  -2.6042\n",
            "Total loss:  -2.5578 | PDE Loss:  -4.5513 | Function Loss:  -2.6043\n",
            "Total loss:  -2.5579 | PDE Loss:  -4.5515 | Function Loss:  -2.6044\n",
            "Total loss:  -2.5581 | PDE Loss:  -4.5513 | Function Loss:  -2.6046\n",
            "Total loss:  -2.5583 | PDE Loss:  -4.5511 | Function Loss:  -2.6048\n",
            "Total loss:  -2.5584 | PDE Loss:  -4.5505 | Function Loss:  -2.6051\n",
            "Total loss:  -2.5586 | PDE Loss:  -4.5496 | Function Loss:  -2.6054\n",
            "Total loss:  -2.5589 | PDE Loss:  -4.5488 | Function Loss:  -2.6058\n",
            "Total loss:  -2.5591 | PDE Loss:  -4.5478 | Function Loss:  -2.6062\n",
            "Total loss:  -2.5595 | PDE Loss:  -4.5466 | Function Loss:  -2.6067\n",
            "Total loss:  -2.5599 | PDE Loss:  -4.5449 | Function Loss:  -2.6073\n",
            "Total loss:  -2.5602 | PDE Loss:  -4.5448 | Function Loss:  -2.6077\n",
            "Total loss:  -2.5606 | PDE Loss:  -4.5445 | Function Loss:  -2.6082\n",
            "Total loss:  -2.5611 | PDE Loss:  -4.5448 | Function Loss:  -2.6087\n",
            "Total loss:  -2.5614 | PDE Loss:  -4.544 | Function Loss:  -2.6092\n",
            "Total loss:  -2.5618 | PDE Loss:  -4.544 | Function Loss:  -2.6096\n",
            "Total loss:  -2.5621 | PDE Loss:  -4.5432 | Function Loss:  -2.61\n",
            "Total loss:  -2.5624 | PDE Loss:  -4.5419 | Function Loss:  -2.6105\n",
            "Total loss:  -2.5628 | PDE Loss:  -4.5414 | Function Loss:  -2.611\n",
            "Total loss:  -2.5631 | PDE Loss:  -4.5385 | Function Loss:  -2.6117\n",
            "Total loss:  -2.5634 | PDE Loss:  -4.5384 | Function Loss:  -2.612\n",
            "Total loss:  -2.5637 | PDE Loss:  -4.5366 | Function Loss:  -2.6125\n",
            "Total loss:  -2.564 | PDE Loss:  -4.5357 | Function Loss:  -2.613\n",
            "Total loss:  -2.5643 | PDE Loss:  -4.5339 | Function Loss:  -2.6135\n",
            "Total loss:  -2.5645 | PDE Loss:  -4.5328 | Function Loss:  -2.6139\n",
            "Total loss:  -2.5647 | PDE Loss:  -4.532 | Function Loss:  -2.6143\n",
            "Total loss:  -2.565 | PDE Loss:  -4.5312 | Function Loss:  -2.6146\n",
            "Total loss:  -2.5652 | PDE Loss:  -4.5311 | Function Loss:  -2.615\n",
            "Total loss:  -2.5655 | PDE Loss:  -4.5305 | Function Loss:  -2.6153\n",
            "Total loss:  -2.5658 | PDE Loss:  -4.5313 | Function Loss:  -2.6156\n",
            "Total loss:  -2.5661 | PDE Loss:  -4.5317 | Function Loss:  -2.6159\n",
            "Total loss:  -2.5666 | PDE Loss:  -4.5338 | Function Loss:  -2.6162\n",
            "Total loss:  -2.5671 | PDE Loss:  -4.5355 | Function Loss:  -2.6165\n",
            "Total loss:  -2.5676 | PDE Loss:  -4.5373 | Function Loss:  -2.6168\n",
            "Total loss:  -2.5675 | PDE Loss:  -4.5435 | Function Loss:  -2.6161\n",
            "Total loss:  -2.5679 | PDE Loss:  -4.5416 | Function Loss:  -2.6166\n",
            "Total loss:  -2.5681 | PDE Loss:  -4.5426 | Function Loss:  -2.6168\n",
            "Total loss:  -2.5684 | PDE Loss:  -4.5444 | Function Loss:  -2.617\n",
            "Total loss:  -2.5688 | PDE Loss:  -4.5434 | Function Loss:  -2.6175\n",
            "Total loss:  -2.5691 | PDE Loss:  -4.546 | Function Loss:  -2.6175\n",
            "Total loss:  -2.5694 | PDE Loss:  -4.5455 | Function Loss:  -2.6179\n",
            "Total loss:  -2.5696 | PDE Loss:  -4.5459 | Function Loss:  -2.6181\n",
            "Total loss:  -2.5698 | PDE Loss:  -4.5457 | Function Loss:  -2.6183\n",
            "Total loss:  -2.5699 | PDE Loss:  -4.5459 | Function Loss:  -2.6185\n",
            "Total loss:  -2.5701 | PDE Loss:  -4.5461 | Function Loss:  -2.6186\n",
            "Total loss:  -2.5703 | PDE Loss:  -4.5463 | Function Loss:  -2.6189\n",
            "Total loss:  -2.5705 | PDE Loss:  -4.547 | Function Loss:  -2.6189\n",
            "Total loss:  -2.5707 | PDE Loss:  -4.5482 | Function Loss:  -2.619\n",
            "Total loss:  -2.5709 | PDE Loss:  -4.5497 | Function Loss:  -2.6191\n",
            "Total loss:  -2.5712 | PDE Loss:  -4.5501 | Function Loss:  -2.6194\n",
            "Total loss:  -2.5715 | PDE Loss:  -4.5512 | Function Loss:  -2.6196\n",
            "Total loss:  -2.5718 | PDE Loss:  -4.5498 | Function Loss:  -2.62\n",
            "Total loss:  -2.572 | PDE Loss:  -4.549 | Function Loss:  -2.6204\n",
            "Total loss:  -2.5722 | PDE Loss:  -4.5474 | Function Loss:  -2.6208\n",
            "Total loss:  -2.5724 | PDE Loss:  -4.5449 | Function Loss:  -2.6214\n",
            "Total loss:  -2.5726 | PDE Loss:  -4.5444 | Function Loss:  -2.6216\n",
            "Total loss:  -2.5728 | PDE Loss:  -4.5433 | Function Loss:  -2.622\n",
            "Total loss:  -2.573 | PDE Loss:  -4.5409 | Function Loss:  -2.6225\n",
            "Total loss:  -2.5731 | PDE Loss:  -4.54 | Function Loss:  -2.6227\n",
            "Total loss:  -2.5733 | PDE Loss:  -4.5384 | Function Loss:  -2.6231\n",
            "Total loss:  -2.5734 | PDE Loss:  -4.537 | Function Loss:  -2.6234\n",
            "Total loss:  -2.5735 | PDE Loss:  -4.5372 | Function Loss:  -2.6235\n",
            "Total loss:  -2.5737 | PDE Loss:  -4.5362 | Function Loss:  -2.6238\n",
            "Total loss:  -2.5738 | PDE Loss:  -4.5364 | Function Loss:  -2.6239\n",
            "Total loss:  -2.574 | PDE Loss:  -4.5352 | Function Loss:  -2.6243\n",
            "Total loss:  -2.5742 | PDE Loss:  -4.5352 | Function Loss:  -2.6245\n",
            "Total loss:  -2.5744 | PDE Loss:  -4.5346 | Function Loss:  -2.6248\n",
            "Total loss:  -2.5746 | PDE Loss:  -4.5342 | Function Loss:  -2.6251\n",
            "Total loss:  -2.5748 | PDE Loss:  -4.5331 | Function Loss:  -2.6255\n",
            "Total loss:  -2.575 | PDE Loss:  -4.5328 | Function Loss:  -2.6257\n",
            "Total loss:  -2.5751 | PDE Loss:  -4.5343 | Function Loss:  -2.6257\n",
            "Total loss:  -2.5753 | PDE Loss:  -4.5357 | Function Loss:  -2.6256\n",
            "Total loss:  -2.5754 | PDE Loss:  -4.5368 | Function Loss:  -2.6257\n",
            "Total loss:  -2.5755 | PDE Loss:  -4.5381 | Function Loss:  -2.6257\n",
            "Total loss:  -2.5757 | PDE Loss:  -4.5394 | Function Loss:  -2.6257\n",
            "Total loss:  -2.5758 | PDE Loss:  -4.5401 | Function Loss:  -2.6257\n",
            "Total loss:  -2.5759 | PDE Loss:  -4.5405 | Function Loss:  -2.6258\n",
            "Total loss:  -2.5761 | PDE Loss:  -4.5414 | Function Loss:  -2.6259\n",
            "Total loss:  -2.5763 | PDE Loss:  -4.5423 | Function Loss:  -2.626\n",
            "Total loss:  -2.5765 | PDE Loss:  -4.5432 | Function Loss:  -2.6261\n",
            "Total loss:  -2.5766 | PDE Loss:  -4.5441 | Function Loss:  -2.6261\n",
            "Total loss:  -2.5768 | PDE Loss:  -4.5452 | Function Loss:  -2.6262\n",
            "Total loss:  -2.577 | PDE Loss:  -4.5464 | Function Loss:  -2.6262\n",
            "Total loss:  -2.5772 | PDE Loss:  -4.547 | Function Loss:  -2.6264\n",
            "Total loss:  -2.5774 | PDE Loss:  -4.5481 | Function Loss:  -2.6266\n",
            "Total loss:  -2.5777 | PDE Loss:  -4.5473 | Function Loss:  -2.627\n",
            "Total loss:  -2.578 | PDE Loss:  -4.5474 | Function Loss:  -2.6272\n",
            "Total loss:  -2.5782 | PDE Loss:  -4.5446 | Function Loss:  -2.6279\n",
            "Total loss:  -2.5785 | PDE Loss:  -4.5455 | Function Loss:  -2.628\n",
            "Total loss:  -2.5787 | PDE Loss:  -4.5445 | Function Loss:  -2.6284\n",
            "Total loss:  -2.579 | PDE Loss:  -4.5464 | Function Loss:  -2.6286\n",
            "Total loss:  -2.5794 | PDE Loss:  -4.5487 | Function Loss:  -2.6287\n",
            "Total loss:  -2.5797 | PDE Loss:  -4.5527 | Function Loss:  -2.6285\n",
            "Total loss:  -2.58 | PDE Loss:  -4.5563 | Function Loss:  -2.6285\n",
            "Total loss:  -2.5804 | PDE Loss:  -4.5606 | Function Loss:  -2.6284\n",
            "Total loss:  -2.5808 | PDE Loss:  -4.564 | Function Loss:  -2.6285\n",
            "Total loss:  -2.5812 | PDE Loss:  -4.5675 | Function Loss:  -2.6285\n",
            "Total loss:  -2.5815 | PDE Loss:  -4.5693 | Function Loss:  -2.6286\n",
            "Total loss:  -2.5818 | PDE Loss:  -4.5707 | Function Loss:  -2.6288\n",
            "Total loss:  -2.5822 | PDE Loss:  -4.5718 | Function Loss:  -2.6291\n",
            "Total loss:  -2.5826 | PDE Loss:  -4.5715 | Function Loss:  -2.6297\n",
            "Total loss:  -2.5831 | PDE Loss:  -4.5715 | Function Loss:  -2.6301\n",
            "Total loss:  -2.5836 | PDE Loss:  -4.5703 | Function Loss:  -2.6309\n",
            "Total loss:  -2.5843 | PDE Loss:  -4.5682 | Function Loss:  -2.6318\n",
            "Total loss:  -2.5848 | PDE Loss:  -4.5701 | Function Loss:  -2.6322\n",
            "Total loss:  -2.5853 | PDE Loss:  -4.5693 | Function Loss:  -2.6329\n",
            "Total loss:  -2.5861 | PDE Loss:  -4.5697 | Function Loss:  -2.6337\n",
            "Total loss:  -2.5867 | PDE Loss:  -4.5698 | Function Loss:  -2.6344\n",
            "Total loss:  -2.5873 | PDE Loss:  -4.5715 | Function Loss:  -2.6348\n",
            "Total loss:  -2.5877 | PDE Loss:  -4.5726 | Function Loss:  -2.6352\n",
            "Total loss:  -2.5881 | PDE Loss:  -4.5742 | Function Loss:  -2.6355\n",
            "Total loss:  -2.5887 | PDE Loss:  -4.576 | Function Loss:  -2.6359\n",
            "Total loss:  -2.5894 | PDE Loss:  -4.5763 | Function Loss:  -2.6367\n",
            "Total loss:  -2.59 | PDE Loss:  -4.5774 | Function Loss:  -2.6372\n",
            "Total loss:  -2.5906 | PDE Loss:  -4.5756 | Function Loss:  -2.6381\n",
            "Total loss:  -2.5911 | PDE Loss:  -4.5732 | Function Loss:  -2.6389\n",
            "Total loss:  -2.5916 | PDE Loss:  -4.5708 | Function Loss:  -2.6398\n",
            "Total loss:  -2.5921 | PDE Loss:  -4.5673 | Function Loss:  -2.6407\n",
            "Total loss:  -2.5923 | PDE Loss:  -4.565 | Function Loss:  -2.6412\n",
            "Total loss:  -2.5927 | PDE Loss:  -4.5645 | Function Loss:  -2.6417\n",
            "Total loss:  -2.593 | PDE Loss:  -4.5648 | Function Loss:  -2.642\n",
            "Total loss:  -2.5933 | PDE Loss:  -4.5657 | Function Loss:  -2.6423\n",
            "Total loss:  -2.5937 | PDE Loss:  -4.5673 | Function Loss:  -2.6424\n",
            "Total loss:  -2.594 | PDE Loss:  -4.5689 | Function Loss:  -2.6426\n",
            "Total loss:  -2.5943 | PDE Loss:  -4.5708 | Function Loss:  -2.6427\n",
            "Total loss:  -2.5946 | PDE Loss:  -4.5718 | Function Loss:  -2.643\n",
            "Total loss:  -2.595 | PDE Loss:  -4.5734 | Function Loss:  -2.6432\n",
            "Total loss:  -2.5953 | PDE Loss:  -4.5732 | Function Loss:  -2.6435\n",
            "Total loss:  -2.5955 | PDE Loss:  -4.5735 | Function Loss:  -2.6438\n",
            "Total loss:  -2.5957 | PDE Loss:  -4.5716 | Function Loss:  -2.6442\n",
            "Total loss:  -2.5959 | PDE Loss:  -4.5724 | Function Loss:  -2.6444\n",
            "Total loss:  -2.5961 | PDE Loss:  -4.5736 | Function Loss:  -2.6444\n",
            "Total loss:  -2.5963 | PDE Loss:  -4.5748 | Function Loss:  -2.6446\n",
            "Total loss:  -2.5966 | PDE Loss:  -4.5763 | Function Loss:  -2.6446\n",
            "Total loss:  -2.5967 | PDE Loss:  -4.5775 | Function Loss:  -2.6447\n",
            "Total loss:  -2.5969 | PDE Loss:  -4.5785 | Function Loss:  -2.6448\n",
            "Total loss:  -2.5971 | PDE Loss:  -4.5796 | Function Loss:  -2.6449\n",
            "Total loss:  -2.5973 | PDE Loss:  -4.5806 | Function Loss:  -2.645\n",
            "Total loss:  -2.5975 | PDE Loss:  -4.581 | Function Loss:  -2.6452\n",
            "Total loss:  -2.5977 | PDE Loss:  -4.5842 | Function Loss:  -2.645\n",
            "Total loss:  -2.5978 | PDE Loss:  -4.5835 | Function Loss:  -2.6452\n",
            "Total loss:  -2.598 | PDE Loss:  -4.5825 | Function Loss:  -2.6455\n",
            "Total loss:  -2.5982 | PDE Loss:  -4.5812 | Function Loss:  -2.6459\n",
            "Total loss:  -2.5983 | PDE Loss:  -4.5801 | Function Loss:  -2.6462\n",
            "Total loss:  -2.5984 | PDE Loss:  -4.5786 | Function Loss:  -2.6464\n",
            "Total loss:  -2.5985 | PDE Loss:  -4.5777 | Function Loss:  -2.6467\n",
            "Total loss:  -2.5986 | PDE Loss:  -4.5763 | Function Loss:  -2.647\n",
            "Total loss:  -2.5988 | PDE Loss:  -4.5754 | Function Loss:  -2.6472\n",
            "Total loss:  -2.5989 | PDE Loss:  -4.5738 | Function Loss:  -2.6475\n",
            "Total loss:  -2.599 | PDE Loss:  -4.5734 | Function Loss:  -2.6477\n",
            "Total loss:  -2.5991 | PDE Loss:  -4.5738 | Function Loss:  -2.6478\n",
            "Total loss:  -2.5993 | PDE Loss:  -4.574 | Function Loss:  -2.6479\n",
            "Total loss:  -2.5994 | PDE Loss:  -4.5748 | Function Loss:  -2.648\n",
            "Total loss:  -2.5995 | PDE Loss:  -4.5754 | Function Loss:  -2.648\n",
            "Total loss:  -2.5997 | PDE Loss:  -4.5757 | Function Loss:  -2.6482\n",
            "Total loss:  -2.5998 | PDE Loss:  -4.5762 | Function Loss:  -2.6483\n",
            "Total loss:  -2.6001 | PDE Loss:  -4.5773 | Function Loss:  -2.6484\n",
            "Total loss:  -2.6003 | PDE Loss:  -4.5771 | Function Loss:  -2.6487\n",
            "Total loss:  -2.6005 | PDE Loss:  -4.5768 | Function Loss:  -2.649\n",
            "Total loss:  -2.6007 | PDE Loss:  -4.5761 | Function Loss:  -2.6493\n",
            "Total loss:  -2.6009 | PDE Loss:  -4.5762 | Function Loss:  -2.6495\n",
            "Total loss:  -2.6011 | PDE Loss:  -4.5752 | Function Loss:  -2.6499\n",
            "Total loss:  -2.6013 | PDE Loss:  -4.5757 | Function Loss:  -2.65\n",
            "Total loss:  -2.6016 | PDE Loss:  -4.5748 | Function Loss:  -2.6504\n",
            "Total loss:  -2.6018 | PDE Loss:  -4.5754 | Function Loss:  -2.6506\n",
            "Total loss:  -2.6021 | PDE Loss:  -4.5752 | Function Loss:  -2.651\n",
            "Total loss:  -2.6024 | PDE Loss:  -4.5758 | Function Loss:  -2.6512\n",
            "Total loss:  -2.6028 | PDE Loss:  -4.5768 | Function Loss:  -2.6515\n",
            "Total loss:  -2.6032 | PDE Loss:  -4.5733 | Function Loss:  -2.6524\n",
            "Total loss:  -2.6037 | PDE Loss:  -4.5795 | Function Loss:  -2.6522\n",
            "Total loss:  -2.6039 | PDE Loss:  -4.5785 | Function Loss:  -2.6526\n",
            "Total loss:  -2.6044 | PDE Loss:  -4.5784 | Function Loss:  -2.6532\n",
            "Total loss:  -2.6051 | PDE Loss:  -4.5798 | Function Loss:  -2.6537\n",
            "Total loss:  -2.6057 | PDE Loss:  -4.5788 | Function Loss:  -2.6546\n",
            "Total loss:  -2.6062 | PDE Loss:  -4.5776 | Function Loss:  -2.6553\n",
            "Total loss:  -2.6067 | PDE Loss:  -4.5771 | Function Loss:  -2.6559\n",
            "Total loss:  -2.6072 | PDE Loss:  -4.576 | Function Loss:  -2.6566\n",
            "Total loss:  -2.6076 | PDE Loss:  -4.5769 | Function Loss:  -2.657\n",
            "Total loss:  -2.6081 | PDE Loss:  -4.579 | Function Loss:  -2.6572\n",
            "Total loss:  -2.6085 | PDE Loss:  -4.5832 | Function Loss:  -2.6572\n",
            "Total loss:  -2.6089 | PDE Loss:  -4.5866 | Function Loss:  -2.6572\n",
            "Total loss:  -2.6093 | PDE Loss:  -4.5919 | Function Loss:  -2.657\n",
            "Total loss:  -2.6097 | PDE Loss:  -4.5965 | Function Loss:  -2.6569\n",
            "Total loss:  -2.61 | PDE Loss:  -4.6019 | Function Loss:  -2.6567\n",
            "Total loss:  -2.6104 | PDE Loss:  -4.6055 | Function Loss:  -2.6567\n",
            "Total loss:  -2.6106 | PDE Loss:  -4.6084 | Function Loss:  -2.6566\n",
            "Total loss:  -2.6107 | PDE Loss:  -4.6094 | Function Loss:  -2.6566\n",
            "Total loss:  -2.6109 | PDE Loss:  -4.61 | Function Loss:  -2.6567\n",
            "Total loss:  -2.611 | PDE Loss:  -4.6097 | Function Loss:  -2.6569\n",
            "Total loss:  -2.6113 | PDE Loss:  -4.6096 | Function Loss:  -2.6572\n",
            "Total loss:  -2.6116 | PDE Loss:  -4.609 | Function Loss:  -2.6576\n",
            "Total loss:  -2.612 | PDE Loss:  -4.608 | Function Loss:  -2.6582\n",
            "Total loss:  -2.6124 | PDE Loss:  -4.6057 | Function Loss:  -2.6589\n",
            "Total loss:  -2.6126 | PDE Loss:  -4.6035 | Function Loss:  -2.6594\n",
            "Total loss:  -2.6128 | PDE Loss:  -4.6003 | Function Loss:  -2.66\n",
            "Total loss:  -2.613 | PDE Loss:  -4.5989 | Function Loss:  -2.6603\n",
            "Total loss:  -2.6131 | PDE Loss:  -4.5969 | Function Loss:  -2.6607\n",
            "Total loss:  -2.6132 | PDE Loss:  -4.5958 | Function Loss:  -2.6609\n",
            "Total loss:  -2.6133 | PDE Loss:  -4.5947 | Function Loss:  -2.6611\n",
            "Total loss:  -2.6134 | PDE Loss:  -4.594 | Function Loss:  -2.6614\n",
            "Total loss:  -2.6136 | PDE Loss:  -4.5927 | Function Loss:  -2.6618\n",
            "Total loss:  -2.6138 | PDE Loss:  -4.5921 | Function Loss:  -2.662\n",
            "Total loss:  -2.614 | PDE Loss:  -4.5912 | Function Loss:  -2.6624\n",
            "Total loss:  -2.6142 | PDE Loss:  -4.5907 | Function Loss:  -2.6627\n",
            "Total loss:  -2.6146 | PDE Loss:  -4.5913 | Function Loss:  -2.663\n",
            "Total loss:  -2.6151 | PDE Loss:  -4.5912 | Function Loss:  -2.6636\n",
            "Total loss:  -2.6157 | PDE Loss:  -4.5921 | Function Loss:  -2.6641\n",
            "Total loss:  -2.6162 | PDE Loss:  -4.5919 | Function Loss:  -2.6648\n",
            "Total loss:  -2.6169 | PDE Loss:  -4.5913 | Function Loss:  -2.6657\n",
            "Total loss:  -2.6176 | PDE Loss:  -4.5912 | Function Loss:  -2.6664\n",
            "Total loss:  -2.6181 | PDE Loss:  -4.589 | Function Loss:  -2.6672\n",
            "Total loss:  -2.6185 | PDE Loss:  -4.5887 | Function Loss:  -2.6677\n",
            "Total loss:  -2.6188 | PDE Loss:  -4.5891 | Function Loss:  -2.668\n",
            "Total loss:  -2.6193 | PDE Loss:  -4.5905 | Function Loss:  -2.6683\n",
            "Total loss:  -2.6195 | PDE Loss:  -4.5912 | Function Loss:  -2.6685\n",
            "Total loss:  -2.62 | PDE Loss:  -4.5961 | Function Loss:  -2.6685\n",
            "Total loss:  -2.6203 | PDE Loss:  -4.5987 | Function Loss:  -2.6686\n",
            "Total loss:  -2.6207 | PDE Loss:  -4.6023 | Function Loss:  -2.6685\n",
            "Total loss:  -2.621 | PDE Loss:  -4.6043 | Function Loss:  -2.6687\n",
            "Total loss:  -2.6214 | PDE Loss:  -4.6076 | Function Loss:  -2.6687\n",
            "Total loss:  -2.6217 | PDE Loss:  -4.6083 | Function Loss:  -2.6689\n",
            "Total loss:  -2.622 | PDE Loss:  -4.6088 | Function Loss:  -2.6692\n",
            "Total loss:  -2.6224 | PDE Loss:  -4.6094 | Function Loss:  -2.6696\n",
            "Total loss:  -2.6228 | PDE Loss:  -4.6065 | Function Loss:  -2.6705\n",
            "Total loss:  -2.6233 | PDE Loss:  -4.6045 | Function Loss:  -2.6712\n",
            "Total loss:  -2.6237 | PDE Loss:  -4.6045 | Function Loss:  -2.6716\n",
            "Total loss:  -2.6243 | PDE Loss:  -4.6039 | Function Loss:  -2.6723\n",
            "Total loss:  -2.6247 | PDE Loss:  -4.6024 | Function Loss:  -2.6731\n",
            "Total loss:  -2.6251 | PDE Loss:  -4.6019 | Function Loss:  -2.6735\n",
            "Total loss:  -2.6256 | PDE Loss:  -4.6014 | Function Loss:  -2.6741\n",
            "Total loss:  -2.6263 | PDE Loss:  -4.6016 | Function Loss:  -2.6749\n",
            "Total loss:  -2.6271 | PDE Loss:  -4.6011 | Function Loss:  -2.6758\n",
            "Total loss:  -2.6277 | PDE Loss:  -4.6023 | Function Loss:  -2.6764\n",
            "Total loss:  -2.6282 | PDE Loss:  -4.5995 | Function Loss:  -2.6773\n",
            "Total loss:  -2.6288 | PDE Loss:  -4.6013 | Function Loss:  -2.6777\n",
            "Total loss:  -2.6292 | PDE Loss:  -4.6042 | Function Loss:  -2.6778\n",
            "Total loss:  -2.6298 | PDE Loss:  -4.605 | Function Loss:  -2.6784\n",
            "Total loss:  -2.6304 | PDE Loss:  -4.6051 | Function Loss:  -2.679\n",
            "Total loss:  -2.6309 | PDE Loss:  -4.6049 | Function Loss:  -2.6796\n",
            "Total loss:  -2.6314 | PDE Loss:  -4.6009 | Function Loss:  -2.6806\n",
            "Total loss:  -2.6319 | PDE Loss:  -4.6015 | Function Loss:  -2.6812\n",
            "Total loss:  -2.6322 | PDE Loss:  -4.5986 | Function Loss:  -2.6819\n",
            "Total loss:  -2.6326 | PDE Loss:  -4.5968 | Function Loss:  -2.6825\n",
            "Total loss:  -2.633 | PDE Loss:  -4.595 | Function Loss:  -2.6831\n",
            "Total loss:  -2.6333 | PDE Loss:  -4.5932 | Function Loss:  -2.6838\n",
            "Total loss:  -2.6337 | PDE Loss:  -4.5922 | Function Loss:  -2.6844\n",
            "Total loss:  -2.6341 | PDE Loss:  -4.5918 | Function Loss:  -2.6849\n",
            "Total loss:  -2.6345 | PDE Loss:  -4.5907 | Function Loss:  -2.6854\n",
            "Total loss:  -2.6348 | PDE Loss:  -4.5921 | Function Loss:  -2.6855\n",
            "Total loss:  -2.6349 | PDE Loss:  -4.592 | Function Loss:  -2.6857\n",
            "Total loss:  -2.6351 | PDE Loss:  -4.5918 | Function Loss:  -2.6859\n",
            "Total loss:  -2.6352 | PDE Loss:  -4.5925 | Function Loss:  -2.686\n",
            "Total loss:  -2.6354 | PDE Loss:  -4.5918 | Function Loss:  -2.6863\n",
            "Total loss:  -2.6356 | PDE Loss:  -4.5926 | Function Loss:  -2.6864\n",
            "Total loss:  -2.6358 | PDE Loss:  -4.5918 | Function Loss:  -2.6868\n",
            "Total loss:  -2.636 | PDE Loss:  -4.5916 | Function Loss:  -2.6869\n",
            "Total loss:  -2.6362 | PDE Loss:  -4.5919 | Function Loss:  -2.6872\n",
            "Total loss:  -2.6365 | PDE Loss:  -4.5923 | Function Loss:  -2.6875\n",
            "Total loss:  -2.6368 | PDE Loss:  -4.5917 | Function Loss:  -2.6879\n",
            "Total loss:  -2.6371 | PDE Loss:  -4.5924 | Function Loss:  -2.6881\n",
            "Total loss:  -2.6373 | PDE Loss:  -4.5914 | Function Loss:  -2.6885\n",
            "Total loss:  -2.6375 | PDE Loss:  -4.5902 | Function Loss:  -2.6889\n",
            "Total loss:  -2.6378 | PDE Loss:  -4.5887 | Function Loss:  -2.6893\n",
            "Total loss:  -2.6381 | PDE Loss:  -4.5857 | Function Loss:  -2.69\n",
            "Total loss:  -2.6383 | PDE Loss:  -4.5847 | Function Loss:  -2.6905\n",
            "Total loss:  -2.6386 | PDE Loss:  -4.5824 | Function Loss:  -2.6911\n",
            "Total loss:  -2.639 | PDE Loss:  -4.5811 | Function Loss:  -2.6917\n",
            "Total loss:  -2.6394 | PDE Loss:  -4.5812 | Function Loss:  -2.6922\n",
            "Total loss:  -2.64 | PDE Loss:  -4.5773 | Function Loss:  -2.6933\n",
            "Total loss:  -2.6404 | PDE Loss:  -4.5798 | Function Loss:  -2.6934\n",
            "Total loss:  -2.6409 | PDE Loss:  -4.5812 | Function Loss:  -2.6938\n",
            "Total loss:  -2.6413 | PDE Loss:  -4.5844 | Function Loss:  -2.6939\n",
            "Total loss:  -2.6417 | PDE Loss:  -4.5878 | Function Loss:  -2.6939\n",
            "Total loss:  -2.6422 | PDE Loss:  -4.5905 | Function Loss:  -2.6941\n",
            "Total loss:  -2.6427 | PDE Loss:  -4.5927 | Function Loss:  -2.6944\n",
            "Total loss:  -2.6432 | PDE Loss:  -4.5931 | Function Loss:  -2.6949\n",
            "Total loss:  -2.6435 | PDE Loss:  -4.594 | Function Loss:  -2.6952\n",
            "Total loss:  -2.644 | PDE Loss:  -4.5947 | Function Loss:  -2.6956\n",
            "Total loss:  -2.6445 | PDE Loss:  -4.5953 | Function Loss:  -2.696\n",
            "Total loss:  -2.6449 | PDE Loss:  -4.5941 | Function Loss:  -2.6967\n",
            "Total loss:  -2.6454 | PDE Loss:  -4.5948 | Function Loss:  -2.6971\n",
            "Total loss:  -2.6458 | PDE Loss:  -4.5929 | Function Loss:  -2.6979\n",
            "Total loss:  -2.6463 | PDE Loss:  -4.5938 | Function Loss:  -2.6983\n",
            "Total loss:  -2.6467 | PDE Loss:  -4.5924 | Function Loss:  -2.699\n",
            "Total loss:  -2.6471 | PDE Loss:  -4.5925 | Function Loss:  -2.6994\n",
            "Total loss:  -2.6474 | PDE Loss:  -4.5923 | Function Loss:  -2.6998\n",
            "Total loss:  -2.6477 | PDE Loss:  -4.5921 | Function Loss:  -2.7002\n",
            "Total loss:  -2.648 | PDE Loss:  -4.5927 | Function Loss:  -2.7004\n",
            "Total loss:  -2.6484 | PDE Loss:  -4.593 | Function Loss:  -2.7008\n",
            "Total loss:  -2.6488 | PDE Loss:  -4.5935 | Function Loss:  -2.7012\n",
            "Total loss:  -2.6493 | PDE Loss:  -4.5956 | Function Loss:  -2.7014\n",
            "Total loss:  -2.6497 | PDE Loss:  -4.5946 | Function Loss:  -2.702\n",
            "Total loss:  -2.6502 | PDE Loss:  -4.5992 | Function Loss:  -2.7021\n",
            "Total loss:  -2.6506 | PDE Loss:  -4.5976 | Function Loss:  -2.7027\n",
            "Total loss:  -2.6511 | PDE Loss:  -4.5981 | Function Loss:  -2.7031\n",
            "Total loss:  -2.6514 | PDE Loss:  -4.5981 | Function Loss:  -2.7035\n",
            "Total loss:  -2.6516 | PDE Loss:  -4.5976 | Function Loss:  -2.7038\n",
            "Total loss:  -2.6518 | PDE Loss:  -4.5981 | Function Loss:  -2.704\n",
            "Total loss:  -2.652 | PDE Loss:  -4.5967 | Function Loss:  -2.7044\n",
            "Total loss:  -2.6522 | PDE Loss:  -4.5963 | Function Loss:  -2.7047\n",
            "Total loss:  -2.6525 | PDE Loss:  -4.5947 | Function Loss:  -2.7051\n",
            "Total loss:  -2.6526 | PDE Loss:  -4.5914 | Function Loss:  -2.7057\n",
            "Total loss:  -2.6529 | PDE Loss:  -4.5899 | Function Loss:  -2.7063\n",
            "Total loss:  -2.6531 | PDE Loss:  -4.5877 | Function Loss:  -2.7068\n",
            "Total loss:  -2.6534 | PDE Loss:  -4.5857 | Function Loss:  -2.7074\n",
            "Total loss:  -2.6536 | PDE Loss:  -4.5822 | Function Loss:  -2.7081\n",
            "Total loss:  -2.6539 | PDE Loss:  -4.5796 | Function Loss:  -2.7087\n",
            "Total loss:  -2.6541 | PDE Loss:  -4.5761 | Function Loss:  -2.7095\n",
            "Total loss:  -2.6543 | PDE Loss:  -4.5734 | Function Loss:  -2.7101\n",
            "Total loss:  -2.6546 | PDE Loss:  -4.5704 | Function Loss:  -2.7108\n",
            "Total loss:  -2.6548 | PDE Loss:  -4.5698 | Function Loss:  -2.7111\n",
            "Total loss:  -2.655 | PDE Loss:  -4.5694 | Function Loss:  -2.7114\n",
            "Total loss:  -2.6552 | PDE Loss:  -4.5702 | Function Loss:  -2.7115\n",
            "Total loss:  -2.6553 | PDE Loss:  -4.5717 | Function Loss:  -2.7115\n",
            "Total loss:  -2.6555 | PDE Loss:  -4.5729 | Function Loss:  -2.7115\n",
            "Total loss:  -2.6557 | PDE Loss:  -4.5744 | Function Loss:  -2.7115\n",
            "Total loss:  -2.6559 | PDE Loss:  -4.575 | Function Loss:  -2.7117\n",
            "Total loss:  -2.6561 | PDE Loss:  -4.5764 | Function Loss:  -2.7117\n",
            "Total loss:  -2.6563 | PDE Loss:  -4.5763 | Function Loss:  -2.712\n",
            "Total loss:  -2.6565 | PDE Loss:  -4.576 | Function Loss:  -2.7122\n",
            "Total loss:  -2.6567 | PDE Loss:  -4.577 | Function Loss:  -2.7123\n",
            "Total loss:  -2.6569 | PDE Loss:  -4.5753 | Function Loss:  -2.7127\n",
            "Total loss:  -2.657 | PDE Loss:  -4.5756 | Function Loss:  -2.7129\n",
            "Total loss:  -2.6572 | PDE Loss:  -4.5749 | Function Loss:  -2.7132\n",
            "Total loss:  -2.6574 | PDE Loss:  -4.5741 | Function Loss:  -2.7135\n",
            "Total loss:  -2.6576 | PDE Loss:  -4.5733 | Function Loss:  -2.7138\n",
            "Total loss:  -2.6578 | PDE Loss:  -4.5722 | Function Loss:  -2.7142\n",
            "Total loss:  -2.6581 | PDE Loss:  -4.5715 | Function Loss:  -2.7146\n",
            "Total loss:  -2.6584 | PDE Loss:  -4.57 | Function Loss:  -2.7152\n",
            "Total loss:  -2.6586 | PDE Loss:  -4.5693 | Function Loss:  -2.7156\n",
            "Total loss:  -2.6589 | PDE Loss:  -4.5681 | Function Loss:  -2.716\n",
            "Total loss:  -2.6593 | PDE Loss:  -4.5677 | Function Loss:  -2.7165\n",
            "Total loss:  -2.6597 | PDE Loss:  -4.5671 | Function Loss:  -2.7171\n",
            "Total loss:  -2.6602 | PDE Loss:  -4.5665 | Function Loss:  -2.7177\n",
            "Total loss:  -2.6607 | PDE Loss:  -4.5661 | Function Loss:  -2.7183\n",
            "Total loss:  -2.6611 | PDE Loss:  -4.565 | Function Loss:  -2.719\n",
            "Total loss:  -2.6614 | PDE Loss:  -4.5648 | Function Loss:  -2.7194\n",
            "Total loss:  -2.6617 | PDE Loss:  -4.5637 | Function Loss:  -2.7198\n",
            "Total loss:  -2.662 | PDE Loss:  -4.5632 | Function Loss:  -2.7202\n",
            "Total loss:  -2.6623 | PDE Loss:  -4.5621 | Function Loss:  -2.7207\n",
            "Total loss:  -2.6625 | PDE Loss:  -4.5618 | Function Loss:  -2.7211\n",
            "Total loss:  -2.6627 | PDE Loss:  -4.561 | Function Loss:  -2.7214\n",
            "Total loss:  -2.663 | PDE Loss:  -4.5604 | Function Loss:  -2.7218\n",
            "Total loss:  -2.6632 | PDE Loss:  -4.5586 | Function Loss:  -2.7224\n",
            "Total loss:  -2.6635 | PDE Loss:  -4.557 | Function Loss:  -2.7229\n",
            "Total loss:  -2.6637 | PDE Loss:  -4.5547 | Function Loss:  -2.7234\n",
            "Total loss:  -2.6638 | PDE Loss:  -4.5538 | Function Loss:  -2.7237\n",
            "Total loss:  -2.6641 | PDE Loss:  -4.5513 | Function Loss:  -2.7244\n",
            "Total loss:  -2.6643 | PDE Loss:  -4.5499 | Function Loss:  -2.7249\n",
            "Total loss:  -2.6645 | PDE Loss:  -4.5478 | Function Loss:  -2.7254\n",
            "Total loss:  -2.6647 | PDE Loss:  -4.546 | Function Loss:  -2.7259\n",
            "Total loss:  -2.665 | PDE Loss:  -4.5438 | Function Loss:  -2.7266\n",
            "Total loss:  -2.6653 | PDE Loss:  -4.5419 | Function Loss:  -2.7272\n",
            "Total loss:  -2.6657 | PDE Loss:  -4.5383 | Function Loss:  -2.7282\n",
            "Total loss:  -2.6661 | PDE Loss:  -4.5394 | Function Loss:  -2.7285\n",
            "Total loss:  -2.6664 | PDE Loss:  -4.5372 | Function Loss:  -2.7292\n",
            "Total loss:  -2.6668 | PDE Loss:  -4.5371 | Function Loss:  -2.7297\n",
            "Total loss:  -2.6672 | PDE Loss:  -4.5374 | Function Loss:  -2.7301\n",
            "Total loss:  -2.6675 | PDE Loss:  -4.5384 | Function Loss:  -2.7303\n",
            "Total loss:  -2.668 | PDE Loss:  -4.5387 | Function Loss:  -2.7308\n",
            "Total loss:  -2.6684 | PDE Loss:  -4.5384 | Function Loss:  -2.7314\n",
            "Total loss:  -2.6688 | PDE Loss:  -4.5376 | Function Loss:  -2.7319\n",
            "Total loss:  -2.6691 | PDE Loss:  -4.5371 | Function Loss:  -2.7323\n",
            "Total loss:  -2.6693 | PDE Loss:  -4.5359 | Function Loss:  -2.7328\n",
            "Total loss:  -2.6696 | PDE Loss:  -4.5355 | Function Loss:  -2.7332\n",
            "Total loss:  -2.67 | PDE Loss:  -4.5346 | Function Loss:  -2.7338\n",
            "Total loss:  -2.6704 | PDE Loss:  -4.535 | Function Loss:  -2.7341\n",
            "Total loss:  -2.6708 | PDE Loss:  -4.5339 | Function Loss:  -2.7349\n",
            "Total loss:  -2.6714 | PDE Loss:  -4.5354 | Function Loss:  -2.7353\n",
            "Total loss:  -2.6718 | PDE Loss:  -4.5359 | Function Loss:  -2.7356\n",
            "Total loss:  -2.6721 | PDE Loss:  -4.535 | Function Loss:  -2.7362\n",
            "Total loss:  -2.6725 | PDE Loss:  -4.5353 | Function Loss:  -2.7365\n",
            "Total loss:  -2.6728 | PDE Loss:  -4.5343 | Function Loss:  -2.7371\n",
            "Total loss:  -2.6732 | PDE Loss:  -4.5334 | Function Loss:  -2.7377\n",
            "Total loss:  -2.6736 | PDE Loss:  -4.5307 | Function Loss:  -2.7385\n",
            "Total loss:  -2.6738 | PDE Loss:  -4.5298 | Function Loss:  -2.739\n",
            "Total loss:  -2.6741 | PDE Loss:  -4.531 | Function Loss:  -2.7391\n",
            "Total loss:  -2.6744 | PDE Loss:  -4.5308 | Function Loss:  -2.7395\n",
            "Total loss:  -2.6748 | PDE Loss:  -4.5308 | Function Loss:  -2.7399\n",
            "Total loss:  -2.6752 | PDE Loss:  -4.53 | Function Loss:  -2.7406\n",
            "Total loss:  -2.6755 | PDE Loss:  -4.5301 | Function Loss:  -2.7408\n",
            "Total loss:  -2.6758 | PDE Loss:  -4.5275 | Function Loss:  -2.7417\n",
            "Total loss:  -2.6761 | PDE Loss:  -4.5264 | Function Loss:  -2.7421\n",
            "Total loss:  -2.6763 | PDE Loss:  -4.5244 | Function Loss:  -2.7427\n",
            "Total loss:  -2.6765 | PDE Loss:  -4.5207 | Function Loss:  -2.7436\n",
            "Total loss:  -2.6767 | PDE Loss:  -4.5185 | Function Loss:  -2.7442\n",
            "Total loss:  -2.6768 | PDE Loss:  -4.5152 | Function Loss:  -2.7449\n",
            "Total loss:  -2.677 | PDE Loss:  -4.5126 | Function Loss:  -2.7456\n",
            "Total loss:  -2.6773 | PDE Loss:  -4.509 | Function Loss:  -2.7465\n",
            "Total loss:  -2.6776 | PDE Loss:  -4.5061 | Function Loss:  -2.7473\n",
            "Total loss:  -2.6778 | PDE Loss:  -4.498 | Function Loss:  -2.7491\n",
            "Total loss:  -2.6782 | PDE Loss:  -4.4976 | Function Loss:  -2.7495\n",
            "Total loss:  -2.6786 | PDE Loss:  -4.4987 | Function Loss:  -2.7498\n",
            "Total loss:  -2.679 | PDE Loss:  -4.4994 | Function Loss:  -2.7502\n",
            "Total loss:  -2.6794 | PDE Loss:  -4.5024 | Function Loss:  -2.7502\n",
            "Total loss:  -2.6799 | PDE Loss:  -4.5028 | Function Loss:  -2.7506\n",
            "Total loss:  -2.6802 | PDE Loss:  -4.5067 | Function Loss:  -2.7504\n",
            "Total loss:  -2.6805 | PDE Loss:  -4.5073 | Function Loss:  -2.7506\n",
            "Total loss:  -2.6808 | PDE Loss:  -4.5084 | Function Loss:  -2.7508\n",
            "Total loss:  -2.6812 | PDE Loss:  -4.5074 | Function Loss:  -2.7514\n",
            "Total loss:  -2.6816 | PDE Loss:  -4.5087 | Function Loss:  -2.7516\n",
            "Total loss:  -2.6818 | PDE Loss:  -4.5082 | Function Loss:  -2.7519\n",
            "Total loss:  -2.682 | PDE Loss:  -4.5072 | Function Loss:  -2.7524\n",
            "Total loss:  -2.6824 | PDE Loss:  -4.507 | Function Loss:  -2.7528\n",
            "Total loss:  -2.6828 | PDE Loss:  -4.5062 | Function Loss:  -2.7534\n",
            "Total loss:  -2.6832 | PDE Loss:  -4.5053 | Function Loss:  -2.7541\n",
            "Total loss:  -2.6835 | PDE Loss:  -4.5039 | Function Loss:  -2.7547\n",
            "Total loss:  -2.6839 | PDE Loss:  -4.503 | Function Loss:  -2.7553\n",
            "Total loss:  -2.6843 | PDE Loss:  -4.5031 | Function Loss:  -2.7557\n",
            "Total loss:  -2.6847 | PDE Loss:  -4.5036 | Function Loss:  -2.7562\n",
            "Total loss:  -2.6851 | PDE Loss:  -4.5046 | Function Loss:  -2.7565\n",
            "Total loss:  -2.6855 | PDE Loss:  -4.5063 | Function Loss:  -2.7566\n",
            "Total loss:  -2.686 | PDE Loss:  -4.5088 | Function Loss:  -2.7567\n",
            "Total loss:  -2.6866 | PDE Loss:  -4.5125 | Function Loss:  -2.7568\n",
            "Total loss:  -2.6873 | PDE Loss:  -4.5181 | Function Loss:  -2.7567\n",
            "Total loss:  -2.688 | PDE Loss:  -4.5222 | Function Loss:  -2.7568\n",
            "Total loss:  -2.6888 | PDE Loss:  -4.5294 | Function Loss:  -2.7565\n",
            "Total loss:  -2.6895 | PDE Loss:  -4.5339 | Function Loss:  -2.7566\n",
            "Total loss:  -2.6902 | PDE Loss:  -4.5391 | Function Loss:  -2.7565\n",
            "Total loss:  -2.6908 | PDE Loss:  -4.5411 | Function Loss:  -2.7569\n",
            "Total loss:  -2.6914 | PDE Loss:  -4.5431 | Function Loss:  -2.7573\n",
            "Total loss:  -2.6921 | PDE Loss:  -4.5418 | Function Loss:  -2.7582\n",
            "Total loss:  -2.6927 | PDE Loss:  -4.5416 | Function Loss:  -2.759\n",
            "Total loss:  -2.6933 | PDE Loss:  -4.5399 | Function Loss:  -2.76\n",
            "Total loss:  -2.6941 | PDE Loss:  -4.5393 | Function Loss:  -2.761\n",
            "Total loss:  -2.6948 | PDE Loss:  -4.5377 | Function Loss:  -2.7621\n",
            "Total loss:  -2.6954 | PDE Loss:  -4.5374 | Function Loss:  -2.7629\n",
            "Total loss:  -2.6958 | PDE Loss:  -4.5374 | Function Loss:  -2.7634\n",
            "Total loss:  -2.6963 | PDE Loss:  -4.5366 | Function Loss:  -2.764\n",
            "Total loss:  -2.6967 | PDE Loss:  -4.5379 | Function Loss:  -2.7643\n",
            "Total loss:  -2.697 | PDE Loss:  -4.5371 | Function Loss:  -2.7648\n",
            "Total loss:  -2.6973 | PDE Loss:  -4.5379 | Function Loss:  -2.765\n",
            "Total loss:  -2.6976 | PDE Loss:  -4.5377 | Function Loss:  -2.7654\n",
            "Total loss:  -2.6979 | PDE Loss:  -4.5381 | Function Loss:  -2.7657\n",
            "Total loss:  -2.6982 | PDE Loss:  -4.5384 | Function Loss:  -2.766\n",
            "Total loss:  -2.6986 | PDE Loss:  -4.5395 | Function Loss:  -2.7662\n",
            "Total loss:  -2.6989 | PDE Loss:  -4.5398 | Function Loss:  -2.7666\n",
            "Total loss:  -2.6993 | PDE Loss:  -4.5412 | Function Loss:  -2.7668\n",
            "Total loss:  -2.6998 | PDE Loss:  -4.5411 | Function Loss:  -2.7674\n",
            "Total loss:  -2.7001 | PDE Loss:  -4.5422 | Function Loss:  -2.7676\n",
            "Total loss:  -2.7004 | PDE Loss:  -4.5419 | Function Loss:  -2.768\n",
            "Total loss:  -2.7007 | PDE Loss:  -4.5412 | Function Loss:  -2.7684\n",
            "Total loss:  -2.7011 | PDE Loss:  -4.5386 | Function Loss:  -2.7693\n",
            "Total loss:  -2.7015 | PDE Loss:  -4.5376 | Function Loss:  -2.77\n",
            "Total loss:  -2.7018 | PDE Loss:  -4.5358 | Function Loss:  -2.7706\n",
            "Total loss:  -2.7023 | PDE Loss:  -4.5348 | Function Loss:  -2.7713\n",
            "Total loss:  -2.7026 | PDE Loss:  -4.5284 | Function Loss:  -2.7729\n",
            "Total loss:  -2.7031 | PDE Loss:  -4.527 | Function Loss:  -2.7737\n",
            "Total loss:  -2.7035 | PDE Loss:  -4.5282 | Function Loss:  -2.7739\n",
            "Total loss:  -2.7039 | PDE Loss:  -4.5301 | Function Loss:  -2.7741\n",
            "Total loss:  -2.7043 | PDE Loss:  -4.5292 | Function Loss:  -2.7748\n",
            "Total loss:  -2.7047 | PDE Loss:  -4.5304 | Function Loss:  -2.7749\n",
            "Total loss:  -2.705 | PDE Loss:  -4.5291 | Function Loss:  -2.7755\n",
            "Total loss:  -2.7053 | PDE Loss:  -4.5285 | Function Loss:  -2.776\n",
            "Total loss:  -2.7056 | PDE Loss:  -4.5275 | Function Loss:  -2.7765\n",
            "Total loss:  -2.706 | PDE Loss:  -4.5267 | Function Loss:  -2.7772\n",
            "Total loss:  -2.7064 | PDE Loss:  -4.5267 | Function Loss:  -2.7776\n",
            "Total loss:  -2.7067 | PDE Loss:  -4.5265 | Function Loss:  -2.778\n",
            "Total loss:  -2.707 | PDE Loss:  -4.5272 | Function Loss:  -2.7782\n",
            "Total loss:  -2.7073 | PDE Loss:  -4.5282 | Function Loss:  -2.7785\n",
            "Total loss:  -2.7077 | PDE Loss:  -4.5293 | Function Loss:  -2.7786\n",
            "Total loss:  -2.708 | PDE Loss:  -4.5304 | Function Loss:  -2.7788\n",
            "Total loss:  -2.7083 | PDE Loss:  -4.5316 | Function Loss:  -2.7789\n",
            "Total loss:  -2.7085 | PDE Loss:  -4.5333 | Function Loss:  -2.779\n",
            "Total loss:  -2.7089 | PDE Loss:  -4.5359 | Function Loss:  -2.779\n",
            "Total loss:  -2.7086 | PDE Loss:  -4.537 | Function Loss:  -2.7784\n",
            "Total loss:  -2.7091 | PDE Loss:  -4.5371 | Function Loss:  -2.7789\n",
            "Total loss:  -2.7094 | PDE Loss:  -4.5402 | Function Loss:  -2.7788\n",
            "Total loss:  -2.7097 | PDE Loss:  -4.5426 | Function Loss:  -2.7787\n",
            "Total loss:  -2.7099 | PDE Loss:  -4.5444 | Function Loss:  -2.7787\n",
            "Total loss:  -2.7102 | PDE Loss:  -4.5467 | Function Loss:  -2.7786\n",
            "Total loss:  -2.7104 | PDE Loss:  -4.547 | Function Loss:  -2.7788\n",
            "Total loss:  -2.7107 | PDE Loss:  -4.5481 | Function Loss:  -2.7789\n",
            "Total loss:  -2.711 | PDE Loss:  -4.5469 | Function Loss:  -2.7794\n",
            "Total loss:  -2.7112 | PDE Loss:  -4.5464 | Function Loss:  -2.7798\n",
            "Total loss:  -2.7115 | PDE Loss:  -4.5445 | Function Loss:  -2.7804\n",
            "Total loss:  -2.7116 | PDE Loss:  -4.5427 | Function Loss:  -2.7809\n",
            "Total loss:  -2.7118 | PDE Loss:  -4.542 | Function Loss:  -2.7813\n",
            "Total loss:  -2.712 | PDE Loss:  -4.5414 | Function Loss:  -2.7816\n",
            "Total loss:  -2.7121 | PDE Loss:  -4.5419 | Function Loss:  -2.7817\n",
            "Total loss:  -2.7123 | PDE Loss:  -4.5426 | Function Loss:  -2.7818\n",
            "Total loss:  -2.7125 | PDE Loss:  -4.5436 | Function Loss:  -2.7818\n",
            "Total loss:  -2.7127 | PDE Loss:  -4.5451 | Function Loss:  -2.7817\n",
            "Total loss:  -2.7128 | PDE Loss:  -4.5458 | Function Loss:  -2.7818\n",
            "Total loss:  -2.713 | PDE Loss:  -4.5467 | Function Loss:  -2.7818\n",
            "Total loss:  -2.7131 | PDE Loss:  -4.5471 | Function Loss:  -2.7819\n",
            "Total loss:  -2.7133 | PDE Loss:  -4.5472 | Function Loss:  -2.7821\n",
            "Total loss:  -2.7134 | PDE Loss:  -4.5472 | Function Loss:  -2.7823\n",
            "Total loss:  -2.7136 | PDE Loss:  -4.5468 | Function Loss:  -2.7825\n",
            "Total loss:  -2.7138 | PDE Loss:  -4.5463 | Function Loss:  -2.7829\n",
            "Total loss:  -2.7141 | PDE Loss:  -4.5449 | Function Loss:  -2.7834\n",
            "Total loss:  -2.7143 | PDE Loss:  -4.544 | Function Loss:  -2.7839\n",
            "Total loss:  -2.7146 | PDE Loss:  -4.5416 | Function Loss:  -2.7846\n",
            "Total loss:  -2.7148 | PDE Loss:  -4.5407 | Function Loss:  -2.7851\n",
            "Total loss:  -2.7151 | PDE Loss:  -4.5393 | Function Loss:  -2.7857\n",
            "Total loss:  -2.7155 | PDE Loss:  -4.5393 | Function Loss:  -2.7861\n",
            "Total loss:  -2.7158 | PDE Loss:  -4.5387 | Function Loss:  -2.7866\n",
            "Total loss:  -2.7161 | PDE Loss:  -4.5384 | Function Loss:  -2.787\n",
            "Total loss:  -2.7163 | PDE Loss:  -4.5393 | Function Loss:  -2.7871\n",
            "Total loss:  -2.7166 | PDE Loss:  -4.5397 | Function Loss:  -2.7873\n",
            "Total loss:  -2.7168 | PDE Loss:  -4.5394 | Function Loss:  -2.7876\n",
            "Total loss:  -2.7168 | PDE Loss:  -4.5414 | Function Loss:  -2.7872\n",
            "Total loss:  -2.7169 | PDE Loss:  -4.5407 | Function Loss:  -2.7875\n",
            "Total loss:  -2.7173 | PDE Loss:  -4.5393 | Function Loss:  -2.7882\n",
            "Total loss:  -2.7174 | PDE Loss:  -4.5381 | Function Loss:  -2.7886\n",
            "Total loss:  -2.7178 | PDE Loss:  -4.5366 | Function Loss:  -2.7893\n",
            "Total loss:  -2.7181 | PDE Loss:  -4.5349 | Function Loss:  -2.79\n",
            "Total loss:  -2.7184 | PDE Loss:  -4.5338 | Function Loss:  -2.7905\n",
            "Total loss:  -2.7186 | PDE Loss:  -4.5331 | Function Loss:  -2.7909\n",
            "Total loss:  -2.7188 | PDE Loss:  -4.5326 | Function Loss:  -2.7912\n",
            "Total loss:  -2.719 | PDE Loss:  -4.5333 | Function Loss:  -2.7913\n",
            "Total loss:  -2.7191 | PDE Loss:  -4.5333 | Function Loss:  -2.7915\n",
            "Total loss:  -2.7193 | PDE Loss:  -4.5343 | Function Loss:  -2.7914\n",
            "Total loss:  -2.7195 | PDE Loss:  -4.5346 | Function Loss:  -2.7916\n",
            "Total loss:  -2.7198 | PDE Loss:  -4.5349 | Function Loss:  -2.792\n",
            "Total loss:  -2.7202 | PDE Loss:  -4.5339 | Function Loss:  -2.7926\n",
            "Total loss:  -2.7205 | PDE Loss:  -4.5328 | Function Loss:  -2.7931\n",
            "Total loss:  -2.7208 | PDE Loss:  -4.5317 | Function Loss:  -2.7937\n",
            "Total loss:  -2.7211 | PDE Loss:  -4.5299 | Function Loss:  -2.7944\n",
            "Total loss:  -2.7215 | PDE Loss:  -4.5286 | Function Loss:  -2.7951\n",
            "Total loss:  -2.7218 | PDE Loss:  -4.5274 | Function Loss:  -2.7957\n",
            "Total loss:  -2.7222 | PDE Loss:  -4.5281 | Function Loss:  -2.796\n",
            "Total loss:  -2.7225 | PDE Loss:  -4.5287 | Function Loss:  -2.7962\n",
            "Total loss:  -2.7229 | PDE Loss:  -4.5314 | Function Loss:  -2.7962\n",
            "Total loss:  -2.7233 | PDE Loss:  -4.5328 | Function Loss:  -2.7964\n",
            "Total loss:  -2.7237 | PDE Loss:  -4.5356 | Function Loss:  -2.7964\n",
            "Total loss:  -2.7242 | PDE Loss:  -4.5376 | Function Loss:  -2.7967\n",
            "Total loss:  -2.7246 | PDE Loss:  -4.5379 | Function Loss:  -2.7971\n",
            "Total loss:  -2.725 | PDE Loss:  -4.5375 | Function Loss:  -2.7976\n",
            "Total loss:  -2.7254 | PDE Loss:  -4.5369 | Function Loss:  -2.7982\n",
            "Total loss:  -2.7256 | PDE Loss:  -4.5348 | Function Loss:  -2.7989\n",
            "Total loss:  -2.7258 | PDE Loss:  -4.5343 | Function Loss:  -2.7992\n",
            "Total loss:  -2.726 | PDE Loss:  -4.5327 | Function Loss:  -2.7997\n",
            "Total loss:  -2.7263 | PDE Loss:  -4.5315 | Function Loss:  -2.8002\n",
            "Total loss:  -2.7265 | PDE Loss:  -4.5301 | Function Loss:  -2.8008\n",
            "Total loss:  -2.7266 | PDE Loss:  -4.5284 | Function Loss:  -2.8013\n",
            "Total loss:  -2.7269 | PDE Loss:  -4.5271 | Function Loss:  -2.8018\n",
            "Total loss:  -2.727 | PDE Loss:  -4.5276 | Function Loss:  -2.8019\n",
            "Total loss:  -2.7273 | PDE Loss:  -4.5273 | Function Loss:  -2.8022\n",
            "Total loss:  -2.7274 | PDE Loss:  -4.5272 | Function Loss:  -2.8024\n",
            "Total loss:  -2.7276 | PDE Loss:  -4.5268 | Function Loss:  -2.8027\n",
            "Total loss:  -2.7278 | PDE Loss:  -4.526 | Function Loss:  -2.803\n",
            "Total loss:  -2.728 | PDE Loss:  -4.5252 | Function Loss:  -2.8034\n",
            "Total loss:  -2.7282 | PDE Loss:  -4.5233 | Function Loss:  -2.804\n",
            "Total loss:  -2.7284 | PDE Loss:  -4.5219 | Function Loss:  -2.8046\n",
            "Total loss:  -2.7287 | PDE Loss:  -4.5201 | Function Loss:  -2.8053\n",
            "Total loss:  -2.7289 | PDE Loss:  -4.5193 | Function Loss:  -2.8057\n",
            "Total loss:  -2.7292 | PDE Loss:  -4.5156 | Function Loss:  -2.8068\n",
            "Total loss:  -2.7294 | PDE Loss:  -4.5165 | Function Loss:  -2.8068\n",
            "Total loss:  -2.7297 | PDE Loss:  -4.5178 | Function Loss:  -2.8069\n",
            "Total loss:  -2.7299 | PDE Loss:  -4.5195 | Function Loss:  -2.8068\n",
            "Total loss:  -2.7301 | PDE Loss:  -4.5214 | Function Loss:  -2.8067\n",
            "Total loss:  -2.7303 | PDE Loss:  -4.5231 | Function Loss:  -2.8066\n",
            "Total loss:  -2.7305 | PDE Loss:  -4.5244 | Function Loss:  -2.8066\n",
            "Total loss:  -2.7304 | PDE Loss:  -4.5241 | Function Loss:  -2.8066\n",
            "Total loss:  -2.7306 | PDE Loss:  -4.5245 | Function Loss:  -2.8067\n",
            "Total loss:  -2.731 | PDE Loss:  -4.527 | Function Loss:  -2.8066\n",
            "Total loss:  -2.7312 | PDE Loss:  -4.527 | Function Loss:  -2.8069\n",
            "Total loss:  -2.7315 | PDE Loss:  -4.526 | Function Loss:  -2.8075\n",
            "Total loss:  -2.7318 | PDE Loss:  -4.5245 | Function Loss:  -2.8082\n",
            "Total loss:  -2.7321 | PDE Loss:  -4.5225 | Function Loss:  -2.8089\n",
            "Total loss:  -2.7323 | PDE Loss:  -4.5212 | Function Loss:  -2.8094\n",
            "Total loss:  -2.7325 | PDE Loss:  -4.5192 | Function Loss:  -2.81\n",
            "Total loss:  -2.7327 | PDE Loss:  -4.5187 | Function Loss:  -2.8103\n",
            "Total loss:  -2.733 | PDE Loss:  -4.5179 | Function Loss:  -2.8108\n",
            "Total loss:  -2.7332 | PDE Loss:  -4.5175 | Function Loss:  -2.8112\n",
            "Total loss:  -2.7335 | PDE Loss:  -4.5189 | Function Loss:  -2.8112\n",
            "Total loss:  -2.7337 | PDE Loss:  -4.5205 | Function Loss:  -2.8112\n",
            "Total loss:  -2.734 | PDE Loss:  -4.5227 | Function Loss:  -2.8111\n",
            "Total loss:  -2.7343 | PDE Loss:  -4.525 | Function Loss:  -2.811\n",
            "Total loss:  -2.7345 | PDE Loss:  -4.5265 | Function Loss:  -2.8109\n",
            "Total loss:  -2.7347 | PDE Loss:  -4.5278 | Function Loss:  -2.8109\n",
            "Total loss:  -2.7348 | PDE Loss:  -4.5287 | Function Loss:  -2.8109\n",
            "Total loss:  -2.735 | PDE Loss:  -4.5292 | Function Loss:  -2.8111\n",
            "Total loss:  -2.7352 | PDE Loss:  -4.5288 | Function Loss:  -2.8114\n",
            "Total loss:  -2.7354 | PDE Loss:  -4.5285 | Function Loss:  -2.8117\n",
            "Total loss:  -2.7356 | PDE Loss:  -4.5278 | Function Loss:  -2.8121\n",
            "Total loss:  -2.7358 | PDE Loss:  -4.5275 | Function Loss:  -2.8123\n",
            "Total loss:  -2.7361 | PDE Loss:  -4.5277 | Function Loss:  -2.8126\n",
            "Total loss:  -2.7364 | PDE Loss:  -4.5276 | Function Loss:  -2.8131\n",
            "Total loss:  -2.7368 | PDE Loss:  -4.5329 | Function Loss:  -2.8125\n",
            "Total loss:  -2.7371 | PDE Loss:  -4.5326 | Function Loss:  -2.8129\n",
            "Total loss:  -2.7378 | PDE Loss:  -4.5349 | Function Loss:  -2.8133\n",
            "Total loss:  -2.7386 | PDE Loss:  -4.5383 | Function Loss:  -2.8136\n",
            "Total loss:  -2.7394 | PDE Loss:  -4.5446 | Function Loss:  -2.8134\n",
            "Total loss:  -2.7401 | PDE Loss:  -4.5502 | Function Loss:  -2.8132\n",
            "Total loss:  -2.7408 | PDE Loss:  -4.5549 | Function Loss:  -2.8131\n",
            "Total loss:  -2.7415 | PDE Loss:  -4.5582 | Function Loss:  -2.8134\n",
            "Total loss:  -2.7422 | PDE Loss:  -4.5608 | Function Loss:  -2.8137\n",
            "Total loss:  -2.7428 | PDE Loss:  -4.5604 | Function Loss:  -2.8145\n",
            "Total loss:  -2.7434 | PDE Loss:  -4.5602 | Function Loss:  -2.8152\n",
            "Total loss:  -2.7439 | PDE Loss:  -4.5584 | Function Loss:  -2.8162\n",
            "Total loss:  -2.7446 | PDE Loss:  -4.5573 | Function Loss:  -2.8171\n",
            "Total loss:  -2.7453 | PDE Loss:  -4.5557 | Function Loss:  -2.8183\n",
            "Total loss:  -2.746 | PDE Loss:  -4.5547 | Function Loss:  -2.8193\n",
            "Total loss:  -2.7466 | PDE Loss:  -4.5545 | Function Loss:  -2.8201\n",
            "Total loss:  -2.7471 | PDE Loss:  -4.554 | Function Loss:  -2.8208\n",
            "Total loss:  -2.7475 | PDE Loss:  -4.555 | Function Loss:  -2.8211\n",
            "Total loss:  -2.7479 | PDE Loss:  -4.5561 | Function Loss:  -2.8213\n",
            "Total loss:  -2.7483 | PDE Loss:  -4.5582 | Function Loss:  -2.8214\n",
            "Total loss:  -2.7487 | PDE Loss:  -4.5604 | Function Loss:  -2.8215\n",
            "Total loss:  -2.7491 | PDE Loss:  -4.5617 | Function Loss:  -2.8217\n",
            "Total loss:  -2.7495 | PDE Loss:  -4.5635 | Function Loss:  -2.8219\n",
            "Total loss:  -2.75 | PDE Loss:  -4.5651 | Function Loss:  -2.8221\n",
            "Total loss:  -2.7507 | PDE Loss:  -4.5664 | Function Loss:  -2.8227\n",
            "Total loss:  -2.7516 | PDE Loss:  -4.5677 | Function Loss:  -2.8236\n",
            "Total loss:  -2.7525 | PDE Loss:  -4.57 | Function Loss:  -2.8242\n",
            "Total loss:  -2.7532 | PDE Loss:  -4.57 | Function Loss:  -2.825\n",
            "Total loss:  -2.7542 | PDE Loss:  -4.5737 | Function Loss:  -2.8256\n",
            "Total loss:  -2.7551 | PDE Loss:  -4.5747 | Function Loss:  -2.8265\n",
            "Total loss:  -2.7557 | PDE Loss:  -4.5771 | Function Loss:  -2.8268\n",
            "Total loss:  -2.7562 | PDE Loss:  -4.5784 | Function Loss:  -2.8271\n",
            "Total loss:  -2.7567 | PDE Loss:  -4.5794 | Function Loss:  -2.8275\n",
            "Total loss:  -2.7572 | PDE Loss:  -4.5801 | Function Loss:  -2.828\n",
            "Total loss:  -2.7581 | PDE Loss:  -4.5799 | Function Loss:  -2.8291\n",
            "Total loss:  -2.7592 | PDE Loss:  -4.5826 | Function Loss:  -2.8299\n",
            "Total loss:  -2.7602 | PDE Loss:  -4.5818 | Function Loss:  -2.8312\n",
            "Total loss:  -2.7611 | PDE Loss:  -4.5851 | Function Loss:  -2.8317\n",
            "Total loss:  -2.7618 | PDE Loss:  -4.5855 | Function Loss:  -2.8325\n",
            "Total loss:  -2.7623 | PDE Loss:  -4.5869 | Function Loss:  -2.8327\n",
            "Total loss:  -2.7627 | PDE Loss:  -4.589 | Function Loss:  -2.8329\n",
            "Total loss:  -2.763 | PDE Loss:  -4.5897 | Function Loss:  -2.8331\n",
            "Total loss:  -2.7632 | PDE Loss:  -4.5905 | Function Loss:  -2.8332\n",
            "Total loss:  -2.7635 | PDE Loss:  -4.5907 | Function Loss:  -2.8335\n",
            "Total loss:  -2.7638 | PDE Loss:  -4.5906 | Function Loss:  -2.8338\n",
            "Total loss:  -2.764 | PDE Loss:  -4.5905 | Function Loss:  -2.8342\n",
            "Total loss:  -2.7643 | PDE Loss:  -4.5902 | Function Loss:  -2.8345\n",
            "Total loss:  -2.7644 | PDE Loss:  -4.5902 | Function Loss:  -2.8347\n",
            "Total loss:  -2.7647 | PDE Loss:  -4.5911 | Function Loss:  -2.8348\n",
            "Total loss:  -2.7649 | PDE Loss:  -4.5914 | Function Loss:  -2.835\n",
            "Total loss:  -2.7652 | PDE Loss:  -4.5921 | Function Loss:  -2.8352\n",
            "Total loss:  -2.7654 | PDE Loss:  -4.5923 | Function Loss:  -2.8355\n",
            "Total loss:  -2.7657 | PDE Loss:  -4.5935 | Function Loss:  -2.8355\n",
            "Total loss:  -2.7659 | PDE Loss:  -4.5938 | Function Loss:  -2.8358\n",
            "Total loss:  -2.7662 | PDE Loss:  -4.5952 | Function Loss:  -2.8358\n",
            "Total loss:  -2.7664 | PDE Loss:  -4.5957 | Function Loss:  -2.836\n",
            "Total loss:  -2.7666 | PDE Loss:  -4.5963 | Function Loss:  -2.8362\n",
            "Total loss:  -2.7668 | PDE Loss:  -4.5974 | Function Loss:  -2.8362\n",
            "Total loss:  -2.767 | PDE Loss:  -4.5972 | Function Loss:  -2.8364\n",
            "Total loss:  -2.7671 | PDE Loss:  -4.5976 | Function Loss:  -2.8365\n",
            "Total loss:  -2.7672 | PDE Loss:  -4.5972 | Function Loss:  -2.8367\n",
            "Total loss:  -2.7673 | PDE Loss:  -4.5971 | Function Loss:  -2.8368\n",
            "Total loss:  -2.7674 | PDE Loss:  -4.5967 | Function Loss:  -2.837\n",
            "Total loss:  -2.7675 | PDE Loss:  -4.596 | Function Loss:  -2.8373\n",
            "Total loss:  -2.7676 | PDE Loss:  -4.595 | Function Loss:  -2.8376\n",
            "Total loss:  -2.7678 | PDE Loss:  -4.5939 | Function Loss:  -2.838\n",
            "Total loss:  -2.768 | PDE Loss:  -4.5925 | Function Loss:  -2.8385\n",
            "Total loss:  -2.7683 | PDE Loss:  -4.5917 | Function Loss:  -2.839\n",
            "Total loss:  -2.7687 | PDE Loss:  -4.5919 | Function Loss:  -2.8395\n",
            "Total loss:  -2.7691 | PDE Loss:  -4.5927 | Function Loss:  -2.8397\n",
            "Total loss:  -2.7694 | PDE Loss:  -4.5937 | Function Loss:  -2.8399\n",
            "Total loss:  -2.7697 | PDE Loss:  -4.5949 | Function Loss:  -2.84\n",
            "Total loss:  -2.7698 | PDE Loss:  -4.596 | Function Loss:  -2.84\n",
            "Total loss:  -2.77 | PDE Loss:  -4.5968 | Function Loss:  -2.8401\n",
            "Total loss:  -2.7702 | PDE Loss:  -4.5973 | Function Loss:  -2.8402\n",
            "Total loss:  -2.7705 | PDE Loss:  -4.5983 | Function Loss:  -2.8404\n",
            "Total loss:  -2.7708 | PDE Loss:  -4.5988 | Function Loss:  -2.8406\n",
            "Total loss:  -2.7711 | PDE Loss:  -4.5993 | Function Loss:  -2.8409\n",
            "Total loss:  -2.7715 | PDE Loss:  -4.5998 | Function Loss:  -2.8413\n",
            "Total loss:  -2.7719 | PDE Loss:  -4.5994 | Function Loss:  -2.8419\n",
            "Total loss:  -2.7723 | PDE Loss:  -4.6006 | Function Loss:  -2.8421\n",
            "Total loss:  -2.7725 | PDE Loss:  -4.601 | Function Loss:  -2.8423\n",
            "Total loss:  -2.7728 | PDE Loss:  -4.6016 | Function Loss:  -2.8426\n",
            "Total loss:  -2.7732 | PDE Loss:  -4.6044 | Function Loss:  -2.8425\n",
            "Total loss:  -2.7735 | PDE Loss:  -4.6019 | Function Loss:  -2.8434\n",
            "Total loss:  -2.7738 | PDE Loss:  -4.604 | Function Loss:  -2.8433\n",
            "Total loss:  -2.7741 | PDE Loss:  -4.6059 | Function Loss:  -2.8433\n",
            "Total loss:  -2.7745 | PDE Loss:  -4.6074 | Function Loss:  -2.8436\n",
            "Total loss:  -2.775 | PDE Loss:  -4.6079 | Function Loss:  -2.844\n",
            "Total loss:  -2.7755 | PDE Loss:  -4.6051 | Function Loss:  -2.845\n",
            "Total loss:  -2.776 | PDE Loss:  -4.6079 | Function Loss:  -2.8452\n",
            "Total loss:  -2.7764 | PDE Loss:  -4.606 | Function Loss:  -2.846\n",
            "Total loss:  -2.7769 | PDE Loss:  -4.6048 | Function Loss:  -2.8467\n",
            "Total loss:  -2.7774 | PDE Loss:  -4.6043 | Function Loss:  -2.8474\n",
            "Total loss:  -2.7777 | PDE Loss:  -4.6047 | Function Loss:  -2.8478\n",
            "Total loss:  -2.7781 | PDE Loss:  -4.6056 | Function Loss:  -2.848\n",
            "Total loss:  -2.7784 | PDE Loss:  -4.6075 | Function Loss:  -2.848\n",
            "Total loss:  -2.7787 | PDE Loss:  -4.6101 | Function Loss:  -2.848\n",
            "Total loss:  -2.7792 | PDE Loss:  -4.613 | Function Loss:  -2.848\n",
            "Total loss:  -2.7796 | PDE Loss:  -4.6183 | Function Loss:  -2.8476\n",
            "Total loss:  -2.7801 | PDE Loss:  -4.6228 | Function Loss:  -2.8475\n",
            "Total loss:  -2.7808 | PDE Loss:  -4.6286 | Function Loss:  -2.8473\n",
            "Total loss:  -2.7814 | PDE Loss:  -4.6337 | Function Loss:  -2.8472\n",
            "Total loss:  -2.7821 | PDE Loss:  -4.6362 | Function Loss:  -2.8475\n",
            "Total loss:  -2.7827 | PDE Loss:  -4.6391 | Function Loss:  -2.8477\n",
            "Total loss:  -2.7831 | PDE Loss:  -4.6379 | Function Loss:  -2.8485\n",
            "Total loss:  -2.7835 | PDE Loss:  -4.6362 | Function Loss:  -2.8492\n",
            "Total loss:  -2.7839 | PDE Loss:  -4.6328 | Function Loss:  -2.8502\n",
            "Total loss:  -2.7842 | PDE Loss:  -4.6301 | Function Loss:  -2.851\n",
            "Total loss:  -2.7846 | PDE Loss:  -4.6271 | Function Loss:  -2.8519\n",
            "Total loss:  -2.7852 | PDE Loss:  -4.6256 | Function Loss:  -2.853\n",
            "Total loss:  -2.7862 | PDE Loss:  -4.6235 | Function Loss:  -2.8545\n",
            "Total loss:  -2.7874 | PDE Loss:  -4.6233 | Function Loss:  -2.8559\n",
            "Total loss:  -2.7888 | PDE Loss:  -4.6231 | Function Loss:  -2.8575\n",
            "Total loss:  -2.7901 | PDE Loss:  -4.6242 | Function Loss:  -2.8589\n",
            "Total loss:  -2.7913 | PDE Loss:  -4.627 | Function Loss:  -2.8598\n",
            "Total loss:  -2.7922 | PDE Loss:  -4.629 | Function Loss:  -2.8606\n",
            "Total loss:  -2.793 | PDE Loss:  -4.634 | Function Loss:  -2.8606\n",
            "Total loss:  -2.7935 | PDE Loss:  -4.6356 | Function Loss:  -2.8609\n",
            "Total loss:  -2.7939 | PDE Loss:  -4.6365 | Function Loss:  -2.8612\n",
            "Total loss:  -2.7944 | PDE Loss:  -4.6393 | Function Loss:  -2.8614\n",
            "Total loss:  -2.7949 | PDE Loss:  -4.6386 | Function Loss:  -2.8621\n",
            "Total loss:  -2.7953 | PDE Loss:  -4.6379 | Function Loss:  -2.8627\n",
            "Total loss:  -2.796 | PDE Loss:  -4.6372 | Function Loss:  -2.8636\n",
            "Total loss:  -2.7968 | PDE Loss:  -4.6327 | Function Loss:  -2.8652\n",
            "Total loss:  -2.7973 | PDE Loss:  -4.6323 | Function Loss:  -2.866\n",
            "Total loss:  -2.7978 | PDE Loss:  -4.6294 | Function Loss:  -2.867\n",
            "Total loss:  -2.7982 | PDE Loss:  -4.629 | Function Loss:  -2.8676\n",
            "Total loss:  -2.7985 | PDE Loss:  -4.6292 | Function Loss:  -2.8679\n",
            "Total loss:  -2.7988 | PDE Loss:  -4.63 | Function Loss:  -2.8681\n",
            "Total loss:  -2.7991 | PDE Loss:  -4.6312 | Function Loss:  -2.8682\n",
            "Total loss:  -2.7994 | PDE Loss:  -4.6319 | Function Loss:  -2.8685\n",
            "Total loss:  -2.8001 | PDE Loss:  -4.6358 | Function Loss:  -2.8686\n",
            "Total loss:  -2.8009 | PDE Loss:  -4.6296 | Function Loss:  -2.8706\n",
            "Total loss:  -2.8016 | PDE Loss:  -4.6348 | Function Loss:  -2.8706\n",
            "Total loss:  -2.8024 | PDE Loss:  -4.6382 | Function Loss:  -2.8709\n",
            "Total loss:  -2.8032 | PDE Loss:  -4.6401 | Function Loss:  -2.8715\n",
            "Total loss:  -2.8039 | PDE Loss:  -4.6399 | Function Loss:  -2.8724\n",
            "Total loss:  -2.8045 | PDE Loss:  -4.6402 | Function Loss:  -2.873\n",
            "Total loss:  -2.8049 | PDE Loss:  -4.6381 | Function Loss:  -2.8738\n",
            "Total loss:  -2.8052 | PDE Loss:  -4.6385 | Function Loss:  -2.8742\n",
            "Total loss:  -2.8057 | PDE Loss:  -4.6386 | Function Loss:  -2.8747\n",
            "Total loss:  -2.8063 | PDE Loss:  -4.6399 | Function Loss:  -2.8752\n",
            "Total loss:  -2.8071 | PDE Loss:  -4.6406 | Function Loss:  -2.876\n",
            "Total loss:  -2.8078 | PDE Loss:  -4.6409 | Function Loss:  -2.8768\n",
            "Total loss:  -2.8086 | PDE Loss:  -4.6411 | Function Loss:  -2.8776\n",
            "Total loss:  -2.8095 | PDE Loss:  -4.6412 | Function Loss:  -2.8788\n",
            "Total loss:  -2.8104 | PDE Loss:  -4.6399 | Function Loss:  -2.88\n",
            "Total loss:  -2.8112 | PDE Loss:  -4.6413 | Function Loss:  -2.8807\n",
            "Total loss:  -2.8121 | PDE Loss:  -4.6413 | Function Loss:  -2.8817\n",
            "Total loss:  -2.8128 | PDE Loss:  -4.6433 | Function Loss:  -2.8822\n",
            "Total loss:  -2.8133 | PDE Loss:  -4.6437 | Function Loss:  -2.8828\n",
            "Total loss:  -2.8138 | PDE Loss:  -4.6464 | Function Loss:  -2.8829\n",
            "Total loss:  -2.8143 | PDE Loss:  -4.6474 | Function Loss:  -2.8832\n",
            "Total loss:  -2.815 | PDE Loss:  -4.6512 | Function Loss:  -2.8834\n",
            "Total loss:  -2.8159 | PDE Loss:  -4.6521 | Function Loss:  -2.8843\n",
            "Total loss:  -2.8167 | PDE Loss:  -4.6567 | Function Loss:  -2.8845\n",
            "Total loss:  -2.8176 | PDE Loss:  -4.6624 | Function Loss:  -2.8846\n",
            "Total loss:  -2.819 | PDE Loss:  -4.6696 | Function Loss:  -2.885\n",
            "Total loss:  -2.8205 | PDE Loss:  -4.675 | Function Loss:  -2.8859\n",
            "Total loss:  -2.822 | PDE Loss:  -4.6806 | Function Loss:  -2.8867\n",
            "Total loss:  -2.8234 | PDE Loss:  -4.6818 | Function Loss:  -2.8881\n",
            "Total loss:  -2.8244 | PDE Loss:  -4.6806 | Function Loss:  -2.8895\n",
            "Total loss:  -2.8252 | PDE Loss:  -4.6803 | Function Loss:  -2.8905\n",
            "Total loss:  -2.826 | PDE Loss:  -4.678 | Function Loss:  -2.8919\n",
            "Total loss:  -2.827 | PDE Loss:  -4.6779 | Function Loss:  -2.893\n",
            "Total loss:  -2.8281 | PDE Loss:  -4.6785 | Function Loss:  -2.8941\n",
            "Total loss:  -2.8293 | PDE Loss:  -4.6801 | Function Loss:  -2.8953\n",
            "Total loss:  -2.8305 | PDE Loss:  -4.6836 | Function Loss:  -2.8961\n",
            "Total loss:  -2.8316 | PDE Loss:  -4.6859 | Function Loss:  -2.897\n",
            "Total loss:  -2.8322 | PDE Loss:  -4.6898 | Function Loss:  -2.8971\n",
            "Total loss:  -2.8329 | PDE Loss:  -4.6943 | Function Loss:  -2.8972\n",
            "Total loss:  -2.8336 | PDE Loss:  -4.6991 | Function Loss:  -2.8972\n",
            "Total loss:  -2.8342 | PDE Loss:  -4.7044 | Function Loss:  -2.8971\n",
            "Total loss:  -2.8347 | PDE Loss:  -4.7068 | Function Loss:  -2.8973\n",
            "Total loss:  -2.8354 | PDE Loss:  -4.7109 | Function Loss:  -2.8974\n",
            "Total loss:  -2.8362 | PDE Loss:  -4.7146 | Function Loss:  -2.8978\n",
            "Total loss:  -2.837 | PDE Loss:  -4.7151 | Function Loss:  -2.8987\n",
            "Total loss:  -2.8377 | PDE Loss:  -4.7168 | Function Loss:  -2.8992\n",
            "Total loss:  -2.8382 | PDE Loss:  -4.7157 | Function Loss:  -2.9\n",
            "Total loss:  -2.8386 | PDE Loss:  -4.7152 | Function Loss:  -2.9005\n",
            "Total loss:  -2.8391 | PDE Loss:  -4.7132 | Function Loss:  -2.9014\n",
            "Total loss:  -2.8395 | PDE Loss:  -4.7124 | Function Loss:  -2.902\n",
            "Total loss:  -2.8401 | PDE Loss:  -4.7107 | Function Loss:  -2.9029\n",
            "Total loss:  -2.8409 | PDE Loss:  -4.7088 | Function Loss:  -2.9041\n",
            "Total loss:  -2.8415 | PDE Loss:  -4.7057 | Function Loss:  -2.9054\n",
            "Total loss:  -2.8424 | PDE Loss:  -4.7034 | Function Loss:  -2.9068\n",
            "Total loss:  -2.843 | PDE Loss:  -4.7031 | Function Loss:  -2.9075\n",
            "Total loss:  -2.844 | PDE Loss:  -4.7032 | Function Loss:  -2.9087\n",
            "Total loss:  -2.8452 | PDE Loss:  -4.7014 | Function Loss:  -2.9103\n",
            "Total loss:  -2.8461 | PDE Loss:  -4.7011 | Function Loss:  -2.9114\n",
            "Total loss:  -2.8469 | PDE Loss:  -4.7006 | Function Loss:  -2.9124\n",
            "Total loss:  -2.8476 | PDE Loss:  -4.6995 | Function Loss:  -2.9134\n",
            "Total loss:  -2.8481 | PDE Loss:  -4.6987 | Function Loss:  -2.9141\n",
            "Total loss:  -2.8483 | PDE Loss:  -4.6981 | Function Loss:  -2.9145\n",
            "Total loss:  -2.8485 | PDE Loss:  -4.6968 | Function Loss:  -2.9149\n",
            "Total loss:  -2.8487 | PDE Loss:  -4.6957 | Function Loss:  -2.9153\n",
            "Total loss:  -2.8489 | PDE Loss:  -4.6955 | Function Loss:  -2.9156\n",
            "Total loss:  -2.8492 | PDE Loss:  -4.6956 | Function Loss:  -2.916\n",
            "Total loss:  -2.8497 | PDE Loss:  -4.6957 | Function Loss:  -2.9165\n",
            "Total loss:  -2.8502 | PDE Loss:  -4.697 | Function Loss:  -2.9169\n",
            "Total loss:  -2.8508 | PDE Loss:  -4.6971 | Function Loss:  -2.9175\n",
            "Total loss:  -2.8513 | PDE Loss:  -4.6979 | Function Loss:  -2.918\n",
            "Total loss:  -2.8518 | PDE Loss:  -4.6988 | Function Loss:  -2.9184\n",
            "Total loss:  -2.8522 | PDE Loss:  -4.6986 | Function Loss:  -2.919\n",
            "Total loss:  -2.8527 | PDE Loss:  -4.6993 | Function Loss:  -2.9194\n",
            "Total loss:  -2.8533 | PDE Loss:  -4.6999 | Function Loss:  -2.92\n",
            "Total loss:  -2.8541 | PDE Loss:  -4.7014 | Function Loss:  -2.9207\n",
            "Total loss:  -2.8549 | PDE Loss:  -4.7033 | Function Loss:  -2.9213\n",
            "Total loss:  -2.8554 | PDE Loss:  -4.7048 | Function Loss:  -2.9217\n",
            "Total loss:  -2.8557 | PDE Loss:  -4.7049 | Function Loss:  -2.922\n",
            "Total loss:  -2.856 | PDE Loss:  -4.7057 | Function Loss:  -2.9221\n",
            "Total loss:  -2.8561 | PDE Loss:  -4.7062 | Function Loss:  -2.9223\n",
            "Total loss:  -2.8563 | PDE Loss:  -4.7069 | Function Loss:  -2.9223\n",
            "Total loss:  -2.8565 | PDE Loss:  -4.7087 | Function Loss:  -2.9223\n",
            "Total loss:  -2.8567 | PDE Loss:  -4.7092 | Function Loss:  -2.9225\n",
            "Total loss:  -2.857 | PDE Loss:  -4.7116 | Function Loss:  -2.9224\n",
            "Total loss:  -2.8573 | PDE Loss:  -4.7129 | Function Loss:  -2.9225\n",
            "Total loss:  -2.8576 | PDE Loss:  -4.7139 | Function Loss:  -2.9228\n",
            "Total loss:  -2.8581 | PDE Loss:  -4.7162 | Function Loss:  -2.9229\n",
            "Total loss:  -2.8586 | PDE Loss:  -4.7189 | Function Loss:  -2.923\n",
            "Total loss:  -2.8589 | PDE Loss:  -4.7218 | Function Loss:  -2.9229\n",
            "Total loss:  -2.8592 | PDE Loss:  -4.725 | Function Loss:  -2.9228\n",
            "Total loss:  -2.8596 | PDE Loss:  -4.7289 | Function Loss:  -2.9226\n",
            "Total loss:  -2.8599 | PDE Loss:  -4.733 | Function Loss:  -2.9223\n",
            "Total loss:  -2.8602 | PDE Loss:  -4.7353 | Function Loss:  -2.9223\n",
            "Total loss:  -2.8604 | PDE Loss:  -4.7381 | Function Loss:  -2.9221\n",
            "Total loss:  -2.8606 | PDE Loss:  -4.7399 | Function Loss:  -2.9221\n",
            "Total loss:  -2.861 | PDE Loss:  -4.7397 | Function Loss:  -2.9226\n",
            "Total loss:  -2.8613 | PDE Loss:  -4.7398 | Function Loss:  -2.9229\n",
            "Total loss:  -2.8618 | PDE Loss:  -4.7397 | Function Loss:  -2.9235\n",
            "Total loss:  -2.8622 | PDE Loss:  -4.739 | Function Loss:  -2.9241\n",
            "Total loss:  -2.8627 | PDE Loss:  -4.7393 | Function Loss:  -2.9247\n",
            "Total loss:  -2.8632 | PDE Loss:  -4.7388 | Function Loss:  -2.9253\n",
            "Total loss:  -2.8637 | PDE Loss:  -4.7392 | Function Loss:  -2.9257\n",
            "Total loss:  -2.8641 | PDE Loss:  -4.7397 | Function Loss:  -2.9262\n",
            "Total loss:  -2.8645 | PDE Loss:  -4.7409 | Function Loss:  -2.9264\n",
            "Total loss:  -2.8648 | PDE Loss:  -4.7423 | Function Loss:  -2.9266\n",
            "Total loss:  -2.8651 | PDE Loss:  -4.7434 | Function Loss:  -2.9268\n",
            "Total loss:  -2.8654 | PDE Loss:  -4.7454 | Function Loss:  -2.9268\n",
            "Total loss:  -2.8656 | PDE Loss:  -4.7462 | Function Loss:  -2.9269\n",
            "Total loss:  -2.866 | PDE Loss:  -4.7481 | Function Loss:  -2.927\n",
            "Total loss:  -2.8663 | PDE Loss:  -4.749 | Function Loss:  -2.9273\n",
            "Total loss:  -2.8666 | PDE Loss:  -4.75 | Function Loss:  -2.9274\n",
            "Total loss:  -2.8668 | PDE Loss:  -4.7505 | Function Loss:  -2.9276\n",
            "Total loss:  -2.8669 | PDE Loss:  -4.7509 | Function Loss:  -2.9277\n",
            "Total loss:  -2.8671 | PDE Loss:  -4.7512 | Function Loss:  -2.9279\n",
            "Total loss:  -2.8672 | PDE Loss:  -4.7518 | Function Loss:  -2.9279\n",
            "Total loss:  -2.8674 | PDE Loss:  -4.7523 | Function Loss:  -2.928\n",
            "Total loss:  -2.8675 | PDE Loss:  -4.7528 | Function Loss:  -2.9282\n",
            "Total loss:  -2.8678 | PDE Loss:  -4.7538 | Function Loss:  -2.9282\n",
            "Total loss:  -2.8679 | PDE Loss:  -4.7505 | Function Loss:  -2.9289\n",
            "Total loss:  -2.8681 | PDE Loss:  -4.7513 | Function Loss:  -2.9291\n",
            "Total loss:  -2.8684 | PDE Loss:  -4.7532 | Function Loss:  -2.9291\n",
            "Total loss:  -2.8689 | PDE Loss:  -4.7563 | Function Loss:  -2.9292\n",
            "Total loss:  -2.8694 | PDE Loss:  -4.759 | Function Loss:  -2.9294\n",
            "Total loss:  -2.8698 | PDE Loss:  -4.7604 | Function Loss:  -2.9296\n",
            "Total loss:  -2.8701 | PDE Loss:  -4.7618 | Function Loss:  -2.9298\n",
            "Total loss:  -2.8704 | PDE Loss:  -4.7622 | Function Loss:  -2.9301\n",
            "Total loss:  -2.8707 | PDE Loss:  -4.7628 | Function Loss:  -2.9303\n",
            "Total loss:  -2.8711 | PDE Loss:  -4.7631 | Function Loss:  -2.9307\n",
            "Total loss:  -2.8713 | PDE Loss:  -4.7628 | Function Loss:  -2.931\n",
            "Total loss:  -2.8717 | PDE Loss:  -4.7625 | Function Loss:  -2.9314\n",
            "Total loss:  -2.8719 | PDE Loss:  -4.7618 | Function Loss:  -2.9319\n",
            "Total loss:  -2.8723 | PDE Loss:  -4.7616 | Function Loss:  -2.9323\n",
            "Total loss:  -2.8727 | PDE Loss:  -4.7616 | Function Loss:  -2.9327\n",
            "Total loss:  -2.873 | PDE Loss:  -4.7619 | Function Loss:  -2.9331\n",
            "Total loss:  -2.8735 | PDE Loss:  -4.7627 | Function Loss:  -2.9335\n",
            "Total loss:  -2.8739 | PDE Loss:  -4.7638 | Function Loss:  -2.9338\n",
            "Total loss:  -2.8745 | PDE Loss:  -4.7647 | Function Loss:  -2.9343\n",
            "Total loss:  -2.875 | PDE Loss:  -4.7653 | Function Loss:  -2.9349\n",
            "Total loss:  -2.8756 | PDE Loss:  -4.7654 | Function Loss:  -2.9356\n",
            "Total loss:  -2.8763 | PDE Loss:  -4.7643 | Function Loss:  -2.9365\n",
            "Total loss:  -2.8769 | PDE Loss:  -4.7644 | Function Loss:  -2.9372\n",
            "Total loss:  -2.8775 | PDE Loss:  -4.7612 | Function Loss:  -2.9384\n",
            "Total loss:  -2.878 | PDE Loss:  -4.7611 | Function Loss:  -2.9389\n",
            "Total loss:  -2.8785 | PDE Loss:  -4.7607 | Function Loss:  -2.9395\n",
            "Total loss:  -2.8791 | PDE Loss:  -4.7591 | Function Loss:  -2.9404\n",
            "Total loss:  -2.8796 | PDE Loss:  -4.7577 | Function Loss:  -2.9413\n",
            "Total loss:  -2.8799 | PDE Loss:  -4.7575 | Function Loss:  -2.9417\n",
            "Total loss:  -2.8804 | PDE Loss:  -4.7563 | Function Loss:  -2.9424\n",
            "Total loss:  -2.8809 | PDE Loss:  -4.7561 | Function Loss:  -2.943\n",
            "Total loss:  -2.8814 | PDE Loss:  -4.7563 | Function Loss:  -2.9435\n",
            "Total loss:  -2.882 | PDE Loss:  -4.7574 | Function Loss:  -2.944\n",
            "Total loss:  -2.8826 | PDE Loss:  -4.7552 | Function Loss:  -2.9451\n",
            "Total loss:  -2.8814 | PDE Loss:  -4.7594 | Function Loss:  -2.9431\n",
            "Total loss:  -2.8832 | PDE Loss:  -4.7593 | Function Loss:  -2.9452\n",
            "Total loss:  -2.8836 | PDE Loss:  -4.7592 | Function Loss:  -2.9457\n",
            "Total loss:  -2.8843 | PDE Loss:  -4.76 | Function Loss:  -2.9463\n",
            "Total loss:  -2.8849 | PDE Loss:  -4.7596 | Function Loss:  -2.9471\n",
            "Total loss:  -2.8854 | PDE Loss:  -4.7593 | Function Loss:  -2.9477\n",
            "Total loss:  -2.8856 | PDE Loss:  -4.76 | Function Loss:  -2.9479\n",
            "Total loss:  -2.886 | PDE Loss:  -4.7601 | Function Loss:  -2.9483\n",
            "Total loss:  -2.8864 | PDE Loss:  -4.7596 | Function Loss:  -2.9489\n",
            "Total loss:  -2.8868 | PDE Loss:  -4.7592 | Function Loss:  -2.9494\n",
            "Total loss:  -2.8871 | PDE Loss:  -4.7581 | Function Loss:  -2.9499\n",
            "Total loss:  -2.8874 | PDE Loss:  -4.757 | Function Loss:  -2.9504\n",
            "Total loss:  -2.8878 | PDE Loss:  -4.7558 | Function Loss:  -2.9511\n",
            "Total loss:  -2.8883 | PDE Loss:  -4.754 | Function Loss:  -2.9519\n",
            "Total loss:  -2.8887 | PDE Loss:  -4.753 | Function Loss:  -2.9526\n",
            "Total loss:  -2.8891 | PDE Loss:  -4.7513 | Function Loss:  -2.9533\n",
            "Total loss:  -2.8894 | PDE Loss:  -4.7512 | Function Loss:  -2.9536\n",
            "Total loss:  -2.8898 | PDE Loss:  -4.7518 | Function Loss:  -2.9541\n",
            "Total loss:  -2.8904 | PDE Loss:  -4.7527 | Function Loss:  -2.9546\n",
            "Total loss:  -2.8909 | PDE Loss:  -4.7535 | Function Loss:  -2.955\n",
            "Total loss:  -2.8914 | PDE Loss:  -4.7531 | Function Loss:  -2.9556\n",
            "Total loss:  -2.8918 | PDE Loss:  -4.7525 | Function Loss:  -2.9562\n",
            "Total loss:  -2.8922 | PDE Loss:  -4.7507 | Function Loss:  -2.9569\n",
            "Total loss:  -2.8925 | PDE Loss:  -4.7485 | Function Loss:  -2.9577\n",
            "Total loss:  -2.8929 | PDE Loss:  -4.7459 | Function Loss:  -2.9585\n",
            "Total loss:  -2.8932 | PDE Loss:  -4.7428 | Function Loss:  -2.9593\n",
            "Total loss:  -2.8935 | PDE Loss:  -4.7402 | Function Loss:  -2.9601\n",
            "Total loss:  -2.8938 | PDE Loss:  -4.7373 | Function Loss:  -2.961\n",
            "Total loss:  -2.8942 | PDE Loss:  -4.7345 | Function Loss:  -2.962\n",
            "Total loss:  -2.8945 | PDE Loss:  -4.7323 | Function Loss:  -2.9627\n",
            "Total loss:  -2.8941 | PDE Loss:  -4.723 | Function Loss:  -2.9638\n",
            "Total loss:  -2.8949 | PDE Loss:  -4.7297 | Function Loss:  -2.9636\n",
            "Total loss:  -2.8951 | PDE Loss:  -4.7307 | Function Loss:  -2.9636\n",
            "Total loss:  -2.8956 | PDE Loss:  -4.7337 | Function Loss:  -2.9637\n",
            "Total loss:  -2.8958 | PDE Loss:  -4.7353 | Function Loss:  -2.9637\n",
            "Total loss:  -2.896 | PDE Loss:  -4.7371 | Function Loss:  -2.9636\n",
            "Total loss:  -2.8961 | PDE Loss:  -4.7373 | Function Loss:  -2.9637\n",
            "Total loss:  -2.8963 | PDE Loss:  -4.7377 | Function Loss:  -2.9638\n",
            "Total loss:  -2.8965 | PDE Loss:  -4.7378 | Function Loss:  -2.9641\n",
            "Total loss:  -2.8967 | PDE Loss:  -4.7375 | Function Loss:  -2.9644\n",
            "Total loss:  -2.8969 | PDE Loss:  -4.7365 | Function Loss:  -2.9648\n",
            "Total loss:  -2.8971 | PDE Loss:  -4.7361 | Function Loss:  -2.9651\n",
            "Total loss:  -2.8973 | PDE Loss:  -4.735 | Function Loss:  -2.9655\n",
            "Total loss:  -2.8976 | PDE Loss:  -4.7336 | Function Loss:  -2.9661\n",
            "Total loss:  -2.8979 | PDE Loss:  -4.7325 | Function Loss:  -2.9666\n",
            "Total loss:  -2.8983 | PDE Loss:  -4.7313 | Function Loss:  -2.9673\n",
            "Total loss:  -2.8987 | PDE Loss:  -4.7307 | Function Loss:  -2.9679\n",
            "Total loss:  -2.899 | PDE Loss:  -4.7306 | Function Loss:  -2.9683\n",
            "Total loss:  -2.8993 | PDE Loss:  -4.7307 | Function Loss:  -2.9686\n",
            "Total loss:  -2.8998 | PDE Loss:  -4.7309 | Function Loss:  -2.9691\n",
            "Total loss:  -2.9003 | PDE Loss:  -4.7316 | Function Loss:  -2.9696\n",
            "Total loss:  -2.9007 | PDE Loss:  -4.7323 | Function Loss:  -2.9699\n",
            "Total loss:  -2.9014 | PDE Loss:  -4.7343 | Function Loss:  -2.9704\n",
            "Total loss:  -2.902 | PDE Loss:  -4.7338 | Function Loss:  -2.9712\n",
            "Total loss:  -2.9025 | PDE Loss:  -4.7359 | Function Loss:  -2.9714\n",
            "Total loss:  -2.9027 | PDE Loss:  -4.7358 | Function Loss:  -2.9717\n",
            "Total loss:  -2.9031 | PDE Loss:  -4.7353 | Function Loss:  -2.9723\n",
            "Total loss:  -2.9035 | PDE Loss:  -4.734 | Function Loss:  -2.9729\n",
            "Total loss:  -2.9039 | PDE Loss:  -4.7326 | Function Loss:  -2.9736\n",
            "Total loss:  -2.9042 | PDE Loss:  -4.731 | Function Loss:  -2.9743\n",
            "Total loss:  -2.9046 | PDE Loss:  -4.7298 | Function Loss:  -2.975\n",
            "Total loss:  -2.905 | PDE Loss:  -4.7284 | Function Loss:  -2.9757\n",
            "Total loss:  -2.9053 | PDE Loss:  -4.728 | Function Loss:  -2.9761\n",
            "Total loss:  -2.9056 | PDE Loss:  -4.7279 | Function Loss:  -2.9765\n",
            "Total loss:  -2.906 | PDE Loss:  -4.7282 | Function Loss:  -2.9768\n",
            "Total loss:  -2.9062 | PDE Loss:  -4.7295 | Function Loss:  -2.9769\n",
            "Total loss:  -2.9065 | PDE Loss:  -4.7294 | Function Loss:  -2.9773\n",
            "Total loss:  -2.9068 | PDE Loss:  -4.7307 | Function Loss:  -2.9773\n",
            "Total loss:  -2.907 | PDE Loss:  -4.7311 | Function Loss:  -2.9776\n",
            "Total loss:  -2.9074 | PDE Loss:  -4.7313 | Function Loss:  -2.9779\n",
            "Total loss:  -2.9075 | PDE Loss:  -4.7296 | Function Loss:  -2.9784\n",
            "Total loss:  -2.9077 | PDE Loss:  -4.7252 | Function Loss:  -2.9794\n",
            "Total loss:  -2.9081 | PDE Loss:  -4.7276 | Function Loss:  -2.9794\n",
            "Total loss:  -2.9084 | PDE Loss:  -4.7292 | Function Loss:  -2.9795\n",
            "Total loss:  -2.9086 | PDE Loss:  -4.7294 | Function Loss:  -2.9797\n",
            "Total loss:  -2.9088 | PDE Loss:  -4.7297 | Function Loss:  -2.9799\n",
            "Total loss:  -2.909 | PDE Loss:  -4.7297 | Function Loss:  -2.9802\n",
            "Total loss:  -2.9093 | PDE Loss:  -4.7305 | Function Loss:  -2.9803\n",
            "Total loss:  -2.9095 | PDE Loss:  -4.7296 | Function Loss:  -2.9807\n",
            "Total loss:  -2.9097 | PDE Loss:  -4.7308 | Function Loss:  -2.9807\n",
            "Total loss:  -2.9099 | PDE Loss:  -4.7309 | Function Loss:  -2.981\n",
            "Total loss:  -2.9102 | PDE Loss:  -4.7313 | Function Loss:  -2.9813\n",
            "Total loss:  -2.9106 | PDE Loss:  -4.7305 | Function Loss:  -2.9819\n",
            "Total loss:  -2.9109 | PDE Loss:  -4.73 | Function Loss:  -2.9823\n",
            "Total loss:  -2.9112 | PDE Loss:  -4.7289 | Function Loss:  -2.9828\n",
            "Total loss:  -2.9114 | PDE Loss:  -4.7283 | Function Loss:  -2.9832\n",
            "Total loss:  -2.9116 | PDE Loss:  -4.7273 | Function Loss:  -2.9837\n",
            "Total loss:  -2.9118 | PDE Loss:  -4.727 | Function Loss:  -2.9839\n",
            "Total loss:  -2.9119 | PDE Loss:  -4.7263 | Function Loss:  -2.9842\n",
            "Total loss:  -2.912 | PDE Loss:  -4.7265 | Function Loss:  -2.9843\n",
            "Total loss:  -2.9121 | PDE Loss:  -4.7262 | Function Loss:  -2.9845\n",
            "Total loss:  -2.9123 | PDE Loss:  -4.7267 | Function Loss:  -2.9846\n",
            "Total loss:  -2.9125 | PDE Loss:  -4.7276 | Function Loss:  -2.9847\n",
            "Total loss:  -2.9128 | PDE Loss:  -4.7284 | Function Loss:  -2.9848\n",
            "Total loss:  -2.9131 | PDE Loss:  -4.7301 | Function Loss:  -2.9849\n",
            "Total loss:  -2.9134 | PDE Loss:  -4.7314 | Function Loss:  -2.985\n",
            "Total loss:  -2.9136 | PDE Loss:  -4.7328 | Function Loss:  -2.9851\n",
            "Total loss:  -2.9139 | PDE Loss:  -4.7343 | Function Loss:  -2.9852\n",
            "Total loss:  -2.9143 | PDE Loss:  -4.7355 | Function Loss:  -2.9853\n",
            "Total loss:  -2.9145 | PDE Loss:  -4.7361 | Function Loss:  -2.9855\n",
            "Total loss:  -2.9149 | PDE Loss:  -4.7374 | Function Loss:  -2.9857\n",
            "Total loss:  -2.9152 | PDE Loss:  -4.738 | Function Loss:  -2.986\n",
            "Total loss:  -2.9156 | PDE Loss:  -4.7389 | Function Loss:  -2.9863\n",
            "Total loss:  -2.916 | PDE Loss:  -4.7416 | Function Loss:  -2.9863\n",
            "Total loss:  -2.9164 | PDE Loss:  -4.7424 | Function Loss:  -2.9866\n",
            "Total loss:  -2.9167 | PDE Loss:  -4.7448 | Function Loss:  -2.9865\n",
            "Total loss:  -2.9169 | PDE Loss:  -4.7464 | Function Loss:  -2.9865\n",
            "Total loss:  -2.9172 | PDE Loss:  -4.748 | Function Loss:  -2.9865\n",
            "Total loss:  -2.9174 | PDE Loss:  -4.7497 | Function Loss:  -2.9865\n",
            "Total loss:  -2.9176 | PDE Loss:  -4.7513 | Function Loss:  -2.9865\n",
            "Total loss:  -2.9178 | PDE Loss:  -4.7534 | Function Loss:  -2.9864\n",
            "Total loss:  -2.9181 | PDE Loss:  -4.7543 | Function Loss:  -2.9865\n",
            "Total loss:  -2.9183 | PDE Loss:  -4.755 | Function Loss:  -2.9866\n",
            "Total loss:  -2.9185 | PDE Loss:  -4.7556 | Function Loss:  -2.9868\n",
            "Total loss:  -2.9188 | PDE Loss:  -4.7562 | Function Loss:  -2.987\n",
            "Total loss:  -2.9191 | PDE Loss:  -4.7571 | Function Loss:  -2.9872\n",
            "Total loss:  -2.9194 | PDE Loss:  -4.7576 | Function Loss:  -2.9875\n",
            "Total loss:  -2.9196 | PDE Loss:  -4.7583 | Function Loss:  -2.9876\n",
            "Total loss:  -2.9198 | PDE Loss:  -4.7587 | Function Loss:  -2.9878\n",
            "Total loss:  -2.92 | PDE Loss:  -4.7596 | Function Loss:  -2.9879\n",
            "Total loss:  -2.9203 | PDE Loss:  -4.7597 | Function Loss:  -2.9882\n",
            "Total loss:  -2.9206 | PDE Loss:  -4.7594 | Function Loss:  -2.9886\n",
            "Total loss:  -2.9209 | PDE Loss:  -4.7585 | Function Loss:  -2.9891\n",
            "Total loss:  -2.9212 | PDE Loss:  -4.7573 | Function Loss:  -2.9896\n",
            "Total loss:  -2.9215 | PDE Loss:  -4.7551 | Function Loss:  -2.9905\n",
            "Total loss:  -2.9219 | PDE Loss:  -4.7538 | Function Loss:  -2.9911\n",
            "Total loss:  -2.9222 | PDE Loss:  -4.7515 | Function Loss:  -2.9918\n",
            "Total loss:  -2.9224 | PDE Loss:  -4.7504 | Function Loss:  -2.9922\n",
            "Total loss:  -2.9225 | PDE Loss:  -4.7492 | Function Loss:  -2.9926\n",
            "Total loss:  -2.9227 | PDE Loss:  -4.7488 | Function Loss:  -2.9928\n",
            "Total loss:  -2.9228 | PDE Loss:  -4.7487 | Function Loss:  -2.9931\n",
            "Total loss:  -2.923 | PDE Loss:  -4.749 | Function Loss:  -2.9932\n",
            "Total loss:  -2.9232 | PDE Loss:  -4.7495 | Function Loss:  -2.9933\n",
            "Total loss:  -2.9234 | PDE Loss:  -4.7507 | Function Loss:  -2.9933\n",
            "Total loss:  -2.9235 | PDE Loss:  -4.7518 | Function Loss:  -2.9933\n",
            "Total loss:  -2.9237 | PDE Loss:  -4.753 | Function Loss:  -2.9934\n",
            "Total loss:  -2.9239 | PDE Loss:  -4.7534 | Function Loss:  -2.9935\n",
            "Total loss:  -2.924 | PDE Loss:  -4.7542 | Function Loss:  -2.9935\n",
            "Total loss:  -2.9242 | PDE Loss:  -4.7549 | Function Loss:  -2.9936\n",
            "Total loss:  -2.9244 | PDE Loss:  -4.7553 | Function Loss:  -2.9938\n",
            "Total loss:  -2.9246 | PDE Loss:  -4.7564 | Function Loss:  -2.9939\n",
            "Total loss:  -2.9248 | PDE Loss:  -4.7565 | Function Loss:  -2.994\n",
            "Total loss:  -2.925 | PDE Loss:  -4.7575 | Function Loss:  -2.9941\n",
            "Total loss:  -2.9252 | PDE Loss:  -4.7574 | Function Loss:  -2.9943\n",
            "Total loss:  -2.9253 | PDE Loss:  -4.7582 | Function Loss:  -2.9943\n",
            "Total loss:  -2.9254 | PDE Loss:  -4.7585 | Function Loss:  -2.9943\n",
            "Total loss:  -2.9254 | PDE Loss:  -4.7576 | Function Loss:  -2.9946\n",
            "Total loss:  -2.9256 | PDE Loss:  -4.7586 | Function Loss:  -2.9946\n",
            "Total loss:  -2.9257 | PDE Loss:  -4.7594 | Function Loss:  -2.9946\n",
            "Total loss:  -2.9259 | PDE Loss:  -4.7603 | Function Loss:  -2.9946\n",
            "Total loss:  -2.926 | PDE Loss:  -4.761 | Function Loss:  -2.9947\n",
            "Total loss:  -2.9263 | PDE Loss:  -4.7614 | Function Loss:  -2.9949\n",
            "Total loss:  -2.9265 | PDE Loss:  -4.7619 | Function Loss:  -2.9951\n",
            "Total loss:  -2.9268 | PDE Loss:  -4.7618 | Function Loss:  -2.9955\n",
            "Total loss:  -2.9271 | PDE Loss:  -4.7612 | Function Loss:  -2.9959\n",
            "Total loss:  -2.9273 | PDE Loss:  -4.7598 | Function Loss:  -2.9964\n",
            "Total loss:  -2.9276 | PDE Loss:  -4.7587 | Function Loss:  -2.9969\n",
            "Total loss:  -2.9278 | PDE Loss:  -4.7572 | Function Loss:  -2.9974\n",
            "Total loss:  -2.9279 | PDE Loss:  -4.7557 | Function Loss:  -2.9978\n",
            "Total loss:  -2.9281 | PDE Loss:  -4.7548 | Function Loss:  -2.9982\n",
            "Total loss:  -2.9282 | PDE Loss:  -4.7542 | Function Loss:  -2.9985\n",
            "Total loss:  -2.9283 | PDE Loss:  -4.754 | Function Loss:  -2.9986\n",
            "Total loss:  -2.9285 | PDE Loss:  -4.7543 | Function Loss:  -2.9987\n",
            "Total loss:  -2.9285 | PDE Loss:  -4.7551 | Function Loss:  -2.9987\n",
            "Total loss:  -2.9287 | PDE Loss:  -4.7555 | Function Loss:  -2.9988\n",
            "Total loss:  -2.9288 | PDE Loss:  -4.7568 | Function Loss:  -2.9987\n",
            "Total loss:  -2.9291 | PDE Loss:  -4.757 | Function Loss:  -2.9989\n",
            "Total loss:  -2.9292 | PDE Loss:  -4.758 | Function Loss:  -2.999\n",
            "Total loss:  -2.9295 | PDE Loss:  -4.7593 | Function Loss:  -2.9991\n",
            "Total loss:  -2.9298 | PDE Loss:  -4.7602 | Function Loss:  -2.9992\n",
            "Total loss:  -2.9301 | PDE Loss:  -4.7613 | Function Loss:  -2.9994\n",
            "Total loss:  -2.9304 | PDE Loss:  -4.7616 | Function Loss:  -2.9997\n",
            "Total loss:  -2.9308 | PDE Loss:  -4.7629 | Function Loss:  -3.0\n",
            "Total loss:  -2.9313 | PDE Loss:  -4.764 | Function Loss:  -3.0003\n",
            "Total loss:  -2.932 | PDE Loss:  -4.7656 | Function Loss:  -3.0008\n",
            "Total loss:  -2.9326 | PDE Loss:  -4.7677 | Function Loss:  -3.0013\n",
            "Total loss:  -2.9332 | PDE Loss:  -4.7687 | Function Loss:  -3.0017\n",
            "Total loss:  -2.9336 | PDE Loss:  -4.7689 | Function Loss:  -3.0023\n",
            "Total loss:  -2.9342 | PDE Loss:  -4.7693 | Function Loss:  -3.0028\n",
            "Total loss:  -2.9346 | PDE Loss:  -4.7689 | Function Loss:  -3.0033\n",
            "Total loss:  -2.9349 | PDE Loss:  -4.7692 | Function Loss:  -3.0037\n",
            "Total loss:  -2.9352 | PDE Loss:  -4.7689 | Function Loss:  -3.0041\n",
            "Total loss:  -2.9355 | PDE Loss:  -4.7687 | Function Loss:  -3.0045\n",
            "Total loss:  -2.9358 | PDE Loss:  -4.7676 | Function Loss:  -3.005\n",
            "Total loss:  -2.9361 | PDE Loss:  -4.7661 | Function Loss:  -3.0056\n",
            "Total loss:  -2.9362 | PDE Loss:  -4.7661 | Function Loss:  -3.0057\n",
            "Total loss:  -2.9363 | PDE Loss:  -4.7658 | Function Loss:  -3.0059\n",
            "Total loss:  -2.9365 | PDE Loss:  -4.7663 | Function Loss:  -3.006\n",
            "Total loss:  -2.9367 | PDE Loss:  -4.7662 | Function Loss:  -3.0062\n",
            "Total loss:  -2.9368 | PDE Loss:  -4.7666 | Function Loss:  -3.0064\n",
            "Total loss:  -2.9371 | PDE Loss:  -4.767 | Function Loss:  -3.0066\n",
            "Total loss:  -2.9373 | PDE Loss:  -4.7669 | Function Loss:  -3.0069\n",
            "Total loss:  -2.9377 | PDE Loss:  -4.7669 | Function Loss:  -3.0074\n",
            "Total loss:  -2.9382 | PDE Loss:  -4.7652 | Function Loss:  -3.0082\n",
            "Total loss:  -2.9387 | PDE Loss:  -4.7639 | Function Loss:  -3.0091\n",
            "Total loss:  -2.9392 | PDE Loss:  -4.7637 | Function Loss:  -3.0097\n",
            "Total loss:  -2.9399 | PDE Loss:  -4.7642 | Function Loss:  -3.0104\n",
            "Total loss:  -2.9405 | PDE Loss:  -4.7627 | Function Loss:  -3.0114\n",
            "Total loss:  -2.9411 | PDE Loss:  -4.7635 | Function Loss:  -3.0119\n",
            "Total loss:  -2.9415 | PDE Loss:  -4.7641 | Function Loss:  -3.0122\n",
            "Total loss:  -2.9419 | PDE Loss:  -4.7646 | Function Loss:  -3.0127\n",
            "Total loss:  -2.9424 | PDE Loss:  -4.767 | Function Loss:  -3.0128\n",
            "Total loss:  -2.9427 | PDE Loss:  -4.7683 | Function Loss:  -3.013\n",
            "Total loss:  -2.9431 | PDE Loss:  -4.7696 | Function Loss:  -3.0133\n",
            "Total loss:  -2.9436 | PDE Loss:  -4.7723 | Function Loss:  -3.0133\n",
            "Total loss:  -2.944 | PDE Loss:  -4.7732 | Function Loss:  -3.0137\n",
            "Total loss:  -2.9444 | PDE Loss:  -4.7751 | Function Loss:  -3.0137\n",
            "Total loss:  -2.9447 | PDE Loss:  -4.7761 | Function Loss:  -3.0139\n",
            "Total loss:  -2.945 | PDE Loss:  -4.7765 | Function Loss:  -3.0142\n",
            "Total loss:  -2.9454 | PDE Loss:  -4.7789 | Function Loss:  -3.0143\n",
            "Total loss:  -2.9456 | PDE Loss:  -4.7793 | Function Loss:  -3.0145\n",
            "Total loss:  -2.946 | PDE Loss:  -4.7809 | Function Loss:  -3.0146\n",
            "Total loss:  -2.9462 | PDE Loss:  -4.7794 | Function Loss:  -3.0152\n",
            "Total loss:  -2.9464 | PDE Loss:  -4.78 | Function Loss:  -3.0153\n",
            "Total loss:  -2.9466 | PDE Loss:  -4.7795 | Function Loss:  -3.0156\n",
            "Total loss:  -2.9469 | PDE Loss:  -4.7792 | Function Loss:  -3.016\n",
            "Total loss:  -2.9473 | PDE Loss:  -4.7784 | Function Loss:  -3.0166\n",
            "Total loss:  -2.9476 | PDE Loss:  -4.7789 | Function Loss:  -3.0169\n",
            "Total loss:  -2.948 | PDE Loss:  -4.7788 | Function Loss:  -3.0173\n",
            "Total loss:  -2.9483 | PDE Loss:  -4.7792 | Function Loss:  -3.0176\n",
            "Total loss:  -2.9486 | PDE Loss:  -4.7797 | Function Loss:  -3.0179\n",
            "Total loss:  -2.9489 | PDE Loss:  -4.7798 | Function Loss:  -3.0182\n",
            "Total loss:  -2.9492 | PDE Loss:  -4.7809 | Function Loss:  -3.0184\n",
            "Total loss:  -2.9495 | PDE Loss:  -4.7811 | Function Loss:  -3.0187\n",
            "Total loss:  -2.9497 | PDE Loss:  -4.7816 | Function Loss:  -3.0189\n",
            "Total loss:  -2.95 | PDE Loss:  -4.7823 | Function Loss:  -3.0191\n",
            "Total loss:  -2.9502 | PDE Loss:  -4.779 | Function Loss:  -3.0199\n",
            "Total loss:  -2.9504 | PDE Loss:  -4.7819 | Function Loss:  -3.0197\n",
            "Total loss:  -2.9506 | PDE Loss:  -4.7824 | Function Loss:  -3.0199\n",
            "Total loss:  -2.9509 | PDE Loss:  -4.7842 | Function Loss:  -3.0199\n",
            "Total loss:  -2.9512 | PDE Loss:  -4.7845 | Function Loss:  -3.0201\n",
            "Total loss:  -2.9514 | PDE Loss:  -4.7841 | Function Loss:  -3.0204\n",
            "Total loss:  -2.9515 | PDE Loss:  -4.7839 | Function Loss:  -3.0206\n",
            "Total loss:  -2.9518 | PDE Loss:  -4.7841 | Function Loss:  -3.0209\n",
            "Total loss:  -2.952 | PDE Loss:  -4.7846 | Function Loss:  -3.021\n",
            "Total loss:  -2.9521 | PDE Loss:  -4.786 | Function Loss:  -3.021\n",
            "Total loss:  -2.9523 | PDE Loss:  -4.7867 | Function Loss:  -3.0211\n",
            "Total loss:  -2.9525 | PDE Loss:  -4.7881 | Function Loss:  -3.021\n",
            "Total loss:  -2.9526 | PDE Loss:  -4.7896 | Function Loss:  -3.0209\n",
            "Total loss:  -2.9528 | PDE Loss:  -4.7918 | Function Loss:  -3.0207\n",
            "Total loss:  -2.9529 | PDE Loss:  -4.7926 | Function Loss:  -3.0208\n",
            "Total loss:  -2.953 | PDE Loss:  -4.7943 | Function Loss:  -3.0206\n",
            "Total loss:  -2.9532 | PDE Loss:  -4.796 | Function Loss:  -3.0205\n",
            "Total loss:  -2.9534 | PDE Loss:  -4.7976 | Function Loss:  -3.0205\n",
            "Total loss:  -2.9535 | PDE Loss:  -4.7981 | Function Loss:  -3.0205\n",
            "Total loss:  -2.9536 | PDE Loss:  -4.7981 | Function Loss:  -3.0207\n",
            "Total loss:  -2.9538 | PDE Loss:  -4.798 | Function Loss:  -3.0208\n",
            "Total loss:  -2.9538 | PDE Loss:  -4.7957 | Function Loss:  -3.0213\n",
            "Total loss:  -2.9539 | PDE Loss:  -4.7966 | Function Loss:  -3.0213\n",
            "Total loss:  -2.9541 | PDE Loss:  -4.7971 | Function Loss:  -3.0214\n",
            "Total loss:  -2.9542 | PDE Loss:  -4.7973 | Function Loss:  -3.0215\n",
            "Total loss:  -2.9544 | PDE Loss:  -4.7975 | Function Loss:  -3.0217\n",
            "Total loss:  -2.9546 | PDE Loss:  -4.7973 | Function Loss:  -3.022\n",
            "Total loss:  -2.9548 | PDE Loss:  -4.7974 | Function Loss:  -3.0222\n",
            "Total loss:  -2.955 | PDE Loss:  -4.7971 | Function Loss:  -3.0225\n",
            "Total loss:  -2.9553 | PDE Loss:  -4.7971 | Function Loss:  -3.0228\n",
            "Total loss:  -2.9556 | PDE Loss:  -4.7965 | Function Loss:  -3.0232\n",
            "Total loss:  -2.9558 | PDE Loss:  -4.7967 | Function Loss:  -3.0234\n",
            "Total loss:  -2.956 | PDE Loss:  -4.7966 | Function Loss:  -3.0237\n",
            "Total loss:  -2.9562 | PDE Loss:  -4.7968 | Function Loss:  -3.0239\n",
            "Total loss:  -2.9564 | PDE Loss:  -4.7967 | Function Loss:  -3.0241\n",
            "Total loss:  -2.9566 | PDE Loss:  -4.7969 | Function Loss:  -3.0244\n",
            "Total loss:  -2.9568 | PDE Loss:  -4.7966 | Function Loss:  -3.0247\n",
            "Total loss:  -2.9571 | PDE Loss:  -4.7972 | Function Loss:  -3.0249\n",
            "Total loss:  -2.9574 | PDE Loss:  -4.7965 | Function Loss:  -3.0253\n",
            "Total loss:  -2.9576 | PDE Loss:  -4.7965 | Function Loss:  -3.0256\n",
            "Total loss:  -2.958 | PDE Loss:  -4.7965 | Function Loss:  -3.026\n",
            "Total loss:  -2.9583 | PDE Loss:  -4.797 | Function Loss:  -3.0263\n",
            "Total loss:  -2.9585 | PDE Loss:  -4.7973 | Function Loss:  -3.0265\n",
            "Total loss:  -2.9587 | PDE Loss:  -4.7975 | Function Loss:  -3.0267\n",
            "Total loss:  -2.9588 | PDE Loss:  -4.7975 | Function Loss:  -3.0268\n",
            "Total loss:  -2.959 | PDE Loss:  -4.7976 | Function Loss:  -3.027\n",
            "Total loss:  -2.9591 | PDE Loss:  -4.7976 | Function Loss:  -3.0272\n",
            "Total loss:  -2.9593 | PDE Loss:  -4.7976 | Function Loss:  -3.0274\n",
            "Total loss:  -2.9595 | PDE Loss:  -4.7974 | Function Loss:  -3.0276\n",
            "Total loss:  -2.9597 | PDE Loss:  -4.7971 | Function Loss:  -3.0279\n",
            "Total loss:  -2.9599 | PDE Loss:  -4.7964 | Function Loss:  -3.0283\n",
            "Total loss:  -2.9603 | PDE Loss:  -4.7965 | Function Loss:  -3.0287\n",
            "Total loss:  -2.9603 | PDE Loss:  -4.7917 | Function Loss:  -3.0296\n",
            "Total loss:  -2.9609 | PDE Loss:  -4.7943 | Function Loss:  -3.0298\n",
            "Total loss:  -2.961 | PDE Loss:  -4.7978 | Function Loss:  -3.0293\n",
            "Total loss:  -2.9613 | PDE Loss:  -4.7987 | Function Loss:  -3.0295\n",
            "Total loss:  -2.9617 | PDE Loss:  -4.7993 | Function Loss:  -3.0299\n",
            "Total loss:  -2.962 | PDE Loss:  -4.7996 | Function Loss:  -3.0302\n",
            "Total loss:  -2.9622 | PDE Loss:  -4.7996 | Function Loss:  -3.0304\n",
            "Total loss:  -2.9624 | PDE Loss:  -4.7999 | Function Loss:  -3.0306\n",
            "Total loss:  -2.9625 | PDE Loss:  -4.7998 | Function Loss:  -3.0307\n",
            "Total loss:  -2.9628 | PDE Loss:  -4.8014 | Function Loss:  -3.0309\n",
            "Total loss:  -2.9631 | PDE Loss:  -4.8022 | Function Loss:  -3.0311\n",
            "Total loss:  -2.9633 | PDE Loss:  -4.8028 | Function Loss:  -3.0312\n",
            "Total loss:  -2.9635 | PDE Loss:  -4.803 | Function Loss:  -3.0313\n",
            "Total loss:  -2.9636 | PDE Loss:  -4.8034 | Function Loss:  -3.0314\n",
            "Total loss:  -2.9637 | PDE Loss:  -4.8031 | Function Loss:  -3.0316\n",
            "Total loss:  -2.9638 | PDE Loss:  -4.8031 | Function Loss:  -3.0317\n",
            "Total loss:  -2.9639 | PDE Loss:  -4.8025 | Function Loss:  -3.032\n",
            "Total loss:  -2.9641 | PDE Loss:  -4.8024 | Function Loss:  -3.0322\n",
            "Total loss:  -2.9642 | PDE Loss:  -4.8019 | Function Loss:  -3.0324\n",
            "Total loss:  -2.9643 | PDE Loss:  -4.8015 | Function Loss:  -3.0326\n",
            "Total loss:  -2.9644 | PDE Loss:  -4.8011 | Function Loss:  -3.0328\n",
            "Total loss:  -2.9645 | PDE Loss:  -4.8008 | Function Loss:  -3.0329\n",
            "Total loss:  -2.9646 | PDE Loss:  -4.8003 | Function Loss:  -3.0331\n",
            "Total loss:  -2.9647 | PDE Loss:  -4.8001 | Function Loss:  -3.0332\n",
            "Total loss:  -2.9648 | PDE Loss:  -4.7996 | Function Loss:  -3.0334\n",
            "Total loss:  -2.9649 | PDE Loss:  -4.7993 | Function Loss:  -3.0337\n",
            "Total loss:  -2.9651 | PDE Loss:  -4.7986 | Function Loss:  -3.034\n",
            "Total loss:  -2.9653 | PDE Loss:  -4.7979 | Function Loss:  -3.0344\n",
            "Total loss:  -2.9656 | PDE Loss:  -4.7975 | Function Loss:  -3.0348\n",
            "Total loss:  -2.966 | PDE Loss:  -4.7959 | Function Loss:  -3.0355\n",
            "Total loss:  -2.9661 | PDE Loss:  -4.7967 | Function Loss:  -3.0355\n",
            "Total loss:  -2.9664 | PDE Loss:  -4.7977 | Function Loss:  -3.0357\n",
            "Total loss:  -2.9666 | PDE Loss:  -4.7992 | Function Loss:  -3.0357\n",
            "Total loss:  -2.9668 | PDE Loss:  -4.8004 | Function Loss:  -3.0357\n",
            "Total loss:  -2.9669 | PDE Loss:  -4.8015 | Function Loss:  -3.0357\n",
            "Total loss:  -2.9671 | PDE Loss:  -4.8025 | Function Loss:  -3.0357\n",
            "Total loss:  -2.9674 | PDE Loss:  -4.8036 | Function Loss:  -3.0359\n",
            "Total loss:  -2.9676 | PDE Loss:  -4.8035 | Function Loss:  -3.0362\n",
            "Total loss:  -2.9679 | PDE Loss:  -4.8043 | Function Loss:  -3.0363\n",
            "Total loss:  -2.9681 | PDE Loss:  -4.8042 | Function Loss:  -3.0366\n",
            "Total loss:  -2.9683 | PDE Loss:  -4.8041 | Function Loss:  -3.0369\n",
            "Total loss:  -2.9686 | PDE Loss:  -4.8043 | Function Loss:  -3.0371\n",
            "Total loss:  -2.9689 | PDE Loss:  -4.8032 | Function Loss:  -3.0377\n",
            "Total loss:  -2.9691 | PDE Loss:  -4.8041 | Function Loss:  -3.0378\n",
            "Total loss:  -2.9694 | PDE Loss:  -4.8042 | Function Loss:  -3.038\n",
            "Total loss:  -2.9697 | PDE Loss:  -4.8053 | Function Loss:  -3.0383\n",
            "Total loss:  -2.9701 | PDE Loss:  -4.806 | Function Loss:  -3.0386\n",
            "Total loss:  -2.9705 | PDE Loss:  -4.8071 | Function Loss:  -3.0388\n",
            "Total loss:  -2.9708 | PDE Loss:  -4.808 | Function Loss:  -3.0391\n",
            "Total loss:  -2.9713 | PDE Loss:  -4.8083 | Function Loss:  -3.0396\n",
            "Total loss:  -2.9717 | PDE Loss:  -4.8094 | Function Loss:  -3.0399\n",
            "Total loss:  -2.9721 | PDE Loss:  -4.8093 | Function Loss:  -3.0403\n",
            "Total loss:  -2.9724 | PDE Loss:  -4.809 | Function Loss:  -3.0408\n",
            "Total loss:  -2.9726 | PDE Loss:  -4.8087 | Function Loss:  -3.0411\n",
            "Total loss:  -2.9728 | PDE Loss:  -4.8077 | Function Loss:  -3.0415\n",
            "Total loss:  -2.973 | PDE Loss:  -4.8075 | Function Loss:  -3.0417\n",
            "Total loss:  -2.9732 | PDE Loss:  -4.8068 | Function Loss:  -3.0421\n",
            "Total loss:  -2.9735 | PDE Loss:  -4.805 | Function Loss:  -3.0428\n",
            "Total loss:  -2.974 | PDE Loss:  -4.8054 | Function Loss:  -3.0433\n",
            "Total loss:  -2.9744 | PDE Loss:  -4.8032 | Function Loss:  -3.0442\n",
            "Total loss:  -2.9749 | PDE Loss:  -4.804 | Function Loss:  -3.0446\n",
            "Total loss:  -2.9754 | PDE Loss:  -4.8049 | Function Loss:  -3.045\n",
            "Total loss:  -2.9758 | PDE Loss:  -4.8058 | Function Loss:  -3.0453\n",
            "Total loss:  -2.9762 | PDE Loss:  -4.8062 | Function Loss:  -3.0457\n",
            "Total loss:  -2.9768 | PDE Loss:  -4.8078 | Function Loss:  -3.0462\n",
            "Total loss:  -2.9775 | PDE Loss:  -4.8081 | Function Loss:  -3.0469\n",
            "Total loss:  -2.9782 | PDE Loss:  -4.8084 | Function Loss:  -3.0477\n",
            "Total loss:  -2.9786 | PDE Loss:  -4.8062 | Function Loss:  -3.0485\n",
            "Total loss:  -2.9794 | PDE Loss:  -4.8073 | Function Loss:  -3.0492\n",
            "Total loss:  -2.9801 | PDE Loss:  -4.8078 | Function Loss:  -3.05\n",
            "Total loss:  -2.9808 | PDE Loss:  -4.8077 | Function Loss:  -3.0508\n",
            "Total loss:  -2.9814 | PDE Loss:  -4.8077 | Function Loss:  -3.0516\n",
            "Total loss:  -2.982 | PDE Loss:  -4.8077 | Function Loss:  -3.0523\n",
            "Total loss:  -2.9825 | PDE Loss:  -4.8082 | Function Loss:  -3.0528\n",
            "Total loss:  -2.983 | PDE Loss:  -4.8086 | Function Loss:  -3.0533\n",
            "Total loss:  -2.9834 | PDE Loss:  -4.8091 | Function Loss:  -3.0537\n",
            "Total loss:  -2.9839 | PDE Loss:  -4.8092 | Function Loss:  -3.0542\n",
            "Total loss:  -2.9843 | PDE Loss:  -4.8105 | Function Loss:  -3.0545\n",
            "Total loss:  -2.9846 | PDE Loss:  -4.8053 | Function Loss:  -3.0557\n",
            "Total loss:  -2.9849 | PDE Loss:  -4.8071 | Function Loss:  -3.0558\n",
            "Total loss:  -2.9853 | PDE Loss:  -4.809 | Function Loss:  -3.056\n",
            "Total loss:  -2.9857 | PDE Loss:  -4.8096 | Function Loss:  -3.0563\n",
            "Total loss:  -2.986 | PDE Loss:  -4.8099 | Function Loss:  -3.0566\n",
            "Total loss:  -2.9863 | PDE Loss:  -4.81 | Function Loss:  -3.057\n",
            "Total loss:  -2.9866 | PDE Loss:  -4.8097 | Function Loss:  -3.0574\n",
            "Total loss:  -2.9869 | PDE Loss:  -4.8099 | Function Loss:  -3.0576\n",
            "Total loss:  -2.9871 | PDE Loss:  -4.8096 | Function Loss:  -3.0579\n",
            "Total loss:  -2.9872 | PDE Loss:  -4.8098 | Function Loss:  -3.058\n",
            "Total loss:  -2.9874 | PDE Loss:  -4.8101 | Function Loss:  -3.0582\n",
            "Total loss:  -2.9876 | PDE Loss:  -4.8105 | Function Loss:  -3.0583\n",
            "Total loss:  -2.9878 | PDE Loss:  -4.8105 | Function Loss:  -3.0586\n",
            "Total loss:  -2.9879 | PDE Loss:  -4.8109 | Function Loss:  -3.0587\n",
            "Total loss:  -2.9881 | PDE Loss:  -4.8112 | Function Loss:  -3.0589\n",
            "Total loss:  -2.9884 | PDE Loss:  -4.811 | Function Loss:  -3.0592\n",
            "Total loss:  -2.9885 | PDE Loss:  -4.8102 | Function Loss:  -3.0594\n",
            "Total loss:  -2.9886 | PDE Loss:  -4.8101 | Function Loss:  -3.0596\n",
            "Total loss:  -2.9887 | PDE Loss:  -4.8095 | Function Loss:  -3.0599\n",
            "Total loss:  -2.9889 | PDE Loss:  -4.8086 | Function Loss:  -3.0602\n",
            "Total loss:  -2.9891 | PDE Loss:  -4.8074 | Function Loss:  -3.0606\n",
            "Total loss:  -2.9893 | PDE Loss:  -4.8064 | Function Loss:  -3.061\n",
            "Total loss:  -2.9895 | PDE Loss:  -4.8039 | Function Loss:  -3.0617\n",
            "Total loss:  -2.9897 | PDE Loss:  -4.8034 | Function Loss:  -3.0621\n",
            "Total loss:  -2.9899 | PDE Loss:  -4.8023 | Function Loss:  -3.0626\n",
            "Total loss:  -2.9894 | PDE Loss:  -4.8004 | Function Loss:  -3.0623\n",
            "Total loss:  -2.99 | PDE Loss:  -4.8024 | Function Loss:  -3.0627\n",
            "Total loss:  -2.9903 | PDE Loss:  -4.8024 | Function Loss:  -3.063\n",
            "Total loss:  -2.9906 | PDE Loss:  -4.8024 | Function Loss:  -3.0633\n",
            "Total loss:  -2.9909 | PDE Loss:  -4.8039 | Function Loss:  -3.0634\n",
            "Total loss:  -2.9913 | PDE Loss:  -4.8031 | Function Loss:  -3.064\n",
            "Total loss:  -2.9915 | PDE Loss:  -4.8044 | Function Loss:  -3.0641\n",
            "Total loss:  -2.9919 | PDE Loss:  -4.8067 | Function Loss:  -3.0642\n",
            "Total loss:  -2.9923 | PDE Loss:  -4.8078 | Function Loss:  -3.0644\n",
            "Total loss:  -2.9926 | PDE Loss:  -4.8083 | Function Loss:  -3.0646\n",
            "Total loss:  -2.9928 | PDE Loss:  -4.8084 | Function Loss:  -3.0648\n",
            "Total loss:  -2.993 | PDE Loss:  -4.8074 | Function Loss:  -3.0653\n",
            "Total loss:  -2.9932 | PDE Loss:  -4.8073 | Function Loss:  -3.0656\n",
            "Total loss:  -2.9935 | PDE Loss:  -4.8061 | Function Loss:  -3.066\n",
            "Total loss:  -2.9937 | PDE Loss:  -4.8053 | Function Loss:  -3.0665\n",
            "Total loss:  -2.994 | PDE Loss:  -4.8043 | Function Loss:  -3.067\n",
            "Total loss:  -2.9942 | PDE Loss:  -4.8038 | Function Loss:  -3.0674\n",
            "Total loss:  -2.9945 | PDE Loss:  -4.8037 | Function Loss:  -3.0677\n",
            "Total loss:  -2.9947 | PDE Loss:  -4.8036 | Function Loss:  -3.0679\n",
            "Total loss:  -2.9948 | PDE Loss:  -4.8044 | Function Loss:  -3.068\n",
            "Total loss:  -2.995 | PDE Loss:  -4.8053 | Function Loss:  -3.0681\n",
            "Total loss:  -2.9952 | PDE Loss:  -4.807 | Function Loss:  -3.068\n",
            "Total loss:  -2.9955 | PDE Loss:  -4.8091 | Function Loss:  -3.0679\n",
            "Total loss:  -2.9958 | PDE Loss:  -4.8112 | Function Loss:  -3.0679\n",
            "Total loss:  -2.996 | PDE Loss:  -4.8132 | Function Loss:  -3.0678\n",
            "Total loss:  -2.9962 | PDE Loss:  -4.8137 | Function Loss:  -3.068\n",
            "Total loss:  -2.9965 | PDE Loss:  -4.8142 | Function Loss:  -3.0682\n",
            "Total loss:  -2.9967 | PDE Loss:  -4.8136 | Function Loss:  -3.0685\n",
            "Total loss:  -2.9969 | PDE Loss:  -4.8128 | Function Loss:  -3.069\n",
            "Total loss:  -2.9972 | PDE Loss:  -4.8112 | Function Loss:  -3.0696\n",
            "Total loss:  -2.9975 | PDE Loss:  -4.8099 | Function Loss:  -3.0702\n",
            "Total loss:  -2.9978 | PDE Loss:  -4.8084 | Function Loss:  -3.0708\n",
            "Total loss:  -2.9981 | PDE Loss:  -4.8079 | Function Loss:  -3.0712\n",
            "Total loss:  -2.9983 | PDE Loss:  -4.8073 | Function Loss:  -3.0715\n",
            "Total loss:  -2.9986 | PDE Loss:  -4.8076 | Function Loss:  -3.0719\n",
            "Total loss:  -2.9992 | PDE Loss:  -4.8084 | Function Loss:  -3.0724\n",
            "Total loss:  -2.9996 | PDE Loss:  -4.8111 | Function Loss:  -3.0724\n",
            "Total loss:  -3.0002 | PDE Loss:  -4.8126 | Function Loss:  -3.0729\n",
            "Total loss:  -3.0008 | PDE Loss:  -4.8152 | Function Loss:  -3.0731\n",
            "Total loss:  -3.0019 | PDE Loss:  -4.8197 | Function Loss:  -3.0735\n",
            "Total loss:  -3.0025 | PDE Loss:  -4.8218 | Function Loss:  -3.0739\n",
            "Total loss:  -3.0029 | PDE Loss:  -4.8246 | Function Loss:  -3.0739\n",
            "Total loss:  -3.0032 | PDE Loss:  -4.8248 | Function Loss:  -3.0742\n",
            "Total loss:  -3.0034 | PDE Loss:  -4.8256 | Function Loss:  -3.0743\n",
            "Total loss:  -3.0036 | PDE Loss:  -4.8256 | Function Loss:  -3.0745\n",
            "Total loss:  -3.0037 | PDE Loss:  -4.8258 | Function Loss:  -3.0747\n",
            "Total loss:  -3.004 | PDE Loss:  -4.8257 | Function Loss:  -3.075\n",
            "Total loss:  -3.0043 | PDE Loss:  -4.8255 | Function Loss:  -3.0754\n",
            "Total loss:  -3.0048 | PDE Loss:  -4.8251 | Function Loss:  -3.076\n",
            "Total loss:  -3.0053 | PDE Loss:  -4.825 | Function Loss:  -3.0767\n",
            "Total loss:  -3.006 | PDE Loss:  -4.8249 | Function Loss:  -3.0774\n",
            "Total loss:  -3.0066 | PDE Loss:  -4.8254 | Function Loss:  -3.0781\n",
            "Total loss:  -3.0071 | PDE Loss:  -4.8265 | Function Loss:  -3.0785\n",
            "Total loss:  -3.0076 | PDE Loss:  -4.8272 | Function Loss:  -3.0789\n",
            "Total loss:  -3.0081 | PDE Loss:  -4.8295 | Function Loss:  -3.0791\n",
            "Total loss:  -3.0086 | PDE Loss:  -4.8305 | Function Loss:  -3.0795\n",
            "Total loss:  -3.009 | PDE Loss:  -4.8335 | Function Loss:  -3.0795\n",
            "Total loss:  -3.0093 | PDE Loss:  -4.8349 | Function Loss:  -3.0796\n",
            "Total loss:  -3.0096 | PDE Loss:  -4.8366 | Function Loss:  -3.0797\n",
            "Total loss:  -3.01 | PDE Loss:  -4.8383 | Function Loss:  -3.0798\n",
            "Total loss:  -3.0102 | PDE Loss:  -4.8395 | Function Loss:  -3.0799\n",
            "Total loss:  -3.0105 | PDE Loss:  -4.8399 | Function Loss:  -3.0801\n",
            "Total loss:  -3.0108 | PDE Loss:  -4.8397 | Function Loss:  -3.0805\n",
            "Total loss:  -3.0112 | PDE Loss:  -4.8386 | Function Loss:  -3.0812\n",
            "Total loss:  -3.0116 | PDE Loss:  -4.8369 | Function Loss:  -3.082\n",
            "Total loss:  -3.0121 | PDE Loss:  -4.8352 | Function Loss:  -3.0828\n",
            "Total loss:  -3.0125 | PDE Loss:  -4.8331 | Function Loss:  -3.0837\n",
            "Total loss:  -3.0131 | PDE Loss:  -4.8315 | Function Loss:  -3.0846\n",
            "Total loss:  -3.0135 | PDE Loss:  -4.8296 | Function Loss:  -3.0855\n",
            "Total loss:  -3.0139 | PDE Loss:  -4.8286 | Function Loss:  -3.0861\n",
            "Total loss:  -3.0142 | PDE Loss:  -4.8278 | Function Loss:  -3.0867\n",
            "Total loss:  -3.0145 | PDE Loss:  -4.8271 | Function Loss:  -3.0871\n",
            "Total loss:  -3.0147 | PDE Loss:  -4.8265 | Function Loss:  -3.0874\n",
            "Total loss:  -3.0148 | PDE Loss:  -4.8266 | Function Loss:  -3.0876\n",
            "Total loss:  -3.015 | PDE Loss:  -4.8262 | Function Loss:  -3.0878\n",
            "Total loss:  -3.0151 | PDE Loss:  -4.8262 | Function Loss:  -3.088\n",
            "Total loss:  -3.0153 | PDE Loss:  -4.826 | Function Loss:  -3.0883\n",
            "Total loss:  -3.0155 | PDE Loss:  -4.8245 | Function Loss:  -3.0887\n",
            "Total loss:  -3.0156 | PDE Loss:  -4.8242 | Function Loss:  -3.089\n",
            "Total loss:  -3.0158 | PDE Loss:  -4.824 | Function Loss:  -3.0892\n",
            "Total loss:  -3.0159 | PDE Loss:  -4.8238 | Function Loss:  -3.0894\n",
            "Total loss:  -3.0161 | PDE Loss:  -4.8242 | Function Loss:  -3.0895\n",
            "Total loss:  -3.0162 | PDE Loss:  -4.8243 | Function Loss:  -3.0896\n",
            "Total loss:  -3.0163 | PDE Loss:  -4.8248 | Function Loss:  -3.0896\n",
            "Total loss:  -3.0164 | PDE Loss:  -4.8249 | Function Loss:  -3.0897\n",
            "Total loss:  -3.0164 | PDE Loss:  -4.8253 | Function Loss:  -3.0897\n",
            "Total loss:  -3.0165 | PDE Loss:  -4.8254 | Function Loss:  -3.0898\n",
            "Total loss:  -3.0166 | PDE Loss:  -4.8255 | Function Loss:  -3.0899\n",
            "Total loss:  -3.0166 | PDE Loss:  -4.8253 | Function Loss:  -3.09\n",
            "Total loss:  -3.0167 | PDE Loss:  -4.8251 | Function Loss:  -3.0901\n",
            "Total loss:  -3.0168 | PDE Loss:  -4.8242 | Function Loss:  -3.0904\n",
            "Total loss:  -3.017 | PDE Loss:  -4.8232 | Function Loss:  -3.0907\n",
            "Total loss:  -3.0171 | PDE Loss:  -4.8209 | Function Loss:  -3.0914\n",
            "Total loss:  -3.0173 | PDE Loss:  -4.8188 | Function Loss:  -3.092\n",
            "Total loss:  -3.0175 | PDE Loss:  -4.8186 | Function Loss:  -3.0922\n",
            "Total loss:  -3.0178 | PDE Loss:  -4.8182 | Function Loss:  -3.0927\n",
            "Total loss:  -3.0181 | PDE Loss:  -4.8183 | Function Loss:  -3.093\n",
            "Total loss:  -3.0184 | PDE Loss:  -4.8176 | Function Loss:  -3.0935\n",
            "Total loss:  -3.0187 | PDE Loss:  -4.8173 | Function Loss:  -3.0939\n",
            "Total loss:  -3.0189 | PDE Loss:  -4.8172 | Function Loss:  -3.0942\n",
            "Total loss:  -3.0193 | PDE Loss:  -4.8173 | Function Loss:  -3.0946\n",
            "Total loss:  -3.0196 | PDE Loss:  -4.817 | Function Loss:  -3.095\n",
            "Total loss:  -3.0199 | PDE Loss:  -4.8176 | Function Loss:  -3.0952\n",
            "Total loss:  -3.0201 | PDE Loss:  -4.8175 | Function Loss:  -3.0955\n",
            "Total loss:  -3.0204 | PDE Loss:  -4.8174 | Function Loss:  -3.096\n",
            "Total loss:  -3.0209 | PDE Loss:  -4.817 | Function Loss:  -3.0966\n",
            "Total loss:  -3.0213 | PDE Loss:  -4.816 | Function Loss:  -3.0972\n",
            "Total loss:  -3.0218 | PDE Loss:  -4.8151 | Function Loss:  -3.098\n",
            "Total loss:  -3.0224 | PDE Loss:  -4.814 | Function Loss:  -3.0989\n",
            "Total loss:  -3.023 | PDE Loss:  -4.8125 | Function Loss:  -3.1\n",
            "Total loss:  -3.0235 | PDE Loss:  -4.8122 | Function Loss:  -3.1006\n",
            "Total loss:  -3.0239 | PDE Loss:  -4.8112 | Function Loss:  -3.1013\n",
            "Total loss:  -3.0242 | PDE Loss:  -4.8113 | Function Loss:  -3.1016\n",
            "Total loss:  -3.0245 | PDE Loss:  -4.8117 | Function Loss:  -3.1019\n",
            "Total loss:  -3.0248 | PDE Loss:  -4.8122 | Function Loss:  -3.1021\n",
            "Total loss:  -3.0252 | PDE Loss:  -4.813 | Function Loss:  -3.1025\n",
            "Total loss:  -3.0257 | PDE Loss:  -4.8131 | Function Loss:  -3.103\n",
            "Total loss:  -3.0261 | PDE Loss:  -4.8141 | Function Loss:  -3.1034\n",
            "Total loss:  -3.0265 | PDE Loss:  -4.8145 | Function Loss:  -3.1037\n",
            "Total loss:  -3.0269 | PDE Loss:  -4.815 | Function Loss:  -3.1041\n",
            "Total loss:  -3.0272 | PDE Loss:  -4.8148 | Function Loss:  -3.1046\n",
            "Total loss:  -3.0276 | PDE Loss:  -4.8152 | Function Loss:  -3.105\n",
            "Total loss:  -3.0279 | PDE Loss:  -4.8153 | Function Loss:  -3.1052\n",
            "Total loss:  -3.0284 | PDE Loss:  -4.8157 | Function Loss:  -3.1058\n",
            "Total loss:  -3.0291 | PDE Loss:  -4.8185 | Function Loss:  -3.1061\n",
            "Total loss:  -3.0298 | PDE Loss:  -4.8194 | Function Loss:  -3.1067\n",
            "Total loss:  -3.0307 | PDE Loss:  -4.8214 | Function Loss:  -3.1074\n",
            "Total loss:  -3.0315 | PDE Loss:  -4.8235 | Function Loss:  -3.108\n",
            "Total loss:  -3.0322 | PDE Loss:  -4.8249 | Function Loss:  -3.1085\n",
            "Total loss:  -3.0326 | PDE Loss:  -4.8261 | Function Loss:  -3.1087\n",
            "Total loss:  -3.0329 | PDE Loss:  -4.8266 | Function Loss:  -3.1091\n",
            "Total loss:  -3.0332 | PDE Loss:  -4.8271 | Function Loss:  -3.1093\n",
            "Total loss:  -3.0335 | PDE Loss:  -4.8271 | Function Loss:  -3.1096\n",
            "Total loss:  -3.0338 | PDE Loss:  -4.8275 | Function Loss:  -3.1099\n",
            "Total loss:  -3.0341 | PDE Loss:  -4.8279 | Function Loss:  -3.1102\n",
            "Total loss:  -3.0344 | PDE Loss:  -4.8282 | Function Loss:  -3.1105\n",
            "Total loss:  -3.0346 | PDE Loss:  -4.8283 | Function Loss:  -3.1107\n",
            "Total loss:  -3.0348 | PDE Loss:  -4.8284 | Function Loss:  -3.1109\n",
            "Total loss:  -3.035 | PDE Loss:  -4.8284 | Function Loss:  -3.1111\n",
            "Total loss:  -3.0352 | PDE Loss:  -4.8284 | Function Loss:  -3.1114\n",
            "Total loss:  -3.0355 | PDE Loss:  -4.8282 | Function Loss:  -3.1118\n",
            "Total loss:  -3.0357 | PDE Loss:  -4.8281 | Function Loss:  -3.1121\n",
            "Total loss:  -3.0359 | PDE Loss:  -4.8277 | Function Loss:  -3.1124\n",
            "Total loss:  -3.0361 | PDE Loss:  -4.8273 | Function Loss:  -3.1128\n",
            "Total loss:  -3.0365 | PDE Loss:  -4.8266 | Function Loss:  -3.1134\n",
            "Total loss:  -3.037 | PDE Loss:  -4.8266 | Function Loss:  -3.1139\n",
            "Total loss:  -3.0374 | PDE Loss:  -4.8258 | Function Loss:  -3.1146\n",
            "Total loss:  -3.0379 | PDE Loss:  -4.8262 | Function Loss:  -3.1151\n",
            "Total loss:  -3.0383 | PDE Loss:  -4.826 | Function Loss:  -3.1156\n",
            "Total loss:  -3.0386 | PDE Loss:  -4.8264 | Function Loss:  -3.1159\n",
            "Total loss:  -3.0389 | PDE Loss:  -4.8261 | Function Loss:  -3.1163\n",
            "Total loss:  -3.039 | PDE Loss:  -4.8263 | Function Loss:  -3.1164\n",
            "Total loss:  -3.0391 | PDE Loss:  -4.8264 | Function Loss:  -3.1164\n",
            "Total loss:  -3.0392 | PDE Loss:  -4.8269 | Function Loss:  -3.1165\n",
            "Total loss:  -3.0393 | PDE Loss:  -4.8274 | Function Loss:  -3.1165\n",
            "Total loss:  -3.0393 | PDE Loss:  -4.8281 | Function Loss:  -3.1164\n",
            "Total loss:  -3.0394 | PDE Loss:  -4.8281 | Function Loss:  -3.1165\n",
            "Total loss:  -3.0395 | PDE Loss:  -4.8285 | Function Loss:  -3.1166\n",
            "Total loss:  -3.0396 | PDE Loss:  -4.829 | Function Loss:  -3.1166\n",
            "Total loss:  -3.0397 | PDE Loss:  -4.8301 | Function Loss:  -3.1165\n",
            "Total loss:  -3.0399 | PDE Loss:  -4.8312 | Function Loss:  -3.1164\n",
            "Total loss:  -3.04 | PDE Loss:  -4.8325 | Function Loss:  -3.1164\n",
            "Total loss:  -3.0402 | PDE Loss:  -4.8341 | Function Loss:  -3.1163\n",
            "Total loss:  -3.0404 | PDE Loss:  -4.8356 | Function Loss:  -3.1162\n",
            "Total loss:  -3.0406 | PDE Loss:  -4.8372 | Function Loss:  -3.1161\n",
            "Total loss:  -3.0407 | PDE Loss:  -4.8381 | Function Loss:  -3.1162\n",
            "Total loss:  -3.0409 | PDE Loss:  -4.8392 | Function Loss:  -3.1162\n",
            "Total loss:  -3.0411 | PDE Loss:  -4.8398 | Function Loss:  -3.1162\n",
            "Total loss:  -3.0413 | PDE Loss:  -4.8403 | Function Loss:  -3.1165\n",
            "Total loss:  -3.0416 | PDE Loss:  -4.841 | Function Loss:  -3.1166\n",
            "Total loss:  -3.0418 | PDE Loss:  -4.8413 | Function Loss:  -3.1168\n",
            "Total loss:  -3.042 | PDE Loss:  -4.8417 | Function Loss:  -3.117\n",
            "Total loss:  -3.0422 | PDE Loss:  -4.842 | Function Loss:  -3.1171\n",
            "Total loss:  -3.0423 | PDE Loss:  -4.8422 | Function Loss:  -3.1172\n",
            "Total loss:  -3.0424 | PDE Loss:  -4.8427 | Function Loss:  -3.1173\n",
            "Total loss:  -3.0426 | PDE Loss:  -4.8429 | Function Loss:  -3.1175\n",
            "Total loss:  -3.0426 | PDE Loss:  -4.8438 | Function Loss:  -3.1174\n",
            "Total loss:  -3.0428 | PDE Loss:  -4.8439 | Function Loss:  -3.1175\n",
            "Total loss:  -3.0429 | PDE Loss:  -4.8437 | Function Loss:  -3.1176\n",
            "Total loss:  -3.043 | PDE Loss:  -4.8437 | Function Loss:  -3.1178\n",
            "Total loss:  -3.043 | PDE Loss:  -4.8437 | Function Loss:  -3.1178\n",
            "Total loss:  -3.0431 | PDE Loss:  -4.8437 | Function Loss:  -3.1179\n",
            "Total loss:  -3.0431 | PDE Loss:  -4.844 | Function Loss:  -3.1179\n",
            "Total loss:  -3.0432 | PDE Loss:  -4.8443 | Function Loss:  -3.1179\n",
            "Total loss:  -3.0433 | PDE Loss:  -4.845 | Function Loss:  -3.1179\n",
            "Total loss:  -3.0434 | PDE Loss:  -4.8454 | Function Loss:  -3.1179\n",
            "Total loss:  -3.0434 | PDE Loss:  -4.8458 | Function Loss:  -3.1179\n",
            "Total loss:  -3.0435 | PDE Loss:  -4.8462 | Function Loss:  -3.1179\n",
            "Total loss:  -3.0436 | PDE Loss:  -4.8465 | Function Loss:  -3.1179\n",
            "Total loss:  -3.0436 | PDE Loss:  -4.847 | Function Loss:  -3.118\n",
            "Total loss:  -3.0437 | PDE Loss:  -4.847 | Function Loss:  -3.118\n",
            "Total loss:  -3.0437 | PDE Loss:  -4.8473 | Function Loss:  -3.118\n",
            "Total loss:  -3.0438 | PDE Loss:  -4.8475 | Function Loss:  -3.118\n",
            "Total loss:  -3.0438 | PDE Loss:  -4.8476 | Function Loss:  -3.118\n",
            "Total loss:  -3.0438 | PDE Loss:  -4.8477 | Function Loss:  -3.1181\n",
            "Total loss:  -3.0439 | PDE Loss:  -4.8481 | Function Loss:  -3.118\n",
            "Total loss:  -3.0439 | PDE Loss:  -4.8484 | Function Loss:  -3.118\n",
            "Total loss:  -3.044 | PDE Loss:  -4.8484 | Function Loss:  -3.1181\n",
            "Total loss:  -3.044 | PDE Loss:  -4.8487 | Function Loss:  -3.1181\n",
            "Total loss:  -3.0441 | PDE Loss:  -4.8492 | Function Loss:  -3.1181\n",
            "Total loss:  -3.0442 | PDE Loss:  -4.8496 | Function Loss:  -3.1181\n",
            "Total loss:  -3.0443 | PDE Loss:  -4.8499 | Function Loss:  -3.1182\n",
            "Total loss:  -3.0444 | PDE Loss:  -4.8506 | Function Loss:  -3.1182\n",
            "Total loss:  -3.0445 | PDE Loss:  -4.8508 | Function Loss:  -3.1182\n",
            "Total loss:  -3.0446 | PDE Loss:  -4.8509 | Function Loss:  -3.1183\n",
            "Total loss:  -3.0447 | PDE Loss:  -4.8507 | Function Loss:  -3.1185\n",
            "Total loss:  -3.0447 | PDE Loss:  -4.8505 | Function Loss:  -3.1186\n",
            "Total loss:  -3.0448 | PDE Loss:  -4.8502 | Function Loss:  -3.1187\n",
            "Total loss:  -3.0448 | PDE Loss:  -4.8494 | Function Loss:  -3.1189\n",
            "Total loss:  -3.0449 | PDE Loss:  -4.8492 | Function Loss:  -3.1191\n",
            "Total loss:  -3.045 | PDE Loss:  -4.8489 | Function Loss:  -3.1191\n",
            "Total loss:  -3.045 | PDE Loss:  -4.8487 | Function Loss:  -3.1193\n",
            "Total loss:  -3.0452 | PDE Loss:  -4.8488 | Function Loss:  -3.1194\n",
            "Total loss:  -3.0452 | PDE Loss:  -4.8486 | Function Loss:  -3.1196\n",
            "Total loss:  -3.0453 | PDE Loss:  -4.8488 | Function Loss:  -3.1196\n",
            "Total loss:  -3.0454 | PDE Loss:  -4.8491 | Function Loss:  -3.1197\n",
            "Total loss:  -3.0456 | PDE Loss:  -4.8494 | Function Loss:  -3.1198\n",
            "Total loss:  -3.0456 | PDE Loss:  -4.8501 | Function Loss:  -3.1198\n",
            "Total loss:  -3.0457 | PDE Loss:  -4.8503 | Function Loss:  -3.1198\n",
            "Total loss:  -3.0458 | PDE Loss:  -4.8505 | Function Loss:  -3.1199\n",
            "Total loss:  -3.0459 | PDE Loss:  -4.8504 | Function Loss:  -3.12\n",
            "Total loss:  -3.046 | PDE Loss:  -4.8505 | Function Loss:  -3.1201\n",
            "Total loss:  -3.0462 | PDE Loss:  -4.85 | Function Loss:  -3.1204\n",
            "Total loss:  -3.0463 | PDE Loss:  -4.849 | Function Loss:  -3.1207\n",
            "Total loss:  -3.0464 | PDE Loss:  -4.8487 | Function Loss:  -3.1209\n",
            "Total loss:  -3.0466 | PDE Loss:  -4.8482 | Function Loss:  -3.1212\n",
            "Total loss:  -3.0467 | PDE Loss:  -4.8477 | Function Loss:  -3.1215\n",
            "Total loss:  -3.0469 | PDE Loss:  -4.8469 | Function Loss:  -3.1218\n",
            "Total loss:  -3.0471 | PDE Loss:  -4.8464 | Function Loss:  -3.1221\n",
            "Total loss:  -3.0473 | PDE Loss:  -4.8457 | Function Loss:  -3.1225\n",
            "Total loss:  -3.0474 | PDE Loss:  -4.8448 | Function Loss:  -3.1229\n",
            "Total loss:  -3.0476 | PDE Loss:  -4.8441 | Function Loss:  -3.1232\n",
            "Total loss:  -3.0478 | PDE Loss:  -4.8437 | Function Loss:  -3.1235\n",
            "Total loss:  -3.0479 | PDE Loss:  -4.8425 | Function Loss:  -3.1238\n",
            "Total loss:  -3.048 | PDE Loss:  -4.8417 | Function Loss:  -3.1241\n",
            "Total loss:  -3.048 | PDE Loss:  -4.8415 | Function Loss:  -3.1242\n",
            "Total loss:  -3.0481 | PDE Loss:  -4.8412 | Function Loss:  -3.1244\n",
            "Total loss:  -3.0482 | PDE Loss:  -4.841 | Function Loss:  -3.1245\n",
            "Total loss:  -3.0483 | PDE Loss:  -4.8406 | Function Loss:  -3.1246\n",
            "Total loss:  -3.0483 | PDE Loss:  -4.8399 | Function Loss:  -3.1248\n",
            "Total loss:  -3.0484 | PDE Loss:  -4.8392 | Function Loss:  -3.125\n",
            "Total loss:  -3.0484 | PDE Loss:  -4.8386 | Function Loss:  -3.1252\n",
            "Total loss:  -3.0485 | PDE Loss:  -4.8374 | Function Loss:  -3.1256\n",
            "Total loss:  -3.0486 | PDE Loss:  -4.8365 | Function Loss:  -3.1259\n",
            "Total loss:  -3.0488 | PDE Loss:  -4.8352 | Function Loss:  -3.1264\n",
            "Total loss:  -3.0487 | PDE Loss:  -4.8312 | Function Loss:  -3.127\n",
            "Total loss:  -3.049 | PDE Loss:  -4.8339 | Function Loss:  -3.1268\n",
            "Total loss:  -3.0491 | PDE Loss:  -4.8334 | Function Loss:  -3.1271\n",
            "Total loss:  -3.0494 | PDE Loss:  -4.833 | Function Loss:  -3.1275\n",
            "Total loss:  -3.0496 | PDE Loss:  -4.8328 | Function Loss:  -3.1278\n",
            "Total loss:  -3.0499 | PDE Loss:  -4.8325 | Function Loss:  -3.1282\n",
            "Total loss:  -3.0501 | PDE Loss:  -4.8323 | Function Loss:  -3.1285\n",
            "Total loss:  -3.0503 | PDE Loss:  -4.8324 | Function Loss:  -3.1286\n",
            "Total loss:  -3.0505 | PDE Loss:  -4.8325 | Function Loss:  -3.1289\n",
            "Total loss:  -3.0507 | PDE Loss:  -4.8328 | Function Loss:  -3.1291\n",
            "Total loss:  -3.0509 | PDE Loss:  -4.8328 | Function Loss:  -3.1293\n",
            "Total loss:  -3.051 | PDE Loss:  -4.8326 | Function Loss:  -3.1295\n",
            "Total loss:  -3.0511 | PDE Loss:  -4.8323 | Function Loss:  -3.1297\n",
            "Total loss:  -3.0513 | PDE Loss:  -4.8322 | Function Loss:  -3.13\n",
            "Total loss:  -3.0515 | PDE Loss:  -4.8318 | Function Loss:  -3.1303\n",
            "Total loss:  -3.0517 | PDE Loss:  -4.8306 | Function Loss:  -3.1307\n",
            "Total loss:  -3.0519 | PDE Loss:  -4.8305 | Function Loss:  -3.131\n",
            "Total loss:  -3.0521 | PDE Loss:  -4.8309 | Function Loss:  -3.1311\n",
            "Total loss:  -3.0522 | PDE Loss:  -4.8309 | Function Loss:  -3.1313\n",
            "Total loss:  -3.0523 | PDE Loss:  -4.8312 | Function Loss:  -3.1313\n",
            "Total loss:  -3.0524 | PDE Loss:  -4.8315 | Function Loss:  -3.1314\n",
            "Total loss:  -3.0525 | PDE Loss:  -4.8318 | Function Loss:  -3.1314\n",
            "Total loss:  -3.0526 | PDE Loss:  -4.8319 | Function Loss:  -3.1315\n",
            "Total loss:  -3.0527 | PDE Loss:  -4.832 | Function Loss:  -3.1316\n",
            "Total loss:  -3.0528 | PDE Loss:  -4.832 | Function Loss:  -3.1318\n",
            "Total loss:  -3.0531 | PDE Loss:  -4.832 | Function Loss:  -3.1321\n",
            "Total loss:  -3.0532 | PDE Loss:  -4.8316 | Function Loss:  -3.1323\n",
            "Total loss:  -3.0533 | PDE Loss:  -4.8315 | Function Loss:  -3.1325\n",
            "Total loss:  -3.0534 | PDE Loss:  -4.8313 | Function Loss:  -3.1326\n",
            "Total loss:  -3.0535 | PDE Loss:  -4.8313 | Function Loss:  -3.1328\n",
            "Total loss:  -3.0536 | PDE Loss:  -4.8315 | Function Loss:  -3.1329\n",
            "Total loss:  -3.0537 | PDE Loss:  -4.8313 | Function Loss:  -3.133\n",
            "Total loss:  -3.0538 | PDE Loss:  -4.8312 | Function Loss:  -3.1331\n",
            "Total loss:  -3.0539 | PDE Loss:  -4.831 | Function Loss:  -3.1332\n",
            "Total loss:  -3.054 | PDE Loss:  -4.8305 | Function Loss:  -3.1335\n",
            "Total loss:  -3.0541 | PDE Loss:  -4.8301 | Function Loss:  -3.1337\n",
            "Total loss:  -3.0543 | PDE Loss:  -4.8298 | Function Loss:  -3.134\n",
            "Total loss:  -3.0544 | PDE Loss:  -4.8295 | Function Loss:  -3.1342\n",
            "Total loss:  -3.0544 | PDE Loss:  -4.8279 | Function Loss:  -3.1346\n",
            "Total loss:  -3.0546 | PDE Loss:  -4.8285 | Function Loss:  -3.1346\n",
            "Total loss:  -3.0547 | PDE Loss:  -4.829 | Function Loss:  -3.1347\n",
            "Total loss:  -3.0549 | PDE Loss:  -4.8294 | Function Loss:  -3.1348\n",
            "Total loss:  -3.055 | PDE Loss:  -4.8299 | Function Loss:  -3.1348\n",
            "Total loss:  -3.0551 | PDE Loss:  -4.8301 | Function Loss:  -3.135\n",
            "Total loss:  -3.0553 | PDE Loss:  -4.8305 | Function Loss:  -3.1351\n",
            "Total loss:  -3.0555 | PDE Loss:  -4.8304 | Function Loss:  -3.1353\n",
            "Total loss:  -3.0557 | PDE Loss:  -4.8302 | Function Loss:  -3.1356\n",
            "Total loss:  -3.0558 | PDE Loss:  -4.8296 | Function Loss:  -3.1359\n",
            "Total loss:  -3.056 | PDE Loss:  -4.8301 | Function Loss:  -3.1359\n",
            "Total loss:  -3.0561 | PDE Loss:  -4.8297 | Function Loss:  -3.1361\n",
            "Total loss:  -3.0562 | PDE Loss:  -4.8289 | Function Loss:  -3.1364\n",
            "Total loss:  -3.0563 | PDE Loss:  -4.8287 | Function Loss:  -3.1367\n",
            "Total loss:  -3.0564 | PDE Loss:  -4.8284 | Function Loss:  -3.1369\n",
            "Total loss:  -3.0566 | PDE Loss:  -4.828 | Function Loss:  -3.1372\n",
            "Total loss:  -3.0569 | PDE Loss:  -4.8278 | Function Loss:  -3.1375\n",
            "Total loss:  -3.0571 | PDE Loss:  -4.8281 | Function Loss:  -3.1378\n",
            "Total loss:  -3.0574 | PDE Loss:  -4.828 | Function Loss:  -3.1381\n",
            "Total loss:  -3.0576 | PDE Loss:  -4.8284 | Function Loss:  -3.1382\n",
            "Total loss:  -3.0577 | PDE Loss:  -4.8289 | Function Loss:  -3.1383\n",
            "Total loss:  -3.0579 | PDE Loss:  -4.8286 | Function Loss:  -3.1386\n",
            "Total loss:  -3.0581 | PDE Loss:  -4.8291 | Function Loss:  -3.1387\n",
            "Total loss:  -3.0583 | PDE Loss:  -4.8298 | Function Loss:  -3.1388\n",
            "Total loss:  -3.0585 | PDE Loss:  -4.8302 | Function Loss:  -3.1389\n",
            "Total loss:  -3.0586 | PDE Loss:  -4.8306 | Function Loss:  -3.1391\n",
            "Total loss:  -3.0588 | PDE Loss:  -4.8304 | Function Loss:  -3.1393\n",
            "Total loss:  -3.059 | PDE Loss:  -4.8304 | Function Loss:  -3.1395\n",
            "Total loss:  -3.0589 | PDE Loss:  -4.8292 | Function Loss:  -3.1397\n",
            "Total loss:  -3.059 | PDE Loss:  -4.8301 | Function Loss:  -3.1396\n",
            "Total loss:  -3.0591 | PDE Loss:  -4.83 | Function Loss:  -3.1398\n",
            "Total loss:  -3.0593 | PDE Loss:  -4.8296 | Function Loss:  -3.1401\n",
            "Total loss:  -3.0595 | PDE Loss:  -4.8292 | Function Loss:  -3.1404\n",
            "Total loss:  -3.0598 | PDE Loss:  -4.8281 | Function Loss:  -3.141\n",
            "Total loss:  -3.06 | PDE Loss:  -4.8275 | Function Loss:  -3.1413\n",
            "Total loss:  -3.0601 | PDE Loss:  -4.8266 | Function Loss:  -3.1417\n",
            "Total loss:  -3.0603 | PDE Loss:  -4.8262 | Function Loss:  -3.1419\n",
            "Total loss:  -3.0604 | PDE Loss:  -4.8257 | Function Loss:  -3.1422\n",
            "Total loss:  -3.0605 | PDE Loss:  -4.8253 | Function Loss:  -3.1424\n",
            "Total loss:  -3.0606 | PDE Loss:  -4.825 | Function Loss:  -3.1426\n",
            "Total loss:  -3.0607 | PDE Loss:  -4.8248 | Function Loss:  -3.1427\n",
            "Total loss:  -3.0608 | PDE Loss:  -4.824 | Function Loss:  -3.1431\n",
            "Total loss:  -3.0609 | PDE Loss:  -4.824 | Function Loss:  -3.1432\n",
            "Total loss:  -3.0611 | PDE Loss:  -4.8241 | Function Loss:  -3.1433\n",
            "Total loss:  -3.0612 | PDE Loss:  -4.8243 | Function Loss:  -3.1435\n",
            "Total loss:  -3.0614 | PDE Loss:  -4.8242 | Function Loss:  -3.1437\n",
            "Total loss:  -3.0615 | PDE Loss:  -4.8241 | Function Loss:  -3.1438\n",
            "Total loss:  -3.0616 | PDE Loss:  -4.8241 | Function Loss:  -3.1439\n",
            "Total loss:  -3.0617 | PDE Loss:  -4.8242 | Function Loss:  -3.144\n",
            "Total loss:  -3.0618 | PDE Loss:  -4.8243 | Function Loss:  -3.1442\n",
            "Total loss:  -3.0619 | PDE Loss:  -4.8248 | Function Loss:  -3.1442\n",
            "Total loss:  -3.062 | PDE Loss:  -4.8255 | Function Loss:  -3.1442\n",
            "Total loss:  -3.0621 | PDE Loss:  -4.8265 | Function Loss:  -3.1441\n",
            "Total loss:  -3.0622 | PDE Loss:  -4.8272 | Function Loss:  -3.144\n",
            "Total loss:  -3.0623 | PDE Loss:  -4.8282 | Function Loss:  -3.1439\n",
            "Total loss:  -3.0624 | PDE Loss:  -4.8292 | Function Loss:  -3.1439\n",
            "Total loss:  -3.0625 | PDE Loss:  -4.8309 | Function Loss:  -3.1436\n",
            "Total loss:  -3.0626 | PDE Loss:  -4.8315 | Function Loss:  -3.1436\n",
            "Total loss:  -3.0626 | PDE Loss:  -4.832 | Function Loss:  -3.1436\n",
            "Total loss:  -3.0627 | PDE Loss:  -4.8324 | Function Loss:  -3.1436\n",
            "Total loss:  -3.0628 | PDE Loss:  -4.8325 | Function Loss:  -3.1437\n",
            "Total loss:  -3.0629 | PDE Loss:  -4.8335 | Function Loss:  -3.1436\n",
            "Total loss:  -3.063 | PDE Loss:  -4.8327 | Function Loss:  -3.1439\n",
            "Total loss:  -3.0631 | PDE Loss:  -4.8319 | Function Loss:  -3.1442\n",
            "Total loss:  -3.0632 | PDE Loss:  -4.8309 | Function Loss:  -3.1445\n",
            "Total loss:  -3.0632 | PDE Loss:  -4.8301 | Function Loss:  -3.1447\n",
            "Total loss:  -3.0633 | PDE Loss:  -4.8293 | Function Loss:  -3.145\n",
            "Total loss:  -3.0634 | PDE Loss:  -4.8286 | Function Loss:  -3.1452\n",
            "Total loss:  -3.0635 | PDE Loss:  -4.8281 | Function Loss:  -3.1454\n",
            "Total loss:  -3.0636 | PDE Loss:  -4.8278 | Function Loss:  -3.1456\n",
            "Total loss:  -3.0637 | PDE Loss:  -4.8278 | Function Loss:  -3.1457\n",
            "Total loss:  -3.0637 | PDE Loss:  -4.8278 | Function Loss:  -3.1458\n",
            "Total loss:  -3.0638 | PDE Loss:  -4.8278 | Function Loss:  -3.1459\n",
            "Total loss:  -3.0639 | PDE Loss:  -4.8279 | Function Loss:  -3.1459\n",
            "Total loss:  -3.064 | PDE Loss:  -4.8281 | Function Loss:  -3.146\n",
            "Total loss:  -3.064 | PDE Loss:  -4.828 | Function Loss:  -3.1461\n",
            "Total loss:  -3.0641 | PDE Loss:  -4.828 | Function Loss:  -3.1462\n",
            "Total loss:  -3.0642 | PDE Loss:  -4.8278 | Function Loss:  -3.1463\n",
            "Total loss:  -3.0643 | PDE Loss:  -4.8277 | Function Loss:  -3.1465\n",
            "Total loss:  -3.0644 | PDE Loss:  -4.827 | Function Loss:  -3.1468\n",
            "Total loss:  -3.0645 | PDE Loss:  -4.8277 | Function Loss:  -3.1468\n",
            "Total loss:  -3.0646 | PDE Loss:  -4.8282 | Function Loss:  -3.1468\n",
            "Total loss:  -3.0647 | PDE Loss:  -4.8289 | Function Loss:  -3.1467\n",
            "Total loss:  -3.0648 | PDE Loss:  -4.8301 | Function Loss:  -3.1466\n",
            "Total loss:  -3.065 | PDE Loss:  -4.8314 | Function Loss:  -3.1465\n",
            "Total loss:  -3.0651 | PDE Loss:  -4.8327 | Function Loss:  -3.1464\n",
            "Total loss:  -3.0652 | PDE Loss:  -4.8338 | Function Loss:  -3.1463\n",
            "Total loss:  -3.0652 | PDE Loss:  -4.8349 | Function Loss:  -3.1461\n",
            "Total loss:  -3.0654 | PDE Loss:  -4.8365 | Function Loss:  -3.146\n",
            "Total loss:  -3.0655 | PDE Loss:  -4.8373 | Function Loss:  -3.146\n",
            "Total loss:  -3.0657 | PDE Loss:  -4.8381 | Function Loss:  -3.146\n",
            "Total loss:  -3.0659 | PDE Loss:  -4.8385 | Function Loss:  -3.1462\n",
            "Total loss:  -3.0661 | PDE Loss:  -4.8387 | Function Loss:  -3.1464\n",
            "Total loss:  -3.0663 | PDE Loss:  -4.8387 | Function Loss:  -3.1466\n",
            "Total loss:  -3.0664 | PDE Loss:  -4.8387 | Function Loss:  -3.1467\n",
            "Total loss:  -3.0665 | PDE Loss:  -4.8388 | Function Loss:  -3.1469\n",
            "Total loss:  -3.0666 | PDE Loss:  -4.8391 | Function Loss:  -3.147\n",
            "Total loss:  -3.0669 | PDE Loss:  -4.8397 | Function Loss:  -3.1471\n",
            "Total loss:  -3.067 | PDE Loss:  -4.8405 | Function Loss:  -3.1472\n",
            "Total loss:  -3.0673 | PDE Loss:  -4.842 | Function Loss:  -3.1472\n",
            "Total loss:  -3.0676 | PDE Loss:  -4.8433 | Function Loss:  -3.1472\n",
            "Total loss:  -3.0678 | PDE Loss:  -4.8445 | Function Loss:  -3.1473\n",
            "Total loss:  -3.0679 | PDE Loss:  -4.8449 | Function Loss:  -3.1474\n",
            "Total loss:  -3.0681 | PDE Loss:  -4.845 | Function Loss:  -3.1475\n",
            "Total loss:  -3.0682 | PDE Loss:  -4.845 | Function Loss:  -3.1477\n",
            "Total loss:  -3.0684 | PDE Loss:  -4.8454 | Function Loss:  -3.1478\n",
            "Total loss:  -3.0686 | PDE Loss:  -4.8458 | Function Loss:  -3.1479\n",
            "Total loss:  -3.0687 | PDE Loss:  -4.8464 | Function Loss:  -3.148\n",
            "Total loss:  -3.0689 | PDE Loss:  -4.8472 | Function Loss:  -3.148\n",
            "Total loss:  -3.069 | PDE Loss:  -4.8485 | Function Loss:  -3.1479\n",
            "Total loss:  -3.0691 | PDE Loss:  -4.8499 | Function Loss:  -3.1478\n",
            "Total loss:  -3.0692 | PDE Loss:  -4.851 | Function Loss:  -3.1477\n",
            "Total loss:  -3.0693 | PDE Loss:  -4.8524 | Function Loss:  -3.1475\n",
            "Total loss:  -3.0694 | PDE Loss:  -4.8534 | Function Loss:  -3.1474\n",
            "Total loss:  -3.0695 | PDE Loss:  -4.8547 | Function Loss:  -3.1473\n",
            "Total loss:  -3.0696 | PDE Loss:  -4.8555 | Function Loss:  -3.1472\n",
            "Total loss:  -3.0696 | PDE Loss:  -4.8559 | Function Loss:  -3.1472\n",
            "Total loss:  -3.0698 | PDE Loss:  -4.8573 | Function Loss:  -3.1471\n",
            "Total loss:  -3.0699 | PDE Loss:  -4.8576 | Function Loss:  -3.1472\n",
            "Total loss:  -3.07 | PDE Loss:  -4.8577 | Function Loss:  -3.1473\n",
            "Total loss:  -3.0702 | PDE Loss:  -4.8581 | Function Loss:  -3.1474\n",
            "Total loss:  -3.0703 | PDE Loss:  -4.858 | Function Loss:  -3.1476\n",
            "Total loss:  -3.0705 | PDE Loss:  -4.8579 | Function Loss:  -3.1479\n",
            "Total loss:  -3.0707 | PDE Loss:  -4.8579 | Function Loss:  -3.1481\n",
            "Total loss:  -3.0708 | PDE Loss:  -4.8577 | Function Loss:  -3.1483\n",
            "Total loss:  -3.071 | PDE Loss:  -4.8583 | Function Loss:  -3.1483\n",
            "Total loss:  -3.0711 | PDE Loss:  -4.8586 | Function Loss:  -3.1484\n",
            "Total loss:  -3.0711 | PDE Loss:  -4.859 | Function Loss:  -3.1484\n",
            "Total loss:  -3.0713 | PDE Loss:  -4.8597 | Function Loss:  -3.1484\n",
            "Total loss:  -3.0714 | PDE Loss:  -4.8603 | Function Loss:  -3.1485\n",
            "Total loss:  -3.0715 | PDE Loss:  -4.8604 | Function Loss:  -3.1486\n",
            "Total loss:  -3.0717 | PDE Loss:  -4.8609 | Function Loss:  -3.1487\n",
            "Total loss:  -3.0719 | PDE Loss:  -4.8597 | Function Loss:  -3.1491\n",
            "Total loss:  -3.072 | PDE Loss:  -4.8603 | Function Loss:  -3.1492\n",
            "Total loss:  -3.0723 | PDE Loss:  -4.86 | Function Loss:  -3.1496\n",
            "Total loss:  -3.0726 | PDE Loss:  -4.8611 | Function Loss:  -3.1497\n",
            "Total loss:  -3.0729 | PDE Loss:  -4.8625 | Function Loss:  -3.1498\n",
            "Total loss:  -3.0732 | PDE Loss:  -4.8644 | Function Loss:  -3.1498\n",
            "Total loss:  -3.0734 | PDE Loss:  -4.866 | Function Loss:  -3.1498\n",
            "Total loss:  -3.0736 | PDE Loss:  -4.8679 | Function Loss:  -3.1496\n",
            "Total loss:  -3.0738 | PDE Loss:  -4.869 | Function Loss:  -3.1496\n",
            "Total loss:  -3.0739 | PDE Loss:  -4.8704 | Function Loss:  -3.1495\n",
            "Total loss:  -3.074 | PDE Loss:  -4.8712 | Function Loss:  -3.1495\n",
            "Total loss:  -3.0741 | PDE Loss:  -4.8724 | Function Loss:  -3.1494\n",
            "Total loss:  -3.0742 | PDE Loss:  -4.8732 | Function Loss:  -3.1494\n",
            "Total loss:  -3.0744 | PDE Loss:  -4.8741 | Function Loss:  -3.1494\n",
            "Total loss:  -3.0746 | PDE Loss:  -4.8743 | Function Loss:  -3.1496\n",
            "Total loss:  -3.0747 | PDE Loss:  -4.8751 | Function Loss:  -3.1496\n",
            "Total loss:  -3.0748 | PDE Loss:  -4.8755 | Function Loss:  -3.1496\n",
            "Total loss:  -3.0749 | PDE Loss:  -4.8759 | Function Loss:  -3.1497\n",
            "Total loss:  -3.075 | PDE Loss:  -4.8765 | Function Loss:  -3.1497\n",
            "Total loss:  -3.0751 | PDE Loss:  -4.8769 | Function Loss:  -3.1497\n",
            "Total loss:  -3.0752 | PDE Loss:  -4.8774 | Function Loss:  -3.1497\n",
            "Total loss:  -3.0753 | PDE Loss:  -4.8778 | Function Loss:  -3.1497\n",
            "Total loss:  -3.0754 | PDE Loss:  -4.8781 | Function Loss:  -3.1498\n",
            "Total loss:  -3.0754 | PDE Loss:  -4.8788 | Function Loss:  -3.1498\n",
            "Total loss:  -3.0755 | PDE Loss:  -4.8792 | Function Loss:  -3.1498\n",
            "Total loss:  -3.0756 | PDE Loss:  -4.8794 | Function Loss:  -3.1498\n",
            "Total loss:  -3.0757 | PDE Loss:  -4.8797 | Function Loss:  -3.1499\n",
            "Total loss:  -3.0759 | PDE Loss:  -4.8802 | Function Loss:  -3.15\n",
            "Total loss:  -3.076 | PDE Loss:  -4.8808 | Function Loss:  -3.1501\n",
            "Total loss:  -3.0762 | PDE Loss:  -4.8811 | Function Loss:  -3.1502\n",
            "Total loss:  -3.0764 | PDE Loss:  -4.8818 | Function Loss:  -3.1503\n",
            "Total loss:  -3.0766 | PDE Loss:  -4.8811 | Function Loss:  -3.1507\n",
            "Total loss:  -3.0768 | PDE Loss:  -4.8819 | Function Loss:  -3.1508\n",
            "Total loss:  -3.0771 | PDE Loss:  -4.8823 | Function Loss:  -3.151\n",
            "Total loss:  -3.0774 | PDE Loss:  -4.8828 | Function Loss:  -3.1513\n",
            "Total loss:  -3.0777 | PDE Loss:  -4.8824 | Function Loss:  -3.1518\n",
            "Total loss:  -3.078 | PDE Loss:  -4.8829 | Function Loss:  -3.152\n",
            "Total loss:  -3.0783 | PDE Loss:  -4.8833 | Function Loss:  -3.1523\n",
            "Total loss:  -3.0787 | PDE Loss:  -4.8846 | Function Loss:  -3.1525\n",
            "Total loss:  -3.079 | PDE Loss:  -4.8853 | Function Loss:  -3.1528\n",
            "Total loss:  -3.0793 | PDE Loss:  -4.8862 | Function Loss:  -3.1529\n",
            "Total loss:  -3.0795 | PDE Loss:  -4.8868 | Function Loss:  -3.1531\n",
            "Total loss:  -3.0797 | PDE Loss:  -4.8874 | Function Loss:  -3.1532\n",
            "Total loss:  -3.0799 | PDE Loss:  -4.8877 | Function Loss:  -3.1534\n",
            "Total loss:  -3.08 | PDE Loss:  -4.8879 | Function Loss:  -3.1535\n",
            "Total loss:  -3.0802 | PDE Loss:  -4.8881 | Function Loss:  -3.1537\n",
            "Total loss:  -3.0803 | PDE Loss:  -4.888 | Function Loss:  -3.1538\n",
            "Total loss:  -3.0804 | PDE Loss:  -4.8881 | Function Loss:  -3.1539\n",
            "Total loss:  -3.0806 | PDE Loss:  -4.8878 | Function Loss:  -3.1542\n",
            "Total loss:  -3.0807 | PDE Loss:  -4.8879 | Function Loss:  -3.1543\n",
            "Total loss:  -3.0808 | PDE Loss:  -4.8874 | Function Loss:  -3.1545\n",
            "Total loss:  -3.0809 | PDE Loss:  -4.8874 | Function Loss:  -3.1546\n",
            "Total loss:  -3.081 | PDE Loss:  -4.887 | Function Loss:  -3.1548\n",
            "Total loss:  -3.081 | PDE Loss:  -4.8869 | Function Loss:  -3.1549\n",
            "Total loss:  -3.0811 | PDE Loss:  -4.8869 | Function Loss:  -3.1549\n",
            "Total loss:  -3.0812 | PDE Loss:  -4.8872 | Function Loss:  -3.155\n",
            "Total loss:  -3.0812 | PDE Loss:  -4.8874 | Function Loss:  -3.155\n",
            "Total loss:  -3.0814 | PDE Loss:  -4.8884 | Function Loss:  -3.155\n",
            "Total loss:  -3.0815 | PDE Loss:  -4.8889 | Function Loss:  -3.1551\n",
            "Total loss:  -3.0817 | PDE Loss:  -4.8905 | Function Loss:  -3.155\n",
            "Total loss:  -3.0818 | PDE Loss:  -4.8914 | Function Loss:  -3.155\n",
            "Total loss:  -3.082 | PDE Loss:  -4.8925 | Function Loss:  -3.155\n",
            "Total loss:  -3.0822 | PDE Loss:  -4.8932 | Function Loss:  -3.155\n",
            "Total loss:  -3.0823 | PDE Loss:  -4.8943 | Function Loss:  -3.155\n",
            "Total loss:  -3.0824 | PDE Loss:  -4.8946 | Function Loss:  -3.1551\n",
            "Total loss:  -3.0825 | PDE Loss:  -4.8949 | Function Loss:  -3.1551\n",
            "Total loss:  -3.0826 | PDE Loss:  -4.8946 | Function Loss:  -3.1553\n",
            "Total loss:  -3.0827 | PDE Loss:  -4.8944 | Function Loss:  -3.1554\n",
            "Total loss:  -3.0828 | PDE Loss:  -4.8938 | Function Loss:  -3.1557\n",
            "Total loss:  -3.0828 | PDE Loss:  -4.8933 | Function Loss:  -3.1558\n",
            "Total loss:  -3.0829 | PDE Loss:  -4.8927 | Function Loss:  -3.156\n",
            "Total loss:  -3.083 | PDE Loss:  -4.8921 | Function Loss:  -3.1562\n",
            "Total loss:  -3.0831 | PDE Loss:  -4.8917 | Function Loss:  -3.1564\n",
            "Total loss:  -3.0829 | PDE Loss:  -4.8888 | Function Loss:  -3.1568\n",
            "Total loss:  -3.0831 | PDE Loss:  -4.891 | Function Loss:  -3.1566\n",
            "Total loss:  -3.0832 | PDE Loss:  -4.8907 | Function Loss:  -3.1568\n",
            "Total loss:  -3.0834 | PDE Loss:  -4.8909 | Function Loss:  -3.1569\n",
            "Total loss:  -3.0835 | PDE Loss:  -4.8911 | Function Loss:  -3.157\n",
            "Total loss:  -3.0837 | PDE Loss:  -4.8924 | Function Loss:  -3.157\n",
            "Total loss:  -3.0838 | PDE Loss:  -4.8932 | Function Loss:  -3.157\n",
            "Total loss:  -3.084 | PDE Loss:  -4.894 | Function Loss:  -3.1571\n",
            "Total loss:  -3.0841 | PDE Loss:  -4.8955 | Function Loss:  -3.157\n",
            "Total loss:  -3.0843 | PDE Loss:  -4.8953 | Function Loss:  -3.1572\n",
            "Total loss:  -3.0845 | PDE Loss:  -4.8956 | Function Loss:  -3.1574\n",
            "Total loss:  -3.0846 | PDE Loss:  -4.8947 | Function Loss:  -3.1577\n",
            "Total loss:  -3.0848 | PDE Loss:  -4.8934 | Function Loss:  -3.1582\n",
            "Total loss:  -3.085 | PDE Loss:  -4.8921 | Function Loss:  -3.1587\n",
            "Total loss:  -3.0853 | PDE Loss:  -4.8904 | Function Loss:  -3.1593\n",
            "Total loss:  -3.0857 | PDE Loss:  -4.8887 | Function Loss:  -3.16\n",
            "Total loss:  -3.086 | PDE Loss:  -4.8872 | Function Loss:  -3.1608\n",
            "Total loss:  -3.0864 | PDE Loss:  -4.8867 | Function Loss:  -3.1613\n",
            "Total loss:  -3.0868 | PDE Loss:  -4.8845 | Function Loss:  -3.1622\n",
            "Total loss:  -3.0871 | PDE Loss:  -4.8856 | Function Loss:  -3.1624\n",
            "Total loss:  -3.0874 | PDE Loss:  -4.8853 | Function Loss:  -3.1628\n",
            "Total loss:  -3.0878 | PDE Loss:  -4.8871 | Function Loss:  -3.1628\n",
            "Total loss:  -3.0881 | PDE Loss:  -4.8879 | Function Loss:  -3.1631\n",
            "Total loss:  -3.0884 | PDE Loss:  -4.8893 | Function Loss:  -3.1631\n",
            "Total loss:  -3.0886 | PDE Loss:  -4.89 | Function Loss:  -3.1632\n",
            "Total loss:  -3.0887 | PDE Loss:  -4.8906 | Function Loss:  -3.1633\n",
            "Total loss:  -3.0888 | PDE Loss:  -4.8912 | Function Loss:  -3.1633\n",
            "Total loss:  -3.089 | PDE Loss:  -4.8901 | Function Loss:  -3.1637\n",
            "Total loss:  -3.0891 | PDE Loss:  -4.8907 | Function Loss:  -3.1637\n",
            "Total loss:  -3.0892 | PDE Loss:  -4.8908 | Function Loss:  -3.1639\n",
            "Total loss:  -3.0894 | PDE Loss:  -4.8915 | Function Loss:  -3.1639\n",
            "Total loss:  -3.0895 | PDE Loss:  -4.8904 | Function Loss:  -3.1643\n",
            "Total loss:  -3.0897 | PDE Loss:  -4.8905 | Function Loss:  -3.1644\n",
            "Total loss:  -3.0898 | PDE Loss:  -4.8905 | Function Loss:  -3.1646\n",
            "Total loss:  -3.09 | PDE Loss:  -4.8908 | Function Loss:  -3.1648\n",
            "Total loss:  -3.0902 | PDE Loss:  -4.8909 | Function Loss:  -3.165\n",
            "Total loss:  -3.0903 | PDE Loss:  -4.8916 | Function Loss:  -3.165\n",
            "Total loss:  -3.0906 | PDE Loss:  -4.8922 | Function Loss:  -3.1652\n",
            "Total loss:  -3.0908 | PDE Loss:  -4.8933 | Function Loss:  -3.1653\n",
            "Total loss:  -3.091 | PDE Loss:  -4.8933 | Function Loss:  -3.1656\n",
            "Total loss:  -3.0912 | PDE Loss:  -4.8935 | Function Loss:  -3.1658\n",
            "Total loss:  -3.0915 | PDE Loss:  -4.8929 | Function Loss:  -3.1661\n",
            "Total loss:  -3.0917 | PDE Loss:  -4.8933 | Function Loss:  -3.1663\n",
            "Total loss:  -3.092 | PDE Loss:  -4.8913 | Function Loss:  -3.1671\n",
            "Total loss:  -3.0923 | PDE Loss:  -4.8921 | Function Loss:  -3.1672\n",
            "Total loss:  -3.0925 | PDE Loss:  -4.8924 | Function Loss:  -3.1674\n",
            "Total loss:  -3.0928 | PDE Loss:  -4.8932 | Function Loss:  -3.1676\n",
            "Total loss:  -3.0931 | PDE Loss:  -4.8937 | Function Loss:  -3.1679\n",
            "Total loss:  -3.0933 | PDE Loss:  -4.8948 | Function Loss:  -3.168\n",
            "Total loss:  -3.0936 | PDE Loss:  -4.8954 | Function Loss:  -3.1682\n",
            "Total loss:  -3.0941 | PDE Loss:  -4.8966 | Function Loss:  -3.1685\n",
            "Total loss:  -3.0945 | PDE Loss:  -4.8976 | Function Loss:  -3.1688\n",
            "Total loss:  -3.095 | PDE Loss:  -4.8984 | Function Loss:  -3.1693\n",
            "Total loss:  -3.0954 | PDE Loss:  -4.8987 | Function Loss:  -3.1697\n",
            "Total loss:  -3.0959 | PDE Loss:  -4.8991 | Function Loss:  -3.1702\n",
            "Total loss:  -3.0962 | PDE Loss:  -4.8983 | Function Loss:  -3.1708\n",
            "Total loss:  -3.0965 | PDE Loss:  -4.8984 | Function Loss:  -3.1711\n",
            "Total loss:  -3.0968 | PDE Loss:  -4.8984 | Function Loss:  -3.1714\n",
            "Total loss:  -3.0972 | PDE Loss:  -4.8985 | Function Loss:  -3.1719\n",
            "Total loss:  -3.0976 | PDE Loss:  -4.8991 | Function Loss:  -3.1723\n",
            "Total loss:  -3.0981 | PDE Loss:  -4.8995 | Function Loss:  -3.1728\n",
            "Total loss:  -3.0986 | PDE Loss:  -4.901 | Function Loss:  -3.1731\n",
            "Total loss:  -3.0991 | PDE Loss:  -4.9024 | Function Loss:  -3.1735\n",
            "Total loss:  -3.0997 | PDE Loss:  -4.9042 | Function Loss:  -3.1738\n",
            "Total loss:  -3.1002 | PDE Loss:  -4.9073 | Function Loss:  -3.1738\n",
            "Total loss:  -3.1005 | PDE Loss:  -4.9094 | Function Loss:  -3.1738\n",
            "Total loss:  -3.1009 | PDE Loss:  -4.9123 | Function Loss:  -3.1737\n",
            "Total loss:  -3.1015 | PDE Loss:  -4.9137 | Function Loss:  -3.1742\n",
            "Total loss:  -3.1021 | PDE Loss:  -4.9177 | Function Loss:  -3.1742\n",
            "Total loss:  -3.1028 | PDE Loss:  -4.9195 | Function Loss:  -3.1747\n",
            "Total loss:  -3.1033 | PDE Loss:  -4.9223 | Function Loss:  -3.1748\n",
            "Total loss:  -3.1038 | PDE Loss:  -4.9233 | Function Loss:  -3.1752\n",
            "Total loss:  -3.1044 | PDE Loss:  -4.9225 | Function Loss:  -3.176\n",
            "Total loss:  -3.1048 | PDE Loss:  -4.9221 | Function Loss:  -3.1765\n",
            "Total loss:  -3.1049 | PDE Loss:  -4.9189 | Function Loss:  -3.1772\n",
            "Total loss:  -3.1052 | PDE Loss:  -4.9188 | Function Loss:  -3.1776\n",
            "Total loss:  -3.1054 | PDE Loss:  -4.9194 | Function Loss:  -3.1777\n",
            "Total loss:  -3.1056 | PDE Loss:  -4.9188 | Function Loss:  -3.1781\n",
            "Total loss:  -3.1057 | PDE Loss:  -4.9184 | Function Loss:  -3.1783\n",
            "Total loss:  -3.1059 | PDE Loss:  -4.9182 | Function Loss:  -3.1786\n",
            "Total loss:  -3.1061 | PDE Loss:  -4.9178 | Function Loss:  -3.1789\n",
            "Total loss:  -3.1062 | PDE Loss:  -4.9179 | Function Loss:  -3.179\n",
            "Total loss:  -3.1064 | PDE Loss:  -4.9178 | Function Loss:  -3.1792\n",
            "Total loss:  -3.1065 | PDE Loss:  -4.9178 | Function Loss:  -3.1793\n",
            "Total loss:  -3.1066 | PDE Loss:  -4.9178 | Function Loss:  -3.1795\n",
            "Total loss:  -3.1067 | PDE Loss:  -4.9178 | Function Loss:  -3.1796\n",
            "Total loss:  -3.1069 | PDE Loss:  -4.9177 | Function Loss:  -3.1799\n",
            "Total loss:  -3.1071 | PDE Loss:  -4.9176 | Function Loss:  -3.1801\n",
            "Total loss:  -3.1074 | PDE Loss:  -4.9174 | Function Loss:  -3.1805\n",
            "Total loss:  -3.1077 | PDE Loss:  -4.9171 | Function Loss:  -3.1808\n",
            "Total loss:  -3.1079 | PDE Loss:  -4.9171 | Function Loss:  -3.1811\n",
            "Total loss:  -3.1081 | PDE Loss:  -4.9171 | Function Loss:  -3.1814\n",
            "Total loss:  -3.1083 | PDE Loss:  -4.9177 | Function Loss:  -3.1815\n",
            "Total loss:  -3.1085 | PDE Loss:  -4.9183 | Function Loss:  -3.1816\n",
            "Total loss:  -3.1087 | PDE Loss:  -4.9197 | Function Loss:  -3.1816\n",
            "Total loss:  -3.1089 | PDE Loss:  -4.9206 | Function Loss:  -3.1816\n",
            "Total loss:  -3.109 | PDE Loss:  -4.9222 | Function Loss:  -3.1815\n",
            "Total loss:  -3.1093 | PDE Loss:  -4.924 | Function Loss:  -3.1815\n",
            "Total loss:  -3.1096 | PDE Loss:  -4.9269 | Function Loss:  -3.1813\n",
            "Total loss:  -3.1098 | PDE Loss:  -4.9309 | Function Loss:  -3.1808\n",
            "Total loss:  -3.11 | PDE Loss:  -4.9316 | Function Loss:  -3.181\n",
            "Total loss:  -3.1102 | PDE Loss:  -4.9331 | Function Loss:  -3.181\n",
            "Total loss:  -3.1105 | PDE Loss:  -4.9345 | Function Loss:  -3.1811\n",
            "Total loss:  -3.1108 | PDE Loss:  -4.9358 | Function Loss:  -3.1812\n",
            "Total loss:  -3.1111 | PDE Loss:  -4.938 | Function Loss:  -3.1812\n",
            "Total loss:  -3.1113 | PDE Loss:  -4.9397 | Function Loss:  -3.1811\n",
            "Total loss:  -3.1116 | PDE Loss:  -4.9413 | Function Loss:  -3.1812\n",
            "Total loss:  -3.1119 | PDE Loss:  -4.9428 | Function Loss:  -3.1813\n",
            "Total loss:  -3.1122 | PDE Loss:  -4.9432 | Function Loss:  -3.1816\n",
            "Total loss:  -3.1125 | PDE Loss:  -4.9433 | Function Loss:  -3.1819\n",
            "Total loss:  -3.1127 | PDE Loss:  -4.9432 | Function Loss:  -3.1822\n",
            "Total loss:  -3.113 | PDE Loss:  -4.9428 | Function Loss:  -3.1825\n",
            "Total loss:  -3.1132 | PDE Loss:  -4.9429 | Function Loss:  -3.1828\n",
            "Total loss:  -3.1134 | PDE Loss:  -4.9428 | Function Loss:  -3.183\n",
            "Total loss:  -3.1136 | PDE Loss:  -4.9436 | Function Loss:  -3.1831\n",
            "Total loss:  -3.1138 | PDE Loss:  -4.9442 | Function Loss:  -3.1833\n",
            "Total loss:  -3.114 | PDE Loss:  -4.9473 | Function Loss:  -3.1829\n",
            "Total loss:  -3.1141 | PDE Loss:  -4.9472 | Function Loss:  -3.1831\n",
            "Total loss:  -3.1143 | PDE Loss:  -4.9465 | Function Loss:  -3.1834\n",
            "Total loss:  -3.1144 | PDE Loss:  -4.9465 | Function Loss:  -3.1836\n",
            "Total loss:  -3.1146 | PDE Loss:  -4.9455 | Function Loss:  -3.1839\n",
            "Total loss:  -3.1147 | PDE Loss:  -4.9455 | Function Loss:  -3.1841\n",
            "Total loss:  -3.1149 | PDE Loss:  -4.9455 | Function Loss:  -3.1843\n",
            "Total loss:  -3.115 | PDE Loss:  -4.946 | Function Loss:  -3.1844\n",
            "Total loss:  -3.1152 | PDE Loss:  -4.9455 | Function Loss:  -3.1847\n",
            "Total loss:  -3.1153 | PDE Loss:  -4.9459 | Function Loss:  -3.1847\n",
            "Total loss:  -3.1154 | PDE Loss:  -4.9465 | Function Loss:  -3.1847\n",
            "Total loss:  -3.1155 | PDE Loss:  -4.947 | Function Loss:  -3.1848\n",
            "Total loss:  -3.1156 | PDE Loss:  -4.9478 | Function Loss:  -3.1848\n",
            "Total loss:  -3.1157 | PDE Loss:  -4.9483 | Function Loss:  -3.1848\n",
            "Total loss:  -3.1158 | PDE Loss:  -4.9492 | Function Loss:  -3.1847\n",
            "Total loss:  -3.1159 | PDE Loss:  -4.9503 | Function Loss:  -3.1847\n",
            "Total loss:  -3.116 | PDE Loss:  -4.9508 | Function Loss:  -3.1847\n",
            "Total loss:  -3.1162 | PDE Loss:  -4.954 | Function Loss:  -3.1844\n",
            "Total loss:  -3.1163 | PDE Loss:  -4.9541 | Function Loss:  -3.1845\n",
            "Total loss:  -3.1166 | PDE Loss:  -4.9543 | Function Loss:  -3.1848\n",
            "Total loss:  -3.1169 | PDE Loss:  -4.9542 | Function Loss:  -3.1852\n",
            "Total loss:  -3.1172 | PDE Loss:  -4.9545 | Function Loss:  -3.1855\n",
            "Total loss:  -3.1175 | PDE Loss:  -4.9544 | Function Loss:  -3.1859\n",
            "Total loss:  -3.1179 | PDE Loss:  -4.9556 | Function Loss:  -3.1861\n",
            "Total loss:  -3.1182 | PDE Loss:  -4.9559 | Function Loss:  -3.1863\n",
            "Total loss:  -3.1184 | PDE Loss:  -4.9572 | Function Loss:  -3.1864\n",
            "Total loss:  -3.1186 | PDE Loss:  -4.9582 | Function Loss:  -3.1865\n",
            "Total loss:  -3.1188 | PDE Loss:  -4.9595 | Function Loss:  -3.1865\n",
            "Total loss:  -3.119 | PDE Loss:  -4.9599 | Function Loss:  -3.1867\n",
            "Total loss:  -3.1192 | PDE Loss:  -4.9606 | Function Loss:  -3.1867\n",
            "Total loss:  -3.1194 | PDE Loss:  -4.9608 | Function Loss:  -3.187\n",
            "Total loss:  -3.1197 | PDE Loss:  -4.9614 | Function Loss:  -3.1872\n",
            "Total loss:  -3.12 | PDE Loss:  -4.9616 | Function Loss:  -3.1876\n",
            "Total loss:  -3.1204 | PDE Loss:  -4.9619 | Function Loss:  -3.1879\n",
            "Total loss:  -3.1207 | PDE Loss:  -4.9618 | Function Loss:  -3.1883\n",
            "Total loss:  -3.1209 | PDE Loss:  -4.962 | Function Loss:  -3.1885\n",
            "Total loss:  -3.1212 | PDE Loss:  -4.9618 | Function Loss:  -3.1889\n",
            "Total loss:  -3.1215 | PDE Loss:  -4.9622 | Function Loss:  -3.1892\n",
            "Total loss:  -3.1219 | PDE Loss:  -4.9631 | Function Loss:  -3.1895\n",
            "Total loss:  -3.1222 | PDE Loss:  -4.9632 | Function Loss:  -3.1898\n",
            "Total loss:  -3.1226 | PDE Loss:  -4.9642 | Function Loss:  -3.1901\n",
            "Total loss:  -3.123 | PDE Loss:  -4.9638 | Function Loss:  -3.1906\n",
            "Total loss:  -3.1233 | PDE Loss:  -4.9646 | Function Loss:  -3.1908\n",
            "Total loss:  -3.1235 | PDE Loss:  -4.9645 | Function Loss:  -3.1911\n",
            "Total loss:  -3.1238 | PDE Loss:  -4.9647 | Function Loss:  -3.1914\n",
            "Total loss:  -3.1241 | PDE Loss:  -4.9651 | Function Loss:  -3.1917\n",
            "Total loss:  -3.1244 | PDE Loss:  -4.9648 | Function Loss:  -3.1921\n",
            "Total loss:  -3.1247 | PDE Loss:  -4.9648 | Function Loss:  -3.1925\n",
            "Total loss:  -3.1251 | PDE Loss:  -4.9652 | Function Loss:  -3.193\n",
            "Total loss:  -3.1256 | PDE Loss:  -4.9658 | Function Loss:  -3.1934\n",
            "Total loss:  -3.126 | PDE Loss:  -4.9668 | Function Loss:  -3.1937\n",
            "Total loss:  -3.1265 | PDE Loss:  -4.9678 | Function Loss:  -3.1941\n",
            "Total loss:  -3.1269 | PDE Loss:  -4.9696 | Function Loss:  -3.1943\n",
            "Total loss:  -3.1274 | PDE Loss:  -4.9703 | Function Loss:  -3.1947\n",
            "Total loss:  -3.1278 | PDE Loss:  -4.9722 | Function Loss:  -3.1949\n",
            "Total loss:  -3.1281 | PDE Loss:  -4.9727 | Function Loss:  -3.1952\n",
            "Total loss:  -3.1284 | PDE Loss:  -4.9739 | Function Loss:  -3.1953\n",
            "Total loss:  -3.1286 | PDE Loss:  -4.9736 | Function Loss:  -3.1956\n",
            "Total loss:  -3.1287 | PDE Loss:  -4.9742 | Function Loss:  -3.1956\n",
            "Total loss:  -3.1288 | PDE Loss:  -4.9743 | Function Loss:  -3.1957\n",
            "Total loss:  -3.129 | PDE Loss:  -4.9742 | Function Loss:  -3.1959\n",
            "Total loss:  -3.1291 | PDE Loss:  -4.9737 | Function Loss:  -3.1961\n",
            "Total loss:  -3.1292 | PDE Loss:  -4.9738 | Function Loss:  -3.1963\n",
            "Total loss:  -3.1294 | PDE Loss:  -4.9708 | Function Loss:  -3.197\n",
            "Total loss:  -3.1295 | PDE Loss:  -4.9703 | Function Loss:  -3.1972\n",
            "Total loss:  -3.1297 | PDE Loss:  -4.9695 | Function Loss:  -3.1975\n",
            "Total loss:  -3.1299 | PDE Loss:  -4.9691 | Function Loss:  -3.1979\n",
            "Total loss:  -3.1301 | PDE Loss:  -4.9682 | Function Loss:  -3.1983\n",
            "Total loss:  -3.1303 | PDE Loss:  -4.9676 | Function Loss:  -3.1986\n",
            "Total loss:  -3.1305 | PDE Loss:  -4.9674 | Function Loss:  -3.1988\n",
            "Total loss:  -3.1307 | PDE Loss:  -4.9672 | Function Loss:  -3.1991\n",
            "Total loss:  -3.1309 | PDE Loss:  -4.9668 | Function Loss:  -3.1994\n",
            "Total loss:  -3.131 | PDE Loss:  -4.9666 | Function Loss:  -3.1996\n",
            "Total loss:  -3.1312 | PDE Loss:  -4.9666 | Function Loss:  -3.1998\n",
            "Total loss:  -3.1313 | PDE Loss:  -4.9667 | Function Loss:  -3.1999\n",
            "Total loss:  -3.1315 | PDE Loss:  -4.9665 | Function Loss:  -3.2002\n",
            "Total loss:  -3.1316 | PDE Loss:  -4.9664 | Function Loss:  -3.2003\n",
            "Total loss:  -3.1318 | PDE Loss:  -4.9654 | Function Loss:  -3.2007\n",
            "Total loss:  -3.1319 | PDE Loss:  -4.9653 | Function Loss:  -3.2009\n",
            "Total loss:  -3.132 | PDE Loss:  -4.964 | Function Loss:  -3.2012\n",
            "Total loss:  -3.1321 | PDE Loss:  -4.9635 | Function Loss:  -3.2014\n",
            "Total loss:  -3.1322 | PDE Loss:  -4.9626 | Function Loss:  -3.2017\n",
            "Total loss:  -3.1324 | PDE Loss:  -4.961 | Function Loss:  -3.2021\n",
            "Total loss:  -3.1325 | PDE Loss:  -4.9592 | Function Loss:  -3.2026\n",
            "Total loss:  -3.1326 | PDE Loss:  -4.9568 | Function Loss:  -3.2032\n",
            "Total loss:  -3.1328 | PDE Loss:  -4.9561 | Function Loss:  -3.2035\n",
            "Total loss:  -3.1329 | PDE Loss:  -4.9556 | Function Loss:  -3.2037\n",
            "Total loss:  -3.133 | PDE Loss:  -4.9559 | Function Loss:  -3.2038\n",
            "Total loss:  -3.1331 | PDE Loss:  -4.9566 | Function Loss:  -3.2038\n",
            "Total loss:  -3.1332 | PDE Loss:  -4.9575 | Function Loss:  -3.2037\n",
            "Total loss:  -3.1333 | PDE Loss:  -4.9589 | Function Loss:  -3.2036\n",
            "Total loss:  -3.1334 | PDE Loss:  -4.9609 | Function Loss:  -3.2034\n",
            "Total loss:  -3.1336 | PDE Loss:  -4.9631 | Function Loss:  -3.2032\n",
            "Total loss:  -3.1338 | PDE Loss:  -4.9647 | Function Loss:  -3.2031\n",
            "Total loss:  -3.1339 | PDE Loss:  -4.9681 | Function Loss:  -3.2027\n",
            "Total loss:  -3.1342 | PDE Loss:  -4.9694 | Function Loss:  -3.2028\n",
            "Total loss:  -3.1343 | PDE Loss:  -4.9694 | Function Loss:  -3.2029\n",
            "Total loss:  -3.1345 | PDE Loss:  -4.9688 | Function Loss:  -3.2033\n",
            "Total loss:  -3.1348 | PDE Loss:  -4.9681 | Function Loss:  -3.2037\n",
            "Total loss:  -3.135 | PDE Loss:  -4.9669 | Function Loss:  -3.2041\n",
            "Total loss:  -3.1351 | PDE Loss:  -4.9662 | Function Loss:  -3.2044\n",
            "Total loss:  -3.1352 | PDE Loss:  -4.9654 | Function Loss:  -3.2047\n",
            "Total loss:  -3.1354 | PDE Loss:  -4.965 | Function Loss:  -3.2049\n",
            "Total loss:  -3.1356 | PDE Loss:  -4.9645 | Function Loss:  -3.2053\n",
            "Total loss:  -3.1357 | PDE Loss:  -4.9621 | Function Loss:  -3.2059\n",
            "Total loss:  -3.1359 | PDE Loss:  -4.9631 | Function Loss:  -3.206\n",
            "Total loss:  -3.1361 | PDE Loss:  -4.9641 | Function Loss:  -3.206\n",
            "Total loss:  -3.1363 | PDE Loss:  -4.9655 | Function Loss:  -3.206\n",
            "Total loss:  -3.1365 | PDE Loss:  -4.9665 | Function Loss:  -3.206\n",
            "Total loss:  -3.1366 | PDE Loss:  -4.9675 | Function Loss:  -3.2059\n",
            "Total loss:  -3.1367 | PDE Loss:  -4.9682 | Function Loss:  -3.2059\n",
            "Total loss:  -3.1368 | PDE Loss:  -4.9691 | Function Loss:  -3.206\n",
            "Total loss:  -3.137 | PDE Loss:  -4.9704 | Function Loss:  -3.206\n",
            "Total loss:  -3.1373 | PDE Loss:  -4.9702 | Function Loss:  -3.2063\n",
            "Total loss:  -3.1375 | PDE Loss:  -4.9716 | Function Loss:  -3.2063\n",
            "Total loss:  -3.1377 | PDE Loss:  -4.9717 | Function Loss:  -3.2066\n",
            "Total loss:  -3.1379 | PDE Loss:  -4.9712 | Function Loss:  -3.2069\n",
            "Total loss:  -3.1381 | PDE Loss:  -4.9708 | Function Loss:  -3.2072\n",
            "Total loss:  -3.1383 | PDE Loss:  -4.9706 | Function Loss:  -3.2074\n",
            "Total loss:  -3.1384 | PDE Loss:  -4.9702 | Function Loss:  -3.2076\n",
            "Total loss:  -3.1385 | PDE Loss:  -4.9703 | Function Loss:  -3.2078\n",
            "Total loss:  -3.1387 | PDE Loss:  -4.9701 | Function Loss:  -3.208\n",
            "Total loss:  -3.139 | PDE Loss:  -4.971 | Function Loss:  -3.2082\n",
            "Total loss:  -3.1392 | PDE Loss:  -4.9712 | Function Loss:  -3.2084\n",
            "Total loss:  -3.1395 | PDE Loss:  -4.9718 | Function Loss:  -3.2086\n",
            "Total loss:  -3.1398 | PDE Loss:  -4.9721 | Function Loss:  -3.2089\n",
            "Total loss:  -3.1401 | PDE Loss:  -4.9729 | Function Loss:  -3.2092\n",
            "Total loss:  -3.1404 | PDE Loss:  -4.9733 | Function Loss:  -3.2095\n",
            "Total loss:  -3.1407 | PDE Loss:  -4.9733 | Function Loss:  -3.2098\n",
            "Total loss:  -3.1411 | PDE Loss:  -4.9738 | Function Loss:  -3.2102\n",
            "Total loss:  -3.1414 | PDE Loss:  -4.9727 | Function Loss:  -3.2107\n",
            "Total loss:  -3.1417 | PDE Loss:  -4.9731 | Function Loss:  -3.2109\n",
            "Total loss:  -3.142 | PDE Loss:  -4.973 | Function Loss:  -3.2113\n",
            "Total loss:  -3.1424 | PDE Loss:  -4.9734 | Function Loss:  -3.2117\n",
            "Total loss:  -3.1426 | PDE Loss:  -4.9731 | Function Loss:  -3.2121\n",
            "Total loss:  -3.143 | PDE Loss:  -4.9733 | Function Loss:  -3.2125\n",
            "Total loss:  -3.1435 | PDE Loss:  -4.9739 | Function Loss:  -3.2129\n",
            "Total loss:  -3.1441 | PDE Loss:  -4.9742 | Function Loss:  -3.2136\n",
            "Total loss:  -3.1446 | PDE Loss:  -4.9761 | Function Loss:  -3.2139\n",
            "Total loss:  -3.1451 | PDE Loss:  -4.9735 | Function Loss:  -3.2149\n",
            "Total loss:  -3.1454 | PDE Loss:  -4.9733 | Function Loss:  -3.2153\n",
            "Total loss:  -3.1458 | PDE Loss:  -4.9757 | Function Loss:  -3.2153\n",
            "Total loss:  -3.1462 | PDE Loss:  -4.9773 | Function Loss:  -3.2155\n",
            "Total loss:  -3.1466 | PDE Loss:  -4.978 | Function Loss:  -3.2159\n",
            "Total loss:  -3.1469 | PDE Loss:  -4.9785 | Function Loss:  -3.2162\n",
            "Total loss:  -3.1474 | PDE Loss:  -4.979 | Function Loss:  -3.2166\n",
            "Total loss:  -3.1478 | PDE Loss:  -4.9789 | Function Loss:  -3.2171\n",
            "Total loss:  -3.1484 | PDE Loss:  -4.979 | Function Loss:  -3.2178\n",
            "Total loss:  -3.1481 | PDE Loss:  -4.9774 | Function Loss:  -3.2177\n",
            "Total loss:  -3.1489 | PDE Loss:  -4.98 | Function Loss:  -3.2182\n",
            "Total loss:  -3.1497 | PDE Loss:  -4.981 | Function Loss:  -3.219\n",
            "Total loss:  -3.1505 | PDE Loss:  -4.981 | Function Loss:  -3.2199\n",
            "Total loss:  -3.1511 | PDE Loss:  -4.982 | Function Loss:  -3.2205\n",
            "Total loss:  -3.1517 | PDE Loss:  -4.9815 | Function Loss:  -3.2213\n",
            "Total loss:  -3.1522 | PDE Loss:  -4.9834 | Function Loss:  -3.2215\n",
            "Total loss:  -3.1526 | PDE Loss:  -4.9854 | Function Loss:  -3.2217\n",
            "Total loss:  -3.1534 | PDE Loss:  -4.9884 | Function Loss:  -3.222\n",
            "Total loss:  -3.1542 | PDE Loss:  -4.9932 | Function Loss:  -3.2222\n",
            "Total loss:  -3.1549 | PDE Loss:  -4.994 | Function Loss:  -3.2229\n",
            "Total loss:  -3.1556 | PDE Loss:  -4.9984 | Function Loss:  -3.2229\n",
            "Total loss:  -3.1561 | PDE Loss:  -5.0005 | Function Loss:  -3.2231\n",
            "Total loss:  -3.1566 | PDE Loss:  -5.0029 | Function Loss:  -3.2233\n",
            "Total loss:  -3.1569 | PDE Loss:  -5.0027 | Function Loss:  -3.2238\n",
            "Total loss:  -3.1573 | PDE Loss:  -5.0028 | Function Loss:  -3.2242\n",
            "Total loss:  -3.1578 | PDE Loss:  -5.003 | Function Loss:  -3.2247\n",
            "Total loss:  -3.1583 | PDE Loss:  -5.0031 | Function Loss:  -3.2253\n",
            "Total loss:  -3.1587 | PDE Loss:  -5.0047 | Function Loss:  -3.2255\n",
            "Total loss:  -3.1592 | PDE Loss:  -5.0064 | Function Loss:  -3.2258\n",
            "Total loss:  -3.1597 | PDE Loss:  -5.0089 | Function Loss:  -3.2259\n",
            "Total loss:  -3.1603 | PDE Loss:  -5.0119 | Function Loss:  -3.2262\n",
            "Total loss:  -3.1609 | PDE Loss:  -5.0142 | Function Loss:  -3.2265\n",
            "Total loss:  -3.1615 | PDE Loss:  -5.0154 | Function Loss:  -3.227\n",
            "Total loss:  -3.162 | PDE Loss:  -5.0155 | Function Loss:  -3.2275\n",
            "Total loss:  -3.1623 | PDE Loss:  -5.0153 | Function Loss:  -3.228\n",
            "Total loss:  -3.1626 | PDE Loss:  -5.0136 | Function Loss:  -3.2286\n",
            "Total loss:  -3.1628 | PDE Loss:  -5.0126 | Function Loss:  -3.229\n",
            "Total loss:  -3.163 | PDE Loss:  -5.0117 | Function Loss:  -3.2294\n",
            "Total loss:  -3.1633 | PDE Loss:  -5.0105 | Function Loss:  -3.2299\n",
            "Total loss:  -3.1635 | PDE Loss:  -5.0096 | Function Loss:  -3.2303\n",
            "Total loss:  -3.164 | PDE Loss:  -5.0073 | Function Loss:  -3.2312\n",
            "Total loss:  -3.1643 | PDE Loss:  -5.0066 | Function Loss:  -3.2318\n",
            "Total loss:  -3.1648 | PDE Loss:  -5.0061 | Function Loss:  -3.2324\n",
            "Total loss:  -3.1655 | PDE Loss:  -5.0071 | Function Loss:  -3.233\n",
            "Total loss:  -3.166 | PDE Loss:  -5.0036 | Function Loss:  -3.2342\n",
            "Total loss:  -3.1665 | PDE Loss:  -5.0057 | Function Loss:  -3.2345\n",
            "Total loss:  -3.167 | PDE Loss:  -5.0073 | Function Loss:  -3.2348\n",
            "Total loss:  -3.1674 | PDE Loss:  -5.009 | Function Loss:  -3.235\n",
            "Total loss:  -3.1677 | PDE Loss:  -5.0089 | Function Loss:  -3.2353\n",
            "Total loss:  -3.1679 | PDE Loss:  -5.0085 | Function Loss:  -3.2356\n",
            "Total loss:  -3.1682 | PDE Loss:  -5.0112 | Function Loss:  -3.2355\n",
            "Total loss:  -3.1686 | PDE Loss:  -5.0118 | Function Loss:  -3.2359\n",
            "Total loss:  -3.1689 | PDE Loss:  -5.0113 | Function Loss:  -3.2363\n",
            "Total loss:  -3.1693 | PDE Loss:  -5.0109 | Function Loss:  -3.2369\n",
            "Total loss:  -3.1697 | PDE Loss:  -5.0103 | Function Loss:  -3.2374\n",
            "Total loss:  -3.1701 | PDE Loss:  -5.0085 | Function Loss:  -3.2382\n",
            "Total loss:  -3.1705 | PDE Loss:  -5.0081 | Function Loss:  -3.2387\n",
            "Total loss:  -3.1707 | PDE Loss:  -5.0078 | Function Loss:  -3.239\n",
            "Total loss:  -3.171 | PDE Loss:  -5.0074 | Function Loss:  -3.2394\n",
            "Total loss:  -3.1714 | PDE Loss:  -5.0072 | Function Loss:  -3.2399\n",
            "Total loss:  -3.1717 | PDE Loss:  -5.0069 | Function Loss:  -3.2403\n",
            "Total loss:  -3.172 | PDE Loss:  -5.0062 | Function Loss:  -3.2408\n",
            "Total loss:  -3.1724 | PDE Loss:  -5.0057 | Function Loss:  -3.2413\n",
            "Total loss:  -3.1727 | PDE Loss:  -5.0031 | Function Loss:  -3.2422\n",
            "Total loss:  -3.1731 | PDE Loss:  -5.0001 | Function Loss:  -3.2432\n",
            "Total loss:  -3.1734 | PDE Loss:  -5.0005 | Function Loss:  -3.2434\n",
            "Total loss:  -3.1739 | PDE Loss:  -5.0003 | Function Loss:  -3.244\n",
            "Total loss:  -3.1743 | PDE Loss:  -5.0008 | Function Loss:  -3.2445\n",
            "Total loss:  -3.1746 | PDE Loss:  -5.0002 | Function Loss:  -3.2449\n",
            "Total loss:  -3.1749 | PDE Loss:  -4.9998 | Function Loss:  -3.2453\n",
            "Total loss:  -3.1751 | PDE Loss:  -4.9991 | Function Loss:  -3.2457\n",
            "Total loss:  -3.1753 | PDE Loss:  -4.9982 | Function Loss:  -3.2461\n",
            "Total loss:  -3.1756 | PDE Loss:  -4.997 | Function Loss:  -3.2466\n",
            "Total loss:  -3.1757 | PDE Loss:  -4.9966 | Function Loss:  -3.2468\n",
            "Total loss:  -3.1759 | PDE Loss:  -4.9961 | Function Loss:  -3.2471\n",
            "Total loss:  -3.1761 | PDE Loss:  -4.9958 | Function Loss:  -3.2475\n",
            "Total loss:  -3.1764 | PDE Loss:  -4.9943 | Function Loss:  -3.248\n",
            "Total loss:  -3.1766 | PDE Loss:  -4.9948 | Function Loss:  -3.2482\n",
            "Total loss:  -3.1769 | PDE Loss:  -4.9937 | Function Loss:  -3.2487\n",
            "Total loss:  -3.1771 | PDE Loss:  -4.9928 | Function Loss:  -3.2492\n",
            "Total loss:  -3.1775 | PDE Loss:  -4.9919 | Function Loss:  -3.2498\n",
            "Total loss:  -3.1778 | PDE Loss:  -4.9911 | Function Loss:  -3.2503\n",
            "Total loss:  -3.178 | PDE Loss:  -4.9899 | Function Loss:  -3.2508\n",
            "Total loss:  -3.1783 | PDE Loss:  -4.9891 | Function Loss:  -3.2513\n",
            "Total loss:  -3.1786 | PDE Loss:  -4.9877 | Function Loss:  -3.2518\n",
            "Total loss:  -3.1789 | PDE Loss:  -4.9865 | Function Loss:  -3.2524\n",
            "Total loss:  -3.1791 | PDE Loss:  -4.9854 | Function Loss:  -3.2528\n",
            "Total loss:  -3.1793 | PDE Loss:  -4.9847 | Function Loss:  -3.2533\n",
            "Total loss:  -3.1796 | PDE Loss:  -4.9841 | Function Loss:  -3.2538\n",
            "Total loss:  -3.18 | PDE Loss:  -4.9835 | Function Loss:  -3.2543\n",
            "Total loss:  -3.1804 | PDE Loss:  -4.9844 | Function Loss:  -3.2546\n",
            "Total loss:  -3.1807 | PDE Loss:  -4.9831 | Function Loss:  -3.2552\n",
            "Total loss:  -3.181 | PDE Loss:  -4.9833 | Function Loss:  -3.2555\n",
            "Total loss:  -3.1813 | PDE Loss:  -4.9831 | Function Loss:  -3.2559\n",
            "Total loss:  -3.1815 | PDE Loss:  -4.9832 | Function Loss:  -3.2561\n",
            "Total loss:  -3.1817 | PDE Loss:  -4.983 | Function Loss:  -3.2564\n",
            "Total loss:  -3.1819 | PDE Loss:  -4.9829 | Function Loss:  -3.2567\n",
            "Total loss:  -3.1822 | PDE Loss:  -4.9825 | Function Loss:  -3.2571\n",
            "Total loss:  -3.1824 | PDE Loss:  -4.9818 | Function Loss:  -3.2575\n",
            "Total loss:  -3.1828 | PDE Loss:  -4.9805 | Function Loss:  -3.2582\n",
            "Total loss:  -3.1832 | PDE Loss:  -4.9795 | Function Loss:  -3.2589\n",
            "Total loss:  -3.1837 | PDE Loss:  -4.9785 | Function Loss:  -3.2596\n",
            "Total loss:  -3.184 | PDE Loss:  -4.9784 | Function Loss:  -3.26\n",
            "Total loss:  -3.1843 | PDE Loss:  -4.9777 | Function Loss:  -3.2605\n",
            "Total loss:  -3.1846 | PDE Loss:  -4.9786 | Function Loss:  -3.2607\n",
            "Total loss:  -3.1849 | PDE Loss:  -4.9782 | Function Loss:  -3.2611\n",
            "Total loss:  -3.1852 | PDE Loss:  -4.9784 | Function Loss:  -3.2614\n",
            "Total loss:  -3.1857 | PDE Loss:  -4.9779 | Function Loss:  -3.2621\n",
            "Total loss:  -3.1863 | PDE Loss:  -4.9759 | Function Loss:  -3.2632\n",
            "Total loss:  -3.1868 | PDE Loss:  -4.9729 | Function Loss:  -3.2644\n",
            "Total loss:  -3.1874 | PDE Loss:  -4.97 | Function Loss:  -3.2656\n",
            "Total loss:  -3.1877 | PDE Loss:  -4.9661 | Function Loss:  -3.2669\n",
            "Total loss:  -3.1881 | PDE Loss:  -4.9634 | Function Loss:  -3.2679\n",
            "Total loss:  -3.1885 | PDE Loss:  -4.9597 | Function Loss:  -3.2691\n",
            "Total loss:  -3.1888 | PDE Loss:  -4.9573 | Function Loss:  -3.27\n",
            "Total loss:  -3.1891 | PDE Loss:  -4.9551 | Function Loss:  -3.2708\n",
            "Total loss:  -3.1894 | PDE Loss:  -4.9533 | Function Loss:  -3.2715\n",
            "Total loss:  -3.1899 | PDE Loss:  -4.9513 | Function Loss:  -3.2725\n",
            "Total loss:  -3.1903 | PDE Loss:  -4.9494 | Function Loss:  -3.2734\n",
            "Total loss:  -3.1905 | PDE Loss:  -4.9484 | Function Loss:  -3.2739\n",
            "Total loss:  -3.1912 | PDE Loss:  -4.9464 | Function Loss:  -3.2751\n",
            "Total loss:  -3.1916 | PDE Loss:  -4.9465 | Function Loss:  -3.2755\n",
            "Total loss:  -3.1922 | PDE Loss:  -4.9471 | Function Loss:  -3.2762\n",
            "Total loss:  -3.193 | PDE Loss:  -4.9479 | Function Loss:  -3.277\n",
            "Total loss:  -3.1936 | PDE Loss:  -4.9485 | Function Loss:  -3.2776\n",
            "Total loss:  -3.1941 | PDE Loss:  -4.9488 | Function Loss:  -3.2781\n",
            "Total loss:  -3.1945 | PDE Loss:  -4.9495 | Function Loss:  -3.2785\n",
            "Total loss:  -3.1949 | PDE Loss:  -4.9495 | Function Loss:  -3.2789\n",
            "Total loss:  -3.1953 | PDE Loss:  -4.9499 | Function Loss:  -3.2793\n",
            "Total loss:  -3.1957 | PDE Loss:  -4.9501 | Function Loss:  -3.2798\n",
            "Total loss:  -3.1961 | PDE Loss:  -4.9482 | Function Loss:  -3.2807\n",
            "Total loss:  -3.1964 | PDE Loss:  -4.9483 | Function Loss:  -3.281\n",
            "Total loss:  -3.1968 | PDE Loss:  -4.9468 | Function Loss:  -3.2818\n",
            "Total loss:  -3.1972 | PDE Loss:  -4.9457 | Function Loss:  -3.2825\n",
            "Total loss:  -3.1974 | PDE Loss:  -4.9455 | Function Loss:  -3.2829\n",
            "Total loss:  -3.1977 | PDE Loss:  -4.9447 | Function Loss:  -3.2834\n",
            "Total loss:  -3.198 | PDE Loss:  -4.9444 | Function Loss:  -3.2839\n",
            "Total loss:  -3.1985 | PDE Loss:  -4.9429 | Function Loss:  -3.2848\n",
            "Total loss:  -3.1989 | PDE Loss:  -4.9432 | Function Loss:  -3.2852\n",
            "Total loss:  -3.1993 | PDE Loss:  -4.9435 | Function Loss:  -3.2856\n",
            "Total loss:  -3.1996 | PDE Loss:  -4.9453 | Function Loss:  -3.2856\n",
            "Total loss:  -3.2 | PDE Loss:  -4.9466 | Function Loss:  -3.2858\n",
            "Total loss:  -3.2002 | PDE Loss:  -4.9478 | Function Loss:  -3.2857\n",
            "Total loss:  -3.2004 | PDE Loss:  -4.9478 | Function Loss:  -3.286\n",
            "Total loss:  -3.2006 | PDE Loss:  -4.9493 | Function Loss:  -3.2859\n",
            "Total loss:  -3.2008 | PDE Loss:  -4.9501 | Function Loss:  -3.286\n",
            "Total loss:  -3.2011 | PDE Loss:  -4.9508 | Function Loss:  -3.2862\n",
            "Total loss:  -3.2013 | PDE Loss:  -4.95 | Function Loss:  -3.2866\n",
            "Total loss:  -3.2015 | PDE Loss:  -4.9495 | Function Loss:  -3.287\n",
            "Total loss:  -3.2017 | PDE Loss:  -4.948 | Function Loss:  -3.2875\n",
            "Total loss:  -3.2018 | PDE Loss:  -4.9467 | Function Loss:  -3.288\n",
            "Total loss:  -3.2021 | PDE Loss:  -4.9457 | Function Loss:  -3.2885\n",
            "Total loss:  -3.2022 | PDE Loss:  -4.9447 | Function Loss:  -3.2889\n",
            "Total loss:  -3.2024 | PDE Loss:  -4.944 | Function Loss:  -3.2892\n",
            "Total loss:  -3.2026 | PDE Loss:  -4.9437 | Function Loss:  -3.2895\n",
            "Total loss:  -3.2028 | PDE Loss:  -4.9432 | Function Loss:  -3.2899\n",
            "Total loss:  -3.2029 | PDE Loss:  -4.9433 | Function Loss:  -3.2901\n",
            "Total loss:  -3.2031 | PDE Loss:  -4.9438 | Function Loss:  -3.2902\n",
            "Total loss:  -3.2033 | PDE Loss:  -4.9439 | Function Loss:  -3.2904\n",
            "Total loss:  -3.2035 | PDE Loss:  -4.9447 | Function Loss:  -3.2905\n",
            "Total loss:  -3.2037 | PDE Loss:  -4.9451 | Function Loss:  -3.2906\n",
            "Total loss:  -3.2038 | PDE Loss:  -4.9452 | Function Loss:  -3.2907\n",
            "Total loss:  -3.2039 | PDE Loss:  -4.9448 | Function Loss:  -3.291\n",
            "Total loss:  -3.2041 | PDE Loss:  -4.9448 | Function Loss:  -3.2911\n",
            "Total loss:  -3.2041 | PDE Loss:  -4.9441 | Function Loss:  -3.2914\n",
            "Total loss:  -3.2043 | PDE Loss:  -4.9443 | Function Loss:  -3.2915\n",
            "Total loss:  -3.2044 | PDE Loss:  -4.9439 | Function Loss:  -3.2918\n",
            "Total loss:  -3.2046 | PDE Loss:  -4.9444 | Function Loss:  -3.2918\n",
            "Total loss:  -3.2048 | PDE Loss:  -4.9446 | Function Loss:  -3.2921\n",
            "Total loss:  -3.205 | PDE Loss:  -4.9455 | Function Loss:  -3.2921\n",
            "Total loss:  -3.2052 | PDE Loss:  -4.9468 | Function Loss:  -3.292\n",
            "Total loss:  -3.2055 | PDE Loss:  -4.9487 | Function Loss:  -3.292\n",
            "Total loss:  -3.2058 | PDE Loss:  -4.9493 | Function Loss:  -3.2923\n",
            "Total loss:  -3.2061 | PDE Loss:  -4.9508 | Function Loss:  -3.2923\n",
            "Total loss:  -3.2062 | PDE Loss:  -4.9514 | Function Loss:  -3.2923\n",
            "Total loss:  -3.2065 | PDE Loss:  -4.9517 | Function Loss:  -3.2926\n",
            "Total loss:  -3.2067 | PDE Loss:  -4.9519 | Function Loss:  -3.2927\n",
            "Total loss:  -3.2069 | PDE Loss:  -4.9516 | Function Loss:  -3.2931\n",
            "Total loss:  -3.2071 | PDE Loss:  -4.9519 | Function Loss:  -3.2933\n",
            "Total loss:  -3.2074 | PDE Loss:  -4.9521 | Function Loss:  -3.2936\n",
            "Total loss:  -3.2076 | PDE Loss:  -4.9529 | Function Loss:  -3.2937\n",
            "Total loss:  -3.2078 | PDE Loss:  -4.953 | Function Loss:  -3.2939\n",
            "Total loss:  -3.2083 | PDE Loss:  -4.9543 | Function Loss:  -3.2942\n",
            "Total loss:  -3.2088 | PDE Loss:  -4.9545 | Function Loss:  -3.2948\n",
            "Total loss:  -3.2093 | PDE Loss:  -4.9558 | Function Loss:  -3.2951\n",
            "Total loss:  -3.2097 | PDE Loss:  -4.9564 | Function Loss:  -3.2955\n",
            "Total loss:  -3.2102 | PDE Loss:  -4.9573 | Function Loss:  -3.2959\n",
            "Total loss:  -3.2106 | PDE Loss:  -4.9582 | Function Loss:  -3.2962\n",
            "Total loss:  -3.2109 | PDE Loss:  -4.9598 | Function Loss:  -3.2962\n",
            "Total loss:  -3.2111 | PDE Loss:  -4.96 | Function Loss:  -3.2964\n",
            "Total loss:  -3.2113 | PDE Loss:  -4.9612 | Function Loss:  -3.2964\n",
            "Total loss:  -3.2115 | PDE Loss:  -4.9622 | Function Loss:  -3.2964\n",
            "Total loss:  -3.2117 | PDE Loss:  -4.9629 | Function Loss:  -3.2964\n",
            "Total loss:  -3.2119 | PDE Loss:  -4.9643 | Function Loss:  -3.2964\n",
            "Total loss:  -3.212 | PDE Loss:  -4.9649 | Function Loss:  -3.2964\n",
            "Total loss:  -3.2122 | PDE Loss:  -4.9655 | Function Loss:  -3.2965\n",
            "Total loss:  -3.2124 | PDE Loss:  -4.9663 | Function Loss:  -3.2966\n",
            "Total loss:  -3.2127 | PDE Loss:  -4.9668 | Function Loss:  -3.2968\n",
            "Total loss:  -3.2129 | PDE Loss:  -4.9673 | Function Loss:  -3.2969\n",
            "Total loss:  -3.213 | PDE Loss:  -4.9675 | Function Loss:  -3.2971\n",
            "Total loss:  -3.2132 | PDE Loss:  -4.9678 | Function Loss:  -3.2972\n",
            "Total loss:  -3.2134 | PDE Loss:  -4.9685 | Function Loss:  -3.2974\n",
            "Total loss:  -3.2136 | PDE Loss:  -4.9683 | Function Loss:  -3.2976\n",
            "Total loss:  -3.2142 | PDE Loss:  -4.9672 | Function Loss:  -3.2986\n",
            "Total loss:  -3.2147 | PDE Loss:  -4.9666 | Function Loss:  -3.2993\n",
            "Total loss:  -3.2149 | PDE Loss:  -4.9657 | Function Loss:  -3.2998\n",
            "Total loss:  -3.2152 | PDE Loss:  -4.9656 | Function Loss:  -3.3002\n",
            "Total loss:  -3.2156 | PDE Loss:  -4.9655 | Function Loss:  -3.3007\n",
            "Total loss:  -3.2161 | PDE Loss:  -4.966 | Function Loss:  -3.3012\n",
            "Total loss:  -3.2166 | PDE Loss:  -4.9663 | Function Loss:  -3.3018\n",
            "Total loss:  -3.2172 | PDE Loss:  -4.9676 | Function Loss:  -3.3021\n",
            "Total loss:  -3.2177 | PDE Loss:  -4.9675 | Function Loss:  -3.3028\n",
            "Total loss:  -3.2183 | PDE Loss:  -4.9683 | Function Loss:  -3.3033\n",
            "Total loss:  -3.219 | PDE Loss:  -4.9699 | Function Loss:  -3.3038\n",
            "Total loss:  -3.22 | PDE Loss:  -4.969 | Function Loss:  -3.3052\n",
            "Total loss:  -3.2207 | PDE Loss:  -4.9692 | Function Loss:  -3.306\n",
            "Total loss:  -3.2213 | PDE Loss:  -4.9675 | Function Loss:  -3.3072\n",
            "Total loss:  -3.2217 | PDE Loss:  -4.9651 | Function Loss:  -3.3082\n",
            "Total loss:  -3.2221 | PDE Loss:  -4.9639 | Function Loss:  -3.3089\n",
            "Total loss:  -3.2226 | PDE Loss:  -4.9623 | Function Loss:  -3.3099\n",
            "Total loss:  -3.2231 | PDE Loss:  -4.9596 | Function Loss:  -3.3111\n",
            "Total loss:  -3.2235 | PDE Loss:  -4.9583 | Function Loss:  -3.3119\n",
            "Total loss:  -3.2239 | PDE Loss:  -4.9561 | Function Loss:  -3.3129\n",
            "Total loss:  -3.2245 | PDE Loss:  -4.9546 | Function Loss:  -3.3139\n",
            "Total loss:  -3.2253 | PDE Loss:  -4.9535 | Function Loss:  -3.3152\n",
            "Total loss:  -3.2259 | PDE Loss:  -4.9515 | Function Loss:  -3.3164\n",
            "Total loss:  -3.2263 | PDE Loss:  -4.9498 | Function Loss:  -3.3173\n",
            "Total loss:  -3.2267 | PDE Loss:  -4.9484 | Function Loss:  -3.3181\n",
            "Total loss:  -3.227 | PDE Loss:  -4.9462 | Function Loss:  -3.319\n",
            "Total loss:  -3.2274 | PDE Loss:  -4.9442 | Function Loss:  -3.32\n",
            "Total loss:  -3.2279 | PDE Loss:  -4.9419 | Function Loss:  -3.3211\n",
            "Total loss:  -3.2284 | PDE Loss:  -4.9398 | Function Loss:  -3.3222\n",
            "Total loss:  -3.229 | PDE Loss:  -4.9371 | Function Loss:  -3.3237\n",
            "Total loss:  -3.2296 | PDE Loss:  -4.9353 | Function Loss:  -3.3249\n",
            "Total loss:  -3.2301 | PDE Loss:  -4.9346 | Function Loss:  -3.3256\n",
            "Total loss:  -3.2306 | PDE Loss:  -4.9347 | Function Loss:  -3.3263\n",
            "Total loss:  -3.2313 | PDE Loss:  -4.9366 | Function Loss:  -3.3266\n",
            "Total loss:  -3.2318 | PDE Loss:  -4.9377 | Function Loss:  -3.327\n",
            "Total loss:  -3.2322 | PDE Loss:  -4.9397 | Function Loss:  -3.327\n",
            "Total loss:  -3.2325 | PDE Loss:  -4.9419 | Function Loss:  -3.3269\n",
            "Total loss:  -3.2329 | PDE Loss:  -4.9433 | Function Loss:  -3.327\n",
            "Total loss:  -3.2333 | PDE Loss:  -4.9463 | Function Loss:  -3.3268\n",
            "Total loss:  -3.2337 | PDE Loss:  -4.9482 | Function Loss:  -3.3268\n",
            "Total loss:  -3.2341 | PDE Loss:  -4.9513 | Function Loss:  -3.3266\n",
            "Total loss:  -3.2346 | PDE Loss:  -4.9552 | Function Loss:  -3.3263\n",
            "Total loss:  -3.235 | PDE Loss:  -4.9574 | Function Loss:  -3.3262\n",
            "Total loss:  -3.2353 | PDE Loss:  -4.9599 | Function Loss:  -3.326\n",
            "Total loss:  -3.2357 | PDE Loss:  -4.9626 | Function Loss:  -3.3258\n",
            "Total loss:  -3.236 | PDE Loss:  -4.9656 | Function Loss:  -3.3256\n",
            "Total loss:  -3.2364 | PDE Loss:  -4.9685 | Function Loss:  -3.3254\n",
            "Total loss:  -3.2367 | PDE Loss:  -4.9708 | Function Loss:  -3.3253\n",
            "Total loss:  -3.2373 | PDE Loss:  -4.9733 | Function Loss:  -3.3254\n",
            "Total loss:  -3.238 | PDE Loss:  -4.9752 | Function Loss:  -3.3259\n",
            "Total loss:  -3.2388 | PDE Loss:  -4.9796 | Function Loss:  -3.3258\n",
            "Total loss:  -3.2393 | PDE Loss:  -4.9795 | Function Loss:  -3.3264\n",
            "Total loss:  -3.2402 | PDE Loss:  -4.9799 | Function Loss:  -3.3275\n",
            "Total loss:  -3.2409 | PDE Loss:  -4.9791 | Function Loss:  -3.3285\n",
            "Total loss:  -3.2414 | PDE Loss:  -4.979 | Function Loss:  -3.3292\n",
            "Total loss:  -3.2417 | PDE Loss:  -4.9769 | Function Loss:  -3.33\n",
            "Total loss:  -3.2423 | PDE Loss:  -4.9779 | Function Loss:  -3.3305\n",
            "Total loss:  -3.2427 | PDE Loss:  -4.979 | Function Loss:  -3.3307\n",
            "Total loss:  -3.2432 | PDE Loss:  -4.9807 | Function Loss:  -3.331\n",
            "Total loss:  -3.2438 | PDE Loss:  -4.982 | Function Loss:  -3.3314\n",
            "Total loss:  -3.2442 | PDE Loss:  -4.9831 | Function Loss:  -3.3316\n",
            "Total loss:  -3.2444 | PDE Loss:  -4.9834 | Function Loss:  -3.3318\n",
            "Total loss:  -3.2446 | PDE Loss:  -4.9842 | Function Loss:  -3.3319\n",
            "Total loss:  -3.2448 | PDE Loss:  -4.9851 | Function Loss:  -3.332\n",
            "Total loss:  -3.245 | PDE Loss:  -4.9864 | Function Loss:  -3.332\n",
            "Total loss:  -3.2453 | PDE Loss:  -4.9873 | Function Loss:  -3.332\n",
            "Total loss:  -3.2454 | PDE Loss:  -4.9881 | Function Loss:  -3.3321\n",
            "Total loss:  -3.2456 | PDE Loss:  -4.9889 | Function Loss:  -3.3321\n",
            "Total loss:  -3.2459 | PDE Loss:  -4.9899 | Function Loss:  -3.3323\n",
            "Total loss:  -3.2462 | PDE Loss:  -4.9907 | Function Loss:  -3.3324\n",
            "Total loss:  -3.2464 | PDE Loss:  -4.9915 | Function Loss:  -3.3325\n",
            "Total loss:  -3.2466 | PDE Loss:  -4.9922 | Function Loss:  -3.3326\n",
            "Total loss:  -3.2469 | PDE Loss:  -4.9929 | Function Loss:  -3.3328\n",
            "Total loss:  -3.2471 | PDE Loss:  -4.994 | Function Loss:  -3.3328\n",
            "Total loss:  -3.2473 | PDE Loss:  -4.9942 | Function Loss:  -3.333\n",
            "Total loss:  -3.2475 | PDE Loss:  -4.995 | Function Loss:  -3.3331\n",
            "Total loss:  -3.2477 | PDE Loss:  -4.9945 | Function Loss:  -3.3334\n",
            "Total loss:  -3.2478 | PDE Loss:  -4.9949 | Function Loss:  -3.3335\n",
            "Total loss:  -3.2479 | PDE Loss:  -4.9944 | Function Loss:  -3.3338\n",
            "Total loss:  -3.2481 | PDE Loss:  -4.9945 | Function Loss:  -3.3339\n",
            "Total loss:  -3.2483 | PDE Loss:  -4.9946 | Function Loss:  -3.3341\n",
            "Total loss:  -3.2485 | PDE Loss:  -4.9954 | Function Loss:  -3.3342\n",
            "Total loss:  -3.2487 | PDE Loss:  -4.9968 | Function Loss:  -3.3341\n",
            "Total loss:  -3.2488 | PDE Loss:  -4.9983 | Function Loss:  -3.3339\n",
            "Total loss:  -3.249 | PDE Loss:  -5.0005 | Function Loss:  -3.3337\n",
            "Total loss:  -3.2492 | PDE Loss:  -5.0024 | Function Loss:  -3.3335\n",
            "Total loss:  -3.2494 | PDE Loss:  -5.0047 | Function Loss:  -3.3333\n",
            "Total loss:  -3.2495 | PDE Loss:  -5.0069 | Function Loss:  -3.333\n",
            "Total loss:  -3.2496 | PDE Loss:  -5.0088 | Function Loss:  -3.3327\n",
            "Total loss:  -3.2497 | PDE Loss:  -5.0107 | Function Loss:  -3.3325\n",
            "Total loss:  -3.2499 | PDE Loss:  -5.0127 | Function Loss:  -3.3322\n",
            "Total loss:  -3.2501 | PDE Loss:  -5.0154 | Function Loss:  -3.3319\n",
            "Total loss:  -3.2504 | PDE Loss:  -5.0177 | Function Loss:  -3.3317\n",
            "Total loss:  -3.2503 | PDE Loss:  -5.0217 | Function Loss:  -3.3309\n",
            "Total loss:  -3.2505 | PDE Loss:  -5.02 | Function Loss:  -3.3315\n",
            "Total loss:  -3.2508 | PDE Loss:  -5.022 | Function Loss:  -3.3314\n",
            "Total loss:  -3.2512 | PDE Loss:  -5.0241 | Function Loss:  -3.3314\n",
            "Total loss:  -3.2515 | PDE Loss:  -5.0268 | Function Loss:  -3.3313\n",
            "Total loss:  -3.2519 | PDE Loss:  -5.0284 | Function Loss:  -3.3314\n",
            "Total loss:  -3.2522 | PDE Loss:  -5.0301 | Function Loss:  -3.3314\n",
            "Total loss:  -3.2523 | PDE Loss:  -5.0316 | Function Loss:  -3.3313\n",
            "Total loss:  -3.2525 | PDE Loss:  -5.0323 | Function Loss:  -3.3314\n",
            "Total loss:  -3.2527 | PDE Loss:  -5.0333 | Function Loss:  -3.3314\n",
            "Total loss:  -3.2529 | PDE Loss:  -5.0341 | Function Loss:  -3.3315\n",
            "Total loss:  -3.2532 | PDE Loss:  -5.035 | Function Loss:  -3.3317\n",
            "Total loss:  -3.2535 | PDE Loss:  -5.0355 | Function Loss:  -3.3319\n",
            "Total loss:  -3.2538 | PDE Loss:  -5.0354 | Function Loss:  -3.3323\n",
            "Total loss:  -3.2541 | PDE Loss:  -5.0357 | Function Loss:  -3.3326\n",
            "Total loss:  -3.2544 | PDE Loss:  -5.0349 | Function Loss:  -3.3331\n",
            "Total loss:  -3.2548 | PDE Loss:  -5.0343 | Function Loss:  -3.3337\n",
            "Total loss:  -3.2551 | PDE Loss:  -5.0341 | Function Loss:  -3.3341\n",
            "Total loss:  -3.2555 | PDE Loss:  -5.0339 | Function Loss:  -3.3346\n",
            "Total loss:  -3.2558 | PDE Loss:  -5.0338 | Function Loss:  -3.335\n",
            "Total loss:  -3.256 | PDE Loss:  -5.0346 | Function Loss:  -3.3351\n",
            "Total loss:  -3.2562 | PDE Loss:  -5.0357 | Function Loss:  -3.3351\n",
            "Total loss:  -3.2564 | PDE Loss:  -5.0377 | Function Loss:  -3.335\n",
            "Total loss:  -3.2567 | PDE Loss:  -5.0401 | Function Loss:  -3.3349\n",
            "Total loss:  -3.2571 | PDE Loss:  -5.044 | Function Loss:  -3.3346\n",
            "Total loss:  -3.2576 | PDE Loss:  -5.0466 | Function Loss:  -3.3346\n",
            "Total loss:  -3.258 | PDE Loss:  -5.0547 | Function Loss:  -3.3335\n",
            "Total loss:  -3.2583 | PDE Loss:  -5.0616 | Function Loss:  -3.3326\n",
            "Total loss:  -3.2587 | PDE Loss:  -5.0588 | Function Loss:  -3.3337\n",
            "Total loss:  -3.2592 | PDE Loss:  -5.0568 | Function Loss:  -3.3347\n",
            "Total loss:  -3.2596 | PDE Loss:  -5.0545 | Function Loss:  -3.3356\n",
            "Total loss:  -3.26 | PDE Loss:  -5.0519 | Function Loss:  -3.3365\n",
            "Total loss:  -3.2604 | PDE Loss:  -5.05 | Function Loss:  -3.3373\n",
            "Total loss:  -3.2607 | PDE Loss:  -5.0489 | Function Loss:  -3.3379\n",
            "Total loss:  -3.2611 | PDE Loss:  -5.0486 | Function Loss:  -3.3384\n",
            "Total loss:  -3.2615 | PDE Loss:  -5.049 | Function Loss:  -3.3388\n",
            "Total loss:  -3.2619 | PDE Loss:  -5.0512 | Function Loss:  -3.3389\n",
            "Total loss:  -3.2623 | PDE Loss:  -5.0538 | Function Loss:  -3.3389\n",
            "Total loss:  -3.2626 | PDE Loss:  -5.0557 | Function Loss:  -3.3388\n",
            "Total loss:  -3.2628 | PDE Loss:  -5.0579 | Function Loss:  -3.3387\n",
            "Total loss:  -3.2631 | PDE Loss:  -5.0623 | Function Loss:  -3.3382\n",
            "Total loss:  -3.2635 | PDE Loss:  -5.0666 | Function Loss:  -3.3378\n",
            "Total loss:  -3.2638 | PDE Loss:  -5.0702 | Function Loss:  -3.3375\n",
            "Total loss:  -3.264 | PDE Loss:  -5.0733 | Function Loss:  -3.3372\n",
            "Total loss:  -3.2643 | PDE Loss:  -5.0746 | Function Loss:  -3.3373\n",
            "Total loss:  -3.2645 | PDE Loss:  -5.0755 | Function Loss:  -3.3373\n",
            "Total loss:  -3.2646 | PDE Loss:  -5.0754 | Function Loss:  -3.3376\n",
            "Total loss:  -3.2649 | PDE Loss:  -5.0756 | Function Loss:  -3.3378\n",
            "Total loss:  -3.2651 | PDE Loss:  -5.0757 | Function Loss:  -3.338\n",
            "Total loss:  -3.2653 | PDE Loss:  -5.0762 | Function Loss:  -3.3382\n",
            "Total loss:  -3.2655 | PDE Loss:  -5.0766 | Function Loss:  -3.3384\n",
            "Total loss:  -3.2657 | PDE Loss:  -5.0769 | Function Loss:  -3.3386\n",
            "Total loss:  -3.2658 | PDE Loss:  -5.0775 | Function Loss:  -3.3386\n",
            "Total loss:  -3.266 | PDE Loss:  -5.0784 | Function Loss:  -3.3386\n",
            "Total loss:  -3.2661 | PDE Loss:  -5.0789 | Function Loss:  -3.3387\n",
            "Total loss:  -3.2663 | PDE Loss:  -5.0801 | Function Loss:  -3.3387\n",
            "Total loss:  -3.2664 | PDE Loss:  -5.0821 | Function Loss:  -3.3385\n",
            "Total loss:  -3.2667 | PDE Loss:  -5.0844 | Function Loss:  -3.3383\n",
            "Total loss:  -3.2669 | PDE Loss:  -5.0871 | Function Loss:  -3.3381\n",
            "Total loss:  -3.2671 | PDE Loss:  -5.0891 | Function Loss:  -3.338\n",
            "Total loss:  -3.2674 | PDE Loss:  -5.0909 | Function Loss:  -3.338\n",
            "Total loss:  -3.2663 | PDE Loss:  -5.0939 | Function Loss:  -3.3362\n",
            "Total loss:  -3.2676 | PDE Loss:  -5.0934 | Function Loss:  -3.3378\n",
            "Total loss:  -3.2679 | PDE Loss:  -5.0944 | Function Loss:  -3.338\n",
            "Total loss:  -3.2683 | PDE Loss:  -5.0946 | Function Loss:  -3.3385\n",
            "Total loss:  -3.2687 | PDE Loss:  -5.0953 | Function Loss:  -3.3388\n",
            "Total loss:  -3.269 | PDE Loss:  -5.0937 | Function Loss:  -3.3394\n",
            "Total loss:  -3.2692 | PDE Loss:  -5.0934 | Function Loss:  -3.3397\n",
            "Total loss:  -3.2694 | PDE Loss:  -5.093 | Function Loss:  -3.3401\n",
            "Total loss:  -3.2698 | PDE Loss:  -5.0943 | Function Loss:  -3.3403\n",
            "Total loss:  -3.2702 | PDE Loss:  -5.0949 | Function Loss:  -3.3406\n",
            "Total loss:  -3.2707 | PDE Loss:  -5.0967 | Function Loss:  -3.3409\n",
            "Total loss:  -3.2712 | PDE Loss:  -5.0987 | Function Loss:  -3.3412\n",
            "Total loss:  -3.2717 | PDE Loss:  -5.1007 | Function Loss:  -3.3415\n",
            "Total loss:  -3.2723 | PDE Loss:  -5.1045 | Function Loss:  -3.3415\n",
            "Total loss:  -3.2729 | PDE Loss:  -5.1018 | Function Loss:  -3.3426\n",
            "Total loss:  -3.2733 | PDE Loss:  -5.1035 | Function Loss:  -3.3428\n",
            "Total loss:  -3.2738 | PDE Loss:  -5.1047 | Function Loss:  -3.3432\n",
            "Total loss:  -3.2743 | PDE Loss:  -5.1038 | Function Loss:  -3.3439\n",
            "Total loss:  -3.2747 | PDE Loss:  -5.1036 | Function Loss:  -3.3444\n",
            "Total loss:  -3.2751 | PDE Loss:  -5.1027 | Function Loss:  -3.3451\n",
            "Total loss:  -3.2757 | PDE Loss:  -5.1017 | Function Loss:  -3.3459\n",
            "Total loss:  -3.2763 | PDE Loss:  -5.101 | Function Loss:  -3.3468\n",
            "Total loss:  -3.2769 | PDE Loss:  -5.1001 | Function Loss:  -3.3476\n",
            "Total loss:  -3.2774 | PDE Loss:  -5.0968 | Function Loss:  -3.3488\n",
            "Total loss:  -3.2778 | PDE Loss:  -5.0956 | Function Loss:  -3.3495\n",
            "Total loss:  -3.2782 | PDE Loss:  -5.0947 | Function Loss:  -3.35\n",
            "Total loss:  -3.2786 | PDE Loss:  -5.0937 | Function Loss:  -3.3508\n",
            "Total loss:  -3.2792 | PDE Loss:  -5.0933 | Function Loss:  -3.3515\n",
            "Total loss:  -3.2797 | PDE Loss:  -5.0931 | Function Loss:  -3.3522\n",
            "Total loss:  -3.2802 | PDE Loss:  -5.0922 | Function Loss:  -3.3529\n",
            "Total loss:  -3.2805 | PDE Loss:  -5.0913 | Function Loss:  -3.3535\n",
            "Total loss:  -3.2809 | PDE Loss:  -5.0898 | Function Loss:  -3.3542\n",
            "Total loss:  -3.2814 | PDE Loss:  -5.0899 | Function Loss:  -3.3548\n",
            "Total loss:  -3.2818 | PDE Loss:  -5.0873 | Function Loss:  -3.3558\n",
            "Total loss:  -3.2821 | PDE Loss:  -5.0882 | Function Loss:  -3.3559\n",
            "Total loss:  -3.2823 | PDE Loss:  -5.0887 | Function Loss:  -3.3561\n",
            "Total loss:  -3.2826 | PDE Loss:  -5.0902 | Function Loss:  -3.3561\n",
            "Total loss:  -3.2827 | PDE Loss:  -5.0914 | Function Loss:  -3.3561\n",
            "Total loss:  -3.2829 | PDE Loss:  -5.0925 | Function Loss:  -3.356\n",
            "Total loss:  -3.2831 | PDE Loss:  -5.0937 | Function Loss:  -3.356\n",
            "Total loss:  -3.2833 | PDE Loss:  -5.095 | Function Loss:  -3.3561\n",
            "Total loss:  -3.2836 | PDE Loss:  -5.0957 | Function Loss:  -3.3563\n",
            "Total loss:  -3.2838 | PDE Loss:  -5.0967 | Function Loss:  -3.3563\n",
            "Total loss:  -3.284 | PDE Loss:  -5.0966 | Function Loss:  -3.3566\n",
            "Total loss:  -3.2841 | PDE Loss:  -5.0964 | Function Loss:  -3.3568\n",
            "Total loss:  -3.2842 | PDE Loss:  -5.0945 | Function Loss:  -3.3572\n",
            "Total loss:  -3.2843 | PDE Loss:  -5.0946 | Function Loss:  -3.3574\n",
            "Total loss:  -3.2845 | PDE Loss:  -5.0947 | Function Loss:  -3.3575\n",
            "Total loss:  -3.2846 | PDE Loss:  -5.0947 | Function Loss:  -3.3577\n",
            "Total loss:  -3.2848 | PDE Loss:  -5.0949 | Function Loss:  -3.3579\n",
            "Total loss:  -3.285 | PDE Loss:  -5.0952 | Function Loss:  -3.358\n",
            "Total loss:  -3.2851 | PDE Loss:  -5.0953 | Function Loss:  -3.3582\n",
            "Total loss:  -3.2854 | PDE Loss:  -5.0956 | Function Loss:  -3.3584\n",
            "Total loss:  -3.2857 | PDE Loss:  -5.095 | Function Loss:  -3.3589\n",
            "Total loss:  -3.2859 | PDE Loss:  -5.0951 | Function Loss:  -3.3592\n",
            "Total loss:  -3.2862 | PDE Loss:  -5.0949 | Function Loss:  -3.3595\n",
            "Total loss:  -3.2864 | PDE Loss:  -5.0951 | Function Loss:  -3.3597\n",
            "Total loss:  -3.2866 | PDE Loss:  -5.0955 | Function Loss:  -3.3599\n",
            "Total loss:  -3.2868 | PDE Loss:  -5.0958 | Function Loss:  -3.3601\n",
            "Total loss:  -3.287 | PDE Loss:  -5.0961 | Function Loss:  -3.3603\n",
            "Total loss:  -3.2872 | PDE Loss:  -5.0957 | Function Loss:  -3.3605\n",
            "Total loss:  -3.2874 | PDE Loss:  -5.0958 | Function Loss:  -3.3608\n",
            "Total loss:  -3.2876 | PDE Loss:  -5.0952 | Function Loss:  -3.3611\n",
            "Total loss:  -3.2874 | PDE Loss:  -5.0912 | Function Loss:  -3.3617\n",
            "Total loss:  -3.2877 | PDE Loss:  -5.0941 | Function Loss:  -3.3614\n",
            "Total loss:  -3.2878 | PDE Loss:  -5.0944 | Function Loss:  -3.3615\n",
            "Total loss:  -3.288 | PDE Loss:  -5.0945 | Function Loss:  -3.3617\n",
            "Total loss:  -3.2881 | PDE Loss:  -5.0947 | Function Loss:  -3.3618\n",
            "Total loss:  -3.2882 | PDE Loss:  -5.0948 | Function Loss:  -3.3619\n",
            "Total loss:  -3.2882 | PDE Loss:  -5.0945 | Function Loss:  -3.362\n",
            "Total loss:  -3.2883 | PDE Loss:  -5.0949 | Function Loss:  -3.362\n",
            "Total loss:  -3.2884 | PDE Loss:  -5.0944 | Function Loss:  -3.3622\n",
            "Total loss:  -3.2885 | PDE Loss:  -5.0947 | Function Loss:  -3.3623\n",
            "Total loss:  -3.2886 | PDE Loss:  -5.094 | Function Loss:  -3.3626\n",
            "Total loss:  -3.2888 | PDE Loss:  -5.0943 | Function Loss:  -3.3627\n",
            "Total loss:  -3.2889 | PDE Loss:  -5.0938 | Function Loss:  -3.363\n",
            "Total loss:  -3.2891 | PDE Loss:  -5.0937 | Function Loss:  -3.3631\n",
            "Total loss:  -3.2892 | PDE Loss:  -5.0936 | Function Loss:  -3.3633\n",
            "Total loss:  -3.2894 | PDE Loss:  -5.0946 | Function Loss:  -3.3633\n",
            "Total loss:  -3.2896 | PDE Loss:  -5.0944 | Function Loss:  -3.3636\n",
            "Total loss:  -3.2897 | PDE Loss:  -5.0946 | Function Loss:  -3.3638\n",
            "Total loss:  -3.2897 | PDE Loss:  -5.0885 | Function Loss:  -3.3649\n",
            "Total loss:  -3.2898 | PDE Loss:  -5.092 | Function Loss:  -3.3644\n",
            "Total loss:  -3.29 | PDE Loss:  -5.0924 | Function Loss:  -3.3645\n",
            "Total loss:  -3.2901 | PDE Loss:  -5.0932 | Function Loss:  -3.3645\n",
            "Total loss:  -3.2904 | PDE Loss:  -5.0927 | Function Loss:  -3.3649\n",
            "Total loss:  -3.2905 | PDE Loss:  -5.093 | Function Loss:  -3.365\n",
            "Total loss:  -3.2907 | PDE Loss:  -5.0938 | Function Loss:  -3.365\n",
            "Total loss:  -3.2909 | PDE Loss:  -5.0943 | Function Loss:  -3.3652\n",
            "Total loss:  -3.2911 | PDE Loss:  -5.0947 | Function Loss:  -3.3653\n",
            "Total loss:  -3.2913 | PDE Loss:  -5.095 | Function Loss:  -3.3656\n",
            "Total loss:  -3.2915 | PDE Loss:  -5.0939 | Function Loss:  -3.366\n",
            "Total loss:  -3.2917 | PDE Loss:  -5.0936 | Function Loss:  -3.3663\n",
            "Total loss:  -3.2919 | PDE Loss:  -5.0932 | Function Loss:  -3.3666\n",
            "Total loss:  -3.2921 | PDE Loss:  -5.0927 | Function Loss:  -3.367\n",
            "Total loss:  -3.2917 | PDE Loss:  -5.0856 | Function Loss:  -3.3677\n",
            "Total loss:  -3.2923 | PDE Loss:  -5.0909 | Function Loss:  -3.3675\n",
            "Total loss:  -3.2925 | PDE Loss:  -5.0911 | Function Loss:  -3.3677\n",
            "Total loss:  -3.2927 | PDE Loss:  -5.0908 | Function Loss:  -3.368\n",
            "Total loss:  -3.293 | PDE Loss:  -5.0909 | Function Loss:  -3.3683\n",
            "Total loss:  -3.2932 | PDE Loss:  -5.0909 | Function Loss:  -3.3686\n",
            "Total loss:  -3.2935 | PDE Loss:  -5.0918 | Function Loss:  -3.3688\n",
            "Total loss:  -3.2937 | PDE Loss:  -5.0921 | Function Loss:  -3.369\n",
            "Total loss:  -3.2939 | PDE Loss:  -5.0937 | Function Loss:  -3.3689\n",
            "Total loss:  -3.2941 | PDE Loss:  -5.0933 | Function Loss:  -3.3692\n",
            "Total loss:  -3.2943 | PDE Loss:  -5.0946 | Function Loss:  -3.3691\n",
            "Total loss:  -3.2944 | PDE Loss:  -5.0949 | Function Loss:  -3.3693\n",
            "Total loss:  -3.2946 | PDE Loss:  -5.0956 | Function Loss:  -3.3693\n",
            "Total loss:  -3.2947 | PDE Loss:  -5.0951 | Function Loss:  -3.3696\n",
            "Total loss:  -3.2948 | PDE Loss:  -5.0948 | Function Loss:  -3.3697\n",
            "Total loss:  -3.2949 | PDE Loss:  -5.0944 | Function Loss:  -3.37\n",
            "Total loss:  -3.295 | PDE Loss:  -5.0939 | Function Loss:  -3.3701\n",
            "Total loss:  -3.2951 | PDE Loss:  -5.094 | Function Loss:  -3.3703\n",
            "Total loss:  -3.2951 | PDE Loss:  -5.094 | Function Loss:  -3.3703\n",
            "Total loss:  -3.2952 | PDE Loss:  -5.0946 | Function Loss:  -3.3703\n",
            "Total loss:  -3.2953 | PDE Loss:  -5.0956 | Function Loss:  -3.3702\n",
            "Total loss:  -3.2954 | PDE Loss:  -5.097 | Function Loss:  -3.37\n",
            "Total loss:  -3.2955 | PDE Loss:  -5.0998 | Function Loss:  -3.3696\n",
            "Total loss:  -3.2955 | PDE Loss:  -5.1004 | Function Loss:  -3.3695\n",
            "Total loss:  -3.2955 | PDE Loss:  -5.1013 | Function Loss:  -3.3694\n",
            "Total loss:  -3.2956 | PDE Loss:  -5.1017 | Function Loss:  -3.3694\n",
            "Total loss:  -3.2956 | PDE Loss:  -5.1024 | Function Loss:  -3.3693\n",
            "Total loss:  -3.2957 | PDE Loss:  -5.103 | Function Loss:  -3.3692\n",
            "Total loss:  -3.2957 | PDE Loss:  -5.104 | Function Loss:  -3.3691\n",
            "Total loss:  -3.2959 | PDE Loss:  -5.1047 | Function Loss:  -3.3692\n",
            "Total loss:  -3.2961 | PDE Loss:  -5.1065 | Function Loss:  -3.3692\n",
            "Total loss:  -3.2963 | PDE Loss:  -5.1076 | Function Loss:  -3.3691\n",
            "Total loss:  -3.2966 | PDE Loss:  -5.1086 | Function Loss:  -3.3693\n",
            "Total loss:  -3.2969 | PDE Loss:  -5.1093 | Function Loss:  -3.3695\n",
            "Total loss:  -3.2971 | PDE Loss:  -5.1102 | Function Loss:  -3.3696\n",
            "Total loss:  -3.2973 | PDE Loss:  -5.1107 | Function Loss:  -3.3697\n",
            "Total loss:  -3.2974 | PDE Loss:  -5.1111 | Function Loss:  -3.3698\n",
            "Total loss:  -3.2975 | PDE Loss:  -5.1115 | Function Loss:  -3.3698\n",
            "Total loss:  -3.2976 | PDE Loss:  -5.1119 | Function Loss:  -3.3699\n",
            "Total loss:  -3.2977 | PDE Loss:  -5.1125 | Function Loss:  -3.3699\n",
            "Total loss:  -3.298 | PDE Loss:  -5.1141 | Function Loss:  -3.3699\n",
            "Total loss:  -3.2982 | PDE Loss:  -5.1151 | Function Loss:  -3.3701\n",
            "Total loss:  -3.2984 | PDE Loss:  -5.1161 | Function Loss:  -3.3701\n",
            "Total loss:  -3.2987 | PDE Loss:  -5.1171 | Function Loss:  -3.3703\n",
            "Total loss:  -3.299 | PDE Loss:  -5.118 | Function Loss:  -3.3705\n",
            "Total loss:  -3.2991 | PDE Loss:  -5.1171 | Function Loss:  -3.3708\n",
            "Total loss:  -3.2993 | PDE Loss:  -5.117 | Function Loss:  -3.3709\n",
            "Total loss:  -3.2994 | PDE Loss:  -5.116 | Function Loss:  -3.3713\n",
            "Total loss:  -3.2995 | PDE Loss:  -5.1155 | Function Loss:  -3.3715\n",
            "Total loss:  -3.2996 | PDE Loss:  -5.1142 | Function Loss:  -3.3719\n",
            "Total loss:  -3.2998 | PDE Loss:  -5.1136 | Function Loss:  -3.3722\n",
            "Total loss:  -3.2998 | PDE Loss:  -5.1146 | Function Loss:  -3.3721\n",
            "Total loss:  -3.3 | PDE Loss:  -5.1145 | Function Loss:  -3.3722\n",
            "Total loss:  -3.3001 | PDE Loss:  -5.1149 | Function Loss:  -3.3723\n",
            "Total loss:  -3.3002 | PDE Loss:  -5.1153 | Function Loss:  -3.3724\n",
            "Total loss:  -3.3003 | PDE Loss:  -5.1157 | Function Loss:  -3.3724\n",
            "Total loss:  -3.3004 | PDE Loss:  -5.1163 | Function Loss:  -3.3724\n",
            "Total loss:  -3.3005 | PDE Loss:  -5.1172 | Function Loss:  -3.3723\n",
            "Total loss:  -3.3006 | PDE Loss:  -5.118 | Function Loss:  -3.3723\n",
            "Total loss:  -3.3007 | PDE Loss:  -5.1193 | Function Loss:  -3.3722\n",
            "Total loss:  -3.3009 | PDE Loss:  -5.1209 | Function Loss:  -3.3721\n",
            "Total loss:  -3.3011 | PDE Loss:  -5.1239 | Function Loss:  -3.3719\n",
            "Total loss:  -3.3014 | PDE Loss:  -5.1274 | Function Loss:  -3.3716\n",
            "Total loss:  -3.3017 | PDE Loss:  -5.1307 | Function Loss:  -3.3714\n",
            "Total loss:  -3.3021 | PDE Loss:  -5.1349 | Function Loss:  -3.3711\n",
            "Total loss:  -3.3021 | PDE Loss:  -5.1419 | Function Loss:  -3.3699\n",
            "Total loss:  -3.3023 | PDE Loss:  -5.139 | Function Loss:  -3.3707\n",
            "Total loss:  -3.3026 | PDE Loss:  -5.1408 | Function Loss:  -3.3707\n",
            "Total loss:  -3.3029 | PDE Loss:  -5.1443 | Function Loss:  -3.3705\n",
            "Total loss:  -3.3032 | PDE Loss:  -5.1455 | Function Loss:  -3.3706\n",
            "Total loss:  -3.3034 | PDE Loss:  -5.146 | Function Loss:  -3.3707\n",
            "Total loss:  -3.3035 | PDE Loss:  -5.1479 | Function Loss:  -3.3706\n",
            "Total loss:  -3.3038 | PDE Loss:  -5.149 | Function Loss:  -3.3707\n",
            "Total loss:  -3.3041 | PDE Loss:  -5.1515 | Function Loss:  -3.3707\n",
            "Total loss:  -3.3045 | PDE Loss:  -5.1525 | Function Loss:  -3.371\n",
            "Total loss:  -3.305 | PDE Loss:  -5.1567 | Function Loss:  -3.3709\n",
            "Total loss:  -3.3052 | PDE Loss:  -5.1569 | Function Loss:  -3.3711\n",
            "Total loss:  -3.3057 | PDE Loss:  -5.1567 | Function Loss:  -3.3717\n",
            "Total loss:  -3.3062 | PDE Loss:  -5.1562 | Function Loss:  -3.3723\n",
            "Total loss:  -3.3067 | PDE Loss:  -5.1539 | Function Loss:  -3.3733\n",
            "Total loss:  -3.307 | PDE Loss:  -5.1533 | Function Loss:  -3.3737\n",
            "Total loss:  -3.3075 | PDE Loss:  -5.152 | Function Loss:  -3.3745\n",
            "Total loss:  -3.3081 | PDE Loss:  -5.1536 | Function Loss:  -3.375\n",
            "Total loss:  -3.3087 | PDE Loss:  -5.1557 | Function Loss:  -3.3753\n",
            "Total loss:  -3.3091 | PDE Loss:  -5.1603 | Function Loss:  -3.375\n",
            "Total loss:  -3.3093 | PDE Loss:  -5.1618 | Function Loss:  -3.375\n",
            "Total loss:  -3.3095 | PDE Loss:  -5.1643 | Function Loss:  -3.3749\n",
            "Total loss:  -3.3098 | PDE Loss:  -5.1671 | Function Loss:  -3.3748\n",
            "Total loss:  -3.3101 | PDE Loss:  -5.17 | Function Loss:  -3.3747\n",
            "Total loss:  -3.3103 | PDE Loss:  -5.1717 | Function Loss:  -3.3746\n",
            "Total loss:  -3.3107 | PDE Loss:  -5.1747 | Function Loss:  -3.3746\n",
            "Total loss:  -3.3111 | PDE Loss:  -5.1766 | Function Loss:  -3.3748\n",
            "Total loss:  -3.3115 | PDE Loss:  -5.1788 | Function Loss:  -3.3748\n",
            "Total loss:  -3.3118 | PDE Loss:  -5.1802 | Function Loss:  -3.375\n",
            "Total loss:  -3.3121 | PDE Loss:  -5.1831 | Function Loss:  -3.3749\n",
            "Total loss:  -3.3124 | PDE Loss:  -5.1855 | Function Loss:  -3.3748\n",
            "Total loss:  -3.3127 | PDE Loss:  -5.1883 | Function Loss:  -3.3747\n",
            "Total loss:  -3.313 | PDE Loss:  -5.1904 | Function Loss:  -3.3748\n",
            "Total loss:  -3.3134 | PDE Loss:  -5.1943 | Function Loss:  -3.3746\n",
            "Total loss:  -3.3135 | PDE Loss:  -5.1954 | Function Loss:  -3.3746\n",
            "Total loss:  -3.3138 | PDE Loss:  -5.1971 | Function Loss:  -3.3747\n",
            "Total loss:  -3.314 | PDE Loss:  -5.1971 | Function Loss:  -3.3749\n",
            "Total loss:  -3.3141 | PDE Loss:  -5.1979 | Function Loss:  -3.375\n",
            "Total loss:  -3.3142 | PDE Loss:  -5.1974 | Function Loss:  -3.3752\n",
            "Total loss:  -3.3144 | PDE Loss:  -5.1976 | Function Loss:  -3.3753\n",
            "Total loss:  -3.3145 | PDE Loss:  -5.1971 | Function Loss:  -3.3755\n",
            "Total loss:  -3.3147 | PDE Loss:  -5.1966 | Function Loss:  -3.3758\n",
            "Total loss:  -3.3149 | PDE Loss:  -5.1956 | Function Loss:  -3.3762\n",
            "Total loss:  -3.3149 | PDE Loss:  -5.1967 | Function Loss:  -3.3761\n",
            "Total loss:  -3.3151 | PDE Loss:  -5.1965 | Function Loss:  -3.3762\n",
            "Total loss:  -3.3152 | PDE Loss:  -5.1967 | Function Loss:  -3.3764\n",
            "Total loss:  -3.3154 | PDE Loss:  -5.1986 | Function Loss:  -3.3763\n",
            "Total loss:  -3.3156 | PDE Loss:  -5.1996 | Function Loss:  -3.3764\n",
            "Total loss:  -3.3158 | PDE Loss:  -5.2017 | Function Loss:  -3.3763\n",
            "Total loss:  -3.316 | PDE Loss:  -5.2036 | Function Loss:  -3.3763\n",
            "Total loss:  -3.3163 | PDE Loss:  -5.2054 | Function Loss:  -3.3763\n",
            "Total loss:  -3.3166 | PDE Loss:  -5.2085 | Function Loss:  -3.3763\n",
            "Total loss:  -3.317 | PDE Loss:  -5.2101 | Function Loss:  -3.3764\n",
            "Total loss:  -3.3173 | PDE Loss:  -5.2116 | Function Loss:  -3.3766\n",
            "Total loss:  -3.3176 | PDE Loss:  -5.2124 | Function Loss:  -3.3768\n",
            "Total loss:  -3.318 | PDE Loss:  -5.2127 | Function Loss:  -3.3772\n",
            "Total loss:  -3.3183 | PDE Loss:  -5.2121 | Function Loss:  -3.3776\n",
            "Total loss:  -3.3186 | PDE Loss:  -5.2111 | Function Loss:  -3.3781\n",
            "Total loss:  -3.3188 | PDE Loss:  -5.2095 | Function Loss:  -3.3786\n",
            "Total loss:  -3.319 | PDE Loss:  -5.2078 | Function Loss:  -3.3791\n",
            "Total loss:  -3.3193 | PDE Loss:  -5.2055 | Function Loss:  -3.3798\n",
            "Total loss:  -3.3197 | PDE Loss:  -5.2025 | Function Loss:  -3.3807\n",
            "Total loss:  -3.3199 | PDE Loss:  -5.2012 | Function Loss:  -3.3811\n",
            "Total loss:  -3.3202 | PDE Loss:  -5.1994 | Function Loss:  -3.3817\n",
            "Total loss:  -3.3205 | PDE Loss:  -5.1977 | Function Loss:  -3.3824\n",
            "Total loss:  -3.3209 | PDE Loss:  -5.1966 | Function Loss:  -3.383\n",
            "Total loss:  -3.3212 | PDE Loss:  -5.1936 | Function Loss:  -3.3838\n",
            "Total loss:  -3.3215 | PDE Loss:  -5.195 | Function Loss:  -3.3839\n",
            "Total loss:  -3.3219 | PDE Loss:  -5.1966 | Function Loss:  -3.3841\n",
            "Total loss:  -3.3223 | PDE Loss:  -5.2 | Function Loss:  -3.3841\n",
            "Total loss:  -3.3228 | PDE Loss:  -5.2036 | Function Loss:  -3.384\n",
            "Total loss:  -3.3231 | PDE Loss:  -5.2073 | Function Loss:  -3.3839\n",
            "Total loss:  -3.3235 | PDE Loss:  -5.2107 | Function Loss:  -3.3838\n",
            "Total loss:  -3.3238 | PDE Loss:  -5.2133 | Function Loss:  -3.3837\n",
            "Total loss:  -3.3242 | PDE Loss:  -5.217 | Function Loss:  -3.3837\n",
            "Total loss:  -3.3246 | PDE Loss:  -5.2202 | Function Loss:  -3.3836\n",
            "Total loss:  -3.3249 | PDE Loss:  -5.2213 | Function Loss:  -3.3838\n",
            "Total loss:  -3.3251 | PDE Loss:  -5.2226 | Function Loss:  -3.3839\n",
            "Total loss:  -3.3253 | PDE Loss:  -5.2243 | Function Loss:  -3.3839\n",
            "Total loss:  -3.3254 | PDE Loss:  -5.2285 | Function Loss:  -3.3834\n",
            "Total loss:  -3.3257 | PDE Loss:  -5.2278 | Function Loss:  -3.3838\n",
            "Total loss:  -3.3259 | PDE Loss:  -5.227 | Function Loss:  -3.3841\n",
            "Total loss:  -3.3261 | PDE Loss:  -5.2247 | Function Loss:  -3.3847\n",
            "Total loss:  -3.3263 | PDE Loss:  -5.2238 | Function Loss:  -3.385\n",
            "Total loss:  -3.3264 | PDE Loss:  -5.2217 | Function Loss:  -3.3856\n",
            "Total loss:  -3.3266 | PDE Loss:  -5.2202 | Function Loss:  -3.386\n",
            "Total loss:  -3.3269 | PDE Loss:  -5.22 | Function Loss:  -3.3863\n",
            "Total loss:  -3.3271 | PDE Loss:  -5.2193 | Function Loss:  -3.3866\n",
            "Total loss:  -3.3273 | PDE Loss:  -5.2193 | Function Loss:  -3.3869\n",
            "Total loss:  -3.3275 | PDE Loss:  -5.2208 | Function Loss:  -3.3869\n",
            "Total loss:  -3.3277 | PDE Loss:  -5.222 | Function Loss:  -3.3869\n",
            "Total loss:  -3.3279 | PDE Loss:  -5.2246 | Function Loss:  -3.3868\n",
            "Total loss:  -3.3281 | PDE Loss:  -5.2265 | Function Loss:  -3.3867\n",
            "Total loss:  -3.3283 | PDE Loss:  -5.2282 | Function Loss:  -3.3867\n",
            "Total loss:  -3.3285 | PDE Loss:  -5.2297 | Function Loss:  -3.3867\n",
            "Total loss:  -3.3287 | PDE Loss:  -5.2312 | Function Loss:  -3.3868\n",
            "Total loss:  -3.329 | PDE Loss:  -5.2327 | Function Loss:  -3.3869\n",
            "Total loss:  -3.3293 | PDE Loss:  -5.2333 | Function Loss:  -3.3872\n",
            "Total loss:  -3.3297 | PDE Loss:  -5.2338 | Function Loss:  -3.3875\n",
            "Total loss:  -3.33 | PDE Loss:  -5.2338 | Function Loss:  -3.3879\n",
            "Total loss:  -3.3304 | PDE Loss:  -5.2349 | Function Loss:  -3.3882\n",
            "Total loss:  -3.3307 | PDE Loss:  -5.2348 | Function Loss:  -3.3886\n",
            "Total loss:  -3.331 | PDE Loss:  -5.2356 | Function Loss:  -3.3888\n",
            "Total loss:  -3.3312 | PDE Loss:  -5.2359 | Function Loss:  -3.3889\n",
            "Total loss:  -3.3313 | PDE Loss:  -5.2362 | Function Loss:  -3.3891\n",
            "Total loss:  -3.3315 | PDE Loss:  -5.2369 | Function Loss:  -3.3892\n",
            "Total loss:  -3.3317 | PDE Loss:  -5.2366 | Function Loss:  -3.3894\n",
            "Total loss:  -3.3318 | PDE Loss:  -5.2377 | Function Loss:  -3.3894\n",
            "Total loss:  -3.3321 | PDE Loss:  -5.2372 | Function Loss:  -3.3898\n",
            "Total loss:  -3.3324 | PDE Loss:  -5.24 | Function Loss:  -3.3897\n",
            "Total loss:  -3.3326 | PDE Loss:  -5.237 | Function Loss:  -3.3903\n",
            "Total loss:  -3.3327 | PDE Loss:  -5.2383 | Function Loss:  -3.3904\n",
            "Total loss:  -3.333 | PDE Loss:  -5.2401 | Function Loss:  -3.3905\n",
            "Total loss:  -3.3334 | PDE Loss:  -5.2406 | Function Loss:  -3.3908\n",
            "Total loss:  -3.3336 | PDE Loss:  -5.241 | Function Loss:  -3.391\n",
            "Total loss:  -3.3338 | PDE Loss:  -5.241 | Function Loss:  -3.3912\n",
            "Total loss:  -3.334 | PDE Loss:  -5.2413 | Function Loss:  -3.3914\n",
            "Total loss:  -3.3343 | PDE Loss:  -5.242 | Function Loss:  -3.3916\n",
            "Total loss:  -3.3345 | PDE Loss:  -5.2424 | Function Loss:  -3.3918\n",
            "Total loss:  -3.3348 | PDE Loss:  -5.2454 | Function Loss:  -3.3918\n",
            "Total loss:  -3.3351 | PDE Loss:  -5.2482 | Function Loss:  -3.3917\n",
            "Total loss:  -3.3354 | PDE Loss:  -5.2509 | Function Loss:  -3.3917\n",
            "Total loss:  -3.3357 | PDE Loss:  -5.2542 | Function Loss:  -3.3915\n",
            "Total loss:  -3.336 | PDE Loss:  -5.2556 | Function Loss:  -3.3917\n",
            "Total loss:  -3.3362 | PDE Loss:  -5.2577 | Function Loss:  -3.3916\n",
            "Total loss:  -3.3365 | PDE Loss:  -5.2593 | Function Loss:  -3.3917\n",
            "Total loss:  -3.3366 | PDE Loss:  -5.259 | Function Loss:  -3.3919\n",
            "Total loss:  -3.3368 | PDE Loss:  -5.2599 | Function Loss:  -3.392\n",
            "Total loss:  -3.3369 | PDE Loss:  -5.2594 | Function Loss:  -3.3922\n",
            "Total loss:  -3.337 | PDE Loss:  -5.2595 | Function Loss:  -3.3923\n",
            "Total loss:  -3.3372 | PDE Loss:  -5.2597 | Function Loss:  -3.3925\n",
            "Total loss:  -3.3374 | PDE Loss:  -5.2599 | Function Loss:  -3.3927\n",
            "Total loss:  -3.3376 | PDE Loss:  -5.2614 | Function Loss:  -3.3927\n",
            "Total loss:  -3.3378 | PDE Loss:  -5.2596 | Function Loss:  -3.3932\n",
            "Total loss:  -3.338 | PDE Loss:  -5.2607 | Function Loss:  -3.3932\n",
            "Total loss:  -3.3382 | PDE Loss:  -5.2617 | Function Loss:  -3.3934\n",
            "Total loss:  -3.3385 | PDE Loss:  -5.2626 | Function Loss:  -3.3935\n",
            "Total loss:  -3.3387 | PDE Loss:  -5.2622 | Function Loss:  -3.3939\n",
            "Total loss:  -3.339 | PDE Loss:  -5.2622 | Function Loss:  -3.3942\n",
            "Total loss:  -3.3393 | PDE Loss:  -5.261 | Function Loss:  -3.3947\n",
            "Total loss:  -3.3397 | PDE Loss:  -5.2594 | Function Loss:  -3.3954\n",
            "Total loss:  -3.3401 | PDE Loss:  -5.2579 | Function Loss:  -3.396\n",
            "Total loss:  -3.3405 | PDE Loss:  -5.2558 | Function Loss:  -3.3968\n",
            "Total loss:  -3.341 | PDE Loss:  -5.2537 | Function Loss:  -3.3976\n",
            "Total loss:  -3.3413 | PDE Loss:  -5.2518 | Function Loss:  -3.3983\n",
            "Total loss:  -3.3416 | PDE Loss:  -5.2508 | Function Loss:  -3.3988\n",
            "Total loss:  -3.3419 | PDE Loss:  -5.2511 | Function Loss:  -3.399\n",
            "Total loss:  -3.3421 | PDE Loss:  -5.2522 | Function Loss:  -3.3992\n",
            "Total loss:  -3.3423 | PDE Loss:  -5.2543 | Function Loss:  -3.3991\n",
            "Total loss:  -3.3425 | PDE Loss:  -5.2559 | Function Loss:  -3.399\n",
            "Total loss:  -3.3426 | PDE Loss:  -5.2574 | Function Loss:  -3.3989\n",
            "Total loss:  -3.3428 | PDE Loss:  -5.2584 | Function Loss:  -3.399\n",
            "Total loss:  -3.3429 | PDE Loss:  -5.2592 | Function Loss:  -3.3991\n",
            "Total loss:  -3.343 | PDE Loss:  -5.2588 | Function Loss:  -3.3992\n",
            "Total loss:  -3.3432 | PDE Loss:  -5.2584 | Function Loss:  -3.3995\n",
            "Total loss:  -3.3433 | PDE Loss:  -5.2581 | Function Loss:  -3.3996\n",
            "Total loss:  -3.3434 | PDE Loss:  -5.2551 | Function Loss:  -3.4001\n",
            "Total loss:  -3.3435 | PDE Loss:  -5.255 | Function Loss:  -3.4003\n",
            "Total loss:  -3.3436 | PDE Loss:  -5.2549 | Function Loss:  -3.4004\n",
            "Total loss:  -3.3437 | PDE Loss:  -5.2535 | Function Loss:  -3.4008\n",
            "Total loss:  -3.3439 | PDE Loss:  -5.2525 | Function Loss:  -3.4011\n",
            "Total loss:  -3.344 | PDE Loss:  -5.2514 | Function Loss:  -3.4014\n",
            "Total loss:  -3.3441 | PDE Loss:  -5.2512 | Function Loss:  -3.4016\n",
            "Total loss:  -3.3443 | PDE Loss:  -5.2509 | Function Loss:  -3.4018\n",
            "Total loss:  -3.3445 | PDE Loss:  -5.2514 | Function Loss:  -3.402\n",
            "Total loss:  -3.3448 | PDE Loss:  -5.2522 | Function Loss:  -3.4022\n",
            "Total loss:  -3.345 | PDE Loss:  -5.2543 | Function Loss:  -3.4021\n",
            "Total loss:  -3.3453 | PDE Loss:  -5.2565 | Function Loss:  -3.4022\n",
            "Total loss:  -3.3456 | PDE Loss:  -5.2594 | Function Loss:  -3.4021\n",
            "Total loss:  -3.346 | PDE Loss:  -5.2624 | Function Loss:  -3.4021\n",
            "Total loss:  -3.3463 | PDE Loss:  -5.265 | Function Loss:  -3.4021\n",
            "Total loss:  -3.3466 | PDE Loss:  -5.2676 | Function Loss:  -3.4021\n",
            "Total loss:  -3.3468 | PDE Loss:  -5.2692 | Function Loss:  -3.4021\n",
            "Total loss:  -3.347 | PDE Loss:  -5.2704 | Function Loss:  -3.4022\n",
            "Total loss:  -3.3473 | PDE Loss:  -5.271 | Function Loss:  -3.4024\n",
            "Total loss:  -3.3476 | PDE Loss:  -5.2715 | Function Loss:  -3.4027\n",
            "Total loss:  -3.3481 | PDE Loss:  -5.2716 | Function Loss:  -3.4032\n",
            "Total loss:  -3.3483 | PDE Loss:  -5.2724 | Function Loss:  -3.4034\n",
            "Total loss:  -3.3489 | PDE Loss:  -5.274 | Function Loss:  -3.4039\n",
            "Total loss:  -3.3496 | PDE Loss:  -5.2765 | Function Loss:  -3.4043\n",
            "Total loss:  -3.3503 | PDE Loss:  -5.2768 | Function Loss:  -3.405\n",
            "Total loss:  -3.3508 | PDE Loss:  -5.2802 | Function Loss:  -3.4051\n",
            "Total loss:  -3.3512 | PDE Loss:  -5.2828 | Function Loss:  -3.4053\n",
            "Total loss:  -3.3519 | PDE Loss:  -5.2863 | Function Loss:  -3.4056\n",
            "Total loss:  -3.3525 | PDE Loss:  -5.2905 | Function Loss:  -3.4057\n",
            "Total loss:  -3.353 | PDE Loss:  -5.293 | Function Loss:  -3.406\n",
            "Total loss:  -3.3537 | PDE Loss:  -5.296 | Function Loss:  -3.4064\n",
            "Total loss:  -3.3544 | PDE Loss:  -5.2986 | Function Loss:  -3.4068\n",
            "Total loss:  -3.3548 | PDE Loss:  -5.2995 | Function Loss:  -3.4072\n",
            "Total loss:  -3.3552 | PDE Loss:  -5.3009 | Function Loss:  -3.4074\n",
            "Total loss:  -3.3555 | PDE Loss:  -5.3016 | Function Loss:  -3.4077\n",
            "Total loss:  -3.3558 | PDE Loss:  -5.3028 | Function Loss:  -3.4079\n",
            "Total loss:  -3.3561 | PDE Loss:  -5.3045 | Function Loss:  -3.408\n",
            "Total loss:  -3.3564 | PDE Loss:  -5.3057 | Function Loss:  -3.4082\n",
            "Total loss:  -3.3568 | PDE Loss:  -5.3081 | Function Loss:  -3.4083\n",
            "Total loss:  -3.3572 | PDE Loss:  -5.3093 | Function Loss:  -3.4086\n",
            "Total loss:  -3.3575 | PDE Loss:  -5.3116 | Function Loss:  -3.4087\n",
            "Total loss:  -3.3579 | PDE Loss:  -5.3131 | Function Loss:  -3.4089\n",
            "Total loss:  -3.3583 | PDE Loss:  -5.3121 | Function Loss:  -3.4095\n",
            "Total loss:  -3.3589 | PDE Loss:  -5.3142 | Function Loss:  -3.4099\n",
            "Total loss:  -3.3594 | PDE Loss:  -5.3156 | Function Loss:  -3.4104\n",
            "Total loss:  -3.36 | PDE Loss:  -5.3151 | Function Loss:  -3.4111\n",
            "Total loss:  -3.3607 | PDE Loss:  -5.3159 | Function Loss:  -3.4117\n",
            "Total loss:  -3.3612 | PDE Loss:  -5.3165 | Function Loss:  -3.4123\n",
            "Total loss:  -3.3618 | PDE Loss:  -5.3171 | Function Loss:  -3.4128\n",
            "Total loss:  -3.3624 | PDE Loss:  -5.3191 | Function Loss:  -3.4132\n",
            "Total loss:  -3.3629 | PDE Loss:  -5.3213 | Function Loss:  -3.4136\n",
            "Total loss:  -3.3637 | PDE Loss:  -5.3247 | Function Loss:  -3.414\n",
            "Total loss:  -3.3645 | PDE Loss:  -5.3314 | Function Loss:  -3.4141\n",
            "Total loss:  -3.3643 | PDE Loss:  -5.3276 | Function Loss:  -3.4143\n",
            "Total loss:  -3.365 | PDE Loss:  -5.3322 | Function Loss:  -3.4145\n",
            "Total loss:  -3.3654 | PDE Loss:  -5.3359 | Function Loss:  -3.4146\n",
            "Total loss:  -3.3659 | PDE Loss:  -5.3393 | Function Loss:  -3.4147\n",
            "Total loss:  -3.3665 | PDE Loss:  -5.3431 | Function Loss:  -3.4149\n",
            "Total loss:  -3.367 | PDE Loss:  -5.3451 | Function Loss:  -3.4153\n",
            "Total loss:  -3.3676 | PDE Loss:  -5.3472 | Function Loss:  -3.4157\n",
            "Total loss:  -3.3682 | PDE Loss:  -5.3485 | Function Loss:  -3.4162\n",
            "Total loss:  -3.3688 | PDE Loss:  -5.3498 | Function Loss:  -3.4167\n",
            "Total loss:  -3.3694 | PDE Loss:  -5.3514 | Function Loss:  -3.4172\n",
            "Total loss:  -3.37 | PDE Loss:  -5.3546 | Function Loss:  -3.4175\n",
            "Total loss:  -3.3704 | PDE Loss:  -5.3565 | Function Loss:  -3.4178\n",
            "Total loss:  -3.3707 | PDE Loss:  -5.3588 | Function Loss:  -3.4178\n",
            "Total loss:  -3.3709 | PDE Loss:  -5.3604 | Function Loss:  -3.4178\n",
            "Total loss:  -3.3709 | PDE Loss:  -5.3617 | Function Loss:  -3.4177\n",
            "Total loss:  -3.3711 | PDE Loss:  -5.3635 | Function Loss:  -3.4177\n",
            "Total loss:  -3.3713 | PDE Loss:  -5.3653 | Function Loss:  -3.4177\n",
            "Total loss:  -3.3714 | PDE Loss:  -5.3676 | Function Loss:  -3.4176\n",
            "Total loss:  -3.3718 | PDE Loss:  -5.3698 | Function Loss:  -3.4177\n",
            "Total loss:  -3.3719 | PDE Loss:  -5.3693 | Function Loss:  -3.418\n",
            "Total loss:  -3.3724 | PDE Loss:  -5.3681 | Function Loss:  -3.4186\n",
            "Total loss:  -3.3727 | PDE Loss:  -5.3653 | Function Loss:  -3.4193\n",
            "Total loss:  -3.373 | PDE Loss:  -5.3622 | Function Loss:  -3.42\n",
            "Total loss:  -3.3733 | PDE Loss:  -5.3609 | Function Loss:  -3.4205\n",
            "Total loss:  -3.3737 | PDE Loss:  -5.357 | Function Loss:  -3.4213\n",
            "Total loss:  -3.374 | PDE Loss:  -5.3578 | Function Loss:  -3.4216\n",
            "Total loss:  -3.3744 | PDE Loss:  -5.3596 | Function Loss:  -3.4218\n",
            "Total loss:  -3.3749 | PDE Loss:  -5.3619 | Function Loss:  -3.4221\n",
            "Total loss:  -3.3753 | PDE Loss:  -5.3652 | Function Loss:  -3.4222\n",
            "Total loss:  -3.3756 | PDE Loss:  -5.3689 | Function Loss:  -3.4221\n",
            "Total loss:  -3.3758 | PDE Loss:  -5.3719 | Function Loss:  -3.422\n",
            "Total loss:  -3.376 | PDE Loss:  -5.3743 | Function Loss:  -3.422\n",
            "Total loss:  -3.3764 | PDE Loss:  -5.3764 | Function Loss:  -3.4221\n",
            "Total loss:  -3.3768 | PDE Loss:  -5.3781 | Function Loss:  -3.4224\n",
            "Total loss:  -3.3772 | PDE Loss:  -5.3758 | Function Loss:  -3.4231\n",
            "Total loss:  -3.3776 | PDE Loss:  -5.3741 | Function Loss:  -3.4237\n",
            "Total loss:  -3.3778 | PDE Loss:  -5.3733 | Function Loss:  -3.424\n",
            "Total loss:  -3.3781 | PDE Loss:  -5.3701 | Function Loss:  -3.4248\n",
            "Total loss:  -3.3784 | PDE Loss:  -5.3675 | Function Loss:  -3.4254\n",
            "Total loss:  -3.3786 | PDE Loss:  -5.365 | Function Loss:  -3.4259\n",
            "Total loss:  -3.3789 | PDE Loss:  -5.3643 | Function Loss:  -3.4263\n",
            "Total loss:  -3.3792 | PDE Loss:  -5.3635 | Function Loss:  -3.4268\n",
            "Total loss:  -3.3797 | PDE Loss:  -5.3639 | Function Loss:  -3.4272\n",
            "Total loss:  -3.38 | PDE Loss:  -5.3624 | Function Loss:  -3.4278\n",
            "Total loss:  -3.3805 | PDE Loss:  -5.3642 | Function Loss:  -3.4281\n",
            "Total loss:  -3.3808 | PDE Loss:  -5.3659 | Function Loss:  -3.4283\n",
            "Total loss:  -3.3813 | PDE Loss:  -5.3687 | Function Loss:  -3.4285\n",
            "Total loss:  -3.3818 | PDE Loss:  -5.3703 | Function Loss:  -3.4288\n",
            "Total loss:  -3.3821 | PDE Loss:  -5.3722 | Function Loss:  -3.429\n",
            "Total loss:  -3.3823 | PDE Loss:  -5.3721 | Function Loss:  -3.4292\n",
            "Total loss:  -3.3826 | PDE Loss:  -5.3726 | Function Loss:  -3.4294\n",
            "Total loss:  -3.3828 | PDE Loss:  -5.3726 | Function Loss:  -3.4297\n",
            "Total loss:  -3.383 | PDE Loss:  -5.3738 | Function Loss:  -3.4298\n",
            "Total loss:  -3.3832 | PDE Loss:  -5.3741 | Function Loss:  -3.43\n",
            "Total loss:  -3.3835 | PDE Loss:  -5.375 | Function Loss:  -3.4302\n",
            "Total loss:  -3.3837 | PDE Loss:  -5.3753 | Function Loss:  -3.4304\n",
            "Total loss:  -3.3838 | PDE Loss:  -5.3763 | Function Loss:  -3.4304\n",
            "Total loss:  -3.384 | PDE Loss:  -5.3771 | Function Loss:  -3.4305\n",
            "Total loss:  -3.3836 | PDE Loss:  -5.3827 | Function Loss:  -3.4295\n",
            "Total loss:  -3.384 | PDE Loss:  -5.3793 | Function Loss:  -3.4303\n",
            "Total loss:  -3.3842 | PDE Loss:  -5.3798 | Function Loss:  -3.4304\n",
            "Total loss:  -3.3843 | PDE Loss:  -5.3801 | Function Loss:  -3.4305\n",
            "Total loss:  -3.3845 | PDE Loss:  -5.3803 | Function Loss:  -3.4307\n",
            "Total loss:  -3.3846 | PDE Loss:  -5.3793 | Function Loss:  -3.4309\n",
            "Total loss:  -3.3847 | PDE Loss:  -5.3789 | Function Loss:  -3.4311\n",
            "Total loss:  -3.3848 | PDE Loss:  -5.3778 | Function Loss:  -3.4313\n",
            "Total loss:  -3.3849 | PDE Loss:  -5.3768 | Function Loss:  -3.4315\n",
            "Total loss:  -3.385 | PDE Loss:  -5.3762 | Function Loss:  -3.4317\n",
            "Total loss:  -3.3851 | PDE Loss:  -5.3744 | Function Loss:  -3.4321\n",
            "Total loss:  -3.3852 | PDE Loss:  -5.3741 | Function Loss:  -3.4323\n",
            "Total loss:  -3.3854 | PDE Loss:  -5.3738 | Function Loss:  -3.4324\n",
            "Total loss:  -3.3856 | PDE Loss:  -5.3737 | Function Loss:  -3.4327\n",
            "Total loss:  -3.3858 | PDE Loss:  -5.3739 | Function Loss:  -3.4329\n",
            "Total loss:  -3.3859 | PDE Loss:  -5.3747 | Function Loss:  -3.433\n",
            "Total loss:  -3.3862 | PDE Loss:  -5.3751 | Function Loss:  -3.4332\n",
            "Total loss:  -3.3864 | PDE Loss:  -5.3756 | Function Loss:  -3.4334\n",
            "Total loss:  -3.3869 | PDE Loss:  -5.3791 | Function Loss:  -3.4335\n",
            "Total loss:  -3.3872 | PDE Loss:  -5.3798 | Function Loss:  -3.4338\n",
            "Total loss:  -3.3875 | PDE Loss:  -5.3805 | Function Loss:  -3.434\n",
            "Total loss:  -3.3878 | PDE Loss:  -5.3816 | Function Loss:  -3.4342\n",
            "Total loss:  -3.388 | PDE Loss:  -5.3819 | Function Loss:  -3.4344\n",
            "Total loss:  -3.3882 | PDE Loss:  -5.3825 | Function Loss:  -3.4345\n",
            "Total loss:  -3.3883 | PDE Loss:  -5.3826 | Function Loss:  -3.4347\n",
            "Total loss:  -3.3885 | PDE Loss:  -5.3831 | Function Loss:  -3.4349\n",
            "Total loss:  -3.3888 | PDE Loss:  -5.384 | Function Loss:  -3.4351\n",
            "Total loss:  -3.3891 | PDE Loss:  -5.3851 | Function Loss:  -3.4353\n",
            "Total loss:  -3.3893 | PDE Loss:  -5.3871 | Function Loss:  -3.4353\n",
            "Total loss:  -3.3894 | PDE Loss:  -5.3886 | Function Loss:  -3.4353\n",
            "Total loss:  -3.3895 | PDE Loss:  -5.3904 | Function Loss:  -3.4352\n",
            "Total loss:  -3.3897 | PDE Loss:  -5.3921 | Function Loss:  -3.4351\n",
            "Total loss:  -3.3897 | PDE Loss:  -5.394 | Function Loss:  -3.435\n",
            "Total loss:  -3.3898 | PDE Loss:  -5.3952 | Function Loss:  -3.435\n",
            "Total loss:  -3.3899 | PDE Loss:  -5.3977 | Function Loss:  -3.4348\n",
            "Total loss:  -3.39 | PDE Loss:  -5.399 | Function Loss:  -3.4348\n",
            "Total loss:  -3.3901 | PDE Loss:  -5.4002 | Function Loss:  -3.4348\n",
            "Total loss:  -3.3903 | PDE Loss:  -5.4014 | Function Loss:  -3.4348\n",
            "Total loss:  -3.3904 | PDE Loss:  -5.404 | Function Loss:  -3.4347\n",
            "Total loss:  -3.3906 | PDE Loss:  -5.4042 | Function Loss:  -3.4348\n",
            "Total loss:  -3.3907 | PDE Loss:  -5.4042 | Function Loss:  -3.435\n",
            "Total loss:  -3.3909 | PDE Loss:  -5.4047 | Function Loss:  -3.4352\n",
            "Total loss:  -3.3911 | PDE Loss:  -5.4054 | Function Loss:  -3.4353\n",
            "Total loss:  -3.3913 | PDE Loss:  -5.4065 | Function Loss:  -3.4354\n",
            "Total loss:  -3.3916 | PDE Loss:  -5.409 | Function Loss:  -3.4354\n",
            "Total loss:  -3.3919 | PDE Loss:  -5.4117 | Function Loss:  -3.4355\n",
            "Total loss:  -3.3922 | PDE Loss:  -5.4141 | Function Loss:  -3.4355\n",
            "Total loss:  -3.3925 | PDE Loss:  -5.4179 | Function Loss:  -3.4355\n",
            "Total loss:  -3.3927 | PDE Loss:  -5.4203 | Function Loss:  -3.4355\n",
            "Total loss:  -3.3929 | PDE Loss:  -5.4208 | Function Loss:  -3.4357\n",
            "Total loss:  -3.3932 | PDE Loss:  -5.4229 | Function Loss:  -3.4358\n",
            "Total loss:  -3.3933 | PDE Loss:  -5.4209 | Function Loss:  -3.4361\n",
            "Total loss:  -3.3935 | PDE Loss:  -5.4211 | Function Loss:  -3.4363\n",
            "Total loss:  -3.3936 | PDE Loss:  -5.4218 | Function Loss:  -3.4364\n",
            "Total loss:  -3.3938 | PDE Loss:  -5.4212 | Function Loss:  -3.4367\n",
            "Total loss:  -3.394 | PDE Loss:  -5.4213 | Function Loss:  -3.4368\n",
            "Total loss:  -3.3942 | PDE Loss:  -5.4209 | Function Loss:  -3.4371\n",
            "Total loss:  -3.3943 | PDE Loss:  -5.4197 | Function Loss:  -3.4373\n",
            "Total loss:  -3.3944 | PDE Loss:  -5.4205 | Function Loss:  -3.4373\n",
            "Total loss:  -3.3944 | PDE Loss:  -5.4205 | Function Loss:  -3.4374\n",
            "Total loss:  -3.3945 | PDE Loss:  -5.4205 | Function Loss:  -3.4375\n",
            "Total loss:  -3.3946 | PDE Loss:  -5.4209 | Function Loss:  -3.4376\n",
            "Total loss:  -3.3947 | PDE Loss:  -5.4208 | Function Loss:  -3.4376\n",
            "Total loss:  -3.3948 | PDE Loss:  -5.421 | Function Loss:  -3.4377\n",
            "Total loss:  -3.3948 | PDE Loss:  -5.4207 | Function Loss:  -3.4378\n",
            "Total loss:  -3.3949 | PDE Loss:  -5.4206 | Function Loss:  -3.4379\n",
            "Total loss:  -3.3949 | PDE Loss:  -5.4204 | Function Loss:  -3.438\n",
            "Total loss:  -3.395 | PDE Loss:  -5.4199 | Function Loss:  -3.4381\n",
            "Total loss:  -3.3951 | PDE Loss:  -5.4192 | Function Loss:  -3.4382\n",
            "Total loss:  -3.3951 | PDE Loss:  -5.4188 | Function Loss:  -3.4383\n",
            "Total loss:  -3.395 | PDE Loss:  -5.4139 | Function Loss:  -3.4387\n",
            "Total loss:  -3.3951 | PDE Loss:  -5.4177 | Function Loss:  -3.4385\n",
            "Total loss:  -3.3952 | PDE Loss:  -5.4171 | Function Loss:  -3.4386\n",
            "Total loss:  -3.3953 | PDE Loss:  -5.4168 | Function Loss:  -3.4387\n",
            "Total loss:  -3.3953 | PDE Loss:  -5.4158 | Function Loss:  -3.4389\n",
            "Total loss:  -3.3954 | PDE Loss:  -5.4157 | Function Loss:  -3.439\n",
            "Total loss:  -3.3955 | PDE Loss:  -5.4153 | Function Loss:  -3.4391\n",
            "Total loss:  -3.3956 | PDE Loss:  -5.415 | Function Loss:  -3.4392\n",
            "Total loss:  -3.3957 | PDE Loss:  -5.4144 | Function Loss:  -3.4394\n",
            "Total loss:  -3.3958 | PDE Loss:  -5.4143 | Function Loss:  -3.4395\n",
            "Total loss:  -3.3959 | PDE Loss:  -5.414 | Function Loss:  -3.4397\n",
            "Total loss:  -3.396 | PDE Loss:  -5.4134 | Function Loss:  -3.4399\n",
            "Total loss:  -3.3962 | PDE Loss:  -5.4138 | Function Loss:  -3.44\n",
            "Total loss:  -3.3963 | PDE Loss:  -5.4139 | Function Loss:  -3.4402\n",
            "Total loss:  -3.3965 | PDE Loss:  -5.4146 | Function Loss:  -3.4403\n",
            "Total loss:  -3.3967 | PDE Loss:  -5.4153 | Function Loss:  -3.4405\n",
            "Total loss:  -3.3969 | PDE Loss:  -5.4158 | Function Loss:  -3.4406\n",
            "Total loss:  -3.397 | PDE Loss:  -5.4164 | Function Loss:  -3.4406\n",
            "Total loss:  -3.3971 | PDE Loss:  -5.4167 | Function Loss:  -3.4408\n",
            "Total loss:  -3.3973 | PDE Loss:  -5.4154 | Function Loss:  -3.4411\n",
            "Total loss:  -3.3974 | PDE Loss:  -5.4155 | Function Loss:  -3.4412\n",
            "Total loss:  -3.3977 | PDE Loss:  -5.415 | Function Loss:  -3.4415\n",
            "Total loss:  -3.3977 | PDE Loss:  -5.4116 | Function Loss:  -3.4419\n",
            "Total loss:  -3.3981 | PDE Loss:  -5.4116 | Function Loss:  -3.4424\n",
            "Total loss:  -3.3983 | PDE Loss:  -5.4115 | Function Loss:  -3.4426\n",
            "Total loss:  -3.3985 | PDE Loss:  -5.4106 | Function Loss:  -3.4429\n",
            "Total loss:  -3.3988 | PDE Loss:  -5.4091 | Function Loss:  -3.4434\n",
            "Total loss:  -3.399 | PDE Loss:  -5.4091 | Function Loss:  -3.4437\n",
            "Total loss:  -3.3992 | PDE Loss:  -5.4081 | Function Loss:  -3.444\n",
            "Total loss:  -3.3994 | PDE Loss:  -5.407 | Function Loss:  -3.4443\n",
            "Total loss:  -3.3996 | PDE Loss:  -5.4064 | Function Loss:  -3.4446\n",
            "Total loss:  -3.3997 | PDE Loss:  -5.4059 | Function Loss:  -3.4448\n",
            "Total loss:  -3.3998 | PDE Loss:  -5.4056 | Function Loss:  -3.4449\n",
            "Total loss:  -3.3999 | PDE Loss:  -5.4053 | Function Loss:  -3.445\n",
            "Total loss:  -3.3999 | PDE Loss:  -5.4047 | Function Loss:  -3.4452\n",
            "Total loss:  -3.4 | PDE Loss:  -5.4048 | Function Loss:  -3.4453\n",
            "Total loss:  -3.4001 | PDE Loss:  -5.4021 | Function Loss:  -3.4457\n",
            "Total loss:  -3.4 | PDE Loss:  -5.4008 | Function Loss:  -3.4457\n",
            "Total loss:  -3.4002 | PDE Loss:  -5.402 | Function Loss:  -3.4458\n",
            "Total loss:  -3.4003 | PDE Loss:  -5.4022 | Function Loss:  -3.4459\n",
            "Total loss:  -3.4004 | PDE Loss:  -5.4012 | Function Loss:  -3.4461\n",
            "Total loss:  -3.4005 | PDE Loss:  -5.4013 | Function Loss:  -3.4462\n",
            "Total loss:  -3.4006 | PDE Loss:  -5.4009 | Function Loss:  -3.4464\n",
            "Total loss:  -3.4007 | PDE Loss:  -5.4005 | Function Loss:  -3.4465\n",
            "Total loss:  -3.4009 | PDE Loss:  -5.4002 | Function Loss:  -3.4467\n",
            "Total loss:  -3.4009 | PDE Loss:  -5.3997 | Function Loss:  -3.4468\n",
            "Total loss:  -3.401 | PDE Loss:  -5.3995 | Function Loss:  -3.447\n",
            "Total loss:  -3.4011 | PDE Loss:  -5.3995 | Function Loss:  -3.4471\n",
            "Total loss:  -3.4012 | PDE Loss:  -5.3994 | Function Loss:  -3.4471\n",
            "Total loss:  -3.4013 | PDE Loss:  -5.3993 | Function Loss:  -3.4473\n",
            "Total loss:  -3.4015 | PDE Loss:  -5.4023 | Function Loss:  -3.4472\n",
            "Total loss:  -3.4016 | PDE Loss:  -5.4018 | Function Loss:  -3.4474\n",
            "Total loss:  -3.4018 | PDE Loss:  -5.4014 | Function Loss:  -3.4476\n",
            "Total loss:  -3.4019 | PDE Loss:  -5.4019 | Function Loss:  -3.4477\n",
            "Total loss:  -3.4021 | PDE Loss:  -5.4024 | Function Loss:  -3.4478\n",
            "Total loss:  -3.4022 | PDE Loss:  -5.403 | Function Loss:  -3.4479\n",
            "Total loss:  -3.4023 | PDE Loss:  -5.4044 | Function Loss:  -3.4478\n",
            "Total loss:  -3.4025 | PDE Loss:  -5.406 | Function Loss:  -3.4478\n",
            "Total loss:  -3.4026 | PDE Loss:  -5.4073 | Function Loss:  -3.4478\n",
            "Total loss:  -3.4029 | PDE Loss:  -5.4098 | Function Loss:  -3.4479\n",
            "Total loss:  -3.4031 | PDE Loss:  -5.4113 | Function Loss:  -3.4479\n",
            "Total loss:  -3.4032 | PDE Loss:  -5.4117 | Function Loss:  -3.448\n",
            "Total loss:  -3.4033 | PDE Loss:  -5.4112 | Function Loss:  -3.4482\n",
            "Total loss:  -3.4034 | PDE Loss:  -5.4111 | Function Loss:  -3.4483\n",
            "Total loss:  -3.4035 | PDE Loss:  -5.4102 | Function Loss:  -3.4485\n",
            "Total loss:  -3.4037 | PDE Loss:  -5.4086 | Function Loss:  -3.4489\n",
            "Total loss:  -3.4038 | PDE Loss:  -5.4071 | Function Loss:  -3.4491\n",
            "Total loss:  -3.4039 | PDE Loss:  -5.4059 | Function Loss:  -3.4494\n",
            "Total loss:  -3.404 | PDE Loss:  -5.4057 | Function Loss:  -3.4496\n",
            "Total loss:  -3.4041 | PDE Loss:  -5.4057 | Function Loss:  -3.4497\n",
            "Total loss:  -3.4041 | PDE Loss:  -5.4062 | Function Loss:  -3.4497\n",
            "Total loss:  -3.4042 | PDE Loss:  -5.4069 | Function Loss:  -3.4497\n",
            "Total loss:  -3.4043 | PDE Loss:  -5.4081 | Function Loss:  -3.4496\n",
            "Total loss:  -3.4043 | PDE Loss:  -5.409 | Function Loss:  -3.4496\n",
            "Total loss:  -3.4044 | PDE Loss:  -5.4101 | Function Loss:  -3.4495\n",
            "Total loss:  -3.4045 | PDE Loss:  -5.4109 | Function Loss:  -3.4495\n",
            "Total loss:  -3.4046 | PDE Loss:  -5.412 | Function Loss:  -3.4495\n",
            "Total loss:  -3.4047 | PDE Loss:  -5.412 | Function Loss:  -3.4496\n",
            "Total loss:  -3.4048 | PDE Loss:  -5.4118 | Function Loss:  -3.4498\n",
            "Total loss:  -3.405 | PDE Loss:  -5.412 | Function Loss:  -3.4499\n",
            "Total loss:  -3.4052 | PDE Loss:  -5.4116 | Function Loss:  -3.4502\n",
            "Total loss:  -3.4055 | PDE Loss:  -5.4117 | Function Loss:  -3.4505\n",
            "Total loss:  -3.4058 | PDE Loss:  -5.4112 | Function Loss:  -3.451\n",
            "Total loss:  -3.406 | PDE Loss:  -5.4119 | Function Loss:  -3.4511\n",
            "Total loss:  -3.4062 | PDE Loss:  -5.4101 | Function Loss:  -3.4515\n",
            "Total loss:  -3.4062 | PDE Loss:  -5.4101 | Function Loss:  -3.4516\n",
            "Total loss:  -3.4064 | PDE Loss:  -5.4112 | Function Loss:  -3.4516\n",
            "Total loss:  -3.4065 | PDE Loss:  -5.4121 | Function Loss:  -3.4517\n",
            "Total loss:  -3.4067 | PDE Loss:  -5.4124 | Function Loss:  -3.4518\n",
            "Total loss:  -3.4067 | PDE Loss:  -5.4125 | Function Loss:  -3.4519\n",
            "Total loss:  -3.4068 | PDE Loss:  -5.4123 | Function Loss:  -3.452\n",
            "Total loss:  -3.407 | PDE Loss:  -5.411 | Function Loss:  -3.4523\n",
            "Total loss:  -3.4071 | PDE Loss:  -5.4101 | Function Loss:  -3.4525\n",
            "Total loss:  -3.4072 | PDE Loss:  -5.4089 | Function Loss:  -3.4528\n",
            "Total loss:  -3.4073 | PDE Loss:  -5.4075 | Function Loss:  -3.453\n",
            "Total loss:  -3.4074 | PDE Loss:  -5.4065 | Function Loss:  -3.4533\n",
            "Total loss:  -3.4074 | PDE Loss:  -5.406 | Function Loss:  -3.4534\n",
            "Total loss:  -3.4075 | PDE Loss:  -5.4056 | Function Loss:  -3.4535\n",
            "Total loss:  -3.4076 | PDE Loss:  -5.406 | Function Loss:  -3.4535\n",
            "Total loss:  -3.4075 | PDE Loss:  -5.4014 | Function Loss:  -3.454\n",
            "Total loss:  -3.4076 | PDE Loss:  -5.4044 | Function Loss:  -3.4538\n",
            "Total loss:  -3.4077 | PDE Loss:  -5.4057 | Function Loss:  -3.4537\n",
            "Total loss:  -3.4078 | PDE Loss:  -5.4069 | Function Loss:  -3.4537\n",
            "Total loss:  -3.4079 | PDE Loss:  -5.4085 | Function Loss:  -3.4536\n",
            "Total loss:  -3.408 | PDE Loss:  -5.41 | Function Loss:  -3.4536\n",
            "Total loss:  -3.4081 | PDE Loss:  -5.4111 | Function Loss:  -3.4535\n",
            "Total loss:  -3.4082 | PDE Loss:  -5.4124 | Function Loss:  -3.4535\n",
            "Total loss:  -3.4083 | PDE Loss:  -5.4131 | Function Loss:  -3.4535\n",
            "Total loss:  -3.4084 | PDE Loss:  -5.4143 | Function Loss:  -3.4535\n",
            "Total loss:  -3.4086 | PDE Loss:  -5.4149 | Function Loss:  -3.4536\n",
            "Total loss:  -3.4087 | PDE Loss:  -5.4149 | Function Loss:  -3.4538\n",
            "Total loss:  -3.4089 | PDE Loss:  -5.4153 | Function Loss:  -3.4539\n",
            "Total loss:  -3.409 | PDE Loss:  -5.4158 | Function Loss:  -3.4541\n",
            "Total loss:  -3.4091 | PDE Loss:  -5.4157 | Function Loss:  -3.4542\n",
            "Total loss:  -3.4093 | PDE Loss:  -5.4158 | Function Loss:  -3.4543\n",
            "Total loss:  -3.4094 | PDE Loss:  -5.4168 | Function Loss:  -3.4544\n",
            "Total loss:  -3.4095 | PDE Loss:  -5.4175 | Function Loss:  -3.4544\n",
            "Total loss:  -3.4097 | PDE Loss:  -5.419 | Function Loss:  -3.4544\n",
            "Total loss:  -3.4098 | PDE Loss:  -5.4211 | Function Loss:  -3.4544\n",
            "Total loss:  -3.41 | PDE Loss:  -5.4235 | Function Loss:  -3.4543\n",
            "Total loss:  -3.4102 | PDE Loss:  -5.4249 | Function Loss:  -3.4543\n",
            "Total loss:  -3.4104 | PDE Loss:  -5.4264 | Function Loss:  -3.4544\n",
            "Total loss:  -3.4106 | PDE Loss:  -5.4271 | Function Loss:  -3.4545\n",
            "Total loss:  -3.4104 | PDE Loss:  -5.4241 | Function Loss:  -3.4547\n",
            "Total loss:  -3.4107 | PDE Loss:  -5.4268 | Function Loss:  -3.4547\n",
            "Total loss:  -3.4108 | PDE Loss:  -5.4264 | Function Loss:  -3.4549\n",
            "Total loss:  -3.411 | PDE Loss:  -5.4259 | Function Loss:  -3.4551\n",
            "Total loss:  -3.411 | PDE Loss:  -5.4254 | Function Loss:  -3.4552\n",
            "Total loss:  -3.4112 | PDE Loss:  -5.4246 | Function Loss:  -3.4555\n",
            "Total loss:  -3.4113 | PDE Loss:  -5.424 | Function Loss:  -3.4557\n",
            "Total loss:  -3.4115 | PDE Loss:  -5.4233 | Function Loss:  -3.456\n",
            "Total loss:  -3.4118 | PDE Loss:  -5.4228 | Function Loss:  -3.4564\n",
            "Total loss:  -3.4122 | PDE Loss:  -5.4228 | Function Loss:  -3.4568\n",
            "Total loss:  -3.4124 | PDE Loss:  -5.4234 | Function Loss:  -3.457\n",
            "Total loss:  -3.4128 | PDE Loss:  -5.4251 | Function Loss:  -3.4572\n",
            "Total loss:  -3.4131 | PDE Loss:  -5.4271 | Function Loss:  -3.4574\n",
            "Total loss:  -3.4133 | PDE Loss:  -5.4282 | Function Loss:  -3.4575\n",
            "Total loss:  -3.4135 | PDE Loss:  -5.4292 | Function Loss:  -3.4575\n",
            "Total loss:  -3.4137 | PDE Loss:  -5.4298 | Function Loss:  -3.4577\n",
            "Total loss:  -3.4139 | PDE Loss:  -5.4308 | Function Loss:  -3.4579\n",
            "Total loss:  -3.4141 | PDE Loss:  -5.4317 | Function Loss:  -3.458\n",
            "Total loss:  -3.4143 | PDE Loss:  -5.4319 | Function Loss:  -3.4581\n",
            "Total loss:  -3.4144 | PDE Loss:  -5.4318 | Function Loss:  -3.4583\n",
            "Total loss:  -3.4145 | PDE Loss:  -5.4318 | Function Loss:  -3.4584\n",
            "Total loss:  -3.4146 | PDE Loss:  -5.4317 | Function Loss:  -3.4585\n",
            "Total loss:  -3.4146 | PDE Loss:  -5.4309 | Function Loss:  -3.4586\n",
            "Total loss:  -3.4147 | PDE Loss:  -5.4313 | Function Loss:  -3.4587\n",
            "Total loss:  -3.4147 | PDE Loss:  -5.4293 | Function Loss:  -3.4589\n",
            "Total loss:  -3.4149 | PDE Loss:  -5.4314 | Function Loss:  -3.4589\n",
            "Total loss:  -3.4151 | PDE Loss:  -5.4328 | Function Loss:  -3.4589\n",
            "Total loss:  -3.4153 | PDE Loss:  -5.4344 | Function Loss:  -3.459\n",
            "Total loss:  -3.4154 | PDE Loss:  -5.4355 | Function Loss:  -3.459\n",
            "Total loss:  -3.4156 | PDE Loss:  -5.4361 | Function Loss:  -3.4591\n",
            "Total loss:  -3.4157 | PDE Loss:  -5.4363 | Function Loss:  -3.4592\n",
            "Total loss:  -3.4158 | PDE Loss:  -5.4363 | Function Loss:  -3.4593\n",
            "Total loss:  -3.4159 | PDE Loss:  -5.436 | Function Loss:  -3.4594\n",
            "Total loss:  -3.416 | PDE Loss:  -5.4359 | Function Loss:  -3.4596\n",
            "Total loss:  -3.4162 | PDE Loss:  -5.4346 | Function Loss:  -3.4599\n",
            "Total loss:  -3.4163 | PDE Loss:  -5.4337 | Function Loss:  -3.4602\n",
            "Total loss:  -3.4164 | PDE Loss:  -5.4324 | Function Loss:  -3.4604\n",
            "Total loss:  -3.4166 | PDE Loss:  -5.4319 | Function Loss:  -3.4607\n",
            "Total loss:  -3.4168 | PDE Loss:  -5.4321 | Function Loss:  -3.4608\n",
            "Total loss:  -3.417 | PDE Loss:  -5.4322 | Function Loss:  -3.4611\n",
            "Total loss:  -3.4172 | PDE Loss:  -5.4321 | Function Loss:  -3.4613\n",
            "Total loss:  -3.4174 | PDE Loss:  -5.4322 | Function Loss:  -3.4615\n",
            "Total loss:  -3.4176 | PDE Loss:  -5.4323 | Function Loss:  -3.4617\n",
            "Total loss:  -3.4177 | PDE Loss:  -5.432 | Function Loss:  -3.4619\n",
            "Total loss:  -3.4179 | PDE Loss:  -5.4323 | Function Loss:  -3.4621\n",
            "Total loss:  -3.4181 | PDE Loss:  -5.4322 | Function Loss:  -3.4623\n",
            "Total loss:  -3.4183 | PDE Loss:  -5.4324 | Function Loss:  -3.4625\n",
            "Total loss:  -3.4185 | PDE Loss:  -5.4317 | Function Loss:  -3.4628\n",
            "Total loss:  -3.4187 | PDE Loss:  -5.4316 | Function Loss:  -3.463\n",
            "Total loss:  -3.4189 | PDE Loss:  -5.4317 | Function Loss:  -3.4632\n",
            "Total loss:  -3.4192 | PDE Loss:  -5.4327 | Function Loss:  -3.4634\n",
            "Total loss:  -3.4194 | PDE Loss:  -5.4321 | Function Loss:  -3.4638\n",
            "Total loss:  -3.4196 | PDE Loss:  -5.4332 | Function Loss:  -3.4639\n",
            "Total loss:  -3.4198 | PDE Loss:  -5.4343 | Function Loss:  -3.464\n",
            "Total loss:  -3.4201 | PDE Loss:  -5.4351 | Function Loss:  -3.4643\n",
            "Total loss:  -3.4203 | PDE Loss:  -5.4357 | Function Loss:  -3.4644\n",
            "Total loss:  -3.4205 | PDE Loss:  -5.4355 | Function Loss:  -3.4646\n",
            "Total loss:  -3.4207 | PDE Loss:  -5.4348 | Function Loss:  -3.4649\n",
            "Total loss:  -3.4208 | PDE Loss:  -5.4348 | Function Loss:  -3.4651\n",
            "Total loss:  -3.421 | PDE Loss:  -5.4326 | Function Loss:  -3.4654\n",
            "Total loss:  -3.4211 | PDE Loss:  -5.4328 | Function Loss:  -3.4656\n",
            "Total loss:  -3.4212 | PDE Loss:  -5.4315 | Function Loss:  -3.4658\n",
            "Total loss:  -3.4213 | PDE Loss:  -5.4317 | Function Loss:  -3.4659\n",
            "Total loss:  -3.4213 | PDE Loss:  -5.4319 | Function Loss:  -3.4659\n",
            "Total loss:  -3.4214 | PDE Loss:  -5.4314 | Function Loss:  -3.466\n",
            "Total loss:  -3.4215 | PDE Loss:  -5.4321 | Function Loss:  -3.466\n",
            "Total loss:  -3.4215 | PDE Loss:  -5.4325 | Function Loss:  -3.466\n",
            "Total loss:  -3.4216 | PDE Loss:  -5.4333 | Function Loss:  -3.4661\n",
            "Total loss:  -3.4216 | PDE Loss:  -5.4339 | Function Loss:  -3.466\n",
            "Total loss:  -3.4216 | PDE Loss:  -5.4351 | Function Loss:  -3.4659\n",
            "Total loss:  -3.4218 | PDE Loss:  -5.4355 | Function Loss:  -3.4661\n",
            "Total loss:  -3.4219 | PDE Loss:  -5.4354 | Function Loss:  -3.4662\n",
            "Total loss:  -3.422 | PDE Loss:  -5.4352 | Function Loss:  -3.4663\n",
            "Total loss:  -3.4221 | PDE Loss:  -5.4349 | Function Loss:  -3.4665\n",
            "Total loss:  -3.4222 | PDE Loss:  -5.4345 | Function Loss:  -3.4666\n",
            "Total loss:  -3.4223 | PDE Loss:  -5.434 | Function Loss:  -3.4668\n",
            "Total loss:  -3.4226 | PDE Loss:  -5.4352 | Function Loss:  -3.4669\n",
            "Total loss:  -3.4228 | PDE Loss:  -5.4349 | Function Loss:  -3.4672\n",
            "Total loss:  -3.4229 | PDE Loss:  -5.4345 | Function Loss:  -3.4674\n",
            "Total loss:  -3.4231 | PDE Loss:  -5.4352 | Function Loss:  -3.4675\n",
            "Total loss:  -3.4232 | PDE Loss:  -5.4352 | Function Loss:  -3.4676\n",
            "Total loss:  -3.4233 | PDE Loss:  -5.4357 | Function Loss:  -3.4677\n",
            "Total loss:  -3.4234 | PDE Loss:  -5.436 | Function Loss:  -3.4677\n",
            "Total loss:  -3.4235 | PDE Loss:  -5.4367 | Function Loss:  -3.4678\n",
            "Total loss:  -3.4237 | PDE Loss:  -5.436 | Function Loss:  -3.4681\n",
            "Total loss:  -3.4238 | PDE Loss:  -5.4356 | Function Loss:  -3.4683\n",
            "Total loss:  -3.424 | PDE Loss:  -5.4357 | Function Loss:  -3.4685\n",
            "Total loss:  -3.4241 | PDE Loss:  -5.435 | Function Loss:  -3.4686\n",
            "Total loss:  -3.4242 | PDE Loss:  -5.4347 | Function Loss:  -3.4688\n",
            "Total loss:  -3.4243 | PDE Loss:  -5.4343 | Function Loss:  -3.469\n",
            "Total loss:  -3.4244 | PDE Loss:  -5.4337 | Function Loss:  -3.4691\n",
            "Total loss:  -3.4245 | PDE Loss:  -5.4339 | Function Loss:  -3.4692\n",
            "Total loss:  -3.4245 | PDE Loss:  -5.4336 | Function Loss:  -3.4693\n",
            "Total loss:  -3.4246 | PDE Loss:  -5.4341 | Function Loss:  -3.4693\n",
            "Total loss:  -3.4247 | PDE Loss:  -5.4343 | Function Loss:  -3.4694\n",
            "Total loss:  -3.4248 | PDE Loss:  -5.4355 | Function Loss:  -3.4693\n",
            "Total loss:  -3.4248 | PDE Loss:  -5.4356 | Function Loss:  -3.4694\n",
            "Total loss:  -3.4249 | PDE Loss:  -5.4364 | Function Loss:  -3.4694\n",
            "Total loss:  -3.4249 | PDE Loss:  -5.4365 | Function Loss:  -3.4694\n",
            "Total loss:  -3.425 | PDE Loss:  -5.4361 | Function Loss:  -3.4696\n",
            "Total loss:  -3.4251 | PDE Loss:  -5.4366 | Function Loss:  -3.4696\n",
            "Total loss:  -3.4251 | PDE Loss:  -5.4363 | Function Loss:  -3.4697\n",
            "Total loss:  -3.4252 | PDE Loss:  -5.4359 | Function Loss:  -3.4698\n",
            "Total loss:  -3.4253 | PDE Loss:  -5.4354 | Function Loss:  -3.47\n",
            "Total loss:  -3.4254 | PDE Loss:  -5.4355 | Function Loss:  -3.4701\n",
            "Total loss:  -3.4255 | PDE Loss:  -5.4352 | Function Loss:  -3.4702\n",
            "Total loss:  -3.4256 | PDE Loss:  -5.4362 | Function Loss:  -3.4702\n",
            "Total loss:  -3.4256 | PDE Loss:  -5.4364 | Function Loss:  -3.4702\n",
            "Total loss:  -3.4258 | PDE Loss:  -5.4382 | Function Loss:  -3.4702\n",
            "Total loss:  -3.4259 | PDE Loss:  -5.4393 | Function Loss:  -3.4702\n",
            "Total loss:  -3.4261 | PDE Loss:  -5.4411 | Function Loss:  -3.4702\n",
            "Total loss:  -3.4263 | PDE Loss:  -5.4447 | Function Loss:  -3.4701\n",
            "Total loss:  -3.4264 | PDE Loss:  -5.4456 | Function Loss:  -3.4701\n",
            "Total loss:  -3.4266 | PDE Loss:  -5.4474 | Function Loss:  -3.4701\n",
            "Total loss:  -3.4268 | PDE Loss:  -5.4485 | Function Loss:  -3.4702\n",
            "Total loss:  -3.427 | PDE Loss:  -5.45 | Function Loss:  -3.4703\n",
            "Total loss:  -3.4272 | PDE Loss:  -5.4509 | Function Loss:  -3.4704\n",
            "Total loss:  -3.4275 | PDE Loss:  -5.4528 | Function Loss:  -3.4705\n",
            "Total loss:  -3.4277 | PDE Loss:  -5.4537 | Function Loss:  -3.4707\n",
            "Total loss:  -3.4278 | PDE Loss:  -5.455 | Function Loss:  -3.4707\n",
            "Total loss:  -3.428 | PDE Loss:  -5.4562 | Function Loss:  -3.4708\n",
            "Total loss:  -3.4282 | PDE Loss:  -5.4579 | Function Loss:  -3.4708\n",
            "Total loss:  -3.4284 | PDE Loss:  -5.4599 | Function Loss:  -3.4708\n",
            "Total loss:  -3.4286 | PDE Loss:  -5.4612 | Function Loss:  -3.4709\n",
            "Total loss:  -3.4288 | PDE Loss:  -5.463 | Function Loss:  -3.4709\n",
            "Total loss:  -3.429 | PDE Loss:  -5.4649 | Function Loss:  -3.471\n",
            "Total loss:  -3.4292 | PDE Loss:  -5.4655 | Function Loss:  -3.4712\n",
            "Total loss:  -3.4294 | PDE Loss:  -5.4663 | Function Loss:  -3.4713\n",
            "Total loss:  -3.4296 | PDE Loss:  -5.4663 | Function Loss:  -3.4715\n",
            "Total loss:  -3.4298 | PDE Loss:  -5.4657 | Function Loss:  -3.4717\n",
            "Total loss:  -3.4299 | PDE Loss:  -5.4649 | Function Loss:  -3.4719\n",
            "Total loss:  -3.43 | PDE Loss:  -5.4647 | Function Loss:  -3.472\n",
            "Total loss:  -3.4301 | PDE Loss:  -5.4632 | Function Loss:  -3.4723\n",
            "Total loss:  -3.4301 | PDE Loss:  -5.4633 | Function Loss:  -3.4724\n",
            "Total loss:  -3.4302 | PDE Loss:  -5.4628 | Function Loss:  -3.4725\n",
            "Total loss:  -3.4303 | PDE Loss:  -5.463 | Function Loss:  -3.4726\n",
            "Total loss:  -3.4305 | PDE Loss:  -5.4641 | Function Loss:  -3.4726\n",
            "Total loss:  -3.4305 | PDE Loss:  -5.4645 | Function Loss:  -3.4727\n",
            "Total loss:  -3.4306 | PDE Loss:  -5.4657 | Function Loss:  -3.4726\n",
            "Total loss:  -3.4307 | PDE Loss:  -5.4667 | Function Loss:  -3.4726\n",
            "Total loss:  -3.4308 | PDE Loss:  -5.468 | Function Loss:  -3.4726\n",
            "Total loss:  -3.4309 | PDE Loss:  -5.4689 | Function Loss:  -3.4726\n",
            "Total loss:  -3.4309 | PDE Loss:  -5.4697 | Function Loss:  -3.4726\n",
            "Total loss:  -3.4311 | PDE Loss:  -5.4709 | Function Loss:  -3.4726\n",
            "Total loss:  -3.4313 | PDE Loss:  -5.4726 | Function Loss:  -3.4727\n",
            "Total loss:  -3.4314 | PDE Loss:  -5.4735 | Function Loss:  -3.4727\n",
            "Total loss:  -3.4316 | PDE Loss:  -5.4747 | Function Loss:  -3.4729\n",
            "Total loss:  -3.4319 | PDE Loss:  -5.4757 | Function Loss:  -3.4731\n",
            "Total loss:  -3.4321 | PDE Loss:  -5.4765 | Function Loss:  -3.4732\n",
            "Total loss:  -3.4323 | PDE Loss:  -5.4781 | Function Loss:  -3.4733\n",
            "Total loss:  -3.4325 | PDE Loss:  -5.4798 | Function Loss:  -3.4733\n",
            "Total loss:  -3.4326 | PDE Loss:  -5.4812 | Function Loss:  -3.4733\n",
            "Total loss:  -3.4328 | PDE Loss:  -5.4828 | Function Loss:  -3.4733\n",
            "Total loss:  -3.4329 | PDE Loss:  -5.4842 | Function Loss:  -3.4734\n",
            "Total loss:  -3.4331 | PDE Loss:  -5.4864 | Function Loss:  -3.4734\n",
            "Total loss:  -3.4332 | PDE Loss:  -5.4831 | Function Loss:  -3.4738\n",
            "Total loss:  -3.4335 | PDE Loss:  -5.4851 | Function Loss:  -3.4739\n",
            "Total loss:  -3.4338 | PDE Loss:  -5.486 | Function Loss:  -3.4741\n",
            "Total loss:  -3.434 | PDE Loss:  -5.4849 | Function Loss:  -3.4745\n",
            "Total loss:  -3.4343 | PDE Loss:  -5.4834 | Function Loss:  -3.475\n",
            "Total loss:  -3.4346 | PDE Loss:  -5.4804 | Function Loss:  -3.4755\n",
            "Total loss:  -3.4348 | PDE Loss:  -5.4774 | Function Loss:  -3.4761\n",
            "Total loss:  -3.4351 | PDE Loss:  -5.4751 | Function Loss:  -3.4766\n",
            "Total loss:  -3.4353 | PDE Loss:  -5.472 | Function Loss:  -3.4772\n",
            "Total loss:  -3.4356 | PDE Loss:  -5.4713 | Function Loss:  -3.4775\n",
            "Total loss:  -3.4359 | PDE Loss:  -5.4716 | Function Loss:  -3.4779\n",
            "Total loss:  -3.4362 | PDE Loss:  -5.4739 | Function Loss:  -3.4779\n",
            "Total loss:  -3.4365 | PDE Loss:  -5.477 | Function Loss:  -3.478\n",
            "Total loss:  -3.4367 | PDE Loss:  -5.4807 | Function Loss:  -3.4778\n",
            "Total loss:  -3.4369 | PDE Loss:  -5.4846 | Function Loss:  -3.4777\n",
            "Total loss:  -3.4372 | PDE Loss:  -5.4887 | Function Loss:  -3.4776\n",
            "Total loss:  -3.4373 | PDE Loss:  -5.4911 | Function Loss:  -3.4775\n",
            "Total loss:  -3.4375 | PDE Loss:  -5.4935 | Function Loss:  -3.4775\n",
            "Total loss:  -3.4377 | PDE Loss:  -5.4935 | Function Loss:  -3.4776\n",
            "Total loss:  -3.4378 | PDE Loss:  -5.4938 | Function Loss:  -3.4778\n",
            "Total loss:  -3.4381 | PDE Loss:  -5.4923 | Function Loss:  -3.4782\n",
            "Total loss:  -3.4383 | PDE Loss:  -5.4919 | Function Loss:  -3.4785\n",
            "Total loss:  -3.4385 | PDE Loss:  -5.4906 | Function Loss:  -3.4788\n",
            "Total loss:  -3.4387 | PDE Loss:  -5.4897 | Function Loss:  -3.4791\n",
            "Total loss:  -3.4389 | PDE Loss:  -5.4895 | Function Loss:  -3.4793\n",
            "Total loss:  -3.439 | PDE Loss:  -5.4897 | Function Loss:  -3.4795\n",
            "Total loss:  -3.4392 | PDE Loss:  -5.4882 | Function Loss:  -3.4798\n",
            "Total loss:  -3.4393 | PDE Loss:  -5.4916 | Function Loss:  -3.4797\n",
            "Total loss:  -3.4394 | PDE Loss:  -5.4924 | Function Loss:  -3.4797\n",
            "Total loss:  -3.4397 | PDE Loss:  -5.4937 | Function Loss:  -3.4798\n",
            "Total loss:  -3.4399 | PDE Loss:  -5.4958 | Function Loss:  -3.4798\n",
            "Total loss:  -3.4401 | PDE Loss:  -5.4984 | Function Loss:  -3.4798\n",
            "Total loss:  -3.4403 | PDE Loss:  -5.5 | Function Loss:  -3.4799\n",
            "Total loss:  -3.4405 | PDE Loss:  -5.5019 | Function Loss:  -3.48\n",
            "Total loss:  -3.4407 | PDE Loss:  -5.5027 | Function Loss:  -3.4801\n",
            "Total loss:  -3.4409 | PDE Loss:  -5.5033 | Function Loss:  -3.4802\n",
            "Total loss:  -3.4411 | PDE Loss:  -5.5034 | Function Loss:  -3.4804\n",
            "Total loss:  -3.4413 | PDE Loss:  -5.5036 | Function Loss:  -3.4807\n",
            "Total loss:  -3.4415 | PDE Loss:  -5.5037 | Function Loss:  -3.4809\n",
            "Total loss:  -3.4418 | PDE Loss:  -5.5032 | Function Loss:  -3.4812\n",
            "Total loss:  -3.442 | PDE Loss:  -5.5041 | Function Loss:  -3.4814\n",
            "Total loss:  -3.4422 | PDE Loss:  -5.5029 | Function Loss:  -3.4817\n",
            "Total loss:  -3.4424 | PDE Loss:  -5.5031 | Function Loss:  -3.4819\n",
            "Total loss:  -3.4425 | PDE Loss:  -5.5028 | Function Loss:  -3.4821\n",
            "Total loss:  -3.4427 | PDE Loss:  -5.502 | Function Loss:  -3.4824\n",
            "Total loss:  -3.4429 | PDE Loss:  -5.5022 | Function Loss:  -3.4825\n",
            "Total loss:  -3.443 | PDE Loss:  -5.5016 | Function Loss:  -3.4828\n",
            "Total loss:  -3.4432 | PDE Loss:  -5.5014 | Function Loss:  -3.483\n",
            "Total loss:  -3.4434 | PDE Loss:  -5.5004 | Function Loss:  -3.4833\n",
            "Total loss:  -3.4437 | PDE Loss:  -5.5011 | Function Loss:  -3.4835\n",
            "Total loss:  -3.4439 | PDE Loss:  -5.499 | Function Loss:  -3.484\n",
            "Total loss:  -3.4442 | PDE Loss:  -5.5007 | Function Loss:  -3.4841\n",
            "Total loss:  -3.4444 | PDE Loss:  -5.5027 | Function Loss:  -3.4841\n",
            "Total loss:  -3.4446 | PDE Loss:  -5.5042 | Function Loss:  -3.4842\n",
            "Total loss:  -3.4448 | PDE Loss:  -5.5065 | Function Loss:  -3.4842\n",
            "Total loss:  -3.4449 | PDE Loss:  -5.5084 | Function Loss:  -3.4842\n",
            "Total loss:  -3.4451 | PDE Loss:  -5.5113 | Function Loss:  -3.4841\n",
            "Total loss:  -3.4453 | PDE Loss:  -5.5136 | Function Loss:  -3.4841\n",
            "Total loss:  -3.4454 | PDE Loss:  -5.5155 | Function Loss:  -3.484\n",
            "Total loss:  -3.4455 | PDE Loss:  -5.5166 | Function Loss:  -3.484\n",
            "Total loss:  -3.4455 | PDE Loss:  -5.5178 | Function Loss:  -3.484\n",
            "Total loss:  -3.4457 | PDE Loss:  -5.5177 | Function Loss:  -3.4841\n",
            "Total loss:  -3.4457 | PDE Loss:  -5.5175 | Function Loss:  -3.4842\n",
            "Total loss:  -3.4458 | PDE Loss:  -5.517 | Function Loss:  -3.4843\n",
            "Total loss:  -3.4459 | PDE Loss:  -5.5161 | Function Loss:  -3.4845\n",
            "Total loss:  -3.446 | PDE Loss:  -5.5152 | Function Loss:  -3.4847\n",
            "Total loss:  -3.4461 | PDE Loss:  -5.5143 | Function Loss:  -3.4849\n",
            "Total loss:  -3.4461 | PDE Loss:  -5.5131 | Function Loss:  -3.485\n",
            "Total loss:  -3.4462 | PDE Loss:  -5.5123 | Function Loss:  -3.4852\n",
            "Total loss:  -3.4453 | PDE Loss:  -5.5078 | Function Loss:  -3.4846\n",
            "Total loss:  -3.4463 | PDE Loss:  -5.5126 | Function Loss:  -3.4852\n",
            "Total loss:  -3.4464 | PDE Loss:  -5.5122 | Function Loss:  -3.4854\n",
            "Total loss:  -3.4464 | PDE Loss:  -5.5119 | Function Loss:  -3.4855\n",
            "Total loss:  -3.4466 | PDE Loss:  -5.5118 | Function Loss:  -3.4856\n",
            "Total loss:  -3.4467 | PDE Loss:  -5.5122 | Function Loss:  -3.4857\n",
            "Total loss:  -3.4468 | PDE Loss:  -5.5124 | Function Loss:  -3.4858\n",
            "Total loss:  -3.4469 | PDE Loss:  -5.5133 | Function Loss:  -3.4859\n",
            "Total loss:  -3.447 | PDE Loss:  -5.5145 | Function Loss:  -3.4859\n",
            "Total loss:  -3.4464 | PDE Loss:  -5.5043 | Function Loss:  -3.4861\n",
            "Total loss:  -3.4471 | PDE Loss:  -5.5133 | Function Loss:  -3.4861\n",
            "Total loss:  -3.4472 | PDE Loss:  -5.5145 | Function Loss:  -3.4861\n",
            "Total loss:  -3.4473 | PDE Loss:  -5.5151 | Function Loss:  -3.4861\n",
            "Total loss:  -3.4474 | PDE Loss:  -5.5156 | Function Loss:  -3.4862\n",
            "Total loss:  -3.4475 | PDE Loss:  -5.5155 | Function Loss:  -3.4863\n",
            "Total loss:  -3.4476 | PDE Loss:  -5.5156 | Function Loss:  -3.4865\n",
            "Total loss:  -3.4478 | PDE Loss:  -5.5155 | Function Loss:  -3.4866\n",
            "Total loss:  -3.448 | PDE Loss:  -5.5151 | Function Loss:  -3.4869\n",
            "Total loss:  -3.4482 | PDE Loss:  -5.5148 | Function Loss:  -3.4871\n",
            "Total loss:  -3.4484 | PDE Loss:  -5.5149 | Function Loss:  -3.4874\n",
            "Total loss:  -3.4487 | PDE Loss:  -5.5151 | Function Loss:  -3.4877\n",
            "Total loss:  -3.449 | PDE Loss:  -5.5157 | Function Loss:  -3.488\n",
            "Total loss:  -3.4493 | PDE Loss:  -5.5161 | Function Loss:  -3.4883\n",
            "Total loss:  -3.4496 | PDE Loss:  -5.5174 | Function Loss:  -3.4884\n",
            "Total loss:  -3.4494 | PDE Loss:  -5.5131 | Function Loss:  -3.4886\n",
            "Total loss:  -3.4498 | PDE Loss:  -5.5169 | Function Loss:  -3.4887\n",
            "Total loss:  -3.45 | PDE Loss:  -5.517 | Function Loss:  -3.4889\n",
            "Total loss:  -3.4503 | PDE Loss:  -5.517 | Function Loss:  -3.4893\n",
            "Total loss:  -3.4506 | PDE Loss:  -5.5159 | Function Loss:  -3.4897\n",
            "Total loss:  -3.4509 | PDE Loss:  -5.5135 | Function Loss:  -3.4902\n",
            "Total loss:  -3.451 | PDE Loss:  -5.5115 | Function Loss:  -3.4906\n",
            "Total loss:  -3.4512 | PDE Loss:  -5.5094 | Function Loss:  -3.4909\n",
            "Total loss:  -3.4513 | PDE Loss:  -5.5075 | Function Loss:  -3.4912\n",
            "Total loss:  -3.4514 | PDE Loss:  -5.5059 | Function Loss:  -3.4915\n",
            "Total loss:  -3.4515 | PDE Loss:  -5.5047 | Function Loss:  -3.4917\n",
            "Total loss:  -3.4515 | PDE Loss:  -5.5027 | Function Loss:  -3.492\n",
            "Total loss:  -3.4517 | PDE Loss:  -5.5014 | Function Loss:  -3.4922\n",
            "Total loss:  -3.4518 | PDE Loss:  -5.5002 | Function Loss:  -3.4925\n",
            "Total loss:  -3.4521 | PDE Loss:  -5.4987 | Function Loss:  -3.4929\n",
            "Total loss:  -3.4524 | PDE Loss:  -5.4955 | Function Loss:  -3.4936\n",
            "Total loss:  -3.4527 | PDE Loss:  -5.4929 | Function Loss:  -3.4942\n",
            "Total loss:  -3.4531 | PDE Loss:  -5.4883 | Function Loss:  -3.4951\n",
            "Total loss:  -3.4535 | PDE Loss:  -5.485 | Function Loss:  -3.4959\n",
            "Total loss:  -3.4538 | PDE Loss:  -5.482 | Function Loss:  -3.4965\n",
            "Total loss:  -3.4541 | PDE Loss:  -5.4805 | Function Loss:  -3.497\n",
            "Total loss:  -3.4544 | PDE Loss:  -5.4788 | Function Loss:  -3.4975\n",
            "Total loss:  -3.4547 | PDE Loss:  -5.4789 | Function Loss:  -3.4978\n",
            "Total loss:  -3.455 | PDE Loss:  -5.4779 | Function Loss:  -3.4983\n",
            "Total loss:  -3.4553 | PDE Loss:  -5.4749 | Function Loss:  -3.4989\n",
            "Total loss:  -3.4555 | PDE Loss:  -5.4765 | Function Loss:  -3.499\n",
            "Total loss:  -3.456 | PDE Loss:  -5.4797 | Function Loss:  -3.4992\n",
            "Total loss:  -3.4563 | PDE Loss:  -5.4822 | Function Loss:  -3.4993\n",
            "Total loss:  -3.4566 | PDE Loss:  -5.4835 | Function Loss:  -3.4995\n",
            "Total loss:  -3.4568 | PDE Loss:  -5.4838 | Function Loss:  -3.4996\n",
            "Total loss:  -3.4571 | PDE Loss:  -5.4835 | Function Loss:  -3.5\n",
            "Total loss:  -3.4574 | PDE Loss:  -5.4842 | Function Loss:  -3.5002\n",
            "Total loss:  -3.4575 | PDE Loss:  -5.4827 | Function Loss:  -3.5006\n",
            "Total loss:  -3.4578 | PDE Loss:  -5.4821 | Function Loss:  -3.5009\n",
            "Total loss:  -3.458 | PDE Loss:  -5.4817 | Function Loss:  -3.5012\n",
            "Total loss:  -3.4582 | PDE Loss:  -5.4775 | Function Loss:  -3.5019\n",
            "Total loss:  -3.4584 | PDE Loss:  -5.4775 | Function Loss:  -3.5021\n",
            "Total loss:  -3.4587 | PDE Loss:  -5.4775 | Function Loss:  -3.5024\n",
            "Total loss:  -3.4591 | PDE Loss:  -5.478 | Function Loss:  -3.5028\n",
            "Total loss:  -3.4594 | PDE Loss:  -5.4792 | Function Loss:  -3.503\n",
            "Total loss:  -3.4598 | PDE Loss:  -5.4803 | Function Loss:  -3.5034\n",
            "Total loss:  -3.4602 | PDE Loss:  -5.4836 | Function Loss:  -3.5034\n",
            "Total loss:  -3.4604 | PDE Loss:  -5.485 | Function Loss:  -3.5036\n",
            "Total loss:  -3.4608 | PDE Loss:  -5.4867 | Function Loss:  -3.5037\n",
            "Total loss:  -3.461 | PDE Loss:  -5.488 | Function Loss:  -3.5038\n",
            "Total loss:  -3.4611 | PDE Loss:  -5.4893 | Function Loss:  -3.5039\n",
            "Total loss:  -3.4613 | PDE Loss:  -5.4908 | Function Loss:  -3.5039\n",
            "Total loss:  -3.4615 | PDE Loss:  -5.4924 | Function Loss:  -3.504\n",
            "Total loss:  -3.4616 | PDE Loss:  -5.4944 | Function Loss:  -3.5039\n",
            "Total loss:  -3.4619 | PDE Loss:  -5.4962 | Function Loss:  -3.504\n",
            "Total loss:  -3.4621 | PDE Loss:  -5.4982 | Function Loss:  -3.504\n",
            "Total loss:  -3.4624 | PDE Loss:  -5.5008 | Function Loss:  -3.504\n",
            "Total loss:  -3.4626 | PDE Loss:  -5.5031 | Function Loss:  -3.5041\n",
            "Total loss:  -3.4628 | PDE Loss:  -5.5055 | Function Loss:  -3.504\n",
            "Total loss:  -3.463 | PDE Loss:  -5.5082 | Function Loss:  -3.504\n",
            "Total loss:  -3.4634 | PDE Loss:  -5.5116 | Function Loss:  -3.5041\n",
            "Total loss:  -3.4637 | PDE Loss:  -5.5159 | Function Loss:  -3.504\n",
            "Total loss:  -3.464 | PDE Loss:  -5.5196 | Function Loss:  -3.504\n",
            "Total loss:  -3.4643 | PDE Loss:  -5.5229 | Function Loss:  -3.504\n",
            "Total loss:  -3.4646 | PDE Loss:  -5.5258 | Function Loss:  -3.5041\n",
            "Total loss:  -3.4649 | PDE Loss:  -5.5289 | Function Loss:  -3.5041\n",
            "Total loss:  -3.4651 | PDE Loss:  -5.5313 | Function Loss:  -3.5041\n",
            "Total loss:  -3.4653 | PDE Loss:  -5.5341 | Function Loss:  -3.5041\n",
            "Total loss:  -3.4655 | PDE Loss:  -5.5358 | Function Loss:  -3.5041\n",
            "Total loss:  -3.4657 | PDE Loss:  -5.5387 | Function Loss:  -3.504\n",
            "Total loss:  -3.4659 | PDE Loss:  -5.5399 | Function Loss:  -3.5041\n",
            "Total loss:  -3.4661 | PDE Loss:  -5.5418 | Function Loss:  -3.5042\n",
            "Total loss:  -3.4663 | PDE Loss:  -5.5414 | Function Loss:  -3.5045\n",
            "Total loss:  -3.4665 | PDE Loss:  -5.539 | Function Loss:  -3.5049\n",
            "Total loss:  -3.4667 | PDE Loss:  -5.5383 | Function Loss:  -3.5052\n",
            "Total loss:  -3.467 | PDE Loss:  -5.5358 | Function Loss:  -3.5057\n",
            "Total loss:  -3.4673 | PDE Loss:  -5.535 | Function Loss:  -3.5061\n",
            "Total loss:  -3.4675 | PDE Loss:  -5.5323 | Function Loss:  -3.5067\n",
            "Total loss:  -3.4677 | PDE Loss:  -5.5307 | Function Loss:  -3.507\n",
            "Total loss:  -3.468 | PDE Loss:  -5.5294 | Function Loss:  -3.5074\n",
            "Total loss:  -3.4682 | PDE Loss:  -5.529 | Function Loss:  -3.5077\n",
            "Total loss:  -3.4685 | PDE Loss:  -5.5293 | Function Loss:  -3.5079\n",
            "Total loss:  -3.4686 | PDE Loss:  -5.5296 | Function Loss:  -3.5081\n",
            "Total loss:  -3.4688 | PDE Loss:  -5.5296 | Function Loss:  -3.5083\n",
            "Total loss:  -3.4692 | PDE Loss:  -5.5301 | Function Loss:  -3.5086\n",
            "Total loss:  -3.4694 | PDE Loss:  -5.5315 | Function Loss:  -3.5088\n",
            "Total loss:  -3.4699 | PDE Loss:  -5.5368 | Function Loss:  -3.5088\n",
            "Total loss:  -3.4702 | PDE Loss:  -5.5377 | Function Loss:  -3.509\n",
            "Total loss:  -3.4703 | PDE Loss:  -5.5388 | Function Loss:  -3.5091\n",
            "Total loss:  -3.4705 | PDE Loss:  -5.5402 | Function Loss:  -3.5092\n",
            "Total loss:  -3.4707 | PDE Loss:  -5.5389 | Function Loss:  -3.5095\n",
            "Total loss:  -3.4709 | PDE Loss:  -5.5377 | Function Loss:  -3.5098\n",
            "Total loss:  -3.471 | PDE Loss:  -5.5359 | Function Loss:  -3.5101\n",
            "Total loss:  -3.4712 | PDE Loss:  -5.5352 | Function Loss:  -3.5104\n",
            "Total loss:  -3.4713 | PDE Loss:  -5.5335 | Function Loss:  -3.5107\n",
            "Total loss:  -3.4715 | PDE Loss:  -5.5347 | Function Loss:  -3.5108\n",
            "Total loss:  -3.4716 | PDE Loss:  -5.5347 | Function Loss:  -3.5109\n",
            "Total loss:  -3.4718 | PDE Loss:  -5.536 | Function Loss:  -3.5109\n",
            "Total loss:  -3.4719 | PDE Loss:  -5.5331 | Function Loss:  -3.5113\n",
            "Total loss:  -3.4721 | PDE Loss:  -5.5337 | Function Loss:  -3.5115\n",
            "Total loss:  -3.4722 | PDE Loss:  -5.5362 | Function Loss:  -3.5114\n",
            "Total loss:  -3.4724 | PDE Loss:  -5.5376 | Function Loss:  -3.5115\n",
            "Total loss:  -3.4726 | PDE Loss:  -5.5371 | Function Loss:  -3.5117\n",
            "Total loss:  -3.4727 | PDE Loss:  -5.5364 | Function Loss:  -3.512\n",
            "Total loss:  -3.4729 | PDE Loss:  -5.5335 | Function Loss:  -3.5124\n",
            "Total loss:  -3.4731 | PDE Loss:  -5.5309 | Function Loss:  -3.5129\n",
            "Total loss:  -3.4734 | PDE Loss:  -5.5273 | Function Loss:  -3.5135\n",
            "Total loss:  -3.4736 | PDE Loss:  -5.523 | Function Loss:  -3.5142\n",
            "Total loss:  -3.4739 | PDE Loss:  -5.521 | Function Loss:  -3.5147\n",
            "Total loss:  -3.4741 | PDE Loss:  -5.5194 | Function Loss:  -3.5151\n",
            "Total loss:  -3.4743 | PDE Loss:  -5.5196 | Function Loss:  -3.5153\n",
            "Total loss:  -3.4745 | PDE Loss:  -5.5199 | Function Loss:  -3.5155\n",
            "Total loss:  -3.4746 | PDE Loss:  -5.5214 | Function Loss:  -3.5155\n",
            "Total loss:  -3.4748 | PDE Loss:  -5.5214 | Function Loss:  -3.5157\n",
            "Total loss:  -3.4749 | PDE Loss:  -5.5223 | Function Loss:  -3.5157\n",
            "Total loss:  -3.4751 | PDE Loss:  -5.5219 | Function Loss:  -3.5159\n",
            "Total loss:  -3.4752 | PDE Loss:  -5.5212 | Function Loss:  -3.5161\n",
            "Total loss:  -3.4754 | PDE Loss:  -5.5197 | Function Loss:  -3.5165\n",
            "Total loss:  -3.4756 | PDE Loss:  -5.5183 | Function Loss:  -3.5168\n",
            "Total loss:  -3.4759 | PDE Loss:  -5.5155 | Function Loss:  -3.5174\n",
            "Total loss:  -3.4761 | PDE Loss:  -5.513 | Function Loss:  -3.518\n",
            "Total loss:  -3.4764 | PDE Loss:  -5.5108 | Function Loss:  -3.5185\n",
            "Total loss:  -3.4767 | PDE Loss:  -5.5058 | Function Loss:  -3.5193\n",
            "Total loss:  -3.4769 | PDE Loss:  -5.5059 | Function Loss:  -3.5195\n",
            "Total loss:  -3.4772 | PDE Loss:  -5.5064 | Function Loss:  -3.5198\n",
            "Total loss:  -3.4774 | PDE Loss:  -5.5077 | Function Loss:  -3.5199\n",
            "Total loss:  -3.4777 | PDE Loss:  -5.5097 | Function Loss:  -3.52\n",
            "Total loss:  -3.4779 | PDE Loss:  -5.5118 | Function Loss:  -3.52\n",
            "Total loss:  -3.4781 | PDE Loss:  -5.514 | Function Loss:  -3.5201\n",
            "Total loss:  -3.4783 | PDE Loss:  -5.5155 | Function Loss:  -3.5201\n",
            "Total loss:  -3.4784 | PDE Loss:  -5.5167 | Function Loss:  -3.5201\n",
            "Total loss:  -3.4785 | PDE Loss:  -5.518 | Function Loss:  -3.5201\n",
            "Total loss:  -3.4786 | PDE Loss:  -5.5188 | Function Loss:  -3.5202\n",
            "Total loss:  -3.4788 | PDE Loss:  -5.5192 | Function Loss:  -3.5203\n",
            "Total loss:  -3.4789 | PDE Loss:  -5.5206 | Function Loss:  -3.5203\n",
            "Total loss:  -3.479 | PDE Loss:  -5.5209 | Function Loss:  -3.5204\n",
            "Total loss:  -3.4792 | PDE Loss:  -5.5219 | Function Loss:  -3.5204\n",
            "Total loss:  -3.4792 | PDE Loss:  -5.5231 | Function Loss:  -3.5204\n",
            "Total loss:  -3.4794 | PDE Loss:  -5.5215 | Function Loss:  -3.5207\n",
            "Total loss:  -3.4795 | PDE Loss:  -5.5221 | Function Loss:  -3.5208\n",
            "Total loss:  -3.4797 | PDE Loss:  -5.5223 | Function Loss:  -3.521\n",
            "Total loss:  -3.4798 | PDE Loss:  -5.5229 | Function Loss:  -3.5211\n",
            "Total loss:  -3.48 | PDE Loss:  -5.5227 | Function Loss:  -3.5212\n",
            "Total loss:  -3.4801 | PDE Loss:  -5.5225 | Function Loss:  -3.5214\n",
            "Total loss:  -3.4803 | PDE Loss:  -5.5226 | Function Loss:  -3.5216\n",
            "Total loss:  -3.4804 | PDE Loss:  -5.5228 | Function Loss:  -3.5217\n",
            "Total loss:  -3.4806 | PDE Loss:  -5.5235 | Function Loss:  -3.5219\n",
            "Total loss:  -3.4808 | PDE Loss:  -5.5244 | Function Loss:  -3.522\n",
            "Total loss:  -3.481 | PDE Loss:  -5.5248 | Function Loss:  -3.5222\n",
            "Total loss:  -3.4812 | PDE Loss:  -5.5258 | Function Loss:  -3.5222\n",
            "Total loss:  -3.4814 | PDE Loss:  -5.5263 | Function Loss:  -3.5224\n",
            "Total loss:  -3.4815 | PDE Loss:  -5.5264 | Function Loss:  -3.5225\n",
            "Total loss:  -3.4816 | PDE Loss:  -5.5261 | Function Loss:  -3.5227\n",
            "Total loss:  -3.4818 | PDE Loss:  -5.5258 | Function Loss:  -3.5229\n",
            "Total loss:  -3.4819 | PDE Loss:  -5.5253 | Function Loss:  -3.523\n",
            "Total loss:  -3.4819 | PDE Loss:  -5.5251 | Function Loss:  -3.5231\n",
            "Total loss:  -3.482 | PDE Loss:  -5.5248 | Function Loss:  -3.5233\n",
            "Total loss:  -3.4821 | PDE Loss:  -5.525 | Function Loss:  -3.5233\n",
            "Total loss:  -3.4821 | PDE Loss:  -5.5253 | Function Loss:  -3.5234\n",
            "Total loss:  -3.4823 | PDE Loss:  -5.5259 | Function Loss:  -3.5234\n",
            "Total loss:  -3.4824 | PDE Loss:  -5.5262 | Function Loss:  -3.5235\n",
            "Total loss:  -3.4824 | PDE Loss:  -5.5275 | Function Loss:  -3.5235\n",
            "Total loss:  -3.4826 | PDE Loss:  -5.5287 | Function Loss:  -3.5235\n",
            "Total loss:  -3.4826 | PDE Loss:  -5.5294 | Function Loss:  -3.5235\n",
            "Total loss:  -3.4828 | PDE Loss:  -5.5307 | Function Loss:  -3.5235\n",
            "Total loss:  -3.4828 | PDE Loss:  -5.5307 | Function Loss:  -3.5236\n",
            "Total loss:  -3.4829 | PDE Loss:  -5.5315 | Function Loss:  -3.5236\n",
            "Total loss:  -3.483 | PDE Loss:  -5.5317 | Function Loss:  -3.5237\n",
            "Total loss:  -3.4831 | PDE Loss:  -5.5321 | Function Loss:  -3.5238\n",
            "Total loss:  -3.4832 | PDE Loss:  -5.5318 | Function Loss:  -3.5239\n",
            "Total loss:  -3.4833 | PDE Loss:  -5.5319 | Function Loss:  -3.5239\n",
            "Total loss:  -3.4833 | PDE Loss:  -5.5315 | Function Loss:  -3.524\n",
            "Total loss:  -3.4834 | PDE Loss:  -5.5314 | Function Loss:  -3.5242\n",
            "Total loss:  -3.4836 | PDE Loss:  -5.5315 | Function Loss:  -3.5243\n",
            "Total loss:  -3.4837 | PDE Loss:  -5.5316 | Function Loss:  -3.5245\n",
            "Total loss:  -3.4838 | PDE Loss:  -5.5311 | Function Loss:  -3.5246\n",
            "Total loss:  -3.484 | PDE Loss:  -5.5317 | Function Loss:  -3.5248\n",
            "Total loss:  -3.4841 | PDE Loss:  -5.5305 | Function Loss:  -3.525\n",
            "Total loss:  -3.4842 | PDE Loss:  -5.5316 | Function Loss:  -3.525\n",
            "Total loss:  -3.4843 | PDE Loss:  -5.5319 | Function Loss:  -3.5251\n",
            "Total loss:  -3.4845 | PDE Loss:  -5.5321 | Function Loss:  -3.5253\n",
            "Total loss:  -3.4846 | PDE Loss:  -5.5323 | Function Loss:  -3.5254\n",
            "Total loss:  -3.4848 | PDE Loss:  -5.5314 | Function Loss:  -3.5256\n",
            "Total loss:  -3.4849 | PDE Loss:  -5.5292 | Function Loss:  -3.526\n",
            "Total loss:  -3.485 | PDE Loss:  -5.5296 | Function Loss:  -3.526\n",
            "Total loss:  -3.4851 | PDE Loss:  -5.5295 | Function Loss:  -3.5261\n",
            "Total loss:  -3.4852 | PDE Loss:  -5.529 | Function Loss:  -3.5263\n",
            "Total loss:  -3.4853 | PDE Loss:  -5.5286 | Function Loss:  -3.5265\n",
            "Total loss:  -3.4853 | PDE Loss:  -5.5275 | Function Loss:  -3.5266\n",
            "Total loss:  -3.4854 | PDE Loss:  -5.5272 | Function Loss:  -3.5268\n",
            "Total loss:  -3.4855 | PDE Loss:  -5.5264 | Function Loss:  -3.527\n",
            "Total loss:  -3.4856 | PDE Loss:  -5.5263 | Function Loss:  -3.5271\n",
            "Total loss:  -3.4857 | PDE Loss:  -5.5261 | Function Loss:  -3.5272\n",
            "Total loss:  -3.4858 | PDE Loss:  -5.5266 | Function Loss:  -3.5273\n",
            "Total loss:  -3.4859 | PDE Loss:  -5.5271 | Function Loss:  -3.5274\n",
            "Total loss:  -3.486 | PDE Loss:  -5.5269 | Function Loss:  -3.5274\n",
            "Total loss:  -3.4861 | PDE Loss:  -5.5281 | Function Loss:  -3.5275\n",
            "Total loss:  -3.4863 | PDE Loss:  -5.5293 | Function Loss:  -3.5275\n",
            "Total loss:  -3.4864 | PDE Loss:  -5.5299 | Function Loss:  -3.5275\n",
            "Total loss:  -3.4864 | PDE Loss:  -5.5301 | Function Loss:  -3.5276\n",
            "Total loss:  -3.4865 | PDE Loss:  -5.5296 | Function Loss:  -3.5277\n",
            "Total loss:  -3.4866 | PDE Loss:  -5.5291 | Function Loss:  -3.5279\n",
            "Total loss:  -3.4867 | PDE Loss:  -5.5278 | Function Loss:  -3.5281\n",
            "Total loss:  -3.4868 | PDE Loss:  -5.5261 | Function Loss:  -3.5284\n",
            "Total loss:  -3.487 | PDE Loss:  -5.5235 | Function Loss:  -3.5289\n",
            "Total loss:  -3.4872 | PDE Loss:  -5.5178 | Function Loss:  -3.5296\n",
            "Total loss:  -3.4872 | PDE Loss:  -5.5175 | Function Loss:  -3.5297\n",
            "Total loss:  -3.4874 | PDE Loss:  -5.5164 | Function Loss:  -3.5301\n",
            "Total loss:  -3.4875 | PDE Loss:  -5.5152 | Function Loss:  -3.5303\n",
            "Total loss:  -3.4876 | PDE Loss:  -5.5144 | Function Loss:  -3.5305\n",
            "Total loss:  -3.4877 | PDE Loss:  -5.5137 | Function Loss:  -3.5307\n",
            "Total loss:  -3.4878 | PDE Loss:  -5.5131 | Function Loss:  -3.5308\n",
            "Total loss:  -3.4879 | PDE Loss:  -5.5135 | Function Loss:  -3.5309\n",
            "Total loss:  -3.488 | PDE Loss:  -5.5124 | Function Loss:  -3.5311\n",
            "Total loss:  -3.4881 | PDE Loss:  -5.5123 | Function Loss:  -3.5312\n",
            "Total loss:  -3.4882 | PDE Loss:  -5.5129 | Function Loss:  -3.5313\n",
            "Total loss:  -3.4883 | PDE Loss:  -5.5128 | Function Loss:  -3.5314\n",
            "Total loss:  -3.4884 | PDE Loss:  -5.512 | Function Loss:  -3.5316\n",
            "Total loss:  -3.4885 | PDE Loss:  -5.515 | Function Loss:  -3.5314\n",
            "Total loss:  -3.4887 | PDE Loss:  -5.5113 | Function Loss:  -3.532\n",
            "Total loss:  -3.4888 | PDE Loss:  -5.5098 | Function Loss:  -3.5323\n",
            "Total loss:  -3.489 | PDE Loss:  -5.5072 | Function Loss:  -3.5328\n",
            "Total loss:  -3.4893 | PDE Loss:  -5.5058 | Function Loss:  -3.5333\n",
            "Total loss:  -3.4897 | PDE Loss:  -5.5051 | Function Loss:  -3.5337\n",
            "Total loss:  -3.4899 | PDE Loss:  -5.5036 | Function Loss:  -3.5342\n",
            "Total loss:  -3.4902 | PDE Loss:  -5.5053 | Function Loss:  -3.5343\n",
            "Total loss:  -3.4904 | PDE Loss:  -5.5072 | Function Loss:  -3.5343\n",
            "Total loss:  -3.4905 | PDE Loss:  -5.5102 | Function Loss:  -3.5342\n",
            "Total loss:  -3.4906 | PDE Loss:  -5.5114 | Function Loss:  -3.5341\n",
            "Total loss:  -3.4907 | PDE Loss:  -5.513 | Function Loss:  -3.5341\n",
            "Total loss:  -3.4908 | PDE Loss:  -5.5139 | Function Loss:  -3.5341\n",
            "Total loss:  -3.4909 | PDE Loss:  -5.5144 | Function Loss:  -3.5342\n",
            "Total loss:  -3.4911 | PDE Loss:  -5.5158 | Function Loss:  -3.5342\n",
            "Total loss:  -3.4912 | PDE Loss:  -5.5147 | Function Loss:  -3.5344\n",
            "Total loss:  -3.4914 | PDE Loss:  -5.5152 | Function Loss:  -3.5346\n",
            "Total loss:  -3.4915 | PDE Loss:  -5.5144 | Function Loss:  -3.5348\n",
            "Total loss:  -3.4917 | PDE Loss:  -5.5129 | Function Loss:  -3.5352\n",
            "Total loss:  -3.492 | PDE Loss:  -5.5111 | Function Loss:  -3.5357\n",
            "Total loss:  -3.4921 | PDE Loss:  -5.5074 | Function Loss:  -3.5362\n",
            "Total loss:  -3.4924 | PDE Loss:  -5.5037 | Function Loss:  -3.5369\n",
            "Total loss:  -3.4926 | PDE Loss:  -5.5034 | Function Loss:  -3.5371\n",
            "Total loss:  -3.4929 | PDE Loss:  -5.5029 | Function Loss:  -3.5376\n",
            "Total loss:  -3.4933 | PDE Loss:  -5.5027 | Function Loss:  -3.538\n",
            "Total loss:  -3.4937 | PDE Loss:  -5.5021 | Function Loss:  -3.5385\n",
            "Total loss:  -3.4939 | PDE Loss:  -5.5014 | Function Loss:  -3.5388\n",
            "Total loss:  -3.4941 | PDE Loss:  -5.5013 | Function Loss:  -3.539\n",
            "Total loss:  -3.4943 | PDE Loss:  -5.5008 | Function Loss:  -3.5393\n",
            "Total loss:  -3.4945 | PDE Loss:  -5.5015 | Function Loss:  -3.5395\n",
            "Total loss:  -3.4947 | PDE Loss:  -5.5009 | Function Loss:  -3.5398\n",
            "Total loss:  -3.4949 | PDE Loss:  -5.5015 | Function Loss:  -3.54\n",
            "Total loss:  -3.4952 | PDE Loss:  -5.5011 | Function Loss:  -3.5403\n",
            "Total loss:  -3.4953 | PDE Loss:  -5.5014 | Function Loss:  -3.5404\n",
            "Total loss:  -3.4955 | PDE Loss:  -5.5028 | Function Loss:  -3.5404\n",
            "Total loss:  -3.4956 | PDE Loss:  -5.5024 | Function Loss:  -3.5406\n",
            "Total loss:  -3.4957 | PDE Loss:  -5.5024 | Function Loss:  -3.5407\n",
            "Total loss:  -3.4959 | PDE Loss:  -5.503 | Function Loss:  -3.5409\n",
            "Total loss:  -3.4961 | PDE Loss:  -5.5028 | Function Loss:  -3.5412\n",
            "Total loss:  -3.4964 | PDE Loss:  -5.503 | Function Loss:  -3.5414\n",
            "Total loss:  -3.4965 | PDE Loss:  -5.5029 | Function Loss:  -3.5416\n",
            "Total loss:  -3.4967 | PDE Loss:  -5.5028 | Function Loss:  -3.5418\n",
            "Total loss:  -3.4969 | PDE Loss:  -5.5025 | Function Loss:  -3.542\n",
            "Total loss:  -3.4971 | PDE Loss:  -5.5024 | Function Loss:  -3.5423\n",
            "Total loss:  -3.4972 | PDE Loss:  -5.5027 | Function Loss:  -3.5424\n",
            "Total loss:  -3.4974 | PDE Loss:  -5.5027 | Function Loss:  -3.5426\n",
            "Total loss:  -3.4976 | PDE Loss:  -5.5032 | Function Loss:  -3.5427\n",
            "Total loss:  -3.4978 | PDE Loss:  -5.5045 | Function Loss:  -3.5428\n",
            "Total loss:  -3.4981 | PDE Loss:  -5.5081 | Function Loss:  -3.5428\n",
            "Total loss:  -3.4984 | PDE Loss:  -5.51 | Function Loss:  -3.5429\n",
            "Total loss:  -3.4986 | PDE Loss:  -5.5123 | Function Loss:  -3.5429\n",
            "Total loss:  -3.4988 | PDE Loss:  -5.5143 | Function Loss:  -3.5429\n",
            "Total loss:  -3.499 | PDE Loss:  -5.5152 | Function Loss:  -3.543\n",
            "Total loss:  -3.4991 | PDE Loss:  -5.5157 | Function Loss:  -3.543\n",
            "Total loss:  -3.4992 | PDE Loss:  -5.5155 | Function Loss:  -3.5432\n",
            "Total loss:  -3.4994 | PDE Loss:  -5.515 | Function Loss:  -3.5434\n",
            "Total loss:  -3.4996 | PDE Loss:  -5.5145 | Function Loss:  -3.5437\n",
            "Total loss:  -3.4998 | PDE Loss:  -5.5135 | Function Loss:  -3.5441\n",
            "Total loss:  -3.5 | PDE Loss:  -5.5123 | Function Loss:  -3.5444\n",
            "Total loss:  -3.5002 | PDE Loss:  -5.5109 | Function Loss:  -3.5447\n",
            "Total loss:  -3.5004 | PDE Loss:  -5.5105 | Function Loss:  -3.545\n",
            "Total loss:  -3.5006 | PDE Loss:  -5.5093 | Function Loss:  -3.5454\n",
            "Total loss:  -3.5008 | PDE Loss:  -5.5091 | Function Loss:  -3.5456\n",
            "Total loss:  -3.501 | PDE Loss:  -5.5081 | Function Loss:  -3.5459\n",
            "Total loss:  -3.5012 | PDE Loss:  -5.507 | Function Loss:  -3.5463\n",
            "Total loss:  -3.5013 | PDE Loss:  -5.5052 | Function Loss:  -3.5467\n",
            "Total loss:  -3.5015 | PDE Loss:  -5.5038 | Function Loss:  -3.547\n",
            "Total loss:  -3.5017 | PDE Loss:  -5.5024 | Function Loss:  -3.5473\n",
            "Total loss:  -3.5018 | PDE Loss:  -5.5016 | Function Loss:  -3.5475\n",
            "Total loss:  -3.5018 | PDE Loss:  -5.5008 | Function Loss:  -3.5477\n",
            "Total loss:  -3.502 | PDE Loss:  -5.4999 | Function Loss:  -3.548\n",
            "Total loss:  -3.5016 | PDE Loss:  -5.4949 | Function Loss:  -3.5481\n",
            "Total loss:  -3.5021 | PDE Loss:  -5.4994 | Function Loss:  -3.5481\n",
            "Total loss:  -3.5023 | PDE Loss:  -5.4974 | Function Loss:  -3.5486\n",
            "Total loss:  -3.5025 | PDE Loss:  -5.4957 | Function Loss:  -3.549\n",
            "Total loss:  -3.5028 | PDE Loss:  -5.4938 | Function Loss:  -3.5495\n",
            "Total loss:  -3.503 | PDE Loss:  -5.4922 | Function Loss:  -3.55\n",
            "Total loss:  -3.5032 | PDE Loss:  -5.49 | Function Loss:  -3.5505\n",
            "Total loss:  -3.5034 | PDE Loss:  -5.4886 | Function Loss:  -3.5508\n",
            "Total loss:  -3.5035 | PDE Loss:  -5.4869 | Function Loss:  -3.5512\n",
            "Total loss:  -3.5036 | PDE Loss:  -5.4858 | Function Loss:  -3.5514\n",
            "Total loss:  -3.5038 | PDE Loss:  -5.4851 | Function Loss:  -3.5516\n",
            "Total loss:  -3.5039 | PDE Loss:  -5.4842 | Function Loss:  -3.5519\n",
            "Total loss:  -3.504 | PDE Loss:  -5.4837 | Function Loss:  -3.552\n",
            "Total loss:  -3.5041 | PDE Loss:  -5.4842 | Function Loss:  -3.5521\n",
            "Total loss:  -3.5042 | PDE Loss:  -5.4843 | Function Loss:  -3.5522\n",
            "Total loss:  -3.5043 | PDE Loss:  -5.4847 | Function Loss:  -3.5522\n",
            "Total loss:  -3.5043 | PDE Loss:  -5.4853 | Function Loss:  -3.5523\n",
            "Total loss:  -3.5039 | PDE Loss:  -5.4838 | Function Loss:  -3.552\n",
            "Total loss:  -3.5044 | PDE Loss:  -5.4855 | Function Loss:  -3.5523\n",
            "Total loss:  -3.5045 | PDE Loss:  -5.4859 | Function Loss:  -3.5524\n",
            "Total loss:  -3.5046 | PDE Loss:  -5.4867 | Function Loss:  -3.5524\n",
            "Total loss:  -3.5048 | PDE Loss:  -5.4866 | Function Loss:  -3.5526\n",
            "Total loss:  -3.5049 | PDE Loss:  -5.4871 | Function Loss:  -3.5526\n",
            "Total loss:  -3.505 | PDE Loss:  -5.4873 | Function Loss:  -3.5528\n",
            "Total loss:  -3.5052 | PDE Loss:  -5.4875 | Function Loss:  -3.5529\n",
            "Total loss:  -3.5053 | PDE Loss:  -5.4877 | Function Loss:  -3.5531\n",
            "Total loss:  -3.5055 | PDE Loss:  -5.4875 | Function Loss:  -3.5532\n",
            "Total loss:  -3.5056 | PDE Loss:  -5.4878 | Function Loss:  -3.5534\n",
            "Total loss:  -3.5058 | PDE Loss:  -5.4875 | Function Loss:  -3.5537\n",
            "Total loss:  -3.506 | PDE Loss:  -5.4876 | Function Loss:  -3.5539\n",
            "Total loss:  -3.5063 | PDE Loss:  -5.4888 | Function Loss:  -3.554\n",
            "Total loss:  -3.5065 | PDE Loss:  -5.4901 | Function Loss:  -3.5541\n",
            "Total loss:  -3.5067 | PDE Loss:  -5.492 | Function Loss:  -3.5541\n",
            "Total loss:  -3.5068 | PDE Loss:  -5.4936 | Function Loss:  -3.5541\n",
            "Total loss:  -3.5069 | PDE Loss:  -5.4946 | Function Loss:  -3.5541\n",
            "Total loss:  -3.5071 | PDE Loss:  -5.4958 | Function Loss:  -3.5541\n",
            "Total loss:  -3.5072 | PDE Loss:  -5.4972 | Function Loss:  -3.5541\n",
            "Total loss:  -3.5075 | PDE Loss:  -5.4984 | Function Loss:  -3.5542\n",
            "Total loss:  -3.5077 | PDE Loss:  -5.4983 | Function Loss:  -3.5545\n",
            "Total loss:  -3.508 | PDE Loss:  -5.499 | Function Loss:  -3.5548\n",
            "Total loss:  -3.5082 | PDE Loss:  -5.4978 | Function Loss:  -3.5551\n",
            "Total loss:  -3.5083 | PDE Loss:  -5.4971 | Function Loss:  -3.5553\n",
            "Total loss:  -3.5085 | PDE Loss:  -5.4967 | Function Loss:  -3.5555\n",
            "Total loss:  -3.5086 | PDE Loss:  -5.496 | Function Loss:  -3.5558\n",
            "Total loss:  -3.5087 | PDE Loss:  -5.4958 | Function Loss:  -3.5559\n",
            "Total loss:  -3.5088 | PDE Loss:  -5.496 | Function Loss:  -3.556\n",
            "Total loss:  -3.5067 | PDE Loss:  -5.4801 | Function Loss:  -3.5555\n",
            "Total loss:  -3.5088 | PDE Loss:  -5.4951 | Function Loss:  -3.5561\n",
            "Total loss:  -3.5089 | PDE Loss:  -5.4954 | Function Loss:  -3.5562\n",
            "Total loss:  -3.509 | PDE Loss:  -5.4961 | Function Loss:  -3.5563\n",
            "Total loss:  -3.5091 | PDE Loss:  -5.4964 | Function Loss:  -3.5563\n",
            "Total loss:  -3.5092 | PDE Loss:  -5.4972 | Function Loss:  -3.5564\n",
            "Total loss:  -3.5093 | PDE Loss:  -5.4978 | Function Loss:  -3.5564\n",
            "Total loss:  -3.5094 | PDE Loss:  -5.4977 | Function Loss:  -3.5565\n",
            "Total loss:  -3.5095 | PDE Loss:  -5.498 | Function Loss:  -3.5565\n",
            "Total loss:  -3.5096 | PDE Loss:  -5.4981 | Function Loss:  -3.5566\n",
            "Total loss:  -3.5097 | PDE Loss:  -5.4981 | Function Loss:  -3.5568\n",
            "Total loss:  -3.5098 | PDE Loss:  -5.4981 | Function Loss:  -3.5569\n",
            "Total loss:  -3.5099 | PDE Loss:  -5.4977 | Function Loss:  -3.5571\n",
            "Total loss:  -3.51 | PDE Loss:  -5.4976 | Function Loss:  -3.5572\n",
            "Total loss:  -3.5101 | PDE Loss:  -5.4973 | Function Loss:  -3.5573\n",
            "Total loss:  -3.5102 | PDE Loss:  -5.4978 | Function Loss:  -3.5574\n",
            "Total loss:  -3.5104 | PDE Loss:  -5.497 | Function Loss:  -3.5577\n",
            "Total loss:  -3.5105 | PDE Loss:  -5.4971 | Function Loss:  -3.5578\n",
            "Total loss:  -3.5106 | PDE Loss:  -5.4967 | Function Loss:  -3.5579\n",
            "Total loss:  -3.5107 | PDE Loss:  -5.4963 | Function Loss:  -3.5581\n",
            "Total loss:  -3.5108 | PDE Loss:  -5.4954 | Function Loss:  -3.5583\n",
            "Total loss:  -3.511 | PDE Loss:  -5.4955 | Function Loss:  -3.5585\n",
            "Total loss:  -3.511 | PDE Loss:  -5.4944 | Function Loss:  -3.5586\n",
            "Total loss:  -3.5111 | PDE Loss:  -5.4943 | Function Loss:  -3.5588\n",
            "Total loss:  -3.5113 | PDE Loss:  -5.4948 | Function Loss:  -3.5589\n",
            "Total loss:  -3.5114 | PDE Loss:  -5.4951 | Function Loss:  -3.559\n",
            "Total loss:  -3.5114 | PDE Loss:  -5.4953 | Function Loss:  -3.559\n",
            "Total loss:  -3.5115 | PDE Loss:  -5.4962 | Function Loss:  -3.559\n",
            "Total loss:  -3.5115 | PDE Loss:  -5.4976 | Function Loss:  -3.5589\n",
            "Total loss:  -3.5116 | PDE Loss:  -5.4988 | Function Loss:  -3.5588\n",
            "Total loss:  -3.5117 | PDE Loss:  -5.5008 | Function Loss:  -3.5586\n",
            "Total loss:  -3.5117 | PDE Loss:  -5.5017 | Function Loss:  -3.5586\n",
            "Total loss:  -3.5118 | PDE Loss:  -5.5038 | Function Loss:  -3.5584\n",
            "Total loss:  -3.5119 | PDE Loss:  -5.5047 | Function Loss:  -3.5584\n",
            "Total loss:  -3.5121 | PDE Loss:  -5.5056 | Function Loss:  -3.5586\n",
            "Total loss:  -3.5122 | PDE Loss:  -5.5053 | Function Loss:  -3.5588\n",
            "Total loss:  -3.5125 | PDE Loss:  -5.5049 | Function Loss:  -3.5591\n",
            "Total loss:  -3.5126 | PDE Loss:  -5.5047 | Function Loss:  -3.5592\n",
            "Total loss:  -3.5128 | PDE Loss:  -5.5039 | Function Loss:  -3.5595\n",
            "Total loss:  -3.5129 | PDE Loss:  -5.5036 | Function Loss:  -3.5597\n",
            "Total loss:  -3.5131 | PDE Loss:  -5.5035 | Function Loss:  -3.5599\n",
            "Total loss:  -3.5131 | PDE Loss:  -5.5033 | Function Loss:  -3.56\n",
            "Total loss:  -3.5133 | PDE Loss:  -5.5042 | Function Loss:  -3.5601\n",
            "Total loss:  -3.5134 | PDE Loss:  -5.5056 | Function Loss:  -3.5601\n",
            "Total loss:  -3.5136 | PDE Loss:  -5.5069 | Function Loss:  -3.5601\n",
            "Total loss:  -3.5137 | PDE Loss:  -5.5082 | Function Loss:  -3.56\n",
            "Total loss:  -3.5137 | PDE Loss:  -5.5097 | Function Loss:  -3.5599\n",
            "Total loss:  -3.5138 | PDE Loss:  -5.5104 | Function Loss:  -3.5599\n",
            "Total loss:  -3.5139 | PDE Loss:  -5.5104 | Function Loss:  -3.56\n",
            "Total loss:  -3.5139 | PDE Loss:  -5.5102 | Function Loss:  -3.5601\n",
            "Total loss:  -3.514 | PDE Loss:  -5.5092 | Function Loss:  -3.5603\n",
            "Total loss:  -3.5141 | PDE Loss:  -5.5081 | Function Loss:  -3.5605\n",
            "Total loss:  -3.5142 | PDE Loss:  -5.5071 | Function Loss:  -3.5607\n",
            "Total loss:  -3.5142 | PDE Loss:  -5.5047 | Function Loss:  -3.5611\n",
            "Total loss:  -3.5144 | PDE Loss:  -5.5034 | Function Loss:  -3.5614\n",
            "Total loss:  -3.5145 | PDE Loss:  -5.5033 | Function Loss:  -3.5615\n",
            "Total loss:  -3.5146 | PDE Loss:  -5.504 | Function Loss:  -3.5616\n",
            "Total loss:  -3.5148 | PDE Loss:  -5.5051 | Function Loss:  -3.5616\n",
            "Total loss:  -3.5149 | PDE Loss:  -5.507 | Function Loss:  -3.5615\n",
            "Total loss:  -3.515 | PDE Loss:  -5.5093 | Function Loss:  -3.5614\n",
            "Total loss:  -3.5152 | PDE Loss:  -5.5121 | Function Loss:  -3.5613\n",
            "Total loss:  -3.5153 | PDE Loss:  -5.515 | Function Loss:  -3.5611\n",
            "Total loss:  -3.5155 | PDE Loss:  -5.5186 | Function Loss:  -3.5609\n",
            "Total loss:  -3.5157 | PDE Loss:  -5.5227 | Function Loss:  -3.5607\n",
            "Total loss:  -3.5159 | PDE Loss:  -5.5254 | Function Loss:  -3.5606\n",
            "Total loss:  -3.516 | PDE Loss:  -5.5277 | Function Loss:  -3.5605\n",
            "Total loss:  -3.5161 | PDE Loss:  -5.5283 | Function Loss:  -3.5606\n",
            "Total loss:  -3.5162 | PDE Loss:  -5.5282 | Function Loss:  -3.5607\n",
            "Total loss:  -3.5163 | PDE Loss:  -5.5271 | Function Loss:  -3.5609\n",
            "Total loss:  -3.5164 | PDE Loss:  -5.526 | Function Loss:  -3.5611\n",
            "Total loss:  -3.5165 | PDE Loss:  -5.5247 | Function Loss:  -3.5614\n",
            "Total loss:  -3.5167 | PDE Loss:  -5.523 | Function Loss:  -3.5618\n",
            "Total loss:  -3.5169 | PDE Loss:  -5.5222 | Function Loss:  -3.5621\n",
            "Total loss:  -3.5171 | PDE Loss:  -5.5207 | Function Loss:  -3.5624\n",
            "Total loss:  -3.5173 | PDE Loss:  -5.5205 | Function Loss:  -3.5627\n",
            "Total loss:  -3.5174 | PDE Loss:  -5.522 | Function Loss:  -3.5627\n",
            "Total loss:  -3.5175 | PDE Loss:  -5.5234 | Function Loss:  -3.5627\n",
            "Total loss:  -3.5177 | PDE Loss:  -5.5251 | Function Loss:  -3.5626\n",
            "Total loss:  -3.5178 | PDE Loss:  -5.5261 | Function Loss:  -3.5626\n",
            "Total loss:  -3.5179 | PDE Loss:  -5.5268 | Function Loss:  -3.5627\n",
            "Total loss:  -3.5181 | PDE Loss:  -5.5268 | Function Loss:  -3.5629\n",
            "Total loss:  -3.5182 | PDE Loss:  -5.526 | Function Loss:  -3.5631\n",
            "Total loss:  -3.5183 | PDE Loss:  -5.5246 | Function Loss:  -3.5634\n",
            "Total loss:  -3.5185 | PDE Loss:  -5.5224 | Function Loss:  -3.5638\n",
            "Total loss:  -3.5186 | PDE Loss:  -5.5189 | Function Loss:  -3.5643\n",
            "Total loss:  -3.5187 | PDE Loss:  -5.5178 | Function Loss:  -3.5646\n",
            "Total loss:  -3.5188 | PDE Loss:  -5.5174 | Function Loss:  -3.5647\n",
            "Total loss:  -3.5189 | PDE Loss:  -5.5173 | Function Loss:  -3.5649\n",
            "Total loss:  -3.519 | PDE Loss:  -5.5175 | Function Loss:  -3.5649\n",
            "Total loss:  -3.5191 | PDE Loss:  -5.5176 | Function Loss:  -3.565\n",
            "Total loss:  -3.5191 | PDE Loss:  -5.5184 | Function Loss:  -3.565\n",
            "Total loss:  -3.5192 | PDE Loss:  -5.5185 | Function Loss:  -3.5651\n",
            "Total loss:  -3.5193 | PDE Loss:  -5.5196 | Function Loss:  -3.565\n",
            "Total loss:  -3.5194 | PDE Loss:  -5.5205 | Function Loss:  -3.5651\n",
            "Total loss:  -3.5195 | PDE Loss:  -5.5211 | Function Loss:  -3.5651\n",
            "Total loss:  -3.5196 | PDE Loss:  -5.5215 | Function Loss:  -3.5651\n",
            "Total loss:  -3.5197 | PDE Loss:  -5.5225 | Function Loss:  -3.5651\n",
            "Total loss:  -3.5197 | PDE Loss:  -5.5228 | Function Loss:  -3.5652\n",
            "Total loss:  -3.5199 | PDE Loss:  -5.5239 | Function Loss:  -3.5652\n",
            "Total loss:  -3.52 | PDE Loss:  -5.5252 | Function Loss:  -3.5652\n",
            "Total loss:  -3.5201 | PDE Loss:  -5.5273 | Function Loss:  -3.5651\n",
            "Total loss:  -3.5202 | PDE Loss:  -5.529 | Function Loss:  -3.565\n",
            "Total loss:  -3.5204 | PDE Loss:  -5.5307 | Function Loss:  -3.565\n",
            "Total loss:  -3.5205 | PDE Loss:  -5.5322 | Function Loss:  -3.565\n",
            "Total loss:  -3.5206 | PDE Loss:  -5.5342 | Function Loss:  -3.5649\n",
            "Total loss:  -3.5208 | PDE Loss:  -5.5351 | Function Loss:  -3.565\n",
            "Total loss:  -3.5207 | PDE Loss:  -5.5367 | Function Loss:  -3.5647\n",
            "Total loss:  -3.5209 | PDE Loss:  -5.5363 | Function Loss:  -3.5649\n",
            "Total loss:  -3.521 | PDE Loss:  -5.5372 | Function Loss:  -3.565\n",
            "Total loss:  -3.5212 | PDE Loss:  -5.5379 | Function Loss:  -3.5651\n",
            "Total loss:  -3.5214 | PDE Loss:  -5.5372 | Function Loss:  -3.5654\n",
            "Total loss:  -3.5215 | PDE Loss:  -5.5378 | Function Loss:  -3.5655\n",
            "Total loss:  -3.5216 | PDE Loss:  -5.5379 | Function Loss:  -3.5656\n",
            "Total loss:  -3.5218 | PDE Loss:  -5.538 | Function Loss:  -3.5658\n",
            "Total loss:  -3.5219 | PDE Loss:  -5.5379 | Function Loss:  -3.5659\n",
            "Total loss:  -3.522 | PDE Loss:  -5.538 | Function Loss:  -3.566\n",
            "Total loss:  -3.5222 | PDE Loss:  -5.5382 | Function Loss:  -3.5662\n",
            "Total loss:  -3.5225 | PDE Loss:  -5.5402 | Function Loss:  -3.5663\n",
            "Total loss:  -3.5226 | PDE Loss:  -5.5372 | Function Loss:  -3.5667\n",
            "Total loss:  -3.5227 | PDE Loss:  -5.5395 | Function Loss:  -3.5667\n",
            "Total loss:  -3.5229 | PDE Loss:  -5.5417 | Function Loss:  -3.5666\n",
            "Total loss:  -3.5231 | PDE Loss:  -5.5431 | Function Loss:  -3.5667\n",
            "Total loss:  -3.5232 | PDE Loss:  -5.5441 | Function Loss:  -3.5667\n",
            "Total loss:  -3.5233 | PDE Loss:  -5.5451 | Function Loss:  -3.5667\n",
            "Total loss:  -3.5234 | PDE Loss:  -5.5443 | Function Loss:  -3.5669\n",
            "Total loss:  -3.5235 | PDE Loss:  -5.5453 | Function Loss:  -3.5669\n",
            "Total loss:  -3.5236 | PDE Loss:  -5.5461 | Function Loss:  -3.5669\n",
            "Total loss:  -3.5237 | PDE Loss:  -5.547 | Function Loss:  -3.5669\n",
            "Total loss:  -3.5238 | PDE Loss:  -5.5477 | Function Loss:  -3.567\n",
            "Total loss:  -3.5239 | PDE Loss:  -5.5483 | Function Loss:  -3.567\n",
            "Total loss:  -3.524 | PDE Loss:  -5.5491 | Function Loss:  -3.567\n",
            "Total loss:  -3.524 | PDE Loss:  -5.5499 | Function Loss:  -3.567\n",
            "Total loss:  -3.5241 | PDE Loss:  -5.5507 | Function Loss:  -3.567\n",
            "Total loss:  -3.5242 | PDE Loss:  -5.5513 | Function Loss:  -3.5671\n",
            "Total loss:  -3.5244 | PDE Loss:  -5.5519 | Function Loss:  -3.5672\n",
            "Total loss:  -3.5245 | PDE Loss:  -5.5525 | Function Loss:  -3.5673\n",
            "Total loss:  -3.5247 | PDE Loss:  -5.553 | Function Loss:  -3.5675\n",
            "Total loss:  -3.525 | PDE Loss:  -5.5545 | Function Loss:  -3.5676\n",
            "Total loss:  -3.5253 | PDE Loss:  -5.5559 | Function Loss:  -3.5677\n",
            "Total loss:  -3.5255 | PDE Loss:  -5.5573 | Function Loss:  -3.5678\n",
            "Total loss:  -3.5256 | PDE Loss:  -5.5579 | Function Loss:  -3.568\n",
            "Total loss:  -3.5258 | PDE Loss:  -5.5587 | Function Loss:  -3.568\n",
            "Total loss:  -3.5258 | PDE Loss:  -5.5585 | Function Loss:  -3.5681\n",
            "Total loss:  -3.5259 | PDE Loss:  -5.5593 | Function Loss:  -3.5682\n",
            "Total loss:  -3.526 | PDE Loss:  -5.5594 | Function Loss:  -3.5682\n",
            "Total loss:  -3.5261 | PDE Loss:  -5.5598 | Function Loss:  -3.5683\n",
            "Total loss:  -3.5262 | PDE Loss:  -5.56 | Function Loss:  -3.5684\n",
            "Total loss:  -3.5263 | PDE Loss:  -5.5605 | Function Loss:  -3.5685\n",
            "Total loss:  -3.5264 | PDE Loss:  -5.5617 | Function Loss:  -3.5685\n",
            "Total loss:  -3.5252 | PDE Loss:  -5.5504 | Function Loss:  -3.5682\n",
            "Total loss:  -3.5265 | PDE Loss:  -5.5608 | Function Loss:  -3.5686\n",
            "Total loss:  -3.5266 | PDE Loss:  -5.5608 | Function Loss:  -3.5687\n",
            "Total loss:  -3.5267 | PDE Loss:  -5.5614 | Function Loss:  -3.5687\n",
            "Total loss:  -3.5268 | PDE Loss:  -5.5625 | Function Loss:  -3.5688\n",
            "Total loss:  -3.5269 | PDE Loss:  -5.5629 | Function Loss:  -3.5688\n",
            "Total loss:  -3.5269 | PDE Loss:  -5.5626 | Function Loss:  -3.5689\n",
            "Total loss:  -3.527 | PDE Loss:  -5.5625 | Function Loss:  -3.569\n",
            "Total loss:  -3.5271 | PDE Loss:  -5.5622 | Function Loss:  -3.5691\n",
            "Total loss:  -3.5272 | PDE Loss:  -5.5616 | Function Loss:  -3.5693\n",
            "Total loss:  -3.5273 | PDE Loss:  -5.5607 | Function Loss:  -3.5695\n",
            "Total loss:  -3.5274 | PDE Loss:  -5.5597 | Function Loss:  -3.5697\n",
            "Total loss:  -3.5276 | PDE Loss:  -5.5588 | Function Loss:  -3.57\n",
            "Total loss:  -3.5277 | PDE Loss:  -5.5566 | Function Loss:  -3.5704\n",
            "Total loss:  -3.5279 | PDE Loss:  -5.5569 | Function Loss:  -3.5706\n",
            "Total loss:  -3.5281 | PDE Loss:  -5.5568 | Function Loss:  -3.5708\n",
            "Total loss:  -3.5283 | PDE Loss:  -5.5571 | Function Loss:  -3.571\n",
            "Total loss:  -3.5285 | PDE Loss:  -5.5571 | Function Loss:  -3.5712\n",
            "Total loss:  -3.5288 | PDE Loss:  -5.5568 | Function Loss:  -3.5715\n",
            "Total loss:  -3.529 | PDE Loss:  -5.5566 | Function Loss:  -3.5718\n",
            "Total loss:  -3.5291 | PDE Loss:  -5.5563 | Function Loss:  -3.572\n",
            "Total loss:  -3.5293 | PDE Loss:  -5.5543 | Function Loss:  -3.5723\n",
            "Total loss:  -3.5293 | PDE Loss:  -5.5546 | Function Loss:  -3.5724\n",
            "Total loss:  -3.5294 | PDE Loss:  -5.5547 | Function Loss:  -3.5725\n",
            "Total loss:  -3.5296 | PDE Loss:  -5.5554 | Function Loss:  -3.5725\n",
            "Total loss:  -3.5297 | PDE Loss:  -5.556 | Function Loss:  -3.5726\n",
            "Total loss:  -3.5298 | PDE Loss:  -5.5573 | Function Loss:  -3.5726\n",
            "Total loss:  -3.5299 | PDE Loss:  -5.5592 | Function Loss:  -3.5726\n",
            "Total loss:  -3.5301 | PDE Loss:  -5.5609 | Function Loss:  -3.5725\n",
            "Total loss:  -3.5302 | PDE Loss:  -5.5635 | Function Loss:  -3.5724\n",
            "Total loss:  -3.5303 | PDE Loss:  -5.5654 | Function Loss:  -3.5724\n",
            "Total loss:  -3.5305 | PDE Loss:  -5.5677 | Function Loss:  -3.5723\n",
            "Total loss:  -3.5306 | PDE Loss:  -5.5696 | Function Loss:  -3.5723\n",
            "Total loss:  -3.5308 | PDE Loss:  -5.5705 | Function Loss:  -3.5723\n",
            "Total loss:  -3.531 | PDE Loss:  -5.571 | Function Loss:  -3.5725\n",
            "Total loss:  -3.5311 | PDE Loss:  -5.571 | Function Loss:  -3.5726\n",
            "Total loss:  -3.5313 | PDE Loss:  -5.5717 | Function Loss:  -3.5728\n",
            "Total loss:  -3.5314 | PDE Loss:  -5.5715 | Function Loss:  -3.5729\n",
            "Total loss:  -3.5315 | PDE Loss:  -5.5722 | Function Loss:  -3.573\n",
            "Total loss:  -3.5317 | PDE Loss:  -5.5725 | Function Loss:  -3.5732\n",
            "Total loss:  -3.5319 | PDE Loss:  -5.5747 | Function Loss:  -3.5732\n",
            "Total loss:  -3.5321 | PDE Loss:  -5.5753 | Function Loss:  -3.5733\n",
            "Total loss:  -3.5322 | PDE Loss:  -5.5769 | Function Loss:  -3.5733\n",
            "Total loss:  -3.5324 | PDE Loss:  -5.5781 | Function Loss:  -3.5733\n",
            "Total loss:  -3.5326 | PDE Loss:  -5.5799 | Function Loss:  -3.5734\n",
            "Total loss:  -3.5327 | PDE Loss:  -5.5816 | Function Loss:  -3.5734\n",
            "Total loss:  -3.5329 | PDE Loss:  -5.5832 | Function Loss:  -3.5734\n",
            "Total loss:  -3.5331 | PDE Loss:  -5.585 | Function Loss:  -3.5734\n",
            "Total loss:  -3.5333 | PDE Loss:  -5.5864 | Function Loss:  -3.5735\n",
            "Total loss:  -3.5334 | PDE Loss:  -5.5874 | Function Loss:  -3.5736\n",
            "Total loss:  -3.5336 | PDE Loss:  -5.588 | Function Loss:  -3.5737\n",
            "Total loss:  -3.5337 | PDE Loss:  -5.5887 | Function Loss:  -3.5738\n",
            "Total loss:  -3.5338 | PDE Loss:  -5.5892 | Function Loss:  -3.5738\n",
            "Total loss:  -3.5339 | PDE Loss:  -5.5899 | Function Loss:  -3.5739\n",
            "Total loss:  -3.534 | PDE Loss:  -5.5901 | Function Loss:  -3.574\n",
            "Total loss:  -3.5341 | PDE Loss:  -5.5901 | Function Loss:  -3.5741\n",
            "Total loss:  -3.5342 | PDE Loss:  -5.5904 | Function Loss:  -3.5742\n",
            "Total loss:  -3.5343 | PDE Loss:  -5.5908 | Function Loss:  -3.5743\n",
            "Total loss:  -3.5345 | PDE Loss:  -5.5906 | Function Loss:  -3.5744\n",
            "Total loss:  -3.5346 | PDE Loss:  -5.5906 | Function Loss:  -3.5745\n",
            "Total loss:  -3.5347 | PDE Loss:  -5.5905 | Function Loss:  -3.5747\n",
            "Total loss:  -3.5348 | PDE Loss:  -5.5897 | Function Loss:  -3.5748\n",
            "Total loss:  -3.5349 | PDE Loss:  -5.5895 | Function Loss:  -3.575\n",
            "Total loss:  -3.535 | PDE Loss:  -5.5885 | Function Loss:  -3.5752\n",
            "Total loss:  -3.5351 | PDE Loss:  -5.5873 | Function Loss:  -3.5755\n",
            "Total loss:  -3.5353 | PDE Loss:  -5.587 | Function Loss:  -3.5756\n",
            "Total loss:  -3.5354 | PDE Loss:  -5.5867 | Function Loss:  -3.5758\n",
            "Total loss:  -3.5355 | PDE Loss:  -5.587 | Function Loss:  -3.5759\n",
            "Total loss:  -3.5356 | PDE Loss:  -5.5876 | Function Loss:  -3.5759\n",
            "Total loss:  -3.5357 | PDE Loss:  -5.5889 | Function Loss:  -3.5759\n",
            "Total loss:  -3.5358 | PDE Loss:  -5.5904 | Function Loss:  -3.5759\n",
            "Total loss:  -3.5359 | PDE Loss:  -5.5923 | Function Loss:  -3.5758\n",
            "Total loss:  -3.536 | PDE Loss:  -5.5953 | Function Loss:  -3.5756\n",
            "Total loss:  -3.5361 | PDE Loss:  -5.5969 | Function Loss:  -3.5757\n",
            "Total loss:  -3.5363 | PDE Loss:  -5.5993 | Function Loss:  -3.5756\n",
            "Total loss:  -3.5364 | PDE Loss:  -5.6003 | Function Loss:  -3.5756\n",
            "Total loss:  -3.5365 | PDE Loss:  -5.6004 | Function Loss:  -3.5757\n",
            "Total loss:  -3.5366 | PDE Loss:  -5.6006 | Function Loss:  -3.5757\n",
            "Total loss:  -3.5367 | PDE Loss:  -5.6009 | Function Loss:  -3.5758\n",
            "Total loss:  -3.5368 | PDE Loss:  -5.6015 | Function Loss:  -3.5759\n",
            "Total loss:  -3.5369 | PDE Loss:  -5.602 | Function Loss:  -3.5759\n",
            "Total loss:  -3.537 | PDE Loss:  -5.6033 | Function Loss:  -3.5759\n",
            "Total loss:  -3.5371 | PDE Loss:  -5.6046 | Function Loss:  -3.5759\n",
            "Total loss:  -3.5371 | PDE Loss:  -5.6056 | Function Loss:  -3.5759\n",
            "Total loss:  -3.5373 | PDE Loss:  -5.6074 | Function Loss:  -3.5759\n",
            "Total loss:  -3.5374 | PDE Loss:  -5.6082 | Function Loss:  -3.5759\n",
            "Total loss:  -3.5375 | PDE Loss:  -5.6088 | Function Loss:  -3.576\n",
            "Total loss:  -3.5376 | PDE Loss:  -5.6091 | Function Loss:  -3.576\n",
            "Total loss:  -3.5377 | PDE Loss:  -5.6095 | Function Loss:  -3.5761\n",
            "Total loss:  -3.5377 | PDE Loss:  -5.6094 | Function Loss:  -3.5762\n",
            "Total loss:  -3.5379 | PDE Loss:  -5.6093 | Function Loss:  -3.5764\n",
            "Total loss:  -3.538 | PDE Loss:  -5.6088 | Function Loss:  -3.5765\n",
            "Total loss:  -3.5381 | PDE Loss:  -5.6082 | Function Loss:  -3.5767\n",
            "Total loss:  -3.5382 | PDE Loss:  -5.6081 | Function Loss:  -3.5769\n",
            "Total loss:  -3.5384 | PDE Loss:  -5.6071 | Function Loss:  -3.5771\n",
            "Total loss:  -3.5385 | PDE Loss:  -5.607 | Function Loss:  -3.5772\n",
            "Total loss:  -3.5386 | PDE Loss:  -5.6075 | Function Loss:  -3.5773\n",
            "Total loss:  -3.5387 | PDE Loss:  -5.6083 | Function Loss:  -3.5773\n",
            "Total loss:  -3.5388 | PDE Loss:  -5.6098 | Function Loss:  -3.5773\n",
            "Total loss:  -3.5388 | PDE Loss:  -5.6106 | Function Loss:  -3.5773\n",
            "Total loss:  -3.5389 | PDE Loss:  -5.6119 | Function Loss:  -3.5773\n",
            "Total loss:  -3.539 | PDE Loss:  -5.6127 | Function Loss:  -3.5773\n",
            "Total loss:  -3.5391 | PDE Loss:  -5.6162 | Function Loss:  -3.5771\n",
            "Total loss:  -3.5392 | PDE Loss:  -5.6158 | Function Loss:  -3.5773\n",
            "Total loss:  -3.5394 | PDE Loss:  -5.6145 | Function Loss:  -3.5775\n",
            "Total loss:  -3.5395 | PDE Loss:  -5.6137 | Function Loss:  -3.5777\n",
            "Total loss:  -3.5396 | PDE Loss:  -5.6133 | Function Loss:  -3.5779\n",
            "Total loss:  -3.5398 | PDE Loss:  -5.6132 | Function Loss:  -3.5781\n",
            "Total loss:  -3.5399 | PDE Loss:  -5.6129 | Function Loss:  -3.5783\n",
            "Total loss:  -3.54 | PDE Loss:  -5.6132 | Function Loss:  -3.5783\n",
            "Total loss:  -3.5402 | PDE Loss:  -5.6138 | Function Loss:  -3.5785\n",
            "Total loss:  -3.5403 | PDE Loss:  -5.6145 | Function Loss:  -3.5785\n",
            "Total loss:  -3.5404 | PDE Loss:  -5.6157 | Function Loss:  -3.5786\n",
            "Total loss:  -3.5406 | PDE Loss:  -5.6167 | Function Loss:  -3.5786\n",
            "Total loss:  -3.5408 | PDE Loss:  -5.618 | Function Loss:  -3.5787\n",
            "Total loss:  -3.5409 | PDE Loss:  -5.6187 | Function Loss:  -3.5788\n",
            "Total loss:  -3.541 | PDE Loss:  -5.6192 | Function Loss:  -3.5789\n",
            "Total loss:  -3.5411 | PDE Loss:  -5.6193 | Function Loss:  -3.579\n",
            "Total loss:  -3.5412 | PDE Loss:  -5.6189 | Function Loss:  -3.5792\n",
            "Total loss:  -3.5413 | PDE Loss:  -5.6183 | Function Loss:  -3.5793\n",
            "Total loss:  -3.5414 | PDE Loss:  -5.6175 | Function Loss:  -3.5795\n",
            "Total loss:  -3.5415 | PDE Loss:  -5.617 | Function Loss:  -3.5796\n",
            "Total loss:  -3.5415 | PDE Loss:  -5.6127 | Function Loss:  -3.58\n",
            "Total loss:  -3.5415 | PDE Loss:  -5.6153 | Function Loss:  -3.5798\n",
            "Total loss:  -3.5416 | PDE Loss:  -5.6154 | Function Loss:  -3.5799\n",
            "Total loss:  -3.5417 | PDE Loss:  -5.6154 | Function Loss:  -3.58\n",
            "Total loss:  -3.5419 | PDE Loss:  -5.6161 | Function Loss:  -3.5801\n",
            "Total loss:  -3.542 | PDE Loss:  -5.6167 | Function Loss:  -3.5802\n",
            "Total loss:  -3.5422 | PDE Loss:  -5.6182 | Function Loss:  -3.5803\n",
            "Total loss:  -3.5424 | PDE Loss:  -5.6195 | Function Loss:  -3.5803\n",
            "Total loss:  -3.5425 | PDE Loss:  -5.6214 | Function Loss:  -3.5803\n",
            "Total loss:  -3.5426 | PDE Loss:  -5.6222 | Function Loss:  -3.5804\n",
            "Total loss:  -3.5428 | PDE Loss:  -5.6234 | Function Loss:  -3.5804\n",
            "Total loss:  -3.5429 | PDE Loss:  -5.6251 | Function Loss:  -3.5805\n",
            "Total loss:  -3.543 | PDE Loss:  -5.6249 | Function Loss:  -3.5805\n",
            "Total loss:  -3.5431 | PDE Loss:  -5.6249 | Function Loss:  -3.5806\n",
            "Total loss:  -3.5431 | PDE Loss:  -5.624 | Function Loss:  -3.5808\n",
            "Total loss:  -3.5432 | PDE Loss:  -5.6234 | Function Loss:  -3.5809\n",
            "Total loss:  -3.5432 | PDE Loss:  -5.6231 | Function Loss:  -3.581\n",
            "Total loss:  -3.5433 | PDE Loss:  -5.6225 | Function Loss:  -3.5811\n",
            "Total loss:  -3.5434 | PDE Loss:  -5.6222 | Function Loss:  -3.5812\n",
            "Total loss:  -3.5435 | PDE Loss:  -5.622 | Function Loss:  -3.5814\n",
            "Total loss:  -3.5436 | PDE Loss:  -5.6217 | Function Loss:  -3.5815\n",
            "Total loss:  -3.5437 | PDE Loss:  -5.621 | Function Loss:  -3.5816\n",
            "Total loss:  -3.5438 | PDE Loss:  -5.6208 | Function Loss:  -3.5818\n",
            "Total loss:  -3.5439 | PDE Loss:  -5.6208 | Function Loss:  -3.5819\n",
            "Total loss:  -3.544 | PDE Loss:  -5.6207 | Function Loss:  -3.582\n",
            "Total loss:  -3.5441 | PDE Loss:  -5.6208 | Function Loss:  -3.5821\n",
            "Total loss:  -3.5442 | PDE Loss:  -5.6211 | Function Loss:  -3.5821\n",
            "Total loss:  -3.5442 | PDE Loss:  -5.6218 | Function Loss:  -3.5821\n",
            "Total loss:  -3.5443 | PDE Loss:  -5.6222 | Function Loss:  -3.5822\n",
            "Total loss:  -3.5443 | PDE Loss:  -5.6232 | Function Loss:  -3.5822\n",
            "Total loss:  -3.5444 | PDE Loss:  -5.6243 | Function Loss:  -3.5821\n",
            "Total loss:  -3.5445 | PDE Loss:  -5.6256 | Function Loss:  -3.5821\n",
            "Total loss:  -3.5446 | PDE Loss:  -5.6265 | Function Loss:  -3.5821\n",
            "Total loss:  -3.5446 | PDE Loss:  -5.6272 | Function Loss:  -3.5821\n",
            "Total loss:  -3.5447 | PDE Loss:  -5.6274 | Function Loss:  -3.5821\n",
            "Total loss:  -3.5447 | PDE Loss:  -5.6275 | Function Loss:  -3.5822\n",
            "Total loss:  -3.5448 | PDE Loss:  -5.6272 | Function Loss:  -3.5823\n",
            "Total loss:  -3.5449 | PDE Loss:  -5.6268 | Function Loss:  -3.5824\n",
            "Total loss:  -3.545 | PDE Loss:  -5.6258 | Function Loss:  -3.5826\n",
            "Total loss:  -3.5451 | PDE Loss:  -5.6256 | Function Loss:  -3.5828\n",
            "Total loss:  -3.5453 | PDE Loss:  -5.6227 | Function Loss:  -3.5833\n",
            "Total loss:  -3.5454 | PDE Loss:  -5.6226 | Function Loss:  -3.5834\n",
            "Total loss:  -3.5457 | PDE Loss:  -5.6223 | Function Loss:  -3.5837\n",
            "Total loss:  -3.5459 | PDE Loss:  -5.6228 | Function Loss:  -3.5839\n",
            "Total loss:  -3.5461 | PDE Loss:  -5.6206 | Function Loss:  -3.5843\n",
            "Total loss:  -3.5462 | PDE Loss:  -5.6212 | Function Loss:  -3.5844\n",
            "Total loss:  -3.5463 | PDE Loss:  -5.6209 | Function Loss:  -3.5846\n",
            "Total loss:  -3.5465 | PDE Loss:  -5.6201 | Function Loss:  -3.5848\n",
            "Total loss:  -3.5466 | PDE Loss:  -5.6197 | Function Loss:  -3.5849\n",
            "Total loss:  -3.5467 | PDE Loss:  -5.619 | Function Loss:  -3.5851\n",
            "Total loss:  -3.5468 | PDE Loss:  -5.6182 | Function Loss:  -3.5853\n",
            "Total loss:  -3.5469 | PDE Loss:  -5.6169 | Function Loss:  -3.5855\n",
            "Total loss:  -3.5469 | PDE Loss:  -5.6163 | Function Loss:  -3.5856\n",
            "Total loss:  -3.547 | PDE Loss:  -5.6155 | Function Loss:  -3.5858\n",
            "Total loss:  -3.5472 | PDE Loss:  -5.6147 | Function Loss:  -3.5861\n",
            "Total loss:  -3.5474 | PDE Loss:  -5.6143 | Function Loss:  -3.5863\n",
            "Total loss:  -3.5475 | PDE Loss:  -5.614 | Function Loss:  -3.5865\n",
            "Total loss:  -3.5477 | PDE Loss:  -5.614 | Function Loss:  -3.5866\n",
            "Total loss:  -3.5478 | PDE Loss:  -5.6143 | Function Loss:  -3.5867\n",
            "Total loss:  -3.5479 | PDE Loss:  -5.6146 | Function Loss:  -3.5868\n",
            "Total loss:  -3.548 | PDE Loss:  -5.6148 | Function Loss:  -3.5869\n",
            "Total loss:  -3.5481 | PDE Loss:  -5.6148 | Function Loss:  -3.587\n",
            "Total loss:  -3.5482 | PDE Loss:  -5.614 | Function Loss:  -3.5873\n",
            "Total loss:  -3.5484 | PDE Loss:  -5.6133 | Function Loss:  -3.5875\n",
            "Total loss:  -3.5486 | PDE Loss:  -5.6124 | Function Loss:  -3.5879\n",
            "Total loss:  -3.5488 | PDE Loss:  -5.608 | Function Loss:  -3.5884\n",
            "Total loss:  -3.5489 | PDE Loss:  -5.6086 | Function Loss:  -3.5885\n",
            "Total loss:  -3.5491 | PDE Loss:  -5.6091 | Function Loss:  -3.5886\n",
            "Total loss:  -3.5492 | PDE Loss:  -5.609 | Function Loss:  -3.5888\n",
            "Total loss:  -3.5493 | PDE Loss:  -5.6088 | Function Loss:  -3.589\n",
            "Total loss:  -3.5495 | PDE Loss:  -5.6082 | Function Loss:  -3.5892\n",
            "Total loss:  -3.5496 | PDE Loss:  -5.6083 | Function Loss:  -3.5893\n",
            "Total loss:  -3.5498 | PDE Loss:  -5.6076 | Function Loss:  -3.5896\n",
            "Total loss:  -3.5499 | PDE Loss:  -5.6075 | Function Loss:  -3.5897\n",
            "Total loss:  -3.55 | PDE Loss:  -5.6005 | Function Loss:  -3.5905\n",
            "Total loss:  -3.5503 | PDE Loss:  -5.6033 | Function Loss:  -3.5905\n",
            "Total loss:  -3.5505 | PDE Loss:  -5.6049 | Function Loss:  -3.5906\n",
            "Total loss:  -3.5506 | PDE Loss:  -5.6066 | Function Loss:  -3.5906\n",
            "Total loss:  -3.5508 | PDE Loss:  -5.607 | Function Loss:  -3.5907\n",
            "Total loss:  -3.5509 | PDE Loss:  -5.607 | Function Loss:  -3.5908\n",
            "Total loss:  -3.551 | PDE Loss:  -5.6067 | Function Loss:  -3.591\n",
            "Total loss:  -3.5512 | PDE Loss:  -5.6052 | Function Loss:  -3.5913\n",
            "Total loss:  -3.5513 | PDE Loss:  -5.6042 | Function Loss:  -3.5916\n",
            "Total loss:  -3.5514 | PDE Loss:  -5.6029 | Function Loss:  -3.5918\n",
            "Total loss:  -3.5515 | PDE Loss:  -5.6018 | Function Loss:  -3.592\n",
            "Total loss:  -3.5516 | PDE Loss:  -5.5996 | Function Loss:  -3.5924\n",
            "Total loss:  -3.5517 | PDE Loss:  -5.5997 | Function Loss:  -3.5925\n",
            "Total loss:  -3.5518 | PDE Loss:  -5.5992 | Function Loss:  -3.5926\n",
            "Total loss:  -3.5519 | PDE Loss:  -5.5991 | Function Loss:  -3.5928\n",
            "Total loss:  -3.5522 | PDE Loss:  -5.5984 | Function Loss:  -3.5931\n",
            "Total loss:  -3.5523 | PDE Loss:  -5.5986 | Function Loss:  -3.5932\n",
            "Total loss:  -3.5525 | PDE Loss:  -5.5991 | Function Loss:  -3.5934\n",
            "Total loss:  -3.5527 | PDE Loss:  -5.6003 | Function Loss:  -3.5935\n",
            "Total loss:  -3.5529 | PDE Loss:  -5.6008 | Function Loss:  -3.5936\n",
            "Total loss:  -3.5529 | PDE Loss:  -5.6017 | Function Loss:  -3.5936\n",
            "Total loss:  -3.553 | PDE Loss:  -5.6018 | Function Loss:  -3.5937\n",
            "Total loss:  -3.5531 | PDE Loss:  -5.6027 | Function Loss:  -3.5937\n",
            "Total loss:  -3.5532 | PDE Loss:  -5.6028 | Function Loss:  -3.5937\n",
            "Total loss:  -3.5533 | PDE Loss:  -5.6033 | Function Loss:  -3.5938\n",
            "Total loss:  -3.5534 | PDE Loss:  -5.603 | Function Loss:  -3.594\n",
            "Total loss:  -3.5535 | PDE Loss:  -5.6035 | Function Loss:  -3.594\n",
            "Total loss:  -3.5536 | PDE Loss:  -5.6027 | Function Loss:  -3.5943\n",
            "Total loss:  -3.5537 | PDE Loss:  -5.6025 | Function Loss:  -3.5944\n",
            "Total loss:  -3.5538 | PDE Loss:  -5.602 | Function Loss:  -3.5945\n",
            "Total loss:  -3.554 | PDE Loss:  -5.6014 | Function Loss:  -3.5947\n",
            "Total loss:  -3.5541 | PDE Loss:  -5.6008 | Function Loss:  -3.5949\n",
            "Total loss:  -3.5542 | PDE Loss:  -5.6006 | Function Loss:  -3.5951\n",
            "Total loss:  -3.5545 | PDE Loss:  -5.5987 | Function Loss:  -3.5956\n",
            "Total loss:  -3.5547 | PDE Loss:  -5.5979 | Function Loss:  -3.596\n",
            "Total loss:  -3.555 | PDE Loss:  -5.5973 | Function Loss:  -3.5963\n",
            "Total loss:  -3.5552 | PDE Loss:  -5.5949 | Function Loss:  -3.5968\n",
            "Total loss:  -3.5555 | PDE Loss:  -5.5963 | Function Loss:  -3.5969\n",
            "Total loss:  -3.5557 | PDE Loss:  -5.5967 | Function Loss:  -3.5971\n",
            "Total loss:  -3.5559 | PDE Loss:  -5.5973 | Function Loss:  -3.5972\n",
            "Total loss:  -3.556 | PDE Loss:  -5.5972 | Function Loss:  -3.5974\n",
            "Total loss:  -3.5561 | PDE Loss:  -5.5972 | Function Loss:  -3.5975\n",
            "Total loss:  -3.5564 | PDE Loss:  -5.5977 | Function Loss:  -3.5978\n",
            "Total loss:  -3.5566 | PDE Loss:  -5.5972 | Function Loss:  -3.5981\n",
            "Total loss:  -3.557 | PDE Loss:  -5.5979 | Function Loss:  -3.5984\n",
            "Total loss:  -3.5574 | PDE Loss:  -5.5992 | Function Loss:  -3.5987\n",
            "Total loss:  -3.5577 | PDE Loss:  -5.5991 | Function Loss:  -3.5991\n",
            "Total loss:  -3.5581 | PDE Loss:  -5.6028 | Function Loss:  -3.5991\n",
            "Total loss:  -3.5583 | PDE Loss:  -5.6031 | Function Loss:  -3.5994\n",
            "Total loss:  -3.5586 | PDE Loss:  -5.605 | Function Loss:  -3.5995\n",
            "Total loss:  -3.5589 | PDE Loss:  -5.6071 | Function Loss:  -3.5996\n",
            "Total loss:  -3.5593 | PDE Loss:  -5.6068 | Function Loss:  -3.6\n",
            "Total loss:  -3.5595 | PDE Loss:  -5.6092 | Function Loss:  -3.6001\n",
            "Total loss:  -3.56 | PDE Loss:  -5.6108 | Function Loss:  -3.6004\n",
            "Total loss:  -3.5605 | PDE Loss:  -5.6124 | Function Loss:  -3.6008\n",
            "Total loss:  -3.561 | PDE Loss:  -5.615 | Function Loss:  -3.6011\n",
            "Total loss:  -3.5614 | PDE Loss:  -5.6139 | Function Loss:  -3.6017\n",
            "Total loss:  -3.5617 | PDE Loss:  -5.6141 | Function Loss:  -3.602\n",
            "Total loss:  -3.562 | PDE Loss:  -5.6131 | Function Loss:  -3.6025\n",
            "Total loss:  -3.5623 | PDE Loss:  -5.6118 | Function Loss:  -3.6029\n",
            "Total loss:  -3.5625 | PDE Loss:  -5.6095 | Function Loss:  -3.6034\n",
            "Total loss:  -3.5627 | PDE Loss:  -5.609 | Function Loss:  -3.6036\n",
            "Total loss:  -3.5629 | PDE Loss:  -5.6072 | Function Loss:  -3.604\n",
            "Total loss:  -3.5631 | PDE Loss:  -5.6069 | Function Loss:  -3.6042\n",
            "Total loss:  -3.5634 | PDE Loss:  -5.6057 | Function Loss:  -3.6047\n",
            "Total loss:  -3.5638 | PDE Loss:  -5.6064 | Function Loss:  -3.605\n",
            "Total loss:  -3.564 | PDE Loss:  -5.6065 | Function Loss:  -3.6053\n",
            "Total loss:  -3.5644 | PDE Loss:  -5.6067 | Function Loss:  -3.6057\n",
            "Total loss:  -3.5649 | PDE Loss:  -5.6095 | Function Loss:  -3.6059\n",
            "Total loss:  -3.5652 | PDE Loss:  -5.6101 | Function Loss:  -3.6062\n",
            "Total loss:  -3.5655 | PDE Loss:  -5.611 | Function Loss:  -3.6065\n",
            "Total loss:  -3.5659 | PDE Loss:  -5.6108 | Function Loss:  -3.6069\n",
            "Total loss:  -3.5662 | PDE Loss:  -5.6082 | Function Loss:  -3.6075\n",
            "Total loss:  -3.5665 | PDE Loss:  -5.6077 | Function Loss:  -3.6079\n",
            "Total loss:  -3.5667 | PDE Loss:  -5.6063 | Function Loss:  -3.6082\n",
            "Total loss:  -3.5669 | PDE Loss:  -5.6051 | Function Loss:  -3.6086\n",
            "Total loss:  -3.5671 | PDE Loss:  -5.6033 | Function Loss:  -3.609\n",
            "Total loss:  -3.5672 | PDE Loss:  -5.6018 | Function Loss:  -3.6093\n",
            "Total loss:  -3.5674 | PDE Loss:  -5.6002 | Function Loss:  -3.6096\n",
            "Total loss:  -3.5675 | PDE Loss:  -5.5988 | Function Loss:  -3.6099\n",
            "Total loss:  -3.5677 | PDE Loss:  -5.5973 | Function Loss:  -3.6103\n",
            "Total loss:  -3.5677 | PDE Loss:  -5.5936 | Function Loss:  -3.6107\n",
            "Total loss:  -3.5679 | PDE Loss:  -5.5938 | Function Loss:  -3.6109\n",
            "Total loss:  -3.5681 | PDE Loss:  -5.5942 | Function Loss:  -3.611\n",
            "Total loss:  -3.5682 | PDE Loss:  -5.5942 | Function Loss:  -3.6112\n",
            "Total loss:  -3.5683 | PDE Loss:  -5.5944 | Function Loss:  -3.6112\n",
            "Total loss:  -3.5684 | PDE Loss:  -5.5946 | Function Loss:  -3.6113\n",
            "Total loss:  -3.5685 | PDE Loss:  -5.5942 | Function Loss:  -3.6114\n",
            "Total loss:  -3.5686 | PDE Loss:  -5.5939 | Function Loss:  -3.6116\n",
            "Total loss:  -3.5686 | PDE Loss:  -5.5929 | Function Loss:  -3.6117\n",
            "Total loss:  -3.5687 | PDE Loss:  -5.5919 | Function Loss:  -3.612\n",
            "Total loss:  -3.5688 | PDE Loss:  -5.5902 | Function Loss:  -3.6122\n",
            "Total loss:  -3.5689 | PDE Loss:  -5.5883 | Function Loss:  -3.6126\n",
            "Total loss:  -3.569 | PDE Loss:  -5.5859 | Function Loss:  -3.6129\n",
            "Total loss:  -3.5691 | PDE Loss:  -5.5838 | Function Loss:  -3.6133\n",
            "Total loss:  -3.5692 | PDE Loss:  -5.581 | Function Loss:  -3.6137\n",
            "Total loss:  -3.5694 | PDE Loss:  -5.5786 | Function Loss:  -3.6141\n",
            "Total loss:  -3.5696 | PDE Loss:  -5.5755 | Function Loss:  -3.6146\n",
            "Total loss:  -3.5697 | PDE Loss:  -5.5733 | Function Loss:  -3.6151\n",
            "Total loss:  -3.5699 | PDE Loss:  -5.5705 | Function Loss:  -3.6155\n",
            "Total loss:  -3.57 | PDE Loss:  -5.5688 | Function Loss:  -3.6159\n",
            "Total loss:  -3.5702 | PDE Loss:  -5.5665 | Function Loss:  -3.6163\n",
            "Total loss:  -3.5703 | PDE Loss:  -5.5655 | Function Loss:  -3.6166\n",
            "Total loss:  -3.5704 | PDE Loss:  -5.565 | Function Loss:  -3.6168\n",
            "Total loss:  -3.5705 | PDE Loss:  -5.5649 | Function Loss:  -3.6169\n",
            "Total loss:  -3.5707 | PDE Loss:  -5.5649 | Function Loss:  -3.6171\n",
            "Total loss:  -3.5708 | PDE Loss:  -5.5657 | Function Loss:  -3.6171\n",
            "Total loss:  -3.571 | PDE Loss:  -5.5662 | Function Loss:  -3.6173\n",
            "Total loss:  -3.5712 | PDE Loss:  -5.5675 | Function Loss:  -3.6173\n",
            "Total loss:  -3.5714 | PDE Loss:  -5.5688 | Function Loss:  -3.6175\n",
            "Total loss:  -3.5716 | PDE Loss:  -5.5689 | Function Loss:  -3.6177\n",
            "Total loss:  -3.5719 | PDE Loss:  -5.5727 | Function Loss:  -3.6176\n",
            "Total loss:  -3.5721 | PDE Loss:  -5.5736 | Function Loss:  -3.6177\n",
            "Total loss:  -3.5723 | PDE Loss:  -5.5739 | Function Loss:  -3.6179\n",
            "Total loss:  -3.5724 | PDE Loss:  -5.5738 | Function Loss:  -3.618\n",
            "Total loss:  -3.5725 | PDE Loss:  -5.5738 | Function Loss:  -3.6181\n",
            "Total loss:  -3.5726 | PDE Loss:  -5.5736 | Function Loss:  -3.6182\n",
            "Total loss:  -3.5727 | PDE Loss:  -5.5739 | Function Loss:  -3.6183\n",
            "Total loss:  -3.5729 | PDE Loss:  -5.5747 | Function Loss:  -3.6184\n",
            "Total loss:  -3.5731 | PDE Loss:  -5.5754 | Function Loss:  -3.6186\n",
            "Total loss:  -3.5734 | PDE Loss:  -5.5764 | Function Loss:  -3.6188\n",
            "Total loss:  -3.5737 | PDE Loss:  -5.5776 | Function Loss:  -3.619\n",
            "Total loss:  -3.5739 | PDE Loss:  -5.579 | Function Loss:  -3.6191\n",
            "Total loss:  -3.5741 | PDE Loss:  -5.5806 | Function Loss:  -3.6191\n",
            "Total loss:  -3.5743 | PDE Loss:  -5.5817 | Function Loss:  -3.6192\n",
            "Total loss:  -3.5744 | PDE Loss:  -5.5828 | Function Loss:  -3.6192\n",
            "Total loss:  -3.5743 | PDE Loss:  -5.5819 | Function Loss:  -3.6193\n",
            "Total loss:  -3.5744 | PDE Loss:  -5.5826 | Function Loss:  -3.6193\n",
            "Total loss:  -3.5745 | PDE Loss:  -5.5827 | Function Loss:  -3.6193\n",
            "Total loss:  -3.5746 | PDE Loss:  -5.5825 | Function Loss:  -3.6195\n",
            "Total loss:  -3.5748 | PDE Loss:  -5.5815 | Function Loss:  -3.6198\n",
            "Total loss:  -3.5749 | PDE Loss:  -5.5804 | Function Loss:  -3.62\n",
            "Total loss:  -3.5749 | PDE Loss:  -5.5786 | Function Loss:  -3.6203\n",
            "Total loss:  -3.575 | PDE Loss:  -5.5774 | Function Loss:  -3.6205\n",
            "Total loss:  -3.5751 | PDE Loss:  -5.5762 | Function Loss:  -3.6207\n",
            "Total loss:  -3.5751 | PDE Loss:  -5.5756 | Function Loss:  -3.6208\n",
            "Total loss:  -3.5752 | PDE Loss:  -5.5748 | Function Loss:  -3.621\n",
            "Total loss:  -3.5753 | PDE Loss:  -5.5744 | Function Loss:  -3.6211\n",
            "Total loss:  -3.5754 | PDE Loss:  -5.5747 | Function Loss:  -3.6212\n",
            "Total loss:  -3.5755 | PDE Loss:  -5.5749 | Function Loss:  -3.6214\n",
            "Total loss:  -3.5757 | PDE Loss:  -5.5778 | Function Loss:  -3.6212\n",
            "Total loss:  -3.5758 | PDE Loss:  -5.5781 | Function Loss:  -3.6213\n",
            "Total loss:  -3.576 | PDE Loss:  -5.5786 | Function Loss:  -3.6215\n",
            "Total loss:  -3.5762 | PDE Loss:  -5.5794 | Function Loss:  -3.6216\n",
            "Total loss:  -3.5763 | PDE Loss:  -5.581 | Function Loss:  -3.6216\n",
            "Total loss:  -3.5766 | PDE Loss:  -5.582 | Function Loss:  -3.6217\n",
            "Total loss:  -3.5767 | PDE Loss:  -5.5829 | Function Loss:  -3.6218\n",
            "Total loss:  -3.5769 | PDE Loss:  -5.5842 | Function Loss:  -3.6218\n",
            "Total loss:  -3.577 | PDE Loss:  -5.5852 | Function Loss:  -3.6218\n",
            "Total loss:  -3.5771 | PDE Loss:  -5.5857 | Function Loss:  -3.6219\n",
            "Total loss:  -3.5772 | PDE Loss:  -5.5879 | Function Loss:  -3.6218\n",
            "Total loss:  -3.5774 | PDE Loss:  -5.5882 | Function Loss:  -3.6219\n",
            "Total loss:  -3.5776 | PDE Loss:  -5.5885 | Function Loss:  -3.6221\n",
            "Total loss:  -3.5778 | PDE Loss:  -5.5884 | Function Loss:  -3.6224\n",
            "Total loss:  -3.5781 | PDE Loss:  -5.5887 | Function Loss:  -3.6227\n",
            "Total loss:  -3.5783 | PDE Loss:  -5.59 | Function Loss:  -3.6228\n",
            "Total loss:  -3.5785 | PDE Loss:  -5.5909 | Function Loss:  -3.6229\n",
            "Total loss:  -3.5787 | PDE Loss:  -5.5922 | Function Loss:  -3.6229\n",
            "Total loss:  -3.5787 | PDE Loss:  -5.5932 | Function Loss:  -3.6229\n",
            "Total loss:  -3.5788 | PDE Loss:  -5.5945 | Function Loss:  -3.6229\n",
            "Total loss:  -3.579 | PDE Loss:  -5.5957 | Function Loss:  -3.6229\n",
            "Total loss:  -3.5791 | PDE Loss:  -5.597 | Function Loss:  -3.6229\n",
            "Total loss:  -3.5792 | PDE Loss:  -5.5976 | Function Loss:  -3.623\n",
            "Total loss:  -3.5793 | PDE Loss:  -5.5985 | Function Loss:  -3.623\n",
            "Total loss:  -3.5795 | PDE Loss:  -5.5992 | Function Loss:  -3.6231\n",
            "Total loss:  -3.5796 | PDE Loss:  -5.5994 | Function Loss:  -3.6232\n",
            "Total loss:  -3.5797 | PDE Loss:  -5.5999 | Function Loss:  -3.6233\n",
            "Total loss:  -3.5798 | PDE Loss:  -5.5994 | Function Loss:  -3.6234\n",
            "Total loss:  -3.5799 | PDE Loss:  -5.5994 | Function Loss:  -3.6235\n",
            "Total loss:  -3.58 | PDE Loss:  -5.5994 | Function Loss:  -3.6236\n",
            "Total loss:  -3.5801 | PDE Loss:  -5.5998 | Function Loss:  -3.6237\n",
            "Total loss:  -3.5801 | PDE Loss:  -5.5993 | Function Loss:  -3.6238\n",
            "Total loss:  -3.5803 | PDE Loss:  -5.6 | Function Loss:  -3.6239\n",
            "Total loss:  -3.5804 | PDE Loss:  -5.6 | Function Loss:  -3.624\n",
            "Total loss:  -3.5805 | PDE Loss:  -5.5996 | Function Loss:  -3.6242\n",
            "Total loss:  -3.5807 | PDE Loss:  -5.5998 | Function Loss:  -3.6244\n",
            "Total loss:  -3.5808 | PDE Loss:  -5.6 | Function Loss:  -3.6245\n",
            "Total loss:  -3.5809 | PDE Loss:  -5.6 | Function Loss:  -3.6246\n",
            "Total loss:  -3.581 | PDE Loss:  -5.6004 | Function Loss:  -3.6247\n",
            "Total loss:  -3.5811 | PDE Loss:  -5.6008 | Function Loss:  -3.6247\n",
            "Total loss:  -3.5812 | PDE Loss:  -5.6015 | Function Loss:  -3.6248\n",
            "Total loss:  -3.5814 | PDE Loss:  -5.6022 | Function Loss:  -3.6249\n",
            "Total loss:  -3.5815 | PDE Loss:  -5.6037 | Function Loss:  -3.6248\n",
            "Total loss:  -3.5815 | PDE Loss:  -5.6043 | Function Loss:  -3.6248\n",
            "Total loss:  -3.5816 | PDE Loss:  -5.6056 | Function Loss:  -3.6248\n",
            "Total loss:  -3.5816 | PDE Loss:  -5.6062 | Function Loss:  -3.6248\n",
            "Total loss:  -3.5817 | PDE Loss:  -5.6068 | Function Loss:  -3.6247\n",
            "Total loss:  -3.5818 | PDE Loss:  -5.6072 | Function Loss:  -3.6248\n",
            "Total loss:  -3.5818 | PDE Loss:  -5.6083 | Function Loss:  -3.6248\n",
            "Total loss:  -3.582 | PDE Loss:  -5.6096 | Function Loss:  -3.6248\n",
            "Total loss:  -3.5821 | PDE Loss:  -5.6116 | Function Loss:  -3.6247\n",
            "Total loss:  -3.5823 | PDE Loss:  -5.6159 | Function Loss:  -3.6245\n",
            "Total loss:  -3.5824 | PDE Loss:  -5.6173 | Function Loss:  -3.6245\n",
            "Total loss:  -3.5827 | PDE Loss:  -5.619 | Function Loss:  -3.6246\n",
            "Total loss:  -3.5828 | PDE Loss:  -5.6199 | Function Loss:  -3.6246\n",
            "Total loss:  -3.5829 | PDE Loss:  -5.6209 | Function Loss:  -3.6247\n",
            "Total loss:  -3.583 | PDE Loss:  -5.6225 | Function Loss:  -3.6246\n",
            "Total loss:  -3.5832 | PDE Loss:  -5.6241 | Function Loss:  -3.6246\n",
            "Total loss:  -3.5834 | PDE Loss:  -5.6275 | Function Loss:  -3.6245\n",
            "Total loss:  -3.5836 | PDE Loss:  -5.6296 | Function Loss:  -3.6246\n",
            "Total loss:  -3.5838 | PDE Loss:  -5.6328 | Function Loss:  -3.6245\n",
            "Total loss:  -3.5841 | PDE Loss:  -5.635 | Function Loss:  -3.6245\n",
            "Total loss:  -3.5843 | PDE Loss:  -5.6364 | Function Loss:  -3.6246\n",
            "Total loss:  -3.5845 | PDE Loss:  -5.6366 | Function Loss:  -3.6249\n",
            "Total loss:  -3.5847 | PDE Loss:  -5.6373 | Function Loss:  -3.625\n",
            "Total loss:  -3.5849 | PDE Loss:  -5.637 | Function Loss:  -3.6252\n",
            "Total loss:  -3.5851 | PDE Loss:  -5.636 | Function Loss:  -3.6255\n",
            "Total loss:  -3.5853 | PDE Loss:  -5.6345 | Function Loss:  -3.6259\n",
            "Total loss:  -3.5855 | PDE Loss:  -5.6336 | Function Loss:  -3.6262\n",
            "Total loss:  -3.5856 | PDE Loss:  -5.6276 | Function Loss:  -3.6269\n",
            "Total loss:  -3.5858 | PDE Loss:  -5.6275 | Function Loss:  -3.6272\n",
            "Total loss:  -3.586 | PDE Loss:  -5.6287 | Function Loss:  -3.6273\n",
            "Total loss:  -3.5862 | PDE Loss:  -5.6293 | Function Loss:  -3.6274\n",
            "Total loss:  -3.5864 | PDE Loss:  -5.6309 | Function Loss:  -3.6275\n",
            "Total loss:  -3.5866 | PDE Loss:  -5.6321 | Function Loss:  -3.6276\n",
            "Total loss:  -3.5868 | PDE Loss:  -5.6339 | Function Loss:  -3.6276\n",
            "Total loss:  -3.5869 | PDE Loss:  -5.6356 | Function Loss:  -3.6276\n",
            "Total loss:  -3.5871 | PDE Loss:  -5.636 | Function Loss:  -3.6277\n",
            "Total loss:  -3.5872 | PDE Loss:  -5.6373 | Function Loss:  -3.6278\n",
            "Total loss:  -3.5875 | PDE Loss:  -5.6379 | Function Loss:  -3.628\n",
            "Total loss:  -3.5876 | PDE Loss:  -5.6378 | Function Loss:  -3.6281\n",
            "Total loss:  -3.5878 | PDE Loss:  -5.638 | Function Loss:  -3.6283\n",
            "Total loss:  -3.588 | PDE Loss:  -5.638 | Function Loss:  -3.6286\n",
            "Total loss:  -3.5883 | PDE Loss:  -5.637 | Function Loss:  -3.629\n",
            "Total loss:  -3.5887 | PDE Loss:  -5.6382 | Function Loss:  -3.6293\n",
            "Total loss:  -3.589 | PDE Loss:  -5.637 | Function Loss:  -3.6297\n",
            "Total loss:  -3.5895 | PDE Loss:  -5.6362 | Function Loss:  -3.6303\n",
            "Total loss:  -3.5899 | PDE Loss:  -5.6366 | Function Loss:  -3.6308\n",
            "Total loss:  -3.5903 | PDE Loss:  -5.6372 | Function Loss:  -3.6312\n",
            "Total loss:  -3.5907 | PDE Loss:  -5.638 | Function Loss:  -3.6315\n",
            "Total loss:  -3.591 | PDE Loss:  -5.639 | Function Loss:  -3.6318\n",
            "Total loss:  -3.5914 | PDE Loss:  -5.6369 | Function Loss:  -3.6323\n",
            "Total loss:  -3.5916 | PDE Loss:  -5.6361 | Function Loss:  -3.6327\n",
            "Total loss:  -3.5921 | PDE Loss:  -5.635 | Function Loss:  -3.6333\n",
            "Total loss:  -3.5926 | PDE Loss:  -5.6326 | Function Loss:  -3.6342\n",
            "Total loss:  -3.593 | PDE Loss:  -5.6319 | Function Loss:  -3.6347\n",
            "Total loss:  -3.5933 | PDE Loss:  -5.6304 | Function Loss:  -3.6351\n",
            "Total loss:  -3.5935 | PDE Loss:  -5.6302 | Function Loss:  -3.6354\n",
            "Total loss:  -3.5937 | PDE Loss:  -5.6295 | Function Loss:  -3.6357\n",
            "Total loss:  -3.594 | PDE Loss:  -5.6302 | Function Loss:  -3.6359\n",
            "Total loss:  -3.5942 | PDE Loss:  -5.6288 | Function Loss:  -3.6362\n",
            "Total loss:  -3.5945 | PDE Loss:  -5.6277 | Function Loss:  -3.6367\n",
            "Total loss:  -3.5947 | PDE Loss:  -5.6266 | Function Loss:  -3.637\n",
            "Total loss:  -3.595 | PDE Loss:  -5.6238 | Function Loss:  -3.6376\n",
            "Total loss:  -3.5953 | PDE Loss:  -5.6201 | Function Loss:  -3.6384\n",
            "Total loss:  -3.5956 | PDE Loss:  -5.6152 | Function Loss:  -3.6392\n",
            "Total loss:  -3.5959 | PDE Loss:  -5.6119 | Function Loss:  -3.6399\n",
            "Total loss:  -3.5962 | PDE Loss:  -5.6096 | Function Loss:  -3.6405\n",
            "Total loss:  -3.5965 | PDE Loss:  -5.6073 | Function Loss:  -3.6411\n",
            "Total loss:  -3.5968 | PDE Loss:  -5.6064 | Function Loss:  -3.6415\n",
            "Total loss:  -3.5971 | PDE Loss:  -5.6059 | Function Loss:  -3.6419\n",
            "Total loss:  -3.5975 | PDE Loss:  -5.6068 | Function Loss:  -3.6422\n",
            "Total loss:  -3.598 | PDE Loss:  -5.6079 | Function Loss:  -3.6426\n",
            "Total loss:  -3.5985 | PDE Loss:  -5.6114 | Function Loss:  -3.6429\n",
            "Total loss:  -3.5989 | PDE Loss:  -5.6133 | Function Loss:  -3.6431\n",
            "Total loss:  -3.5993 | PDE Loss:  -5.6149 | Function Loss:  -3.6434\n",
            "Total loss:  -3.5997 | PDE Loss:  -5.6155 | Function Loss:  -3.6437\n",
            "Total loss:  -3.5999 | PDE Loss:  -5.6163 | Function Loss:  -3.6439\n",
            "Total loss:  -3.6 | PDE Loss:  -5.6152 | Function Loss:  -3.6441\n",
            "Total loss:  -3.6001 | PDE Loss:  -5.6148 | Function Loss:  -3.6443\n",
            "Total loss:  -3.6003 | PDE Loss:  -5.6135 | Function Loss:  -3.6446\n",
            "Total loss:  -3.6005 | PDE Loss:  -5.6116 | Function Loss:  -3.645\n",
            "Total loss:  -3.6007 | PDE Loss:  -5.6098 | Function Loss:  -3.6455\n",
            "Total loss:  -3.6009 | PDE Loss:  -5.6077 | Function Loss:  -3.646\n",
            "Total loss:  -3.6013 | PDE Loss:  -5.6068 | Function Loss:  -3.6465\n",
            "Total loss:  -3.6017 | PDE Loss:  -5.6056 | Function Loss:  -3.647\n",
            "Total loss:  -3.6022 | PDE Loss:  -5.6037 | Function Loss:  -3.6477\n",
            "Total loss:  -3.6025 | PDE Loss:  -5.6034 | Function Loss:  -3.6482\n",
            "Total loss:  -3.6028 | PDE Loss:  -5.6031 | Function Loss:  -3.6485\n",
            "Total loss:  -3.603 | PDE Loss:  -5.6042 | Function Loss:  -3.6487\n",
            "Total loss:  -3.6032 | PDE Loss:  -5.6048 | Function Loss:  -3.6488\n",
            "Total loss:  -3.6033 | PDE Loss:  -5.6052 | Function Loss:  -3.6489\n",
            "Total loss:  -3.6036 | PDE Loss:  -5.6053 | Function Loss:  -3.6491\n",
            "Total loss:  -3.604 | PDE Loss:  -5.6056 | Function Loss:  -3.6496\n",
            "Total loss:  -3.6043 | PDE Loss:  -5.605 | Function Loss:  -3.65\n",
            "Total loss:  -3.6048 | PDE Loss:  -5.604 | Function Loss:  -3.6506\n",
            "Total loss:  -3.6052 | PDE Loss:  -5.6038 | Function Loss:  -3.6511\n",
            "Total loss:  -3.6056 | PDE Loss:  -5.6045 | Function Loss:  -3.6515\n",
            "Total loss:  -3.6059 | PDE Loss:  -5.604 | Function Loss:  -3.6519\n",
            "Total loss:  -3.6062 | PDE Loss:  -5.6051 | Function Loss:  -3.652\n",
            "Total loss:  -3.6064 | PDE Loss:  -5.6057 | Function Loss:  -3.6523\n",
            "Total loss:  -3.6067 | PDE Loss:  -5.6064 | Function Loss:  -3.6525\n",
            "Total loss:  -3.6071 | PDE Loss:  -5.6065 | Function Loss:  -3.6529\n",
            "Total loss:  -3.6075 | PDE Loss:  -5.6046 | Function Loss:  -3.6536\n",
            "Total loss:  -3.6079 | PDE Loss:  -5.6035 | Function Loss:  -3.6542\n",
            "Total loss:  -3.6082 | PDE Loss:  -5.6006 | Function Loss:  -3.6548\n",
            "Total loss:  -3.6087 | PDE Loss:  -5.5992 | Function Loss:  -3.6555\n",
            "Total loss:  -3.6091 | PDE Loss:  -5.5958 | Function Loss:  -3.6563\n",
            "Total loss:  -3.6094 | PDE Loss:  -5.5936 | Function Loss:  -3.657\n",
            "Total loss:  -3.6098 | PDE Loss:  -5.5915 | Function Loss:  -3.6576\n",
            "Total loss:  -3.61 | PDE Loss:  -5.5889 | Function Loss:  -3.6582\n",
            "Total loss:  -3.6104 | PDE Loss:  -5.5882 | Function Loss:  -3.6587\n",
            "Total loss:  -3.6106 | PDE Loss:  -5.5866 | Function Loss:  -3.6591\n",
            "Total loss:  -3.6109 | PDE Loss:  -5.5867 | Function Loss:  -3.6595\n",
            "Total loss:  -3.6113 | PDE Loss:  -5.5867 | Function Loss:  -3.6599\n",
            "Total loss:  -3.6116 | PDE Loss:  -5.5877 | Function Loss:  -3.6601\n",
            "Total loss:  -3.6119 | PDE Loss:  -5.5888 | Function Loss:  -3.6603\n",
            "Total loss:  -3.6122 | PDE Loss:  -5.5897 | Function Loss:  -3.6605\n",
            "Total loss:  -3.6124 | PDE Loss:  -5.5912 | Function Loss:  -3.6606\n",
            "Total loss:  -3.6125 | PDE Loss:  -5.5914 | Function Loss:  -3.6607\n",
            "Total loss:  -3.6128 | PDE Loss:  -5.5923 | Function Loss:  -3.6609\n",
            "Total loss:  -3.6131 | PDE Loss:  -5.5924 | Function Loss:  -3.6612\n",
            "Total loss:  -3.6134 | PDE Loss:  -5.5925 | Function Loss:  -3.6615\n",
            "Total loss:  -3.6137 | PDE Loss:  -5.5918 | Function Loss:  -3.6619\n",
            "Total loss:  -3.614 | PDE Loss:  -5.5924 | Function Loss:  -3.6622\n",
            "Total loss:  -3.6142 | PDE Loss:  -5.5925 | Function Loss:  -3.6624\n",
            "Total loss:  -3.6144 | PDE Loss:  -5.5931 | Function Loss:  -3.6626\n",
            "Total loss:  -3.6146 | PDE Loss:  -5.5932 | Function Loss:  -3.6628\n",
            "Total loss:  -3.6148 | PDE Loss:  -5.5935 | Function Loss:  -3.663\n",
            "Total loss:  -3.6149 | PDE Loss:  -5.5935 | Function Loss:  -3.6631\n",
            "Total loss:  -3.615 | PDE Loss:  -5.5934 | Function Loss:  -3.6633\n",
            "Total loss:  -3.6153 | PDE Loss:  -5.5937 | Function Loss:  -3.6635\n",
            "Total loss:  -3.6155 | PDE Loss:  -5.5935 | Function Loss:  -3.6637\n",
            "Total loss:  -3.6157 | PDE Loss:  -5.5936 | Function Loss:  -3.664\n",
            "Total loss:  -3.616 | PDE Loss:  -5.5946 | Function Loss:  -3.6642\n",
            "Total loss:  -3.6161 | PDE Loss:  -5.5945 | Function Loss:  -3.6644\n",
            "Total loss:  -3.6163 | PDE Loss:  -5.5959 | Function Loss:  -3.6644\n",
            "Total loss:  -3.6165 | PDE Loss:  -5.5968 | Function Loss:  -3.6645\n",
            "Total loss:  -3.6166 | PDE Loss:  -5.5982 | Function Loss:  -3.6645\n",
            "Total loss:  -3.6168 | PDE Loss:  -5.5995 | Function Loss:  -3.6645\n",
            "Total loss:  -3.6171 | PDE Loss:  -5.6018 | Function Loss:  -3.6646\n",
            "Total loss:  -3.6172 | PDE Loss:  -5.6032 | Function Loss:  -3.6646\n",
            "Total loss:  -3.6175 | PDE Loss:  -5.6043 | Function Loss:  -3.6648\n",
            "Total loss:  -3.6178 | PDE Loss:  -5.605 | Function Loss:  -3.665\n",
            "Total loss:  -3.6181 | PDE Loss:  -5.605 | Function Loss:  -3.6654\n",
            "Total loss:  -3.6185 | PDE Loss:  -5.6048 | Function Loss:  -3.6658\n",
            "Total loss:  -3.6188 | PDE Loss:  -5.6036 | Function Loss:  -3.6663\n",
            "Total loss:  -3.6191 | PDE Loss:  -5.6033 | Function Loss:  -3.6666\n",
            "Total loss:  -3.6193 | PDE Loss:  -5.6018 | Function Loss:  -3.667\n",
            "Total loss:  -3.6195 | PDE Loss:  -5.6014 | Function Loss:  -3.6673\n",
            "Total loss:  -3.6196 | PDE Loss:  -5.6009 | Function Loss:  -3.6675\n",
            "Total loss:  -3.6198 | PDE Loss:  -5.6007 | Function Loss:  -3.6677\n",
            "Total loss:  -3.6199 | PDE Loss:  -5.6013 | Function Loss:  -3.6678\n",
            "Total loss:  -3.62 | PDE Loss:  -5.6022 | Function Loss:  -3.6678\n",
            "Total loss:  -3.6201 | PDE Loss:  -5.6032 | Function Loss:  -3.6678\n",
            "Total loss:  -3.6202 | PDE Loss:  -5.6048 | Function Loss:  -3.6678\n",
            "Total loss:  -3.6202 | PDE Loss:  -5.6018 | Function Loss:  -3.668\n",
            "Total loss:  -3.6202 | PDE Loss:  -5.604 | Function Loss:  -3.6678\n",
            "Total loss:  -3.6202 | PDE Loss:  -5.6045 | Function Loss:  -3.6678\n",
            "Total loss:  -3.6202 | PDE Loss:  -5.6047 | Function Loss:  -3.6677\n",
            "Total loss:  -3.6202 | PDE Loss:  -5.6048 | Function Loss:  -3.6677\n",
            "Total loss:  -3.6202 | PDE Loss:  -5.6048 | Function Loss:  -3.6678\n",
            "Final Loss:  -3.620249032974243\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x28128b8e0>"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 960x720 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "hist = pinn.fit(num_epochs=n_epochs,\n",
        "                optimizer=optimizer_LBFGS,\n",
        "                verbose=True)\n",
        "\n",
        "plt.figure(dpi=150)\n",
        "plt.grid(True, which=\"both\", ls=\":\")\n",
        "plt.plot(np.arange(1, len(hist) + 1), hist, label=\"Train Loss\")\n",
        "plt.xscale(\"log\")\n",
        "plt.legend()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Save and load the training hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.save(pinn.approximate_solution.state_dict(), 'hyperparameters.pth')\n",
        "pinn.approximate_solution.load_state_dict(torch.load(\"hyperparameters.pth\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "'c' argument has 200000 elements, which is inconsistent with 'x' and 'y' with size 100000.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/matplotlib/axes/_axes.py:4439\u001b[0m, in \u001b[0;36mAxes._parse_scatter_color_args\u001b[0;34m(c, edgecolors, kwargs, xsize, get_next_color_func)\u001b[0m\n\u001b[1;32m   4438\u001b[0m \u001b[39mtry\u001b[39;00m:  \u001b[39m# Is 'c' acceptable as PathCollection facecolors?\u001b[39;00m\n\u001b[0;32m-> 4439\u001b[0m     colors \u001b[39m=\u001b[39m mcolors\u001b[39m.\u001b[39;49mto_rgba_array(c)\n\u001b[1;32m   4440\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m) \u001b[39mas\u001b[39;00m err:\n",
            "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/matplotlib/colors.py:487\u001b[0m, in \u001b[0;36mto_rgba_array\u001b[0;34m(c, alpha)\u001b[0m\n\u001b[1;32m    486\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 487\u001b[0m     rgba \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([to_rgba(cc) \u001b[39mfor\u001b[39;00m cc \u001b[39min\u001b[39;00m c])\n\u001b[1;32m    489\u001b[0m \u001b[39mif\u001b[39;00m alpha \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
            "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/matplotlib/colors.py:487\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    486\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 487\u001b[0m     rgba \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([to_rgba(cc) \u001b[39mfor\u001b[39;00m cc \u001b[39min\u001b[39;00m c])\n\u001b[1;32m    489\u001b[0m \u001b[39mif\u001b[39;00m alpha \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
            "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/matplotlib/colors.py:299\u001b[0m, in \u001b[0;36mto_rgba\u001b[0;34m(c, alpha)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[39mif\u001b[39;00m rgba \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:  \u001b[39m# Suppress exception chaining of cache lookup failure.\u001b[39;00m\n\u001b[0;32m--> 299\u001b[0m     rgba \u001b[39m=\u001b[39m _to_rgba_no_colorcycle(c, alpha)\n\u001b[1;32m    300\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
            "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/matplotlib/colors.py:381\u001b[0m, in \u001b[0;36m_to_rgba_no_colorcycle\u001b[0;34m(c, alpha)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39miterable(c):\n\u001b[0;32m--> 381\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInvalid RGBA argument: \u001b[39m\u001b[39m{\u001b[39;00morig_c\u001b[39m!r}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    382\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(c) \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m [\u001b[39m3\u001b[39m, \u001b[39m4\u001b[39m]:\n",
            "\u001b[0;31mValueError\u001b[0m: Invalid RGBA argument: 0.9997645020484924",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m pinn\u001b[39m.\u001b[39;49mplotting()\n",
            "Cell \u001b[0;32mIn[2], line 441\u001b[0m, in \u001b[0;36mPinns.plotting\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[39m# plt.colorbar(im1, ax=axs[0])\u001b[39;00m\n\u001b[1;32m    440\u001b[0m axs[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mgrid(\u001b[39mTrue\u001b[39;00m, which\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mboth\u001b[39m\u001b[39m\"\u001b[39m, ls\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m:\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 441\u001b[0m im2 \u001b[39m=\u001b[39m axs[\u001b[39m1\u001b[39;49m]\u001b[39m.\u001b[39;49mscatter(inputs[:, \u001b[39m1\u001b[39;49m]\u001b[39m.\u001b[39;49mdetach(), inputs[:, \u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49mdetach(), c\u001b[39m=\u001b[39;49moutput\u001b[39m.\u001b[39;49mdetach(), cmap\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mjet\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m    442\u001b[0m axs[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mset_xlabel(\u001b[39m\"\u001b[39m\u001b[39mx\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    443\u001b[0m axs[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mset_ylabel(\u001b[39m\"\u001b[39m\u001b[39mt\u001b[39m\u001b[39m\"\u001b[39m)\n",
            "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/matplotlib/__init__.py:1442\u001b[0m, in \u001b[0;36m_preprocess_data.<locals>.inner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1439\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m   1440\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minner\u001b[39m(ax, \u001b[39m*\u001b[39margs, data\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m   1441\u001b[0m     \u001b[39mif\u001b[39;00m data \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1442\u001b[0m         \u001b[39mreturn\u001b[39;00m func(ax, \u001b[39m*\u001b[39;49m\u001b[39mmap\u001b[39;49m(sanitize_sequence, args), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1444\u001b[0m     bound \u001b[39m=\u001b[39m new_sig\u001b[39m.\u001b[39mbind(ax, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1445\u001b[0m     auto_label \u001b[39m=\u001b[39m (bound\u001b[39m.\u001b[39marguments\u001b[39m.\u001b[39mget(label_namer)\n\u001b[1;32m   1446\u001b[0m                   \u001b[39mor\u001b[39;00m bound\u001b[39m.\u001b[39mkwargs\u001b[39m.\u001b[39mget(label_namer))\n",
            "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/matplotlib/axes/_axes.py:4602\u001b[0m, in \u001b[0;36mAxes.scatter\u001b[0;34m(self, x, y, s, c, marker, cmap, norm, vmin, vmax, alpha, linewidths, edgecolors, plotnonfinite, **kwargs)\u001b[0m\n\u001b[1;32m   4599\u001b[0m \u001b[39mif\u001b[39;00m edgecolors \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   4600\u001b[0m     orig_edgecolor \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39medgecolor\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m   4601\u001b[0m c, colors, edgecolors \u001b[39m=\u001b[39m \\\n\u001b[0;32m-> 4602\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_parse_scatter_color_args(\n\u001b[1;32m   4603\u001b[0m         c, edgecolors, kwargs, x\u001b[39m.\u001b[39;49msize,\n\u001b[1;32m   4604\u001b[0m         get_next_color_func\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_patches_for_fill\u001b[39m.\u001b[39;49mget_next_color)\n\u001b[1;32m   4606\u001b[0m \u001b[39mif\u001b[39;00m plotnonfinite \u001b[39mand\u001b[39;00m colors \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   4607\u001b[0m     c \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mma\u001b[39m.\u001b[39mmasked_invalid(c)\n",
            "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.10/site-packages/matplotlib/axes/_axes.py:4445\u001b[0m, in \u001b[0;36mAxes._parse_scatter_color_args\u001b[0;34m(c, edgecolors, kwargs, xsize, get_next_color_func)\u001b[0m\n\u001b[1;32m   4443\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   4444\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m valid_shape:\n\u001b[0;32m-> 4445\u001b[0m         \u001b[39mraise\u001b[39;00m invalid_shape_exception(c\u001b[39m.\u001b[39msize, xsize) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m   4446\u001b[0m     \u001b[39m# Both the mapping *and* the RGBA conversion failed: pretty\u001b[39;00m\n\u001b[1;32m   4447\u001b[0m     \u001b[39m# severe failure => one may appreciate a verbose feedback.\u001b[39;00m\n\u001b[1;32m   4448\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   4449\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39mc\u001b[39m\u001b[39m'\u001b[39m\u001b[39m argument must be a color, a sequence of colors, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   4450\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mor a sequence of numbers, not \u001b[39m\u001b[39m{\u001b[39;00mc\u001b[39m!r}\u001b[39;00m\u001b[39m\"\u001b[39m) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n",
            "\u001b[0;31mValueError\u001b[0m: 'c' argument has 200000 elements, which is inconsistent with 'x' and 'y' with size 100000."
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 2400x1200 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "pinn.plotting()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Get the predictions on the test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "U_q8DqdznK6z"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>t</th>\n",
              "      <th>x</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.624062</td>\n",
              "      <td>0.895</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.783512</td>\n",
              "      <td>0.778</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.398106</td>\n",
              "      <td>0.469</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.415906</td>\n",
              "      <td>0.948</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.949750</td>\n",
              "      <td>0.457</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99995</th>\n",
              "      <td>0.811444</td>\n",
              "      <td>0.697</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99996</th>\n",
              "      <td>0.498512</td>\n",
              "      <td>0.248</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99997</th>\n",
              "      <td>0.718912</td>\n",
              "      <td>0.100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99998</th>\n",
              "      <td>0.953963</td>\n",
              "      <td>0.247</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99999</th>\n",
              "      <td>0.463612</td>\n",
              "      <td>0.740</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100000 rows  2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "              t      x\n",
              "0      0.624062  0.895\n",
              "1      0.783512  0.778\n",
              "2      0.398106  0.469\n",
              "3      0.415906  0.948\n",
              "4      0.949750  0.457\n",
              "...         ...    ...\n",
              "99995  0.811444  0.697\n",
              "99996  0.498512  0.248\n",
              "99997  0.718912  0.100\n",
              "99998  0.953963  0.247\n",
              "99999  0.463612  0.740\n",
              "\n",
              "[100000 rows x 2 columns]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# test_data = pandas.loadtxt(\"Task1/TestingData.txt\", delimiter=',')\n",
        "\n",
        "test_data = pd.read_csv(\"Task1/TestingData.txt\", delimiter=',')\n",
        "display(test_data)\n",
        "\n",
        "t_test = torch.tensor(test_data['t'].values, dtype=torch.float32).reshape(-1, 1)\n",
        "x_test = torch.tensor(test_data['x'].values, dtype=torch.float32).reshape(-1, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for i in t_test:\n",
        "    print(i)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
