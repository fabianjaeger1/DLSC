{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "6u_j73SUfQxG",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "# Task 1: PINNs for solving PDEs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NOPdRWmOfQxI",
        "outputId": "7e48ccf5-8b35-43b2-fa3c-ebf973323f50",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x118599950>"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.optim as optim\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from Common import NeuralNet, MultiVariatePoly\n",
        "import time\n",
        "torch.autograd.set_detect_anomaly(True)\n",
        "torch.manual_seed(128)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We consider the following PDE (corresponding to the evolution of the Temperature of the solid and liquid phase in the first cycle for the charging phase) governed by the following two equations $$\\begin{aligned} \\frac{\\partial \\bar{T}_{f}}{\\partial t}+U_{f} \\frac{\\partial \\bar{T}_{f}}{\\partial x}=\\alpha_{f} \\frac{\\partial^{2} \\bar{T}_{f}}{\\partial x^{2}}-h_{f}\\left(\\bar{T}_{f}-\\bar{T}_{s}\\right) & x \\in[0,1], t \\in[0,1] \\\\ \\frac{\\partial \\bar{T}_{s}}{\\partial t}=\\alpha_{s} \\frac{\\partial^{2} \\bar{T}_{s}}{\\partial x^{2}}+h_{s}\\left(\\bar{T}_{f}-\\bar{T}_{s}\\right) & x \\in[0,1], t \\in[0,1]\\end{aligned}$$ with initial conditions $$\\bar{T}_{f}(x, t=0)=\\bar{T}_{s}(x, t=0)=T_{0}, \\quad x \\in[0,1]$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "HgRzd8_wfQxK",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "class Pinns:\n",
        "    def __init__(self, n_int_, n_sb_, n_tb_):\n",
        "        self.n_int = n_int_\n",
        "        self.n_sb = n_sb_\n",
        "        self.n_tb = n_tb_\n",
        "\n",
        "        self.alpha_f = 0.05\n",
        "        self.alpha_s = 0.08\n",
        "        self.h_f = 5\n",
        "        self.h_s = 6\n",
        "        self.T_hot = 4\n",
        "        self.T_0 = 1\n",
        "        self.U_f = 1 \n",
        "\n",
        "        # Extrema of the solution domain (t,x) in [0,0.1]x[-1,1]\n",
        "        self.domain_extrema = torch.tensor([[0, 1],  # Time dimension\n",
        "                                            [0, 1]])  # Space dimension\n",
        "\n",
        "        # Number of space dimensions\n",
        "        self.space_dimensions = 1\n",
        "\n",
        "        # Parameter to balance role of data and PDE\n",
        "        self.lambda_u = 10\n",
        "\n",
        "        # F Dense NN to approximate the solution of the underlying heat equation\n",
        "        self.approximate_solution = NeuralNet(input_dimension=self.domain_extrema.shape[0], output_dimension=2,\n",
        "                                              n_hidden_layers=4,\n",
        "                                              neurons=20,\n",
        "                                              regularization_param=0.,\n",
        "                                              regularization_exp=2.,\n",
        "                                              retrain_seed=42)\n",
        "        '''self.approximate_solution = MultiVariatePoly(self.domain_extrema.shape[0], 3)'''\n",
        "\n",
        "        # Generator of Sobol sequences\n",
        "        self.soboleng = torch.quasirandom.SobolEngine(dimension=self.domain_extrema.shape[0])\n",
        "\n",
        "        # Training sets S_sb, S_tb, S_int as torch dataloader\n",
        "        self.training_set_sb, self.training_set_tb, self.training_set_int = self.assemble_datasets()\n",
        "\n",
        "    ################################################################################################\n",
        "    # Function to linearly transform a tensor whose value are between 0 and 1\n",
        "    # to a tensor whose values are between the domain extrema\n",
        "    def convert(self, tens):\n",
        "        assert (tens.shape[1] == self.domain_extrema.shape[0])\n",
        "        return tens * (self.domain_extrema[:, 1] - self.domain_extrema[:, 0]) + self.domain_extrema[:, 0]\n",
        "    \n",
        "    # Initial condition T_f(x,t = 0) = T_s(x,t=0) = T_0\n",
        "    # def initial_condition(self, x):\n",
        "    #     return torch.tensor(self.T_0)\n",
        "\n",
        "    ################################################################################################\n",
        "    # Function returning the input-output tensor required to assemble the training set S_tb corresponding to the temporal boundary\n",
        "    def add_temporal_boundary_points(self):\n",
        "        # t0 = self.domain_extrema[0, 0]\n",
        "        # input_tb = self.soboleng.draw(self.n_tb)    # input_sb has two columns (t, x) both with random numbers in the two respective domains\n",
        "        # input_tb[:, 0] = torch.full(input_tb[:, 0].shape, t0)   # overwrite the entier column of time with t0\n",
        "        # output_tb = torch.full(input_tb.shape, self.T_0)\n",
        "        t0 = self.domain_extrema[0, 0]\n",
        "        input_tb = self.convert(self.soboleng.draw(self.n_tb))\n",
        "        input_tb[:, 0] = torch.full(input_tb[:, 0].shape, t0)\n",
        "        # print(\"Input temporal boundary shape: \", input_tb.shape)\n",
        "        # output_tb = self.initial_condition(input_tb[:, 1]).reshape(-1, 1)\n",
        "        output_tb = torch.randn(self.n_tb, 2).fill_(self.T_0)\n",
        "        # print( \"Output temporal boundary shape: \", output_tb.shape)\n",
        "        return input_tb, output_tb\n",
        "\n",
        "    # Function returning the input-output tensor required to assemble the training set S_sb corresponding to the spatial boundary\n",
        "    def add_spatial_boundary_points(self):\n",
        "        #! Should be fixed\n",
        "\n",
        "        #set first column of output to be T_f\n",
        "        #set second column of output to be T_s\n",
        "\n",
        "        # Get x-coordinates of the extrema of the domain\n",
        "        x0 = self.domain_extrema[1, 0]\n",
        "        xL = self.domain_extrema[1, 1]\n",
        "\n",
        "        # input_sb = self.convert(self.soboleng.draw(self.n_sb))\n",
        "        input_sb = self.soboleng.draw(self.n_sb)\n",
        "\n",
        "        # Corresponds to the dirichlet boundary condition u_b(t,-1) = 0\n",
        "        input_sb_0 = torch.clone(input_sb)\n",
        "        input_sb_0[:, 1] = torch.full(input_sb_0[:, 1].shape, x0)\n",
        "\n",
        "        # Corresponds to the dirichlet boundary condition u_b(t, 1) = 0\n",
        "        input_sb_L = torch.clone(input_sb)\n",
        "        input_sb_L[:, 1] = torch.full(input_sb_L[:, 1].shape, xL)\n",
        "\n",
        "        # These are the Dirichlet boundary conditions at the two extrema domain points -1 and 1\n",
        "        # output_sb_0 = torch.zeros((input_sb.shape[0], 1))\n",
        "        output_sb_L = torch.zeros((input_sb.shape[0], 2))\n",
        "        # output_sb_0 = ((self.T_hot-self.T_0)/(1+torch.exp(-200*(input_sb-0.25)))+self.T_0) \n",
        "\n",
        "        input_sb_t =  input_sb[:, 0]\n",
        "        # print(((self.T_hot-self.T_0)/(1+torch.exp(-200*(input_sb_t-0.25)))+self.T_0))\n",
        "        # print(input_sb_t)\n",
        "        # output_sb_0 = torch.full(((self.T_hot-self.T_0)/(1+torch.exp(-200*(input_sb_t-0.25)))+self.T_0), x0)\n",
        "\n",
        "        \n",
        "        output_sb_0 = ((self.T_hot-self.T_0)/(1+torch.exp(-200*(input_sb_t-0.25)))+self.T_0).reshape(-1,1)\n",
        "        new_tensor = torch.zeros_like(output_sb_0)\n",
        "\n",
        "        output_sb_0 = torch.cat((output_sb_0, new_tensor), dim=1)\n",
        "        # print(\"Output sb: {}\".format(output_sb_0))\n",
        "        # output_sb_0 = torch.full(output_sb_0[:, 1].shape, 0)\n",
        "\n",
        "        # conditions T_xL_dx\n",
        "\n",
        "        # print(\"Input sb 0: {}\".format(input_sb_0))\n",
        "        # print(\"Input sb L: {}\".format(input_sb_L))\n",
        "        # print(\"Output sb 0: {}\".format(output_sb_0))\n",
        "        # print(\"Output sb L: {}\".format(output_sb_L))\n",
        "\n",
        "        # print(\"Concatenated input: {}\".format(torch.cat([input_sb_0, input_sb_L], 0)))\n",
        "        # print(\"Concatenated output: {}\".format(torch.cat([output_sb_0, output_sb_L], 0)))\n",
        "    \n",
        "        # print(output_sb_0)\n",
        "\n",
        "        return torch.cat([input_sb_0, input_sb_L], 0), torch.cat([output_sb_0, output_sb_L], 0)\n",
        "\n",
        "    #  Function returning the input-output tensor required to assemble the training set S_int corresponding to the interior domain where the PDE is enforced\n",
        "    def add_interior_points(self):\n",
        "        # input_int = self.convert(self.soboleng.draw(self.n_int))\n",
        "        input_int = self.soboleng.draw(self.n_int)\n",
        "        output_int = torch.zeros((input_int.shape[0], 1))\n",
        "        return input_int, output_int\n",
        "\n",
        "    # Function returning the training sets S_sb, S_tb, S_int as dataloader\n",
        "    def assemble_datasets(self):\n",
        "        input_sb, output_sb = self.add_spatial_boundary_points()   # S_sb\n",
        "        input_tb, output_tb = self.add_temporal_boundary_points()  # S_tb\n",
        "        input_int, output_int = self.add_interior_points()         # S_int\n",
        "\n",
        "        training_set_sb = DataLoader(torch.utils.data.TensorDataset(input_sb, output_sb), batch_size=2*self.space_dimensions*self.n_sb, shuffle=False)\n",
        "        training_set_tb = DataLoader(torch.utils.data.TensorDataset(input_tb, output_tb), batch_size=self.n_tb, shuffle=False)\n",
        "        training_set_int = DataLoader(torch.utils.data.TensorDataset(input_int, output_int), batch_size=self.n_int, shuffle=False)\n",
        "\n",
        "        return training_set_sb, training_set_tb, training_set_int\n",
        "\n",
        "    ################################################################################################\n",
        "    # Function to compute the terms required in the definition of the TEMPORAL boundary residual\n",
        "    # Takes as input the temporal boundary points and returns the prediction of the neural network at the temporal boundary points\n",
        "    def apply_initial_condition(self, input_tb):\n",
        "        u_pred_tb = self.approximate_solution(input_tb)\n",
        "        #print(u_pred_tb)\n",
        "        return u_pred_tb\n",
        "\n",
        "    # Function to compute the terms required in the definition of the SPATIAL boundary residual\n",
        "    # Takes as input the spatial boundary points and returns the prediction of the neural network at the temporal boundary points\n",
        "    def apply_boundary_conditions(self, input_sb):\n",
        "        #! Error probably here\n",
        "        input_sb.requires_grad = True\n",
        "        # input_sb = input_sb.detach()\n",
        "        # input_sb.requires_grad = True\n",
        "\n",
        "\n",
        "        # u_pred_sb = self.approximate_solution(input_sb) \n",
        "        # print(u_pred_sb)\n",
        "\n",
        "        #CORRECT\n",
        "        input_sb_x0 = input_sb[:int(input_sb.shape[0]/2), :]\n",
        "        input_sb_xL = input_sb[int(input_sb.shape[0]/2):, :]\n",
        "\n",
        "\n",
        "        # print(\"Input sb x0: {}\".format(input_sb_x0))\n",
        "        # print(\"Input sb xL: {}\".format(input_sb_xL))\n",
        "       \n",
        "        #condtion on Tf_x0\n",
        "\n",
        "        #CORRECT\n",
        "\n",
        "        # pre\n",
        "\n",
        "        u_pred_Tf_x0 = self.approximate_solution(input_sb_x0)[:, 0]\n",
        "        u_pred_Ts_x0 = self.approximate_solution(input_sb_x0)[:, 1]\n",
        "\n",
        "        # print(\"T_f x0 pred: {}\".format(u_pred_Tf_x0))\n",
        "\n",
        "\n",
        "        grad_Ts_x0 = torch.autograd.grad(u_pred_Ts_x0.sum(), input_sb_x0, create_graph=True)[0][:, 1]\n",
        "\n",
        "        # u_pred_Ts_x0 = u_pred_Ts_x0.view(-1, 1)\n",
        "        # grad_Ts_x0 = grad_Ts_x0.view(-1, 1).squeeze()\n",
        "\n",
        "        # print(\"pred Grad T_s x0: {}\".format(grad_Ts_x0))\n",
        "\n",
        "\n",
        "\n",
        "        # u_pred_xL = self.approximate_solution(input_sb_xL)\n",
        "        u_pred_Tf_xL = self.approximate_solution(input_sb_xL)[:, 0]\n",
        "        u_pred_Ts_xL = self.approximate_solution(input_sb_xL)[:, 1]\n",
        "\n",
        "        # print(\"U_pred_Tf_x0: {}\".format(u_pred_Tf_x0))\n",
        "        # print(\"U_pred_Ts_x0: {}\".format(u_pred_Ts_x0))\n",
        "\n",
        "        #conditidion on grad_Ts_x0\n",
        "        \n",
        " \n",
        "\n",
        "        #condition on grad_Tf/s_xL\n",
        "      \n",
        "\n",
        "        grad_Tf_xL = torch.autograd.grad(u_pred_Tf_xL.sum(), input_sb_xL, create_graph=True)[0][:, 1]\n",
        "        grad_Ts_xL = torch.autograd.grad(u_pred_Ts_xL.sum(), input_sb_xL, create_graph=True)[0][:, 1]\n",
        "\n",
        "        # print(\"pred Grad T_f xL: {}\".format(grad_Tf_xL))\n",
        "        # print(\"pred Grad T_s xL: {}\".format(grad_Ts_xL))\n",
        "\n",
        "        # grad_Ts_xL = u_pred_Tf_x0.view(-1, 1).squeeze()\n",
        "        # grad_Tf_xL = grad_Tf_xL.view(-1, 1).squeeze()\n",
        "\n",
        "\n",
        "\n",
        "        # print(\"grad_Tf_xL: \", grad_Tf_xL.shape)\n",
        "        # print(\"grad_Ts_xL: \", grad_Ts_xL.shape)\n",
        "\n",
        "        # print(\"u_pred_Tf_x0: \", u_pred_Tf_x0.shape)\n",
        "        # print(\"grad_Ts_x0: \", grad_Ts_x0.shape)\n",
        "\n",
        "        # print('u_pred: ', u_pred_sb.requires_grad)\n",
        "        # print('inp: ', input_sb.requires_grad)\n",
        "\n",
        "        u_pred_concat_1 = torch.cat((u_pred_Tf_x0.reshape(-1,1), grad_Ts_x0.reshape(-1,1)), dim =1)\n",
        "        # print(\"U_pred_concat_1: \", u_pred_concat_1)\n",
        "        u_pred_concat_2 = torch.cat((grad_Tf_xL.reshape(-1,1), grad_Ts_xL.reshape(-1,1)), dim =1)\n",
        "        # print(\"U_pred_concat_2: \", u_pred_concat_2)\n",
        "        total_concat = torch.cat((u_pred_concat_1, u_pred_concat_2), dim =0)\n",
        "        # print(\"Total_concat: \", total_concat.shape)\n",
        "\n",
        "\n",
        "        #obtain the prediction for (T_f, T_s) at the boundary points (x=0, t) for the first 64 and (x=1, t) for the second 64\n",
        "        # print(u_pred_sb.shape)\n",
        "        # print(input_sb.shape)\n",
        "\n",
        "        # print(u_pred_sb[:self.n_sb, 1].reshape(-1, 1).shape)\n",
        "        # print(input_sb[:self.n_sb, :].shape)\n",
        "        \n",
        "        # T_s_1= u_pred_sb[:self.n_sb, 1].reshape(-1, 1)\n",
        "        # T_s_2= u_pred_sb[:self.n_sb, 1].reshape(-1, 1) \n",
        "\n",
        "        # input_sb_1 = input_sb[:self.n_sb, :]\n",
        "        # input_sb_2 = input_sb[self.n_sb:-1, :]\n",
        "\n",
        "        # print(input_sb[:,0])\n",
        "        # # print('u_pred: ', T_s_1.requires_grad)\n",
        "        # # print('inp: ', input_sb_1.requires_grad)\n",
        "\n",
        "        # grad = torch.autograd.grad(u_pred_sb.sum(), input_sb, create_graph=True)[0]\n",
        "        # print(grad)\n",
        "\n",
        "        # grad_T_s_1 = grad[:self.n_sb, 1]\n",
        "        # grad_T_s_2 = grad[self.n_sb:, 1]\n",
        "        # print(\"Shape grad_T_s_1: \", grad_T_s_1.shape)\n",
        "        # print(\"Shape grad_T_s_2: \", grad_T_s_2.shape)\n",
        "        # pred_T_f_1 = (self.T_hot - self.T_0)/(1 + torch.exp(-200*(input_sb[:self.n_sb,0]))) + self.T_0\n",
        "        # grad_T_f_2 = grad[self.n_sb:, 0]\n",
        "        \n",
        "        # # pred_T_f_2 = (self.T_hot - self.T_0)/(1 + torch.exp(-200*(input_sb[self.n_sb:,0]))) + self.T_0\n",
        "        # print(\"Shape pred_T_f_1: \", pred_T_f_1.shape)\n",
        "        # # print(\"Shape pred_T_f_2: \", pred_T_f_2.shape)\n",
        "        # print(pred_T_f_1)\n",
        "        # print(pred_T_f_2)\n",
        "\n",
        "        # grad_T_s_1 = torch.autograd.grad(T_s_1.sum(), input_sb_1, create_graph=True)[0][:,1]\n",
        "        # grad_T_s_2 = torch.autograd.grad(T_s_2.sum(), input_sb_2, create_graph=True)[0][:,1]\n",
        "        # # grad_T_s = torch.clone(T_s_1)\n",
        "        # pred_T_f_1 = (self.T_hot - self.T_0)/(1 + torch.exp(-200*(u_pred_sb[:self.n_t, 0]))) + self.T_0\n",
        "        # pred_T_f_2 = (self.T_hot - self.T_0)/(1 + torch.exp(-200*(u_pred_sb[self.n_t:-1, 0]))) + self.T_0\n",
        "\n",
        "        # part_1 = torch.cat([pred_T_f_1, grad_T_s_1],1)\n",
        "        # part_2 = torch.cat([pred_T_f_2, grad_T_s_2],1)\n",
        "        # print(part_1)\n",
        "        # print(part_2)\n",
        "        # concat = torch.cat([part_1, part_2],0)\n",
        "        # print(pred_T_f)\n",
        "        # print(grad_T_s)\n",
        "\n",
        "        return torch.cat([u_pred_concat_1, u_pred_concat_2],0)\n",
        "\n",
        "    # Function to compute the PDE residuals\n",
        "    def compute_pde_residual(self, input_int):\n",
        "        #! Should be correct\n",
        "\n",
        "        input_int.requires_grad = True\n",
        "\n",
        "        # Obtain the prediction from the neural network\n",
        "        u = self.approximate_solution(input_int)\n",
        "        \n",
        "        # First column of prediction will be the temperature of the fluid\n",
        "        # Second column of prediction will be the temperature of the solid\n",
        "        T_f = u[:,0]\n",
        "        T_s = u[:,1]\n",
        "\n",
        "        # grad compute the gradient of a \"SCALAR\" function L with respect to some input nxm TENSOR Z=[[x1, y1],[x2,y2],[x3,y3],...,[xn,yn]], m=2\n",
        "        # it returns grad_L = [[dL/dx1, dL/dy1],[dL/dx2, dL/dy2],[dL/dx3, dL/dy3],...,[dL/dxn, dL/dyn]]\n",
        "        # Note: pytorch considers a tensor [u1, u2,u3, ... ,un] a vectorial function\n",
        "        # whereas sum_u = u1 + u2 + u3 + u4 + ... + un as a \"scalar\" one\n",
        "\n",
        "        # In our case ui = u(xi), therefore the line below returns:\n",
        "        # grad_u = [[dsum_u/dx1, dsum_u/dy1],[dsum_u/dx2, dsum_u/dy2],[dsum_u/dx3, dL/dy3],...,[dsum_u/dxm, dsum_u/dyn]]\n",
        "        # and dsum_u/dxi = d(u1 + u2 + u3 + u4 + ... + un)/dxi = d(u(x1) + u(x2) u3(x3) + u4(x4) + ... + u(xn))/dxi = dui/dxi\n",
        "        grad_tf = torch.autograd.grad(T_f.sum(), input_int, create_graph=True)[0]\n",
        "        grad_ts = torch.autograd.grad(T_s.sum(), input_int, create_graph=True)[0]\n",
        "        \n",
        "        grad_ts_t = grad_ts[:,0] \n",
        "        grad_ts_x = grad_ts[:,1]\n",
        "        grad_tf_t = grad_tf[:,0]\n",
        "        grad_tf_x = grad_tf[:,1 ]\n",
        "        \n",
        "        grad_tf_xx = torch.autograd.grad(grad_tf_x.sum(), input_int, create_graph=True)[0][:,1]\n",
        "        grad_ts_xx = torch.autograd.grad(grad_ts_x.sum(), input_int, create_graph=True)[0][:,1]\n",
        "\n",
        "\n",
        "        residual1 = (grad_tf_t) + (self.U_f*grad_tf_x) - (self.alpha_f*grad_tf_xx) + (self.h_f*(T_f - T_s))\n",
        "        residual2 = (grad_ts_t) - (self.alpha_s*grad_ts_xx) - self.h_s*(T_f - T_s)\n",
        "\n",
        "        # print(residual1, residual2)\n",
        "\n",
        "        # residual = residual1 + residual2\n",
        "        # grad_u_t = grad_u[:, 0]\n",
        "        # grad_u_x = grad_u[:, 1]\n",
        "        # grad_u_xx = torch.autograd.grad(grad_u_x.sum(), input_int, create_graph=True)[0][:, 1]\n",
        "\n",
        "        #grad_u_sq_x = torch.autograd.grad(u_sq.sum(), input_int, create_graph=True)[0][:,1]\n",
        "\n",
        "        # residual = grad_u_t - grad_u_xx\n",
        "        return residual1.reshape(-1, ), residual2.reshape(-1, )\n",
        "\n",
        "    # Function to compute the total loss (weighted sum of spatial boundary loss, temporal boundary loss and interior loss)\n",
        "    def compute_loss(self, inp_train_sb, u_train_sb, inp_train_tb, u_train_tb, inp_train_int, verbose=True):\n",
        "        u_pred_sb = self.apply_boundary_conditions(inp_train_sb)\n",
        "        u_pred_tb = self.apply_initial_condition(inp_train_tb)\n",
        "        # print(u_pred_sb.shape)\n",
        "\n",
        "\n",
        "        # print(\"U_pred_sb: {}\".format(u_pred_sb))\n",
        "        # Seems correct\n",
        "        # print(\"U_train_sb: {}\".format(u_train_sb))\n",
        "\n",
        "        assert (u_pred_sb.shape[1] == u_train_sb.shape[1])\n",
        "        assert (u_pred_tb.shape[1] == u_train_tb.shape[1])\n",
        "\n",
        "        res1, res2 = self.compute_pde_residual(inp_train_int)\n",
        "        # r_int = self.compute_pde_residual(inp_train_int)\n",
        "\n",
        "        # Initial Condition loss\n",
        "        res_tb = u_train_tb - u_pred_tb\n",
        "        \n",
        "        # This corresponds to the condition with the exponential in the boundary conditions\n",
        "        ##########################################\n",
        "\n",
        "        len_train_half = int(u_train_sb.shape[0]/2)\n",
        "        # print(\"len_train: {}\".format(len_train_half))\n",
        "        #! ERROR?\n",
        "        Tf_train_sb_0 = u_train_sb[:len_train_half, 0]\n",
        "        Tf_pred_sb_0 = u_pred_sb[:len_train_half, 0]\n",
        "        # print(\"Tf_train_sb_0: {}\".format(Tf_train_sb_0))\n",
        "        # print(\"Tf_pred_sb_0: {}\".format(Tf_pred_sb_0))\n",
        "        res_Tf_sb_0 = Tf_train_sb_0 - Tf_pred_sb_0\n",
        "\n",
        "        # Condition grad T_s/x at x = 0\n",
        "        Ts_train_sb_0 = u_train_sb[:len_train_half, 1]\n",
        "        Ts_pred_sb_0 = u_pred_sb[:len_train_half, 1]\n",
        "        # print(\"Ts_train_sb_0: {}\".format(Ts_train_sb_0))\n",
        "        # print(\"Ts_pred_sb_0: {}\".format(Ts_pred_sb_0))\n",
        "        res_Ts_sb_0 = Ts_train_sb_0 - Ts_pred_sb_0\n",
        "\n",
        "        # Condition grad T_s/x at x = 1\n",
        "        Ts_train_sb_L = u_train_sb[len_train_half:, 1]\n",
        "        Ts_pred_sb_L = u_pred_sb[len_train_half:, 1]\n",
        "        # print(\"Ts_train_sb_L: {}\".format(Ts_train_sb_L))\n",
        "        # print(\"Ts_pred_sb_L: {}\".format(Ts_pred_sb_L))\n",
        "        res_Ts_sb_L = Ts_train_sb_L - Ts_pred_sb_L\n",
        "\n",
        "        # Condition grad T_f/x at x = 1\n",
        "        Tf_train_sb_L = u_train_sb[len_train_half:, 0]\n",
        "        Tf_pred_sb_L = u_pred_sb[len_train_half:, 0]\n",
        "        # print(\"Tf_train_sb_L: {}\".format(Tf_train_sb_L))\n",
        "        # print(\"Tf_pred_sb_L: {}\".format(Tf_pred_sb_L))\n",
        "        res_Tf_sb_L = Tf_train_sb_L - Tf_pred_sb_L\n",
        "\n",
        "        # print(res_Tf_sb_0)\n",
        "        # print(res_Ts_sb_0)\n",
        "        # print(res_Ts_sb_L)\n",
        "        # print(res_Tf_sb_L)\n",
        "        ############################``\n",
        "\n",
        "        loss_sb = torch.mean(abs(res_Tf_sb_0) ** 2) + torch.mean(abs(res_Ts_sb_0) ** 2) + torch.mean(abs(res_Ts_sb_L) ** 2) + torch.mean(abs(res_Tf_sb_L) ** 2)\n",
        "        loss_tb = torch.mean(abs(res_tb) ** 2)\n",
        "        loss_int = torch.mean(abs(res1) ** 2) + torch.mean(abs(res2) ** 2)\n",
        "\n",
        "        # print(\"Loss int: {}\".format(loss_int))\n",
        "        # print(\"Loss sb: {}\".format(loss_sb))\n",
        "        # print(\"Loss tb: {}\".format(loss_tb))\n",
        "\n",
        "        loss_u = loss_sb + loss_tb\n",
        "\n",
        "        # loss = torch.log10(self.lambda_u * (loss_sb + loss_tb) + loss_int)\n",
        "        loss = torch.log10(self.lambda_u * (1*loss_sb + 1*loss_tb) + 1*loss_int)\n",
        "        if verbose: print(\"Total loss: \", round(loss.item(), 4), \"| PDE Loss: \", round(torch.log10(loss_u).item(), 4), \"| Function Loss: \", round(torch.log10(loss_int).item(), 4))\n",
        "\n",
        "        return loss\n",
        "\n",
        "    ################################################################################################\n",
        "    def fit(self, num_epochs, optimizer, verbose=True):\n",
        "        history = list()\n",
        "\n",
        "        # Loop over epochs\n",
        "        for epoch in range(num_epochs):\n",
        "            if verbose: print(\"################################ \", epoch, \" ################################\")\n",
        "\n",
        "            for j, ((inp_train_sb, u_train_sb), (inp_train_tb, u_train_tb), (inp_train_int, u_train_int)) in enumerate(zip(self.training_set_sb, self.training_set_tb, self.training_set_int)):\n",
        "                def closure():\n",
        "                    optimizer.zero_grad()\n",
        "                    loss = self.compute_loss(inp_train_sb, u_train_sb, inp_train_tb, u_train_tb, inp_train_int, verbose=verbose)\n",
        "                    loss.backward()\n",
        "\n",
        "                    history.append(loss.item())\n",
        "                    return loss\n",
        "\n",
        "                optimizer.step(closure=closure)\n",
        "\n",
        "        print('Final Loss: ', history[-1])\n",
        "\n",
        "        return history\n",
        "\n",
        "    ################################################################################################\n",
        "    def plotting(self):\n",
        "        inputs = self.soboleng.draw(100000)\n",
        "        #inputs = self.convert(inputs)\n",
        "\n",
        "        output = self.approximate_solution(inputs).reshape(-1, )\n",
        "        # exact_output = self.exact_solution(inputs).reshape(-1, )\n",
        "\n",
        "        fig, axs = plt.subplots(1, 2, figsize=(16, 8), dpi=150)\n",
        "        # im1 = axs[0].scatter(inputs[:, 1].detach(), inputs[:, 0].detach(), c=exact_output.detach(), cmap=\"jet\")\n",
        "        axs[0].set_xlabel(\"x\")\n",
        "        axs[0].set_ylabel(\"t\")\n",
        "        # plt.colorbar(im1, ax=axs[0])\n",
        "        axs[0].grid(True, which=\"both\", ls=\":\")\n",
        "        im2 = axs[1].scatter(inputs[:, 1].detach(), inputs[:, 0].detach(), c=output.detach(), cmap=\"jet\")\n",
        "        axs[1].set_xlabel(\"x\")\n",
        "        axs[1].set_ylabel(\"t\")\n",
        "        plt.colorbar(im2, ax=axs[1])\n",
        "        axs[1].grid(True, which=\"both\", ls=\":\")\n",
        "        axs[0].set_title(\"Exact Solution\")\n",
        "        axs[1].set_title(\"Approximate Solution\")\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "        # err = (torch.mean((output - exact_output) ** 2) / torch.mean(exact_output ** 2)) ** 0.5 * 100\n",
        "        # print(\"L2 Relative Error Norm: \", err.item(), \"%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "M3ug4ztBfQxM",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# Solve the heat equation:\n",
        "# u_t = u_xx, (t,x) in [0, 0.1]x[-1,1]\n",
        "# with zero dirichlet BC and\n",
        "# u(x,0)= -sin(pi x)\n",
        "\n",
        "n_int = 256\n",
        "n_sb = 64\n",
        "n_tb = 64\n",
        "\n",
        "pinn = Pinns(n_int, n_sb, n_tb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 859
        },
        "id": "l4gxwi51fQxM",
        "outputId": "048a7506-4f92-447d-e58d-e39f05548b23",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# # Plot the input training points\n",
        "# input_sb_, output_sb_ = pinn.add_spatial_boundary_points()\n",
        "# input_tb_, output_tb_ = pinn.add_temporal_boundary_points()\n",
        "# input_int_, output_int_ = pinn.add_interior_points()\n",
        "\n",
        "# plt.figure(figsize=(16, 8), dpi=150)\n",
        "# plt.scatter(input_sb_[:, 1].detach().numpy(), input_sb_[:, 0].detach().numpy(), label=\"Boundary Points\")\n",
        "# plt.scatter(input_int_[:, 1].detach().numpy(), input_int_[:, 0].detach().numpy(), label=\"Interior Points\")\n",
        "# plt.scatter(input_tb_[:, 1].detach().numpy(), input_tb_[:, 0].detach().numpy(), label=\"Initial Points\")\n",
        "# plt.xlabel(\"x\")\n",
        "# plt.ylabel(\"t\")\n",
        "# plt.legend()\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "0CPnHdKtfQxN",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "n_epochs = 1\n",
        "optimizer_LBFGS = optim.LBFGS(pinn.approximate_solution.parameters(),\n",
        "                              lr=float(0.5),\n",
        "                              max_iter=50000,\n",
        "                              max_eval=50000,\n",
        "                              history_size=150,\n",
        "                              line_search_fn=\"strong_wolfe\",\n",
        "                              tolerance_change=1.0 * np.finfo(float).eps)\n",
        "optimizer_ADAM = optim.Adam(pinn.approximate_solution.parameters(),\n",
        "                            lr=float(0.001))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "qub-M5jqfQxN",
        "outputId": "6cf2b62f-f8f2-4e80-b0c4-c3a736fa93eb",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "################################  0  ################################\n",
            "Total loss:  2.3624 | PDE Loss:  1.1527 | Function Loss:  1.9457\n",
            "Total loss:  2.3071 | PDE Loss:  1.1268 | Function Loss:  1.8382\n",
            "Total loss:  2.1081 | PDE Loss:  0.911 | Function Loss:  1.6701\n",
            "Total loss:  1.9951 | PDE Loss:  0.8484 | Function Loss:  1.4523\n",
            "Total loss:  1.8096 | PDE Loss:  0.7492 | Function Loss:  0.923\n",
            "Total loss:  1.7431 | PDE Loss:  0.7111 | Function Loss:  0.5944\n",
            "Total loss:  1.7065 | PDE Loss:  0.6671 | Function Loss:  0.6439\n",
            "Total loss:  1.5212 | PDE Loss:  0.2948 | Function Loss:  1.1301\n",
            "Total loss:  1.4731 | PDE Loss:  0.2881 | Function Loss:  1.0132\n",
            "Total loss:  1.4377 | PDE Loss:  0.2781 | Function Loss:  0.9256\n",
            "Total loss:  1.4281 | PDE Loss:  0.2721 | Function Loss:  0.9078\n",
            "Total loss:  1.414 | PDE Loss:  0.2566 | Function Loss:  0.8969\n",
            "Total loss:  1.385 | PDE Loss:  0.2167 | Function Loss:  0.8917\n",
            "Total loss:  1.3577 | PDE Loss:  0.1723 | Function Loss:  0.8986\n",
            "Total loss:  1.3273 | PDE Loss:  0.1241 | Function Loss:  0.8999\n",
            "Total loss:  1.299 | PDE Loss:  0.0842 | Function Loss:  0.8902\n",
            "Total loss:  1.2791 | PDE Loss:  0.0559 | Function Loss:  0.8831\n",
            "Total loss:  1.262 | PDE Loss:  0.0349 | Function Loss:  0.8718\n",
            "Total loss:  1.2422 | PDE Loss:  0.0227 | Function Loss:  0.8406\n",
            "Total loss:  1.2078 | PDE Loss:  0.0157 | Function Loss:  0.761\n",
            "Total loss:  1.1696 | PDE Loss:  0.0221 | Function Loss:  0.6289\n",
            "Total loss:  1.1309 | PDE Loss:  -0.0008 | Function Loss:  0.5484\n",
            "Total loss:  1.0733 | PDE Loss:  -0.0259 | Function Loss:  0.3831\n",
            "Total loss:  1.0477 | PDE Loss:  -0.0542 | Function Loss:  0.3682\n",
            "Total loss:  1.0327 | PDE Loss:  -0.0607 | Function Loss:  0.3197\n",
            "Total loss:  1.0194 | PDE Loss:  -0.0728 | Function Loss:  0.3012\n",
            "Total loss:  1.0042 | PDE Loss:  -0.0881 | Function Loss:  0.286\n",
            "Total loss:  0.9722 | PDE Loss:  -0.138 | Function Loss:  0.3227\n",
            "Total loss:  0.9346 | PDE Loss:  -0.1915 | Function Loss:  0.3358\n",
            "Total loss:  0.9026 | PDE Loss:  -0.2368 | Function Loss:  0.3412\n",
            "Total loss:  0.8773 | PDE Loss:  -0.26 | Function Loss:  0.3103\n",
            "Total loss:  0.8489 | PDE Loss:  -0.2717 | Function Loss:  0.2337\n",
            "Total loss:  0.8282 | PDE Loss:  -0.2955 | Function Loss:  0.2226\n",
            "Total loss:  0.8156 | PDE Loss:  -0.3033 | Function Loss:  0.195\n",
            "Total loss:  0.81 | PDE Loss:  -0.3123 | Function Loss:  0.1997\n",
            "Total loss:  0.8054 | PDE Loss:  -0.325 | Function Loss:  0.2191\n",
            "Total loss:  0.799 | PDE Loss:  -0.3371 | Function Loss:  0.2287\n",
            "Total loss:  0.7896 | PDE Loss:  -0.3541 | Function Loss:  0.2396\n",
            "Total loss:  0.7753 | PDE Loss:  -0.3772 | Function Loss:  0.2467\n",
            "Total loss:  0.7633 | PDE Loss:  -0.3927 | Function Loss:  0.2432\n",
            "Total loss:  0.7528 | PDE Loss:  -0.402 | Function Loss:  0.2299\n",
            "Total loss:  0.7408 | PDE Loss:  -0.4105 | Function Loss:  0.2092\n",
            "Total loss:  0.7299 | PDE Loss:  -0.4164 | Function Loss:  0.1863\n",
            "Total loss:  0.7175 | PDE Loss:  -0.4272 | Function Loss:  0.17\n",
            "Total loss:  0.7021 | PDE Loss:  -0.4366 | Function Loss:  0.139\n",
            "Total loss:  0.682 | PDE Loss:  -0.4521 | Function Loss:  0.1064\n",
            "Total loss:  0.6586 | PDE Loss:  -0.4609 | Function Loss:  0.0401\n",
            "Total loss:  0.6366 | PDE Loss:  -0.4799 | Function Loss:  0.0083\n",
            "Total loss:  0.628 | PDE Loss:  -0.4834 | Function Loss:  -0.0173\n",
            "Total loss:  0.6231 | PDE Loss:  -0.4882 | Function Loss:  -0.0228\n",
            "Total loss:  0.6164 | PDE Loss:  -0.4957 | Function Loss:  -0.0264\n",
            "Total loss:  0.6068 | PDE Loss:  -0.5038 | Function Loss:  -0.0415\n",
            "Total loss:  0.5978 | PDE Loss:  -0.5079 | Function Loss:  -0.068\n",
            "Total loss:  0.5899 | PDE Loss:  -0.5092 | Function Loss:  -0.1001\n",
            "Total loss:  0.5832 | PDE Loss:  -0.5122 | Function Loss:  -0.1218\n",
            "Total loss:  0.5745 | PDE Loss:  -0.5161 | Function Loss:  -0.1505\n",
            "Total loss:  0.5647 | PDE Loss:  -0.5232 | Function Loss:  -0.1726\n",
            "Total loss:  0.5541 | PDE Loss:  -0.5285 | Function Loss:  -0.2075\n",
            "Total loss:  0.5427 | PDE Loss:  -0.5345 | Function Loss:  -0.2454\n",
            "Total loss:  0.5267 | PDE Loss:  -0.5457 | Function Loss:  -0.287\n",
            "Total loss:  0.5156 | PDE Loss:  -0.5538 | Function Loss:  -0.3155\n",
            "Total loss:  0.5041 | PDE Loss:  -0.5683 | Function Loss:  -0.3098\n",
            "Total loss:  0.4916 | PDE Loss:  -0.5854 | Function Loss:  -0.298\n",
            "Total loss:  0.4808 | PDE Loss:  -0.6011 | Function Loss:  -0.2837\n",
            "Total loss:  0.4721 | PDE Loss:  -0.6115 | Function Loss:  -0.2846\n",
            "Total loss:  0.4602 | PDE Loss:  -0.6251 | Function Loss:  -0.2886\n",
            "Total loss:  0.4481 | PDE Loss:  -0.6324 | Function Loss:  -0.3236\n",
            "Total loss:  0.4378 | PDE Loss:  -0.6372 | Function Loss:  -0.3622\n",
            "Total loss:  0.4301 | PDE Loss:  -0.6406 | Function Loss:  -0.3931\n",
            "Total loss:  0.4259 | PDE Loss:  -0.6423 | Function Loss:  -0.4115\n",
            "Total loss:  0.4216 | PDE Loss:  -0.6453 | Function Loss:  -0.4234\n",
            "Total loss:  0.4157 | PDE Loss:  -0.6505 | Function Loss:  -0.434\n",
            "Total loss:  0.4078 | PDE Loss:  -0.6604 | Function Loss:  -0.4301\n",
            "Total loss:  0.3998 | PDE Loss:  -0.6708 | Function Loss:  -0.4236\n",
            "Total loss:  0.3921 | PDE Loss:  -0.6854 | Function Loss:  -0.3942\n",
            "Total loss:  0.3856 | PDE Loss:  -0.6962 | Function Loss:  -0.3801\n",
            "Total loss:  0.3773 | PDE Loss:  -0.7092 | Function Loss:  -0.3664\n",
            "Total loss:  0.371 | PDE Loss:  -0.7218 | Function Loss:  -0.3446\n",
            "Total loss:  0.3667 | PDE Loss:  -0.7286 | Function Loss:  -0.3391\n",
            "Total loss:  0.3644 | PDE Loss:  -0.7334 | Function Loss:  -0.3309\n",
            "Total loss:  0.3598 | PDE Loss:  -0.7408 | Function Loss:  -0.3249\n",
            "Total loss:  0.35 | PDE Loss:  -0.7588 | Function Loss:  -0.3042\n",
            "Total loss:  0.3389 | PDE Loss:  -0.7762 | Function Loss:  -0.294\n",
            "Total loss:  0.331 | PDE Loss:  -0.7881 | Function Loss:  -0.2892\n",
            "Total loss:  0.3249 | PDE Loss:  -0.793 | Function Loss:  -0.2991\n",
            "Total loss:  0.3212 | PDE Loss:  -0.795 | Function Loss:  -0.308\n",
            "Total loss:  0.3176 | PDE Loss:  -0.7961 | Function Loss:  -0.3197\n",
            "Total loss:  0.3152 | PDE Loss:  -0.7968 | Function Loss:  -0.3284\n",
            "Total loss:  0.3123 | PDE Loss:  -0.8001 | Function Loss:  -0.3295\n",
            "Total loss:  0.3072 | PDE Loss:  -0.804 | Function Loss:  -0.3391\n",
            "Total loss:  0.2958 | PDE Loss:  -0.8124 | Function Loss:  -0.3608\n",
            "Total loss:  0.2775 | PDE Loss:  -0.8253 | Function Loss:  -0.3989\n",
            "Total loss:  0.2642 | PDE Loss:  -0.8338 | Function Loss:  -0.4302\n",
            "Total loss:  0.2507 | PDE Loss:  -0.845 | Function Loss:  -0.4529\n",
            "Total loss:  0.2396 | PDE Loss:  -0.8548 | Function Loss:  -0.4699\n",
            "Total loss:  0.2213 | PDE Loss:  -0.8704 | Function Loss:  -0.4992\n",
            "Total loss:  0.2045 | PDE Loss:  -0.8848 | Function Loss:  -0.5263\n",
            "Total loss:  0.1919 | PDE Loss:  -0.9012 | Function Loss:  -0.5223\n",
            "Total loss:  0.1848 | PDE Loss:  -0.9103 | Function Loss:  -0.5217\n",
            "Total loss:  0.1814 | PDE Loss:  -0.9187 | Function Loss:  -0.5047\n",
            "Total loss:  0.1772 | PDE Loss:  -0.9227 | Function Loss:  -0.51\n",
            "Total loss:  0.1747 | PDE Loss:  -0.9247 | Function Loss:  -0.5148\n",
            "Total loss:  0.1715 | PDE Loss:  -0.9279 | Function Loss:  -0.5178\n",
            "Total loss:  0.1668 | PDE Loss:  -0.9315 | Function Loss:  -0.5269\n",
            "Total loss:  0.1582 | PDE Loss:  -0.9378 | Function Loss:  -0.5444\n",
            "Total loss:  0.1479 | PDE Loss:  -0.9467 | Function Loss:  -0.5604\n",
            "Total loss:  0.136 | PDE Loss:  -0.9565 | Function Loss:  -0.5812\n",
            "Total loss:  0.1216 | PDE Loss:  -0.9704 | Function Loss:  -0.5975\n",
            "Total loss:  0.1075 | PDE Loss:  -0.9888 | Function Loss:  -0.5937\n",
            "Total loss:  0.0948 | PDE Loss:  -1.01 | Function Loss:  -0.5741\n",
            "Total loss:  0.0882 | PDE Loss:  -1.0152 | Function Loss:  -0.586\n",
            "Total loss:  0.0818 | PDE Loss:  -1.0284 | Function Loss:  -0.5675\n",
            "Total loss:  0.0782 | PDE Loss:  -1.0425 | Function Loss:  -0.5368\n",
            "Total loss:  0.0736 | PDE Loss:  -1.0471 | Function Loss:  -0.5414\n",
            "Total loss:  0.071 | PDE Loss:  -1.0489 | Function Loss:  -0.5465\n",
            "Total loss:  0.0677 | PDE Loss:  -1.0509 | Function Loss:  -0.5541\n",
            "Total loss:  0.063 | PDE Loss:  -1.0584 | Function Loss:  -0.5501\n",
            "Total loss:  0.0572 | PDE Loss:  -1.066 | Function Loss:  -0.5504\n",
            "Total loss:  0.0506 | PDE Loss:  -1.078 | Function Loss:  -0.5406\n",
            "Total loss:  0.0424 | PDE Loss:  -1.0911 | Function Loss:  -0.5351\n",
            "Total loss:  0.0329 | PDE Loss:  -1.1078 | Function Loss:  -0.5253\n",
            "Total loss:  0.0246 | PDE Loss:  -1.1185 | Function Loss:  -0.527\n",
            "Total loss:  0.0197 | PDE Loss:  -1.1253 | Function Loss:  -0.5272\n",
            "Total loss:  0.0144 | PDE Loss:  -1.1303 | Function Loss:  -0.5333\n",
            "Total loss:  0.0093 | PDE Loss:  -1.1332 | Function Loss:  -0.5438\n",
            "Total loss:  0.0027 | PDE Loss:  -1.1393 | Function Loss:  -0.5518\n",
            "Total loss:  -0.0065 | PDE Loss:  -1.1437 | Function Loss:  -0.5738\n",
            "Total loss:  -0.0132 | PDE Loss:  -1.1502 | Function Loss:  -0.5809\n",
            "Total loss:  -0.0162 | PDE Loss:  -1.1519 | Function Loss:  -0.5873\n",
            "Total loss:  -0.0188 | PDE Loss:  -1.1555 | Function Loss:  -0.5872\n",
            "Total loss:  -0.021 | PDE Loss:  -1.1562 | Function Loss:  -0.5936\n",
            "Total loss:  -0.0246 | PDE Loss:  -1.1584 | Function Loss:  -0.6013\n",
            "Total loss:  -0.0305 | PDE Loss:  -1.1561 | Function Loss:  -0.6304\n",
            "Total loss:  -0.038 | PDE Loss:  -1.1584 | Function Loss:  -0.6538\n",
            "Total loss:  -0.0449 | PDE Loss:  -1.1613 | Function Loss:  -0.6738\n",
            "Total loss:  -0.0545 | PDE Loss:  -1.1677 | Function Loss:  -0.6937\n",
            "Total loss:  -0.0645 | PDE Loss:  -1.1802 | Function Loss:  -0.6957\n",
            "Total loss:  -0.0726 | PDE Loss:  -1.1953 | Function Loss:  -0.6814\n",
            "Total loss:  -0.0802 | PDE Loss:  -1.2101 | Function Loss:  -0.6675\n",
            "Total loss:  -0.0886 | PDE Loss:  -1.2311 | Function Loss:  -0.6417\n",
            "Total loss:  -0.0983 | PDE Loss:  -1.2516 | Function Loss:  -0.625\n",
            "Total loss:  -0.1055 | PDE Loss:  -1.272 | Function Loss:  -0.6025\n",
            "Total loss:  -0.1093 | PDE Loss:  -1.2809 | Function Loss:  -0.5955\n",
            "Total loss:  -0.1128 | PDE Loss:  -1.2867 | Function Loss:  -0.5945\n",
            "Total loss:  -0.1175 | PDE Loss:  -1.2914 | Function Loss:  -0.5988\n",
            "Total loss:  -0.1237 | PDE Loss:  -1.2942 | Function Loss:  -0.6121\n",
            "Total loss:  -0.1327 | PDE Loss:  -1.2993 | Function Loss:  -0.6296\n",
            "Total loss:  -0.139 | PDE Loss:  -1.2998 | Function Loss:  -0.6485\n",
            "Total loss:  -0.1427 | PDE Loss:  -1.3032 | Function Loss:  -0.6529\n",
            "Total loss:  -0.1459 | PDE Loss:  -1.3053 | Function Loss:  -0.6585\n",
            "Total loss:  -0.1495 | PDE Loss:  -1.31 | Function Loss:  -0.6595\n",
            "Total loss:  -0.1525 | PDE Loss:  -1.3091 | Function Loss:  -0.6713\n",
            "Total loss:  -0.1554 | PDE Loss:  -1.3112 | Function Loss:  -0.6761\n",
            "Total loss:  -0.1607 | PDE Loss:  -1.314 | Function Loss:  -0.6873\n",
            "Total loss:  -0.1671 | PDE Loss:  -1.3202 | Function Loss:  -0.6942\n",
            "Total loss:  -0.1753 | PDE Loss:  -1.3293 | Function Loss:  -0.7004\n",
            "Total loss:  -0.1841 | PDE Loss:  -1.341 | Function Loss:  -0.7022\n",
            "Total loss:  -0.1913 | PDE Loss:  -1.3515 | Function Loss:  -0.702\n",
            "Total loss:  -0.2017 | PDE Loss:  -1.3617 | Function Loss:  -0.7127\n",
            "Total loss:  -0.2177 | PDE Loss:  -1.3721 | Function Loss:  -0.7418\n",
            "Total loss:  -0.2317 | PDE Loss:  -1.3761 | Function Loss:  -0.7803\n",
            "Total loss:  -0.2442 | PDE Loss:  -1.3808 | Function Loss:  -0.8129\n",
            "Total loss:  -0.2585 | PDE Loss:  -1.3917 | Function Loss:  -0.8367\n",
            "Total loss:  -0.2696 | PDE Loss:  -1.3939 | Function Loss:  -0.8734\n",
            "Total loss:  -0.2757 | PDE Loss:  -1.3963 | Function Loss:  -0.8913\n",
            "Total loss:  -0.2807 | PDE Loss:  -1.4017 | Function Loss:  -0.8945\n",
            "Total loss:  -0.2847 | PDE Loss:  -1.4062 | Function Loss:  -0.8972\n",
            "Total loss:  -0.289 | PDE Loss:  -1.4076 | Function Loss:  -0.9108\n",
            "Total loss:  -0.2928 | PDE Loss:  -1.4131 | Function Loss:  -0.909\n",
            "Total loss:  -0.2956 | PDE Loss:  -1.4176 | Function Loss:  -0.9068\n",
            "Total loss:  -0.2991 | PDE Loss:  -1.4214 | Function Loss:  -0.909\n",
            "Total loss:  -0.3043 | PDE Loss:  -1.43 | Function Loss:  -0.9041\n",
            "Total loss:  -0.3091 | PDE Loss:  -1.4362 | Function Loss:  -0.9048\n",
            "Total loss:  -0.3178 | PDE Loss:  -1.445 | Function Loss:  -0.9133\n",
            "Total loss:  -0.3303 | PDE Loss:  -1.4581 | Function Loss:  -0.924\n",
            "Total loss:  -0.3424 | PDE Loss:  -1.4709 | Function Loss:  -0.934\n",
            "Total loss:  -0.3522 | PDE Loss:  -1.483 | Function Loss:  -0.9372\n",
            "Total loss:  -0.365 | PDE Loss:  -1.501 | Function Loss:  -0.9355\n",
            "Total loss:  -0.3752 | PDE Loss:  -1.5167 | Function Loss:  -0.9312\n",
            "Total loss:  -0.3839 | PDE Loss:  -1.5339 | Function Loss:  -0.9183\n",
            "Total loss:  -0.3908 | PDE Loss:  -1.5445 | Function Loss:  -0.9166\n",
            "Total loss:  -0.3991 | PDE Loss:  -1.5596 | Function Loss:  -0.9092\n",
            "Total loss:  -0.4087 | PDE Loss:  -1.5761 | Function Loss:  -0.9038\n",
            "Total loss:  -0.4186 | PDE Loss:  -1.5901 | Function Loss:  -0.9051\n",
            "Total loss:  -0.4301 | PDE Loss:  -1.599 | Function Loss:  -0.9219\n",
            "Total loss:  -0.439 | PDE Loss:  -1.6072 | Function Loss:  -0.9321\n",
            "Total loss:  -0.4436 | PDE Loss:  -1.6052 | Function Loss:  -0.951\n",
            "Total loss:  -0.4466 | PDE Loss:  -1.6089 | Function Loss:  -0.9525\n",
            "Total loss:  -0.4493 | PDE Loss:  -1.6094 | Function Loss:  -0.9605\n",
            "Total loss:  -0.4523 | PDE Loss:  -1.6108 | Function Loss:  -0.9668\n",
            "Total loss:  -0.4561 | PDE Loss:  -1.6138 | Function Loss:  -0.9724\n",
            "Total loss:  -0.4609 | PDE Loss:  -1.6188 | Function Loss:  -0.9767\n",
            "Total loss:  -0.4657 | PDE Loss:  -1.6258 | Function Loss:  -0.9768\n",
            "Total loss:  -0.4727 | PDE Loss:  -1.6391 | Function Loss:  -0.9699\n",
            "Total loss:  -0.4799 | PDE Loss:  -1.6527 | Function Loss:  -0.9636\n",
            "Total loss:  -0.4835 | PDE Loss:  -1.6609 | Function Loss:  -0.9578\n",
            "Total loss:  -0.487 | PDE Loss:  -1.6681 | Function Loss:  -0.9543\n",
            "Total loss:  -0.4905 | PDE Loss:  -1.676 | Function Loss:  -0.9495\n",
            "Total loss:  -0.4937 | PDE Loss:  -1.6824 | Function Loss:  -0.9467\n",
            "Total loss:  -0.4976 | PDE Loss:  -1.6895 | Function Loss:  -0.9449\n",
            "Total loss:  -0.5019 | PDE Loss:  -1.6976 | Function Loss:  -0.9422\n",
            "Total loss:  -0.5069 | PDE Loss:  -1.7061 | Function Loss:  -0.9411\n",
            "Total loss:  -0.5116 | PDE Loss:  -1.7173 | Function Loss:  -0.9348\n",
            "Total loss:  -0.515 | PDE Loss:  -1.7216 | Function Loss:  -0.9369\n",
            "Total loss:  -0.5178 | PDE Loss:  -1.7293 | Function Loss:  -0.9317\n",
            "Total loss:  -0.5204 | PDE Loss:  -1.7335 | Function Loss:  -0.9318\n",
            "Total loss:  -0.5231 | PDE Loss:  -1.7364 | Function Loss:  -0.9342\n",
            "Total loss:  -0.5267 | PDE Loss:  -1.7413 | Function Loss:  -0.9357\n",
            "Total loss:  -0.5315 | PDE Loss:  -1.7459 | Function Loss:  -0.9409\n",
            "Total loss:  -0.5394 | PDE Loss:  -1.7551 | Function Loss:  -0.9468\n",
            "Total loss:  -0.5508 | PDE Loss:  -1.7706 | Function Loss:  -0.9518\n",
            "Total loss:  -0.5619 | PDE Loss:  -1.7868 | Function Loss:  -0.9554\n",
            "Total loss:  -0.5709 | PDE Loss:  -1.8011 | Function Loss:  -0.9565\n",
            "Total loss:  -0.5778 | PDE Loss:  -1.8116 | Function Loss:  -0.9584\n",
            "Total loss:  -0.5826 | PDE Loss:  -1.8221 | Function Loss:  -0.9553\n",
            "Total loss:  -0.5861 | PDE Loss:  -1.8253 | Function Loss:  -0.9594\n",
            "Total loss:  -0.5886 | PDE Loss:  -1.8331 | Function Loss:  -0.9546\n",
            "Total loss:  -0.5908 | PDE Loss:  -1.836 | Function Loss:  -0.956\n",
            "Total loss:  -0.594 | PDE Loss:  -1.8426 | Function Loss:  -0.9547\n",
            "Total loss:  -0.5985 | PDE Loss:  -1.8493 | Function Loss:  -0.9563\n",
            "Total loss:  -0.6044 | PDE Loss:  -1.8568 | Function Loss:  -0.9601\n",
            "Total loss:  -0.6105 | PDE Loss:  -1.8633 | Function Loss:  -0.9657\n",
            "Total loss:  -0.6164 | PDE Loss:  -1.8675 | Function Loss:  -0.9738\n",
            "Total loss:  -0.6226 | PDE Loss:  -1.8688 | Function Loss:  -0.9864\n",
            "Total loss:  -0.6285 | PDE Loss:  -1.8773 | Function Loss:  -0.9889\n",
            "Total loss:  -0.6327 | PDE Loss:  -1.8786 | Function Loss:  -0.9969\n",
            "Total loss:  -0.6356 | PDE Loss:  -1.8819 | Function Loss:  -0.9992\n",
            "Total loss:  -0.6379 | PDE Loss:  -1.8864 | Function Loss:  -0.9987\n",
            "Total loss:  -0.6396 | PDE Loss:  -1.888 | Function Loss:  -1.0004\n",
            "Total loss:  -0.6417 | PDE Loss:  -1.8916 | Function Loss:  -1.0008\n",
            "Total loss:  -0.6469 | PDE Loss:  -1.8972 | Function Loss:  -1.0054\n",
            "Total loss:  -0.6537 | PDE Loss:  -1.9066 | Function Loss:  -1.0088\n",
            "Total loss:  -0.6603 | PDE Loss:  -1.9163 | Function Loss:  -1.0115\n",
            "Total loss:  -0.6671 | PDE Loss:  -1.9243 | Function Loss:  -1.0169\n",
            "Total loss:  -0.6757 | PDE Loss:  -1.9368 | Function Loss:  -1.0206\n",
            "Total loss:  -0.6829 | PDE Loss:  -1.9427 | Function Loss:  -1.0296\n",
            "Total loss:  -0.6888 | PDE Loss:  -1.9515 | Function Loss:  -1.0318\n",
            "Total loss:  -0.6918 | PDE Loss:  -1.9543 | Function Loss:  -1.0351\n",
            "Total loss:  -0.6948 | PDE Loss:  -1.9556 | Function Loss:  -1.0403\n",
            "Total loss:  -0.6986 | PDE Loss:  -1.9572 | Function Loss:  -1.0466\n",
            "Total loss:  -0.7036 | PDE Loss:  -1.9635 | Function Loss:  -1.0502\n",
            "Total loss:  -0.7094 | PDE Loss:  -1.9676 | Function Loss:  -1.0579\n",
            "Total loss:  -0.7146 | PDE Loss:  -1.9782 | Function Loss:  -1.0565\n",
            "Total loss:  -0.721 | PDE Loss:  -1.9882 | Function Loss:  -1.0587\n",
            "Total loss:  -0.7266 | PDE Loss:  -2.0042 | Function Loss:  -1.0525\n",
            "Total loss:  -0.7311 | PDE Loss:  -2.0143 | Function Loss:  -1.0507\n",
            "Total loss:  -0.7351 | PDE Loss:  -2.0283 | Function Loss:  -1.0441\n",
            "Total loss:  -0.7379 | PDE Loss:  -2.0351 | Function Loss:  -1.0428\n",
            "Total loss:  -0.7406 | PDE Loss:  -2.0438 | Function Loss:  -1.0394\n",
            "Total loss:  -0.7426 | PDE Loss:  -2.0462 | Function Loss:  -1.041\n",
            "Total loss:  -0.7451 | PDE Loss:  -2.0542 | Function Loss:  -1.0381\n",
            "Total loss:  -0.7473 | PDE Loss:  -2.0571 | Function Loss:  -1.0398\n",
            "Total loss:  -0.7525 | PDE Loss:  -2.0631 | Function Loss:  -1.0443\n",
            "Total loss:  -0.7566 | PDE Loss:  -2.0661 | Function Loss:  -1.0493\n",
            "Total loss:  -0.7595 | PDE Loss:  -2.0689 | Function Loss:  -1.0524\n",
            "Total loss:  -0.7625 | PDE Loss:  -2.0718 | Function Loss:  -1.0554\n",
            "Total loss:  -0.7649 | PDE Loss:  -2.0778 | Function Loss:  -1.0543\n",
            "Total loss:  -0.7676 | PDE Loss:  -2.0812 | Function Loss:  -1.0563\n",
            "Total loss:  -0.7713 | PDE Loss:  -2.0916 | Function Loss:  -1.0538\n",
            "Total loss:  -0.7755 | PDE Loss:  -2.0991 | Function Loss:  -1.0551\n",
            "Total loss:  -0.7809 | PDE Loss:  -2.1116 | Function Loss:  -1.0541\n",
            "Total loss:  -0.7874 | PDE Loss:  -2.1219 | Function Loss:  -1.0574\n",
            "Total loss:  -0.7937 | PDE Loss:  -2.1381 | Function Loss:  -1.0552\n",
            "Total loss:  -0.7986 | PDE Loss:  -2.1444 | Function Loss:  -1.0592\n",
            "Total loss:  -0.8019 | PDE Loss:  -2.1532 | Function Loss:  -1.0579\n",
            "Total loss:  -0.8054 | PDE Loss:  -2.1572 | Function Loss:  -1.061\n",
            "Total loss:  -0.8083 | PDE Loss:  -2.1626 | Function Loss:  -1.0619\n",
            "Total loss:  -0.8132 | PDE Loss:  -2.17 | Function Loss:  -1.0648\n",
            "Total loss:  -0.8176 | PDE Loss:  -2.1812 | Function Loss:  -1.0639\n",
            "Total loss:  -0.8215 | PDE Loss:  -2.1886 | Function Loss:  -1.0652\n",
            "Total loss:  -0.825 | PDE Loss:  -2.1977 | Function Loss:  -1.0646\n",
            "Total loss:  -0.8279 | PDE Loss:  -2.203 | Function Loss:  -1.0656\n",
            "Total loss:  -0.8299 | PDE Loss:  -2.2057 | Function Loss:  -1.0672\n",
            "Total loss:  -0.8315 | PDE Loss:  -2.2059 | Function Loss:  -1.0697\n",
            "Total loss:  -0.8343 | PDE Loss:  -2.2084 | Function Loss:  -1.0728\n",
            "Total loss:  -0.841 | PDE Loss:  -2.2172 | Function Loss:  -1.0781\n",
            "Total loss:  -0.8495 | PDE Loss:  -2.2211 | Function Loss:  -1.0898\n",
            "Total loss:  -0.8585 | PDE Loss:  -2.2336 | Function Loss:  -1.0963\n",
            "Total loss:  -0.8707 | PDE Loss:  -2.2518 | Function Loss:  -1.1041\n",
            "Total loss:  -0.8861 | PDE Loss:  -2.2617 | Function Loss:  -1.1236\n",
            "Total loss:  -0.8993 | PDE Loss:  -2.2724 | Function Loss:  -1.1385\n",
            "Total loss:  -0.9071 | PDE Loss:  -2.2925 | Function Loss:  -1.1376\n",
            "Total loss:  -0.9117 | PDE Loss:  -2.2938 | Function Loss:  -1.1444\n",
            "Total loss:  -0.9146 | PDE Loss:  -2.3036 | Function Loss:  -1.1424\n",
            "Total loss:  -0.9168 | PDE Loss:  -2.3102 | Function Loss:  -1.1417\n",
            "Total loss:  -0.9195 | PDE Loss:  -2.315 | Function Loss:  -1.143\n",
            "Total loss:  -0.923 | PDE Loss:  -2.3235 | Function Loss:  -1.1431\n",
            "Total loss:  -0.9258 | PDE Loss:  -2.3256 | Function Loss:  -1.1464\n",
            "Total loss:  -0.9287 | PDE Loss:  -2.3276 | Function Loss:  -1.1499\n",
            "Total loss:  -0.9342 | PDE Loss:  -2.3326 | Function Loss:  -1.1557\n",
            "Total loss:  -0.9426 | PDE Loss:  -2.3413 | Function Loss:  -1.1639\n",
            "Total loss:  -0.9541 | PDE Loss:  -2.3574 | Function Loss:  -1.1724\n",
            "Total loss:  -0.9669 | PDE Loss:  -2.3672 | Function Loss:  -1.1872\n",
            "Total loss:  -0.9747 | PDE Loss:  -2.3797 | Function Loss:  -1.1919\n",
            "Total loss:  -0.9828 | PDE Loss:  -2.4166 | Function Loss:  -1.1823\n",
            "Total loss:  -0.9911 | PDE Loss:  -2.4266 | Function Loss:  -1.1896\n",
            "Total loss:  -0.9977 | PDE Loss:  -2.4521 | Function Loss:  -1.1856\n",
            "Total loss:  -1.0025 | PDE Loss:  -2.4589 | Function Loss:  -1.1893\n",
            "Total loss:  -1.0051 | PDE Loss:  -2.4649 | Function Loss:  -1.1901\n",
            "Total loss:  -1.0071 | PDE Loss:  -2.4651 | Function Loss:  -1.193\n",
            "Total loss:  -1.0092 | PDE Loss:  -2.4631 | Function Loss:  -1.1974\n",
            "Total loss:  -1.0113 | PDE Loss:  -2.4621 | Function Loss:  -1.2012\n",
            "Total loss:  -1.014 | PDE Loss:  -2.4611 | Function Loss:  -1.2059\n",
            "Total loss:  -1.0177 | PDE Loss:  -2.458 | Function Loss:  -1.2135\n",
            "Total loss:  -1.0217 | PDE Loss:  -2.4556 | Function Loss:  -1.2212\n",
            "Total loss:  -1.0263 | PDE Loss:  -2.4595 | Function Loss:  -1.2261\n",
            "Total loss:  -1.0307 | PDE Loss:  -2.4636 | Function Loss:  -1.2308\n",
            "Total loss:  -1.0355 | PDE Loss:  -2.4748 | Function Loss:  -1.2318\n",
            "Total loss:  -1.0406 | PDE Loss:  -2.485 | Function Loss:  -1.234\n",
            "Total loss:  -1.0465 | PDE Loss:  -2.4996 | Function Loss:  -1.2351\n",
            "Total loss:  -1.0526 | PDE Loss:  -2.5109 | Function Loss:  -1.2384\n",
            "Total loss:  -1.0605 | PDE Loss:  -2.5277 | Function Loss:  -1.2417\n",
            "Total loss:  -1.0701 | PDE Loss:  -2.538 | Function Loss:  -1.2508\n",
            "Total loss:  -1.0785 | PDE Loss:  -2.5559 | Function Loss:  -1.2544\n",
            "Total loss:  -1.0837 | PDE Loss:  -2.5611 | Function Loss:  -1.2596\n",
            "Total loss:  -1.0876 | PDE Loss:  -2.5602 | Function Loss:  -1.2659\n",
            "Total loss:  -1.0905 | PDE Loss:  -2.5661 | Function Loss:  -1.2674\n",
            "Total loss:  -1.094 | PDE Loss:  -2.5606 | Function Loss:  -1.2754\n",
            "Total loss:  -1.0985 | PDE Loss:  -2.5679 | Function Loss:  -1.2785\n",
            "Total loss:  -1.1045 | PDE Loss:  -2.5639 | Function Loss:  -1.2897\n",
            "Total loss:  -1.1113 | PDE Loss:  -2.5677 | Function Loss:  -1.2982\n",
            "Total loss:  -1.1178 | PDE Loss:  -2.5749 | Function Loss:  -1.3043\n",
            "Total loss:  -1.123 | PDE Loss:  -2.5837 | Function Loss:  -1.3076\n",
            "Total loss:  -1.128 | PDE Loss:  -2.5923 | Function Loss:  -1.3107\n",
            "Total loss:  -1.1321 | PDE Loss:  -2.6054 | Function Loss:  -1.3101\n",
            "Total loss:  -1.1355 | PDE Loss:  -2.6101 | Function Loss:  -1.3128\n",
            "Total loss:  -1.1378 | PDE Loss:  -2.623 | Function Loss:  -1.3099\n",
            "Total loss:  -1.1402 | PDE Loss:  -2.6256 | Function Loss:  -1.3122\n",
            "Total loss:  -1.1423 | PDE Loss:  -2.6287 | Function Loss:  -1.3138\n",
            "Total loss:  -1.145 | PDE Loss:  -2.6312 | Function Loss:  -1.3166\n",
            "Total loss:  -1.1476 | PDE Loss:  -2.6368 | Function Loss:  -1.3177\n",
            "Total loss:  -1.1493 | PDE Loss:  -2.6423 | Function Loss:  -1.3176\n",
            "Total loss:  -1.1503 | PDE Loss:  -2.6446 | Function Loss:  -1.3181\n",
            "Total loss:  -1.1512 | PDE Loss:  -2.6471 | Function Loss:  -1.3182\n",
            "Total loss:  -1.1523 | PDE Loss:  -2.6492 | Function Loss:  -1.3188\n",
            "Total loss:  -1.1537 | PDE Loss:  -2.6513 | Function Loss:  -1.32\n",
            "Total loss:  -1.1559 | PDE Loss:  -2.6548 | Function Loss:  -1.3214\n",
            "Total loss:  -1.1589 | PDE Loss:  -2.657 | Function Loss:  -1.3248\n",
            "Total loss:  -1.1626 | PDE Loss:  -2.6622 | Function Loss:  -1.3279\n",
            "Total loss:  -1.167 | PDE Loss:  -2.665 | Function Loss:  -1.333\n",
            "Total loss:  -1.1724 | PDE Loss:  -2.6684 | Function Loss:  -1.3394\n",
            "Total loss:  -1.1787 | PDE Loss:  -2.6708 | Function Loss:  -1.3475\n",
            "Total loss:  -1.1834 | PDE Loss:  -2.6756 | Function Loss:  -1.3521\n",
            "Total loss:  -1.187 | PDE Loss:  -2.6733 | Function Loss:  -1.3585\n",
            "Total loss:  -1.1896 | PDE Loss:  -2.6785 | Function Loss:  -1.3598\n",
            "Total loss:  -1.1911 | PDE Loss:  -2.6765 | Function Loss:  -1.3632\n",
            "Total loss:  -1.1922 | PDE Loss:  -2.6764 | Function Loss:  -1.3649\n",
            "Total loss:  -1.1934 | PDE Loss:  -2.679 | Function Loss:  -1.3653\n",
            "Total loss:  -1.195 | PDE Loss:  -2.6804 | Function Loss:  -1.367\n",
            "Total loss:  -1.1969 | PDE Loss:  -2.6839 | Function Loss:  -1.3681\n",
            "Total loss:  -1.1991 | PDE Loss:  -2.6878 | Function Loss:  -1.3694\n",
            "Total loss:  -1.2021 | PDE Loss:  -2.6917 | Function Loss:  -1.372\n",
            "Total loss:  -1.2059 | PDE Loss:  -2.6984 | Function Loss:  -1.3745\n",
            "Total loss:  -1.2101 | PDE Loss:  -2.7009 | Function Loss:  -1.3795\n",
            "Total loss:  -1.2134 | PDE Loss:  -2.7066 | Function Loss:  -1.3816\n",
            "Total loss:  -1.2155 | PDE Loss:  -2.7034 | Function Loss:  -1.3863\n",
            "Total loss:  -1.2166 | PDE Loss:  -2.7046 | Function Loss:  -1.3873\n",
            "Total loss:  -1.2171 | PDE Loss:  -2.7026 | Function Loss:  -1.389\n",
            "Total loss:  -1.2175 | PDE Loss:  -2.7017 | Function Loss:  -1.39\n",
            "Total loss:  -1.2179 | PDE Loss:  -2.7008 | Function Loss:  -1.3911\n",
            "Total loss:  -1.2184 | PDE Loss:  -2.6994 | Function Loss:  -1.3926\n",
            "Total loss:  -1.2192 | PDE Loss:  -2.6987 | Function Loss:  -1.3941\n",
            "Total loss:  -1.2205 | PDE Loss:  -2.6986 | Function Loss:  -1.3961\n",
            "Total loss:  -1.2223 | PDE Loss:  -2.6981 | Function Loss:  -1.3991\n",
            "Total loss:  -1.2254 | PDE Loss:  -2.7028 | Function Loss:  -1.4013\n",
            "Total loss:  -1.2296 | PDE Loss:  -2.707 | Function Loss:  -1.4056\n",
            "Total loss:  -1.235 | PDE Loss:  -2.7197 | Function Loss:  -1.4074\n",
            "Total loss:  -1.2414 | PDE Loss:  -2.7346 | Function Loss:  -1.4096\n",
            "Total loss:  -1.2474 | PDE Loss:  -2.7497 | Function Loss:  -1.4114\n",
            "Total loss:  -1.252 | PDE Loss:  -2.7622 | Function Loss:  -1.4125\n",
            "Total loss:  -1.2548 | PDE Loss:  -2.7673 | Function Loss:  -1.4142\n",
            "Total loss:  -1.256 | PDE Loss:  -2.7692 | Function Loss:  -1.4152\n",
            "Total loss:  -1.2566 | PDE Loss:  -2.7698 | Function Loss:  -1.4158\n",
            "Total loss:  -1.257 | PDE Loss:  -2.7696 | Function Loss:  -1.4164\n",
            "Total loss:  -1.2574 | PDE Loss:  -2.7695 | Function Loss:  -1.417\n",
            "Total loss:  -1.2579 | PDE Loss:  -2.7696 | Function Loss:  -1.4177\n",
            "Total loss:  -1.2589 | PDE Loss:  -2.7694 | Function Loss:  -1.4192\n",
            "Total loss:  -1.2607 | PDE Loss:  -2.7715 | Function Loss:  -1.4209\n",
            "Total loss:  -1.2636 | PDE Loss:  -2.7717 | Function Loss:  -1.425\n",
            "Total loss:  -1.2681 | PDE Loss:  -2.7799 | Function Loss:  -1.4279\n",
            "Total loss:  -1.2737 | PDE Loss:  -2.7848 | Function Loss:  -1.4337\n",
            "Total loss:  -1.2788 | PDE Loss:  -2.7943 | Function Loss:  -1.4369\n",
            "Total loss:  -1.2836 | PDE Loss:  -2.8088 | Function Loss:  -1.4376\n",
            "Total loss:  -1.2876 | PDE Loss:  -2.8178 | Function Loss:  -1.4394\n",
            "Total loss:  -1.2896 | PDE Loss:  -2.8287 | Function Loss:  -1.4377\n",
            "Total loss:  -1.2902 | PDE Loss:  -2.8313 | Function Loss:  -1.4376\n",
            "Total loss:  -1.2906 | PDE Loss:  -2.8313 | Function Loss:  -1.4381\n",
            "Total loss:  -1.2909 | PDE Loss:  -2.8311 | Function Loss:  -1.4385\n",
            "Total loss:  -1.2913 | PDE Loss:  -2.8297 | Function Loss:  -1.4397\n",
            "Total loss:  -1.2921 | PDE Loss:  -2.8295 | Function Loss:  -1.4409\n",
            "Total loss:  -1.2932 | PDE Loss:  -2.8251 | Function Loss:  -1.4443\n",
            "Total loss:  -1.2949 | PDE Loss:  -2.8245 | Function Loss:  -1.447\n",
            "Total loss:  -1.2976 | PDE Loss:  -2.8217 | Function Loss:  -1.4519\n",
            "Total loss:  -1.3019 | PDE Loss:  -2.8195 | Function Loss:  -1.4591\n",
            "Total loss:  -1.3076 | PDE Loss:  -2.8197 | Function Loss:  -1.4671\n",
            "Total loss:  -1.3128 | PDE Loss:  -2.8161 | Function Loss:  -1.4763\n",
            "Total loss:  -1.3165 | PDE Loss:  -2.819 | Function Loss:  -1.4805\n",
            "Total loss:  -1.3201 | PDE Loss:  -2.8222 | Function Loss:  -1.4843\n",
            "Total loss:  -1.3229 | PDE Loss:  -2.8256 | Function Loss:  -1.4868\n",
            "Total loss:  -1.3246 | PDE Loss:  -2.8348 | Function Loss:  -1.4851\n",
            "Total loss:  -1.3259 | PDE Loss:  -2.838 | Function Loss:  -1.4855\n",
            "Total loss:  -1.3268 | PDE Loss:  -2.8453 | Function Loss:  -1.4836\n",
            "Total loss:  -1.328 | PDE Loss:  -2.8522 | Function Loss:  -1.4823\n",
            "Total loss:  -1.3296 | PDE Loss:  -2.8612 | Function Loss:  -1.4808\n",
            "Total loss:  -1.3316 | PDE Loss:  -2.8717 | Function Loss:  -1.4793\n",
            "Total loss:  -1.3342 | PDE Loss:  -2.8826 | Function Loss:  -1.4786\n",
            "Total loss:  -1.3377 | PDE Loss:  -2.8933 | Function Loss:  -1.4793\n",
            "Total loss:  -1.3423 | PDE Loss:  -2.9061 | Function Loss:  -1.4808\n",
            "Total loss:  -1.3479 | PDE Loss:  -2.9169 | Function Loss:  -1.4845\n",
            "Total loss:  -1.3536 | PDE Loss:  -2.9249 | Function Loss:  -1.4893\n",
            "Total loss:  -1.3587 | PDE Loss:  -2.9322 | Function Loss:  -1.4936\n",
            "Total loss:  -1.3617 | PDE Loss:  -2.9266 | Function Loss:  -1.4998\n",
            "Total loss:  -1.3637 | PDE Loss:  -2.9248 | Function Loss:  -1.5032\n",
            "Total loss:  -1.365 | PDE Loss:  -2.9232 | Function Loss:  -1.5056\n",
            "Total loss:  -1.3657 | PDE Loss:  -2.9215 | Function Loss:  -1.5073\n",
            "Total loss:  -1.3664 | PDE Loss:  -2.9215 | Function Loss:  -1.5082\n",
            "Total loss:  -1.3668 | PDE Loss:  -2.9214 | Function Loss:  -1.5087\n",
            "Total loss:  -1.367 | PDE Loss:  -2.9227 | Function Loss:  -1.5086\n",
            "Total loss:  -1.3673 | PDE Loss:  -2.9236 | Function Loss:  -1.5086\n",
            "Total loss:  -1.3676 | PDE Loss:  -2.9252 | Function Loss:  -1.5085\n",
            "Total loss:  -1.3681 | PDE Loss:  -2.9264 | Function Loss:  -1.5086\n",
            "Total loss:  -1.3688 | PDE Loss:  -2.9285 | Function Loss:  -1.5088\n",
            "Total loss:  -1.3698 | PDE Loss:  -2.9294 | Function Loss:  -1.5099\n",
            "Total loss:  -1.3714 | PDE Loss:  -2.9331 | Function Loss:  -1.5106\n",
            "Total loss:  -1.3739 | PDE Loss:  -2.9348 | Function Loss:  -1.5135\n",
            "Total loss:  -1.3776 | PDE Loss:  -2.9393 | Function Loss:  -1.5169\n",
            "Total loss:  -1.3829 | PDE Loss:  -2.945 | Function Loss:  -1.522\n",
            "Total loss:  -1.3892 | PDE Loss:  -2.9495 | Function Loss:  -1.529\n",
            "Total loss:  -1.3949 | PDE Loss:  -2.9544 | Function Loss:  -1.5351\n",
            "Total loss:  -1.3994 | PDE Loss:  -2.9576 | Function Loss:  -1.54\n",
            "Total loss:  -1.4027 | PDE Loss:  -2.9587 | Function Loss:  -1.5442\n",
            "Total loss:  -1.4045 | PDE Loss:  -2.9646 | Function Loss:  -1.5444\n",
            "Total loss:  -1.4061 | PDE Loss:  -2.9594 | Function Loss:  -1.5486\n",
            "Total loss:  -1.4073 | PDE Loss:  -2.9661 | Function Loss:  -1.5477\n",
            "Total loss:  -1.4082 | PDE Loss:  -2.9705 | Function Loss:  -1.5472\n",
            "Total loss:  -1.4092 | PDE Loss:  -2.9718 | Function Loss:  -1.5482\n",
            "Total loss:  -1.4101 | PDE Loss:  -2.974 | Function Loss:  -1.5485\n",
            "Total loss:  -1.4107 | PDE Loss:  -2.9742 | Function Loss:  -1.5493\n",
            "Total loss:  -1.4112 | PDE Loss:  -2.9748 | Function Loss:  -1.5497\n",
            "Total loss:  -1.4117 | PDE Loss:  -2.974 | Function Loss:  -1.5508\n",
            "Total loss:  -1.4125 | PDE Loss:  -2.9746 | Function Loss:  -1.5516\n",
            "Total loss:  -1.4135 | PDE Loss:  -2.9732 | Function Loss:  -1.5536\n",
            "Total loss:  -1.415 | PDE Loss:  -2.9753 | Function Loss:  -1.5547\n",
            "Total loss:  -1.4168 | PDE Loss:  -2.9758 | Function Loss:  -1.5571\n",
            "Total loss:  -1.4191 | PDE Loss:  -2.9801 | Function Loss:  -1.5587\n",
            "Total loss:  -1.4223 | PDE Loss:  -2.9933 | Function Loss:  -1.5581\n",
            "Total loss:  -1.4256 | PDE Loss:  -2.9972 | Function Loss:  -1.5612\n",
            "Total loss:  -1.4282 | PDE Loss:  -3.0073 | Function Loss:  -1.5611\n",
            "Total loss:  -1.4323 | PDE Loss:  -3.0146 | Function Loss:  -1.564\n",
            "Total loss:  -1.4358 | PDE Loss:  -3.0422 | Function Loss:  -1.5593\n",
            "Total loss:  -1.4377 | PDE Loss:  -3.0456 | Function Loss:  -1.5606\n",
            "Total loss:  -1.4386 | PDE Loss:  -3.0502 | Function Loss:  -1.5604\n",
            "Total loss:  -1.4394 | PDE Loss:  -3.0554 | Function Loss:  -1.5598\n",
            "Total loss:  -1.4398 | PDE Loss:  -3.0572 | Function Loss:  -1.5598\n",
            "Total loss:  -1.4403 | PDE Loss:  -3.0597 | Function Loss:  -1.5596\n",
            "Total loss:  -1.4409 | PDE Loss:  -3.0609 | Function Loss:  -1.56\n",
            "Total loss:  -1.4416 | PDE Loss:  -3.0623 | Function Loss:  -1.5605\n",
            "Total loss:  -1.4424 | PDE Loss:  -3.0626 | Function Loss:  -1.5615\n",
            "Total loss:  -1.4433 | PDE Loss:  -3.0634 | Function Loss:  -1.5624\n",
            "Total loss:  -1.4446 | PDE Loss:  -3.0651 | Function Loss:  -1.5636\n",
            "Total loss:  -1.4466 | PDE Loss:  -3.0667 | Function Loss:  -1.5657\n",
            "Total loss:  -1.4489 | PDE Loss:  -3.073 | Function Loss:  -1.5667\n",
            "Total loss:  -1.4508 | PDE Loss:  -3.0797 | Function Loss:  -1.5671\n",
            "Total loss:  -1.4524 | PDE Loss:  -3.084 | Function Loss:  -1.5679\n",
            "Total loss:  -1.4542 | PDE Loss:  -3.0915 | Function Loss:  -1.568\n",
            "Total loss:  -1.4562 | PDE Loss:  -3.0972 | Function Loss:  -1.5689\n",
            "Total loss:  -1.4582 | PDE Loss:  -3.103 | Function Loss:  -1.5698\n",
            "Total loss:  -1.46 | PDE Loss:  -3.107 | Function Loss:  -1.571\n",
            "Total loss:  -1.4617 | PDE Loss:  -3.1106 | Function Loss:  -1.5721\n",
            "Total loss:  -1.4628 | PDE Loss:  -3.1087 | Function Loss:  -1.574\n",
            "Total loss:  -1.4634 | PDE Loss:  -3.1108 | Function Loss:  -1.5743\n",
            "Total loss:  -1.4638 | PDE Loss:  -3.1099 | Function Loss:  -1.575\n",
            "Total loss:  -1.4641 | PDE Loss:  -3.11 | Function Loss:  -1.5754\n",
            "Total loss:  -1.4644 | PDE Loss:  -3.1101 | Function Loss:  -1.5757\n",
            "Total loss:  -1.4646 | PDE Loss:  -3.1107 | Function Loss:  -1.5758\n",
            "Total loss:  -1.4649 | PDE Loss:  -3.111 | Function Loss:  -1.5761\n",
            "Total loss:  -1.4654 | PDE Loss:  -3.1128 | Function Loss:  -1.5762\n",
            "Total loss:  -1.4661 | PDE Loss:  -3.1152 | Function Loss:  -1.5765\n",
            "Total loss:  -1.4669 | PDE Loss:  -3.1178 | Function Loss:  -1.5768\n",
            "Total loss:  -1.4684 | PDE Loss:  -3.1214 | Function Loss:  -1.5776\n",
            "Total loss:  -1.4701 | PDE Loss:  -3.1317 | Function Loss:  -1.5769\n",
            "Total loss:  -1.472 | PDE Loss:  -3.1292 | Function Loss:  -1.58\n",
            "Total loss:  -1.4747 | PDE Loss:  -3.1294 | Function Loss:  -1.5834\n",
            "Total loss:  -1.4781 | PDE Loss:  -3.1246 | Function Loss:  -1.5892\n",
            "Total loss:  -1.481 | PDE Loss:  -3.1181 | Function Loss:  -1.5949\n",
            "Total loss:  -1.4838 | PDE Loss:  -3.12 | Function Loss:  -1.5979\n",
            "Total loss:  -1.4862 | PDE Loss:  -3.1212 | Function Loss:  -1.6006\n",
            "Total loss:  -1.4887 | PDE Loss:  -3.1276 | Function Loss:  -1.6021\n",
            "Total loss:  -1.4906 | PDE Loss:  -3.136 | Function Loss:  -1.602\n",
            "Total loss:  -1.4917 | PDE Loss:  -3.1408 | Function Loss:  -1.6021\n",
            "Total loss:  -1.4924 | PDE Loss:  -3.1474 | Function Loss:  -1.601\n",
            "Total loss:  -1.4928 | PDE Loss:  -3.1507 | Function Loss:  -1.6006\n",
            "Total loss:  -1.4931 | PDE Loss:  -3.1553 | Function Loss:  -1.5997\n",
            "Total loss:  -1.4935 | PDE Loss:  -3.1581 | Function Loss:  -1.5994\n",
            "Total loss:  -1.4938 | PDE Loss:  -3.1619 | Function Loss:  -1.5987\n",
            "Total loss:  -1.4941 | PDE Loss:  -3.1636 | Function Loss:  -1.5987\n",
            "Total loss:  -1.4945 | PDE Loss:  -3.1663 | Function Loss:  -1.5984\n",
            "Total loss:  -1.495 | PDE Loss:  -3.1677 | Function Loss:  -1.5987\n",
            "Total loss:  -1.4956 | PDE Loss:  -3.1702 | Function Loss:  -1.5989\n",
            "Total loss:  -1.4966 | PDE Loss:  -3.1724 | Function Loss:  -1.5994\n",
            "Total loss:  -1.4978 | PDE Loss:  -3.1787 | Function Loss:  -1.5994\n",
            "Total loss:  -1.4994 | PDE Loss:  -3.1846 | Function Loss:  -1.5998\n",
            "Total loss:  -1.5009 | PDE Loss:  -3.1925 | Function Loss:  -1.5997\n",
            "Total loss:  -1.503 | PDE Loss:  -3.2041 | Function Loss:  -1.5994\n",
            "Total loss:  -1.5053 | PDE Loss:  -3.2195 | Function Loss:  -1.5985\n",
            "Total loss:  -1.507 | PDE Loss:  -3.2298 | Function Loss:  -1.5982\n",
            "Total loss:  -1.509 | PDE Loss:  -3.245 | Function Loss:  -1.5971\n",
            "Total loss:  -1.5107 | PDE Loss:  -3.2574 | Function Loss:  -1.5964\n",
            "Total loss:  -1.512 | PDE Loss:  -3.2619 | Function Loss:  -1.5971\n",
            "Total loss:  -1.5133 | PDE Loss:  -3.2712 | Function Loss:  -1.5966\n",
            "Total loss:  -1.5144 | PDE Loss:  -3.2691 | Function Loss:  -1.5985\n",
            "Total loss:  -1.5158 | PDE Loss:  -3.2672 | Function Loss:  -1.6005\n",
            "Total loss:  -1.5173 | PDE Loss:  -3.261 | Function Loss:  -1.6037\n",
            "Total loss:  -1.5188 | PDE Loss:  -3.2568 | Function Loss:  -1.6065\n",
            "Total loss:  -1.5202 | PDE Loss:  -3.2514 | Function Loss:  -1.6094\n",
            "Total loss:  -1.5215 | PDE Loss:  -3.2514 | Function Loss:  -1.611\n",
            "Total loss:  -1.5224 | PDE Loss:  -3.25 | Function Loss:  -1.6124\n",
            "Total loss:  -1.523 | PDE Loss:  -3.2584 | Function Loss:  -1.6113\n",
            "Total loss:  -1.5236 | PDE Loss:  -3.2578 | Function Loss:  -1.6122\n",
            "Total loss:  -1.524 | PDE Loss:  -3.2607 | Function Loss:  -1.612\n",
            "Total loss:  -1.5246 | PDE Loss:  -3.2651 | Function Loss:  -1.6117\n",
            "Total loss:  -1.5253 | PDE Loss:  -3.2733 | Function Loss:  -1.6108\n",
            "Total loss:  -1.5261 | PDE Loss:  -3.2809 | Function Loss:  -1.6102\n",
            "Total loss:  -1.5272 | PDE Loss:  -3.2923 | Function Loss:  -1.6091\n",
            "Total loss:  -1.5285 | PDE Loss:  -3.3017 | Function Loss:  -1.6086\n",
            "Total loss:  -1.5298 | PDE Loss:  -3.3118 | Function Loss:  -1.6082\n",
            "Total loss:  -1.5313 | PDE Loss:  -3.319 | Function Loss:  -1.6086\n",
            "Total loss:  -1.5332 | PDE Loss:  -3.3237 | Function Loss:  -1.6099\n",
            "Total loss:  -1.5353 | PDE Loss:  -3.3308 | Function Loss:  -1.6111\n",
            "Total loss:  -1.5376 | PDE Loss:  -3.3271 | Function Loss:  -1.6145\n",
            "Total loss:  -1.5397 | PDE Loss:  -3.3261 | Function Loss:  -1.6172\n",
            "Total loss:  -1.5423 | PDE Loss:  -3.3237 | Function Loss:  -1.6208\n",
            "Total loss:  -1.545 | PDE Loss:  -3.3237 | Function Loss:  -1.6241\n",
            "Total loss:  -1.5475 | PDE Loss:  -3.3288 | Function Loss:  -1.6261\n",
            "Total loss:  -1.5496 | PDE Loss:  -3.3346 | Function Loss:  -1.6275\n",
            "Total loss:  -1.5513 | PDE Loss:  -3.343 | Function Loss:  -1.6278\n",
            "Total loss:  -1.5524 | PDE Loss:  -3.3569 | Function Loss:  -1.6265\n",
            "Total loss:  -1.5533 | PDE Loss:  -3.3616 | Function Loss:  -1.6267\n",
            "Total loss:  -1.5543 | PDE Loss:  -3.3811 | Function Loss:  -1.6243\n",
            "Total loss:  -1.5551 | PDE Loss:  -3.3847 | Function Loss:  -1.6247\n",
            "Total loss:  -1.5559 | PDE Loss:  -3.3897 | Function Loss:  -1.6247\n",
            "Total loss:  -1.5566 | PDE Loss:  -3.3918 | Function Loss:  -1.6253\n",
            "Total loss:  -1.5573 | PDE Loss:  -3.387 | Function Loss:  -1.6269\n",
            "Total loss:  -1.5581 | PDE Loss:  -3.3808 | Function Loss:  -1.6289\n",
            "Total loss:  -1.5589 | PDE Loss:  -3.3698 | Function Loss:  -1.6318\n",
            "Total loss:  -1.5595 | PDE Loss:  -3.3629 | Function Loss:  -1.6338\n",
            "Total loss:  -1.5603 | PDE Loss:  -3.3573 | Function Loss:  -1.6358\n",
            "Total loss:  -1.5614 | PDE Loss:  -3.3518 | Function Loss:  -1.6382\n",
            "Total loss:  -1.5629 | PDE Loss:  -3.3493 | Function Loss:  -1.6405\n",
            "Total loss:  -1.5649 | PDE Loss:  -3.3488 | Function Loss:  -1.6429\n",
            "Total loss:  -1.5672 | PDE Loss:  -3.351 | Function Loss:  -1.6453\n",
            "Total loss:  -1.5701 | PDE Loss:  -3.3539 | Function Loss:  -1.6482\n",
            "Total loss:  -1.5726 | PDE Loss:  -3.3647 | Function Loss:  -1.6491\n",
            "Total loss:  -1.576 | PDE Loss:  -3.3703 | Function Loss:  -1.652\n",
            "Total loss:  -1.5791 | PDE Loss:  -3.3823 | Function Loss:  -1.6534\n",
            "Total loss:  -1.5815 | PDE Loss:  -3.3826 | Function Loss:  -1.6562\n",
            "Total loss:  -1.5832 | PDE Loss:  -3.3867 | Function Loss:  -1.6575\n",
            "Total loss:  -1.5843 | PDE Loss:  -3.3847 | Function Loss:  -1.6592\n",
            "Total loss:  -1.5852 | PDE Loss:  -3.3779 | Function Loss:  -1.6616\n",
            "Total loss:  -1.5862 | PDE Loss:  -3.3736 | Function Loss:  -1.6635\n",
            "Total loss:  -1.587 | PDE Loss:  -3.3648 | Function Loss:  -1.6663\n",
            "Total loss:  -1.5877 | PDE Loss:  -3.3557 | Function Loss:  -1.6689\n",
            "Total loss:  -1.5884 | PDE Loss:  -3.3507 | Function Loss:  -1.6708\n",
            "Total loss:  -1.5888 | PDE Loss:  -3.3438 | Function Loss:  -1.6728\n",
            "Total loss:  -1.5892 | PDE Loss:  -3.3422 | Function Loss:  -1.6736\n",
            "Total loss:  -1.5896 | PDE Loss:  -3.3428 | Function Loss:  -1.6739\n",
            "Total loss:  -1.5899 | PDE Loss:  -3.346 | Function Loss:  -1.6737\n",
            "Total loss:  -1.5902 | PDE Loss:  -3.3502 | Function Loss:  -1.6732\n",
            "Total loss:  -1.5905 | PDE Loss:  -3.3542 | Function Loss:  -1.6726\n",
            "Total loss:  -1.5909 | PDE Loss:  -3.3591 | Function Loss:  -1.6721\n",
            "Total loss:  -1.5914 | PDE Loss:  -3.3648 | Function Loss:  -1.6715\n",
            "Total loss:  -1.592 | PDE Loss:  -3.3695 | Function Loss:  -1.6713\n",
            "Total loss:  -1.5926 | PDE Loss:  -3.3731 | Function Loss:  -1.6713\n",
            "Total loss:  -1.5932 | PDE Loss:  -3.3725 | Function Loss:  -1.6722\n",
            "Total loss:  -1.594 | PDE Loss:  -3.373 | Function Loss:  -1.673\n",
            "Total loss:  -1.595 | PDE Loss:  -3.3672 | Function Loss:  -1.6754\n",
            "Total loss:  -1.5962 | PDE Loss:  -3.3646 | Function Loss:  -1.6773\n",
            "Total loss:  -1.5977 | PDE Loss:  -3.3585 | Function Loss:  -1.6804\n",
            "Total loss:  -1.5994 | PDE Loss:  -3.3556 | Function Loss:  -1.6831\n",
            "Total loss:  -1.6014 | PDE Loss:  -3.3535 | Function Loss:  -1.686\n",
            "Total loss:  -1.6036 | PDE Loss:  -3.3536 | Function Loss:  -1.6886\n",
            "Total loss:  -1.6055 | PDE Loss:  -3.3583 | Function Loss:  -1.6899\n",
            "Total loss:  -1.6071 | PDE Loss:  -3.3617 | Function Loss:  -1.6911\n",
            "Total loss:  -1.6084 | PDE Loss:  -3.3686 | Function Loss:  -1.6912\n",
            "Total loss:  -1.6094 | PDE Loss:  -3.3747 | Function Loss:  -1.6912\n",
            "Total loss:  -1.6101 | PDE Loss:  -3.3795 | Function Loss:  -1.6911\n",
            "Total loss:  -1.6106 | PDE Loss:  -3.3824 | Function Loss:  -1.6911\n",
            "Total loss:  -1.6111 | PDE Loss:  -3.3837 | Function Loss:  -1.6915\n",
            "Total loss:  -1.6116 | PDE Loss:  -3.3822 | Function Loss:  -1.6923\n",
            "Total loss:  -1.6121 | PDE Loss:  -3.3805 | Function Loss:  -1.6933\n",
            "Total loss:  -1.6126 | PDE Loss:  -3.3754 | Function Loss:  -1.6949\n",
            "Total loss:  -1.6131 | PDE Loss:  -3.3722 | Function Loss:  -1.6962\n",
            "Total loss:  -1.6135 | PDE Loss:  -3.3683 | Function Loss:  -1.6976\n",
            "Total loss:  -1.6141 | PDE Loss:  -3.3643 | Function Loss:  -1.6991\n",
            "Total loss:  -1.6147 | PDE Loss:  -3.36 | Function Loss:  -1.7008\n",
            "Total loss:  -1.6155 | PDE Loss:  -3.3601 | Function Loss:  -1.7017\n",
            "Total loss:  -1.6165 | PDE Loss:  -3.3578 | Function Loss:  -1.7034\n",
            "Total loss:  -1.6179 | PDE Loss:  -3.3581 | Function Loss:  -1.705\n",
            "Total loss:  -1.6193 | PDE Loss:  -3.3653 | Function Loss:  -1.7052\n",
            "Total loss:  -1.6213 | PDE Loss:  -3.3728 | Function Loss:  -1.7061\n",
            "Total loss:  -1.6232 | PDE Loss:  -3.3791 | Function Loss:  -1.7069\n",
            "Total loss:  -1.6259 | PDE Loss:  -3.3952 | Function Loss:  -1.7069\n",
            "Total loss:  -1.6292 | PDE Loss:  -3.4014 | Function Loss:  -1.7096\n",
            "Total loss:  -1.629 | PDE Loss:  -3.4134 | Function Loss:  -1.7069\n",
            "Total loss:  -1.6307 | PDE Loss:  -3.4135 | Function Loss:  -1.7089\n",
            "Total loss:  -1.6325 | PDE Loss:  -3.4309 | Function Loss:  -1.7078\n",
            "Total loss:  -1.6349 | PDE Loss:  -3.4307 | Function Loss:  -1.7107\n",
            "Total loss:  -1.637 | PDE Loss:  -3.4269 | Function Loss:  -1.7138\n",
            "Total loss:  -1.6385 | PDE Loss:  -3.4312 | Function Loss:  -1.7148\n",
            "Total loss:  -1.6396 | PDE Loss:  -3.4267 | Function Loss:  -1.717\n",
            "Total loss:  -1.6406 | PDE Loss:  -3.4333 | Function Loss:  -1.717\n",
            "Total loss:  -1.6415 | PDE Loss:  -3.4356 | Function Loss:  -1.7176\n",
            "Total loss:  -1.6423 | PDE Loss:  -3.4331 | Function Loss:  -1.7191\n",
            "Total loss:  -1.6432 | PDE Loss:  -3.4379 | Function Loss:  -1.7191\n",
            "Total loss:  -1.6438 | PDE Loss:  -3.44 | Function Loss:  -1.7194\n",
            "Total loss:  -1.6441 | PDE Loss:  -3.4435 | Function Loss:  -1.7191\n",
            "Total loss:  -1.6445 | PDE Loss:  -3.446 | Function Loss:  -1.7191\n",
            "Total loss:  -1.6449 | PDE Loss:  -3.4478 | Function Loss:  -1.7193\n",
            "Total loss:  -1.6454 | PDE Loss:  -3.4512 | Function Loss:  -1.7192\n",
            "Total loss:  -1.6457 | PDE Loss:  -3.4511 | Function Loss:  -1.7196\n",
            "Total loss:  -1.6461 | PDE Loss:  -3.451 | Function Loss:  -1.7201\n",
            "Total loss:  -1.6465 | PDE Loss:  -3.4499 | Function Loss:  -1.7208\n",
            "Total loss:  -1.6471 | PDE Loss:  -3.4496 | Function Loss:  -1.7216\n",
            "Total loss:  -1.6482 | PDE Loss:  -3.4465 | Function Loss:  -1.7235\n",
            "Total loss:  -1.6497 | PDE Loss:  -3.4442 | Function Loss:  -1.7256\n",
            "Total loss:  -1.6516 | PDE Loss:  -3.4384 | Function Loss:  -1.729\n",
            "Total loss:  -1.6539 | PDE Loss:  -3.4356 | Function Loss:  -1.7323\n",
            "Total loss:  -1.6562 | PDE Loss:  -3.4313 | Function Loss:  -1.736\n",
            "Total loss:  -1.6585 | PDE Loss:  -3.4381 | Function Loss:  -1.7374\n",
            "Total loss:  -1.6603 | PDE Loss:  -3.4394 | Function Loss:  -1.7393\n",
            "Total loss:  -1.6614 | PDE Loss:  -3.4467 | Function Loss:  -1.7392\n",
            "Total loss:  -1.6622 | PDE Loss:  -3.4532 | Function Loss:  -1.7389\n",
            "Total loss:  -1.6628 | PDE Loss:  -3.4581 | Function Loss:  -1.7386\n",
            "Total loss:  -1.6633 | PDE Loss:  -3.4611 | Function Loss:  -1.7386\n",
            "Total loss:  -1.6637 | PDE Loss:  -3.4628 | Function Loss:  -1.7388\n",
            "Total loss:  -1.664 | PDE Loss:  -3.4631 | Function Loss:  -1.7392\n",
            "Total loss:  -1.6645 | PDE Loss:  -3.4632 | Function Loss:  -1.7397\n",
            "Total loss:  -1.665 | PDE Loss:  -3.4617 | Function Loss:  -1.7405\n",
            "Total loss:  -1.6656 | PDE Loss:  -3.4603 | Function Loss:  -1.7416\n",
            "Total loss:  -1.6665 | PDE Loss:  -3.4571 | Function Loss:  -1.7432\n",
            "Total loss:  -1.6675 | PDE Loss:  -3.4556 | Function Loss:  -1.7447\n",
            "Total loss:  -1.6686 | PDE Loss:  -3.4535 | Function Loss:  -1.7465\n",
            "Total loss:  -1.6704 | PDE Loss:  -3.4516 | Function Loss:  -1.749\n",
            "Total loss:  -1.6727 | PDE Loss:  -3.4519 | Function Loss:  -1.7516\n",
            "Total loss:  -1.675 | PDE Loss:  -3.45 | Function Loss:  -1.7548\n",
            "Total loss:  -1.6768 | PDE Loss:  -3.4578 | Function Loss:  -1.7554\n",
            "Total loss:  -1.6784 | PDE Loss:  -3.4617 | Function Loss:  -1.7566\n",
            "Total loss:  -1.6795 | PDE Loss:  -3.4621 | Function Loss:  -1.7578\n",
            "Total loss:  -1.6805 | PDE Loss:  -3.4689 | Function Loss:  -1.7577\n",
            "Total loss:  -1.6814 | PDE Loss:  -3.4748 | Function Loss:  -1.7576\n",
            "Total loss:  -1.6821 | PDE Loss:  -3.4725 | Function Loss:  -1.7589\n",
            "Total loss:  -1.6827 | PDE Loss:  -3.4759 | Function Loss:  -1.7589\n",
            "Total loss:  -1.6832 | PDE Loss:  -3.4729 | Function Loss:  -1.7601\n",
            "Total loss:  -1.6837 | PDE Loss:  -3.4712 | Function Loss:  -1.761\n",
            "Total loss:  -1.6842 | PDE Loss:  -3.4689 | Function Loss:  -1.7621\n",
            "Total loss:  -1.6848 | PDE Loss:  -3.4684 | Function Loss:  -1.7629\n",
            "Total loss:  -1.6855 | PDE Loss:  -3.4657 | Function Loss:  -1.7643\n",
            "Total loss:  -1.6861 | PDE Loss:  -3.4667 | Function Loss:  -1.7647\n",
            "Total loss:  -1.6866 | PDE Loss:  -3.4684 | Function Loss:  -1.7651\n",
            "Total loss:  -1.6871 | PDE Loss:  -3.4676 | Function Loss:  -1.7658\n",
            "Total loss:  -1.6877 | PDE Loss:  -3.4718 | Function Loss:  -1.7657\n",
            "Total loss:  -1.6884 | PDE Loss:  -3.4744 | Function Loss:  -1.766\n",
            "Total loss:  -1.689 | PDE Loss:  -3.4785 | Function Loss:  -1.766\n",
            "Total loss:  -1.6897 | PDE Loss:  -3.4805 | Function Loss:  -1.7664\n",
            "Total loss:  -1.6903 | PDE Loss:  -3.4818 | Function Loss:  -1.7668\n",
            "Total loss:  -1.6908 | PDE Loss:  -3.4837 | Function Loss:  -1.7671\n",
            "Total loss:  -1.6912 | PDE Loss:  -3.4806 | Function Loss:  -1.7682\n",
            "Total loss:  -1.6916 | PDE Loss:  -3.4814 | Function Loss:  -1.7685\n",
            "Total loss:  -1.6922 | PDE Loss:  -3.4828 | Function Loss:  -1.7689\n",
            "Total loss:  -1.6929 | PDE Loss:  -3.4816 | Function Loss:  -1.77\n",
            "Total loss:  -1.6937 | PDE Loss:  -3.4847 | Function Loss:  -1.7704\n",
            "Total loss:  -1.6948 | PDE Loss:  -3.4832 | Function Loss:  -1.7719\n",
            "Total loss:  -1.6958 | PDE Loss:  -3.4863 | Function Loss:  -1.7725\n",
            "Total loss:  -1.6969 | PDE Loss:  -3.4898 | Function Loss:  -1.7732\n",
            "Total loss:  -1.6982 | PDE Loss:  -3.4942 | Function Loss:  -1.7739\n",
            "Total loss:  -1.6993 | PDE Loss:  -3.5001 | Function Loss:  -1.7741\n",
            "Total loss:  -1.7003 | PDE Loss:  -3.5064 | Function Loss:  -1.7742\n",
            "Total loss:  -1.7018 | PDE Loss:  -3.5196 | Function Loss:  -1.7735\n",
            "Total loss:  -1.7032 | PDE Loss:  -3.5273 | Function Loss:  -1.7737\n",
            "Total loss:  -1.7043 | PDE Loss:  -3.5363 | Function Loss:  -1.7735\n",
            "Total loss:  -1.7056 | PDE Loss:  -3.5424 | Function Loss:  -1.774\n",
            "Total loss:  -1.7074 | PDE Loss:  -3.5514 | Function Loss:  -1.7745\n",
            "Total loss:  -1.7092 | PDE Loss:  -3.5521 | Function Loss:  -1.7765\n",
            "Total loss:  -1.7106 | PDE Loss:  -3.5586 | Function Loss:  -1.777\n",
            "Total loss:  -1.7116 | PDE Loss:  -3.5592 | Function Loss:  -1.7782\n",
            "Total loss:  -1.7127 | PDE Loss:  -3.5579 | Function Loss:  -1.7796\n",
            "Total loss:  -1.7137 | PDE Loss:  -3.5612 | Function Loss:  -1.7803\n",
            "Total loss:  -1.7145 | PDE Loss:  -3.5559 | Function Loss:  -1.7821\n",
            "Total loss:  -1.7151 | PDE Loss:  -3.5571 | Function Loss:  -1.7826\n",
            "Total loss:  -1.7156 | PDE Loss:  -3.5568 | Function Loss:  -1.7833\n",
            "Total loss:  -1.7162 | PDE Loss:  -3.5567 | Function Loss:  -1.7839\n",
            "Total loss:  -1.7168 | PDE Loss:  -3.5594 | Function Loss:  -1.7842\n",
            "Total loss:  -1.7174 | PDE Loss:  -3.5595 | Function Loss:  -1.7849\n",
            "Total loss:  -1.718 | PDE Loss:  -3.5665 | Function Loss:  -1.7844\n",
            "Total loss:  -1.7189 | PDE Loss:  -3.5659 | Function Loss:  -1.7855\n",
            "Total loss:  -1.7199 | PDE Loss:  -3.5743 | Function Loss:  -1.7854\n",
            "Total loss:  -1.7214 | PDE Loss:  -3.5759 | Function Loss:  -1.7868\n",
            "Total loss:  -1.7228 | PDE Loss:  -3.5867 | Function Loss:  -1.7867\n",
            "Total loss:  -1.7243 | PDE Loss:  -3.5957 | Function Loss:  -1.787\n",
            "Total loss:  -1.7258 | PDE Loss:  -3.6049 | Function Loss:  -1.7873\n",
            "Total loss:  -1.7269 | PDE Loss:  -3.6169 | Function Loss:  -1.7868\n",
            "Total loss:  -1.7277 | PDE Loss:  -3.6167 | Function Loss:  -1.7878\n",
            "Total loss:  -1.7284 | PDE Loss:  -3.6234 | Function Loss:  -1.7875\n",
            "Total loss:  -1.7288 | PDE Loss:  -3.6219 | Function Loss:  -1.7883\n",
            "Total loss:  -1.7291 | PDE Loss:  -3.623 | Function Loss:  -1.7885\n",
            "Total loss:  -1.7295 | PDE Loss:  -3.6229 | Function Loss:  -1.7889\n",
            "Total loss:  -1.73 | PDE Loss:  -3.6238 | Function Loss:  -1.7894\n",
            "Total loss:  -1.7307 | PDE Loss:  -3.622 | Function Loss:  -1.7904\n",
            "Total loss:  -1.7312 | PDE Loss:  -3.6243 | Function Loss:  -1.7907\n",
            "Total loss:  -1.7318 | PDE Loss:  -3.6226 | Function Loss:  -1.7916\n",
            "Total loss:  -1.7326 | PDE Loss:  -3.6224 | Function Loss:  -1.7926\n",
            "Total loss:  -1.7335 | PDE Loss:  -3.6222 | Function Loss:  -1.7935\n",
            "Total loss:  -1.7347 | PDE Loss:  -3.6214 | Function Loss:  -1.795\n",
            "Total loss:  -1.7362 | PDE Loss:  -3.6223 | Function Loss:  -1.7966\n",
            "Total loss:  -1.7379 | PDE Loss:  -3.6255 | Function Loss:  -1.7981\n",
            "Total loss:  -1.7395 | PDE Loss:  -3.615 | Function Loss:  -1.8016\n",
            "Total loss:  -1.7408 | PDE Loss:  -3.622 | Function Loss:  -1.802\n",
            "Total loss:  -1.7418 | PDE Loss:  -3.6296 | Function Loss:  -1.802\n",
            "Total loss:  -1.7426 | PDE Loss:  -3.6335 | Function Loss:  -1.8023\n",
            "Total loss:  -1.7431 | PDE Loss:  -3.6408 | Function Loss:  -1.8019\n",
            "Total loss:  -1.7435 | PDE Loss:  -3.6446 | Function Loss:  -1.8018\n",
            "Total loss:  -1.744 | PDE Loss:  -3.6483 | Function Loss:  -1.8018\n",
            "Total loss:  -1.7446 | PDE Loss:  -3.6563 | Function Loss:  -1.8014\n",
            "Total loss:  -1.7454 | PDE Loss:  -3.6588 | Function Loss:  -1.802\n",
            "Total loss:  -1.7463 | PDE Loss:  -3.6667 | Function Loss:  -1.8018\n",
            "Total loss:  -1.747 | PDE Loss:  -3.67 | Function Loss:  -1.8022\n",
            "Total loss:  -1.7476 | PDE Loss:  -3.6704 | Function Loss:  -1.8028\n",
            "Total loss:  -1.748 | PDE Loss:  -3.6747 | Function Loss:  -1.8027\n",
            "Total loss:  -1.7485 | PDE Loss:  -3.6733 | Function Loss:  -1.8034\n",
            "Total loss:  -1.749 | PDE Loss:  -3.674 | Function Loss:  -1.8039\n",
            "Total loss:  -1.7495 | PDE Loss:  -3.6737 | Function Loss:  -1.8045\n",
            "Total loss:  -1.75 | PDE Loss:  -3.6715 | Function Loss:  -1.8054\n",
            "Total loss:  -1.7505 | PDE Loss:  -3.6732 | Function Loss:  -1.8058\n",
            "Total loss:  -1.7511 | PDE Loss:  -3.6704 | Function Loss:  -1.8068\n",
            "Total loss:  -1.7516 | PDE Loss:  -3.669 | Function Loss:  -1.8076\n",
            "Total loss:  -1.7524 | PDE Loss:  -3.6682 | Function Loss:  -1.8086\n",
            "Total loss:  -1.7532 | PDE Loss:  -3.6673 | Function Loss:  -1.8097\n",
            "Total loss:  -1.7541 | PDE Loss:  -3.6646 | Function Loss:  -1.811\n",
            "Total loss:  -1.755 | PDE Loss:  -3.6637 | Function Loss:  -1.8122\n",
            "Total loss:  -1.7557 | PDE Loss:  -3.664 | Function Loss:  -1.8129\n",
            "Total loss:  -1.7566 | PDE Loss:  -3.6643 | Function Loss:  -1.814\n",
            "Total loss:  -1.7578 | PDE Loss:  -3.6667 | Function Loss:  -1.815\n",
            "Total loss:  -1.759 | PDE Loss:  -3.666 | Function Loss:  -1.8165\n",
            "Total loss:  -1.7601 | PDE Loss:  -3.6634 | Function Loss:  -1.818\n",
            "Total loss:  -1.761 | PDE Loss:  -3.6667 | Function Loss:  -1.8186\n",
            "Total loss:  -1.7617 | PDE Loss:  -3.6625 | Function Loss:  -1.82\n",
            "Total loss:  -1.7624 | PDE Loss:  -3.6646 | Function Loss:  -1.8206\n",
            "Total loss:  -1.7632 | PDE Loss:  -3.6636 | Function Loss:  -1.8216\n",
            "Total loss:  -1.764 | PDE Loss:  -3.6698 | Function Loss:  -1.8216\n",
            "Total loss:  -1.7648 | PDE Loss:  -3.6697 | Function Loss:  -1.8225\n",
            "Total loss:  -1.7657 | PDE Loss:  -3.6762 | Function Loss:  -1.8227\n",
            "Total loss:  -1.7671 | PDE Loss:  -3.6807 | Function Loss:  -1.8236\n",
            "Total loss:  -1.7685 | PDE Loss:  -3.6872 | Function Loss:  -1.8243\n",
            "Total loss:  -1.7697 | PDE Loss:  -3.6904 | Function Loss:  -1.8252\n",
            "Total loss:  -1.7705 | PDE Loss:  -3.6935 | Function Loss:  -1.8257\n",
            "Total loss:  -1.7711 | PDE Loss:  -3.697 | Function Loss:  -1.8259\n",
            "Total loss:  -1.77 | PDE Loss:  -3.6758 | Function Loss:  -1.8276\n",
            "Total loss:  -1.7713 | PDE Loss:  -3.693 | Function Loss:  -1.8267\n",
            "Total loss:  -1.7719 | PDE Loss:  -3.6959 | Function Loss:  -1.827\n",
            "Total loss:  -1.7725 | PDE Loss:  -3.6986 | Function Loss:  -1.8273\n",
            "Total loss:  -1.7732 | PDE Loss:  -3.7012 | Function Loss:  -1.8277\n",
            "Total loss:  -1.7739 | PDE Loss:  -3.7034 | Function Loss:  -1.8283\n",
            "Total loss:  -1.7746 | PDE Loss:  -3.7067 | Function Loss:  -1.8286\n",
            "Total loss:  -1.7754 | PDE Loss:  -3.7106 | Function Loss:  -1.829\n",
            "Total loss:  -1.7762 | PDE Loss:  -3.7133 | Function Loss:  -1.8295\n",
            "Total loss:  -1.7769 | PDE Loss:  -3.7183 | Function Loss:  -1.8297\n",
            "Total loss:  -1.7776 | PDE Loss:  -3.7204 | Function Loss:  -1.8302\n",
            "Total loss:  -1.7783 | PDE Loss:  -3.7229 | Function Loss:  -1.8306\n",
            "Total loss:  -1.779 | PDE Loss:  -3.7267 | Function Loss:  -1.8309\n",
            "Total loss:  -1.7796 | PDE Loss:  -3.7247 | Function Loss:  -1.8319\n",
            "Total loss:  -1.7801 | PDE Loss:  -3.726 | Function Loss:  -1.8323\n",
            "Total loss:  -1.7808 | PDE Loss:  -3.7305 | Function Loss:  -1.8325\n",
            "Total loss:  -1.7814 | PDE Loss:  -3.7248 | Function Loss:  -1.8339\n",
            "Total loss:  -1.782 | PDE Loss:  -3.7313 | Function Loss:  -1.8337\n",
            "Total loss:  -1.7824 | PDE Loss:  -3.7325 | Function Loss:  -1.8341\n",
            "Total loss:  -1.7831 | PDE Loss:  -3.735 | Function Loss:  -1.8346\n",
            "Total loss:  -1.7838 | PDE Loss:  -3.7337 | Function Loss:  -1.8355\n",
            "Total loss:  -1.7844 | PDE Loss:  -3.7359 | Function Loss:  -1.8359\n",
            "Total loss:  -1.7848 | PDE Loss:  -3.738 | Function Loss:  -1.8361\n",
            "Total loss:  -1.7854 | PDE Loss:  -3.7397 | Function Loss:  -1.8365\n",
            "Total loss:  -1.786 | PDE Loss:  -3.7431 | Function Loss:  -1.8368\n",
            "Total loss:  -1.7866 | PDE Loss:  -3.745 | Function Loss:  -1.8373\n",
            "Total loss:  -1.7872 | PDE Loss:  -3.7484 | Function Loss:  -1.8375\n",
            "Total loss:  -1.7877 | PDE Loss:  -3.7498 | Function Loss:  -1.8379\n",
            "Total loss:  -1.7881 | PDE Loss:  -3.7516 | Function Loss:  -1.8382\n",
            "Total loss:  -1.7886 | PDE Loss:  -3.7555 | Function Loss:  -1.8382\n",
            "Total loss:  -1.7891 | PDE Loss:  -3.7545 | Function Loss:  -1.8389\n",
            "Total loss:  -1.7896 | PDE Loss:  -3.7599 | Function Loss:  -1.8388\n",
            "Total loss:  -1.7903 | PDE Loss:  -3.7593 | Function Loss:  -1.8397\n",
            "Total loss:  -1.791 | PDE Loss:  -3.7617 | Function Loss:  -1.8401\n",
            "Total loss:  -1.7918 | PDE Loss:  -3.7632 | Function Loss:  -1.8408\n",
            "Total loss:  -1.7925 | PDE Loss:  -3.7602 | Function Loss:  -1.8419\n",
            "Total loss:  -1.793 | PDE Loss:  -3.7664 | Function Loss:  -1.8418\n",
            "Total loss:  -1.7934 | PDE Loss:  -3.76 | Function Loss:  -1.843\n",
            "Total loss:  -1.7938 | PDE Loss:  -3.7637 | Function Loss:  -1.843\n",
            "Total loss:  -1.7941 | PDE Loss:  -3.7632 | Function Loss:  -1.8434\n",
            "Total loss:  -1.7945 | PDE Loss:  -3.7632 | Function Loss:  -1.8439\n",
            "Total loss:  -1.7949 | PDE Loss:  -3.7616 | Function Loss:  -1.8445\n",
            "Total loss:  -1.7952 | PDE Loss:  -3.7639 | Function Loss:  -1.8446\n",
            "Total loss:  -1.7954 | PDE Loss:  -3.7636 | Function Loss:  -1.8449\n",
            "Total loss:  -1.7958 | PDE Loss:  -3.7645 | Function Loss:  -1.8451\n",
            "Total loss:  -1.7961 | PDE Loss:  -3.7653 | Function Loss:  -1.8454\n",
            "Total loss:  -1.7964 | PDE Loss:  -3.7653 | Function Loss:  -1.8458\n",
            "Total loss:  -1.7967 | PDE Loss:  -3.7679 | Function Loss:  -1.8458\n",
            "Total loss:  -1.7972 | PDE Loss:  -3.7687 | Function Loss:  -1.8462\n",
            "Total loss:  -1.7976 | PDE Loss:  -3.7718 | Function Loss:  -1.8463\n",
            "Total loss:  -1.7982 | PDE Loss:  -3.7782 | Function Loss:  -1.8462\n",
            "Total loss:  -1.7989 | PDE Loss:  -3.7853 | Function Loss:  -1.8462\n",
            "Total loss:  -1.7996 | PDE Loss:  -3.7914 | Function Loss:  -1.8462\n",
            "Total loss:  -1.8002 | PDE Loss:  -3.7983 | Function Loss:  -1.8461\n",
            "Total loss:  -1.8009 | PDE Loss:  -3.8019 | Function Loss:  -1.8465\n",
            "Total loss:  -1.8017 | PDE Loss:  -3.8075 | Function Loss:  -1.8469\n",
            "Total loss:  -1.8028 | PDE Loss:  -3.8091 | Function Loss:  -1.8479\n",
            "Total loss:  -1.8043 | PDE Loss:  -3.81 | Function Loss:  -1.8494\n",
            "Total loss:  -1.8059 | PDE Loss:  -3.8038 | Function Loss:  -1.8518\n",
            "Total loss:  -1.8071 | PDE Loss:  -3.7977 | Function Loss:  -1.8539\n",
            "Total loss:  -1.8079 | PDE Loss:  -3.7935 | Function Loss:  -1.8553\n",
            "Total loss:  -1.8089 | PDE Loss:  -3.7914 | Function Loss:  -1.8566\n",
            "Total loss:  -1.8097 | PDE Loss:  -3.7829 | Function Loss:  -1.8585\n",
            "Total loss:  -1.8102 | PDE Loss:  -3.7823 | Function Loss:  -1.8592\n",
            "Total loss:  -1.8108 | PDE Loss:  -3.7834 | Function Loss:  -1.8597\n",
            "Total loss:  -1.8114 | PDE Loss:  -3.7736 | Function Loss:  -1.8615\n",
            "Total loss:  -1.8118 | PDE Loss:  -3.7803 | Function Loss:  -1.8613\n",
            "Total loss:  -1.8122 | PDE Loss:  -3.7848 | Function Loss:  -1.8611\n",
            "Total loss:  -1.8127 | PDE Loss:  -3.789 | Function Loss:  -1.8612\n",
            "Total loss:  -1.8132 | PDE Loss:  -3.7939 | Function Loss:  -1.8611\n",
            "Total loss:  -1.8137 | PDE Loss:  -3.7976 | Function Loss:  -1.8612\n",
            "Total loss:  -1.8144 | PDE Loss:  -3.8024 | Function Loss:  -1.8615\n",
            "Total loss:  -1.8153 | PDE Loss:  -3.806 | Function Loss:  -1.8621\n",
            "Total loss:  -1.8162 | PDE Loss:  -3.8122 | Function Loss:  -1.8624\n",
            "Total loss:  -1.8172 | PDE Loss:  -3.8119 | Function Loss:  -1.8636\n",
            "Total loss:  -1.8183 | PDE Loss:  -3.8157 | Function Loss:  -1.8644\n",
            "Total loss:  -1.8193 | PDE Loss:  -3.8057 | Function Loss:  -1.8666\n",
            "Total loss:  -1.82 | PDE Loss:  -3.8032 | Function Loss:  -1.8677\n",
            "Total loss:  -1.8206 | PDE Loss:  -3.7963 | Function Loss:  -1.8692\n",
            "Total loss:  -1.8213 | PDE Loss:  -3.7883 | Function Loss:  -1.8709\n",
            "Total loss:  -1.822 | PDE Loss:  -3.7878 | Function Loss:  -1.8717\n",
            "Total loss:  -1.8227 | PDE Loss:  -3.7762 | Function Loss:  -1.874\n",
            "Total loss:  -1.8232 | PDE Loss:  -3.7742 | Function Loss:  -1.8747\n",
            "Total loss:  -1.8236 | PDE Loss:  -3.7728 | Function Loss:  -1.8754\n",
            "Total loss:  -1.8239 | PDE Loss:  -3.7771 | Function Loss:  -1.8752\n",
            "Total loss:  -1.8241 | PDE Loss:  -3.7786 | Function Loss:  -1.8753\n",
            "Total loss:  -1.8244 | PDE Loss:  -3.7825 | Function Loss:  -1.875\n",
            "Total loss:  -1.8246 | PDE Loss:  -3.7847 | Function Loss:  -1.875\n",
            "Total loss:  -1.825 | PDE Loss:  -3.7887 | Function Loss:  -1.875\n",
            "Total loss:  -1.8254 | PDE Loss:  -3.7898 | Function Loss:  -1.8753\n",
            "Total loss:  -1.8258 | PDE Loss:  -3.7869 | Function Loss:  -1.8761\n",
            "Total loss:  -1.8261 | PDE Loss:  -3.7856 | Function Loss:  -1.8766\n",
            "Total loss:  -1.8264 | PDE Loss:  -3.7786 | Function Loss:  -1.8778\n",
            "Total loss:  -1.8266 | PDE Loss:  -3.7757 | Function Loss:  -1.8785\n",
            "Total loss:  -1.8269 | PDE Loss:  -3.7707 | Function Loss:  -1.8794\n",
            "Total loss:  -1.8271 | PDE Loss:  -3.7675 | Function Loss:  -1.8801\n",
            "Total loss:  -1.8274 | PDE Loss:  -3.7639 | Function Loss:  -1.8809\n",
            "Total loss:  -1.8277 | PDE Loss:  -3.763 | Function Loss:  -1.8813\n",
            "Total loss:  -1.8279 | PDE Loss:  -3.7624 | Function Loss:  -1.8816\n",
            "Total loss:  -1.8281 | PDE Loss:  -3.7638 | Function Loss:  -1.8817\n",
            "Total loss:  -1.8283 | PDE Loss:  -3.7656 | Function Loss:  -1.8816\n",
            "Total loss:  -1.8285 | PDE Loss:  -3.769 | Function Loss:  -1.8814\n",
            "Total loss:  -1.8288 | PDE Loss:  -3.7711 | Function Loss:  -1.8814\n",
            "Total loss:  -1.829 | PDE Loss:  -3.7746 | Function Loss:  -1.8813\n",
            "Total loss:  -1.8294 | PDE Loss:  -3.7763 | Function Loss:  -1.8814\n",
            "Total loss:  -1.8301 | PDE Loss:  -3.7844 | Function Loss:  -1.8812\n",
            "Total loss:  -1.8308 | PDE Loss:  -3.7873 | Function Loss:  -1.8817\n",
            "Total loss:  -1.8321 | PDE Loss:  -3.7915 | Function Loss:  -1.8826\n",
            "Total loss:  -1.8334 | PDE Loss:  -3.7921 | Function Loss:  -1.884\n",
            "Total loss:  -1.8345 | PDE Loss:  -3.7969 | Function Loss:  -1.8847\n",
            "Total loss:  -1.8356 | PDE Loss:  -3.7971 | Function Loss:  -1.8858\n",
            "Total loss:  -1.8367 | PDE Loss:  -3.8002 | Function Loss:  -1.8867\n",
            "Total loss:  -1.8375 | PDE Loss:  -3.7947 | Function Loss:  -1.8882\n",
            "Total loss:  -1.8381 | PDE Loss:  -3.795 | Function Loss:  -1.8889\n",
            "Total loss:  -1.8385 | PDE Loss:  -3.7891 | Function Loss:  -1.8901\n",
            "Total loss:  -1.8389 | PDE Loss:  -3.7919 | Function Loss:  -1.8902\n",
            "Total loss:  -1.8393 | PDE Loss:  -3.7856 | Function Loss:  -1.8914\n",
            "Total loss:  -1.8395 | PDE Loss:  -3.786 | Function Loss:  -1.8916\n",
            "Total loss:  -1.8399 | PDE Loss:  -3.7872 | Function Loss:  -1.892\n",
            "Total loss:  -1.8405 | PDE Loss:  -3.7929 | Function Loss:  -1.8919\n",
            "Total loss:  -1.841 | PDE Loss:  -3.7921 | Function Loss:  -1.8926\n",
            "Total loss:  -1.8415 | PDE Loss:  -3.7941 | Function Loss:  -1.8928\n",
            "Total loss:  -1.8418 | PDE Loss:  -3.7935 | Function Loss:  -1.8933\n",
            "Total loss:  -1.8421 | PDE Loss:  -3.7932 | Function Loss:  -1.8937\n",
            "Total loss:  -1.8423 | PDE Loss:  -3.793 | Function Loss:  -1.8939\n",
            "Total loss:  -1.8425 | PDE Loss:  -3.7932 | Function Loss:  -1.8941\n",
            "Total loss:  -1.8427 | PDE Loss:  -3.7921 | Function Loss:  -1.8945\n",
            "Total loss:  -1.8429 | PDE Loss:  -3.792 | Function Loss:  -1.8947\n",
            "Total loss:  -1.8432 | PDE Loss:  -3.7903 | Function Loss:  -1.8952\n",
            "Total loss:  -1.8435 | PDE Loss:  -3.7901 | Function Loss:  -1.8956\n",
            "Total loss:  -1.8437 | PDE Loss:  -3.7891 | Function Loss:  -1.896\n",
            "Total loss:  -1.844 | PDE Loss:  -3.7888 | Function Loss:  -1.8963\n",
            "Total loss:  -1.8443 | PDE Loss:  -3.7882 | Function Loss:  -1.8968\n",
            "Total loss:  -1.8446 | PDE Loss:  -3.7888 | Function Loss:  -1.8971\n",
            "Total loss:  -1.8449 | PDE Loss:  -3.7905 | Function Loss:  -1.8972\n",
            "Total loss:  -1.8453 | PDE Loss:  -3.7929 | Function Loss:  -1.8973\n",
            "Total loss:  -1.8457 | PDE Loss:  -3.7977 | Function Loss:  -1.8972\n",
            "Total loss:  -1.8462 | PDE Loss:  -3.8005 | Function Loss:  -1.8973\n",
            "Total loss:  -1.8465 | PDE Loss:  -3.8036 | Function Loss:  -1.8973\n",
            "Total loss:  -1.8469 | PDE Loss:  -3.8053 | Function Loss:  -1.8975\n",
            "Total loss:  -1.8472 | PDE Loss:  -3.8073 | Function Loss:  -1.8976\n",
            "Total loss:  -1.8474 | PDE Loss:  -3.8064 | Function Loss:  -1.898\n",
            "Total loss:  -1.8477 | PDE Loss:  -3.8053 | Function Loss:  -1.8984\n",
            "Total loss:  -1.8479 | PDE Loss:  -3.8037 | Function Loss:  -1.8989\n",
            "Total loss:  -1.8482 | PDE Loss:  -3.8016 | Function Loss:  -1.8995\n",
            "Total loss:  -1.8485 | PDE Loss:  -3.8018 | Function Loss:  -1.8997\n",
            "Total loss:  -1.8487 | PDE Loss:  -3.8024 | Function Loss:  -1.8999\n",
            "Total loss:  -1.849 | PDE Loss:  -3.8047 | Function Loss:  -1.9\n",
            "Total loss:  -1.8494 | PDE Loss:  -3.8087 | Function Loss:  -1.8999\n",
            "Total loss:  -1.8497 | PDE Loss:  -3.8131 | Function Loss:  -1.8998\n",
            "Total loss:  -1.85 | PDE Loss:  -3.8168 | Function Loss:  -1.8997\n",
            "Total loss:  -1.8503 | PDE Loss:  -3.8198 | Function Loss:  -1.8996\n",
            "Total loss:  -1.8506 | PDE Loss:  -3.8204 | Function Loss:  -1.8999\n",
            "Total loss:  -1.8511 | PDE Loss:  -3.8197 | Function Loss:  -1.9004\n",
            "Total loss:  -1.8516 | PDE Loss:  -3.8165 | Function Loss:  -1.9014\n",
            "Total loss:  -1.8521 | PDE Loss:  -3.8109 | Function Loss:  -1.9027\n",
            "Total loss:  -1.8528 | PDE Loss:  -3.807 | Function Loss:  -1.904\n",
            "Total loss:  -1.8535 | PDE Loss:  -3.7969 | Function Loss:  -1.9061\n",
            "Total loss:  -1.8542 | PDE Loss:  -3.795 | Function Loss:  -1.907\n",
            "Total loss:  -1.8548 | PDE Loss:  -3.7899 | Function Loss:  -1.9084\n",
            "Total loss:  -1.8553 | PDE Loss:  -3.7925 | Function Loss:  -1.9086\n",
            "Total loss:  -1.8557 | PDE Loss:  -3.7917 | Function Loss:  -1.9091\n",
            "Total loss:  -1.856 | PDE Loss:  -3.797 | Function Loss:  -1.9088\n",
            "Total loss:  -1.8562 | PDE Loss:  -3.8011 | Function Loss:  -1.9086\n",
            "Total loss:  -1.8565 | PDE Loss:  -3.8055 | Function Loss:  -1.9083\n",
            "Total loss:  -1.8569 | PDE Loss:  -3.8106 | Function Loss:  -1.9081\n",
            "Total loss:  -1.8575 | PDE Loss:  -3.8195 | Function Loss:  -1.9076\n",
            "Total loss:  -1.858 | PDE Loss:  -3.8281 | Function Loss:  -1.9072\n",
            "Total loss:  -1.8587 | PDE Loss:  -3.8398 | Function Loss:  -1.9066\n",
            "Total loss:  -1.8593 | PDE Loss:  -3.8497 | Function Loss:  -1.9062\n",
            "Total loss:  -1.8599 | PDE Loss:  -3.8539 | Function Loss:  -1.9063\n",
            "Total loss:  -1.8603 | PDE Loss:  -3.8605 | Function Loss:  -1.906\n",
            "Total loss:  -1.8606 | PDE Loss:  -3.8561 | Function Loss:  -1.9069\n",
            "Total loss:  -1.8609 | PDE Loss:  -3.8587 | Function Loss:  -1.9069\n",
            "Total loss:  -1.8611 | PDE Loss:  -3.8573 | Function Loss:  -1.9073\n",
            "Total loss:  -1.8614 | PDE Loss:  -3.8578 | Function Loss:  -1.9075\n",
            "Total loss:  -1.8617 | PDE Loss:  -3.8578 | Function Loss:  -1.9078\n",
            "Total loss:  -1.8619 | PDE Loss:  -3.8599 | Function Loss:  -1.9079\n",
            "Total loss:  -1.8621 | PDE Loss:  -3.862 | Function Loss:  -1.9079\n",
            "Total loss:  -1.8623 | PDE Loss:  -3.8644 | Function Loss:  -1.9078\n",
            "Total loss:  -1.8625 | PDE Loss:  -3.8671 | Function Loss:  -1.9077\n",
            "Total loss:  -1.8627 | PDE Loss:  -3.8694 | Function Loss:  -1.9078\n",
            "Total loss:  -1.863 | PDE Loss:  -3.8719 | Function Loss:  -1.9078\n",
            "Total loss:  -1.8633 | PDE Loss:  -3.874 | Function Loss:  -1.9078\n",
            "Total loss:  -1.8635 | PDE Loss:  -3.8759 | Function Loss:  -1.9079\n",
            "Total loss:  -1.8638 | PDE Loss:  -3.8774 | Function Loss:  -1.9081\n",
            "Total loss:  -1.8641 | PDE Loss:  -3.8813 | Function Loss:  -1.908\n",
            "Total loss:  -1.8644 | PDE Loss:  -3.8794 | Function Loss:  -1.9085\n",
            "Total loss:  -1.8646 | PDE Loss:  -3.884 | Function Loss:  -1.9082\n",
            "Total loss:  -1.8648 | PDE Loss:  -3.8888 | Function Loss:  -1.908\n",
            "Total loss:  -1.8652 | PDE Loss:  -3.8925 | Function Loss:  -1.908\n",
            "Total loss:  -1.8655 | PDE Loss:  -3.8988 | Function Loss:  -1.9077\n",
            "Total loss:  -1.8659 | PDE Loss:  -3.8998 | Function Loss:  -1.9081\n",
            "Total loss:  -1.8664 | PDE Loss:  -3.903 | Function Loss:  -1.9083\n",
            "Total loss:  -1.8672 | PDE Loss:  -3.907 | Function Loss:  -1.9087\n",
            "Total loss:  -1.8679 | PDE Loss:  -3.9039 | Function Loss:  -1.9099\n",
            "Total loss:  -1.8686 | PDE Loss:  -3.9132 | Function Loss:  -1.9097\n",
            "Total loss:  -1.8694 | PDE Loss:  -3.903 | Function Loss:  -1.9116\n",
            "Total loss:  -1.87 | PDE Loss:  -3.9077 | Function Loss:  -1.9118\n",
            "Total loss:  -1.8707 | PDE Loss:  -3.9005 | Function Loss:  -1.9133\n",
            "Total loss:  -1.8714 | PDE Loss:  -3.9026 | Function Loss:  -1.9138\n",
            "Total loss:  -1.8721 | PDE Loss:  -3.9018 | Function Loss:  -1.9147\n",
            "Total loss:  -1.8729 | PDE Loss:  -3.9015 | Function Loss:  -1.9156\n",
            "Total loss:  -1.874 | PDE Loss:  -3.9076 | Function Loss:  -1.9162\n",
            "Total loss:  -1.875 | PDE Loss:  -3.9076 | Function Loss:  -1.9173\n",
            "Total loss:  -1.8757 | PDE Loss:  -3.9125 | Function Loss:  -1.9175\n",
            "Total loss:  -1.8762 | PDE Loss:  -3.92 | Function Loss:  -1.9174\n",
            "Total loss:  -1.8766 | PDE Loss:  -3.9214 | Function Loss:  -1.9177\n",
            "Total loss:  -1.8771 | PDE Loss:  -3.9245 | Function Loss:  -1.9179\n",
            "Total loss:  -1.8775 | PDE Loss:  -3.9289 | Function Loss:  -1.9179\n",
            "Total loss:  -1.8779 | PDE Loss:  -3.9289 | Function Loss:  -1.9183\n",
            "Total loss:  -1.8782 | PDE Loss:  -3.9319 | Function Loss:  -1.9183\n",
            "Total loss:  -1.8785 | PDE Loss:  -3.9317 | Function Loss:  -1.9187\n",
            "Total loss:  -1.8788 | PDE Loss:  -3.9339 | Function Loss:  -1.9189\n",
            "Total loss:  -1.8792 | PDE Loss:  -3.9361 | Function Loss:  -1.919\n",
            "Total loss:  -1.8796 | PDE Loss:  -3.943 | Function Loss:  -1.9188\n",
            "Total loss:  -1.88 | PDE Loss:  -3.9442 | Function Loss:  -1.9192\n",
            "Total loss:  -1.8805 | PDE Loss:  -3.9519 | Function Loss:  -1.919\n",
            "Total loss:  -1.8811 | PDE Loss:  -3.9586 | Function Loss:  -1.919\n",
            "Total loss:  -1.8817 | PDE Loss:  -3.9713 | Function Loss:  -1.9185\n",
            "Total loss:  -1.8822 | PDE Loss:  -3.9799 | Function Loss:  -1.9183\n",
            "Total loss:  -1.8828 | PDE Loss:  -3.9911 | Function Loss:  -1.9181\n",
            "Total loss:  -1.8837 | PDE Loss:  -4.0021 | Function Loss:  -1.9181\n",
            "Total loss:  -1.8843 | PDE Loss:  -4.0103 | Function Loss:  -1.9181\n",
            "Total loss:  -1.8851 | PDE Loss:  -4.0169 | Function Loss:  -1.9184\n",
            "Total loss:  -1.8858 | PDE Loss:  -4.0173 | Function Loss:  -1.9192\n",
            "Total loss:  -1.8865 | PDE Loss:  -4.0177 | Function Loss:  -1.9199\n",
            "Total loss:  -1.8871 | PDE Loss:  -4.0098 | Function Loss:  -1.9211\n",
            "Total loss:  -1.8874 | PDE Loss:  -4.0083 | Function Loss:  -1.9216\n",
            "Total loss:  -1.8877 | PDE Loss:  -3.9975 | Function Loss:  -1.9229\n",
            "Total loss:  -1.8881 | PDE Loss:  -3.9964 | Function Loss:  -1.9233\n",
            "Total loss:  -1.8884 | PDE Loss:  -3.9913 | Function Loss:  -1.9241\n",
            "Total loss:  -1.8889 | PDE Loss:  -3.989 | Function Loss:  -1.9248\n",
            "Total loss:  -1.8893 | PDE Loss:  -3.9849 | Function Loss:  -1.9256\n",
            "Total loss:  -1.8898 | PDE Loss:  -3.9838 | Function Loss:  -1.9263\n",
            "Total loss:  -1.8903 | PDE Loss:  -3.9816 | Function Loss:  -1.927\n",
            "Total loss:  -1.8908 | PDE Loss:  -3.9797 | Function Loss:  -1.9277\n",
            "Total loss:  -1.8915 | PDE Loss:  -3.9781 | Function Loss:  -1.9286\n",
            "Total loss:  -1.8921 | PDE Loss:  -3.975 | Function Loss:  -1.9296\n",
            "Total loss:  -1.893 | PDE Loss:  -3.9711 | Function Loss:  -1.9308\n",
            "Total loss:  -1.8939 | PDE Loss:  -3.9714 | Function Loss:  -1.9319\n",
            "Total loss:  -1.8947 | PDE Loss:  -3.9724 | Function Loss:  -1.9327\n",
            "Total loss:  -1.8953 | PDE Loss:  -3.9481 | Function Loss:  -1.9356\n",
            "Total loss:  -1.8959 | PDE Loss:  -3.9602 | Function Loss:  -1.9351\n",
            "Total loss:  -1.8963 | PDE Loss:  -3.9662 | Function Loss:  -1.9349\n",
            "Total loss:  -1.8967 | PDE Loss:  -3.9635 | Function Loss:  -1.9357\n",
            "Total loss:  -1.8971 | PDE Loss:  -3.96 | Function Loss:  -1.9364\n",
            "Total loss:  -1.8974 | PDE Loss:  -3.9537 | Function Loss:  -1.9374\n",
            "Total loss:  -1.8978 | PDE Loss:  -3.9489 | Function Loss:  -1.9382\n",
            "Total loss:  -1.8981 | PDE Loss:  -3.9429 | Function Loss:  -1.9392\n",
            "Total loss:  -1.8985 | PDE Loss:  -3.9377 | Function Loss:  -1.9402\n",
            "Total loss:  -1.899 | PDE Loss:  -3.932 | Function Loss:  -1.9412\n",
            "Total loss:  -1.8994 | PDE Loss:  -3.9274 | Function Loss:  -1.9422\n",
            "Total loss:  -1.8997 | PDE Loss:  -3.9263 | Function Loss:  -1.9426\n",
            "Total loss:  -1.9 | PDE Loss:  -3.9201 | Function Loss:  -1.9436\n",
            "Total loss:  -1.9002 | PDE Loss:  -3.9207 | Function Loss:  -1.9438\n",
            "Total loss:  -1.9005 | PDE Loss:  -3.9204 | Function Loss:  -1.9441\n",
            "Total loss:  -1.9008 | PDE Loss:  -3.9212 | Function Loss:  -1.9444\n",
            "Total loss:  -1.9012 | PDE Loss:  -3.9214 | Function Loss:  -1.9448\n",
            "Total loss:  -1.9018 | PDE Loss:  -3.922 | Function Loss:  -1.9454\n",
            "Total loss:  -1.9023 | PDE Loss:  -3.9212 | Function Loss:  -1.946\n",
            "Total loss:  -1.903 | PDE Loss:  -3.9233 | Function Loss:  -1.9465\n",
            "Total loss:  -1.9037 | PDE Loss:  -3.9222 | Function Loss:  -1.9474\n",
            "Total loss:  -1.9043 | PDE Loss:  -3.9246 | Function Loss:  -1.9478\n",
            "Total loss:  -1.9048 | PDE Loss:  -3.9264 | Function Loss:  -1.9482\n",
            "Total loss:  -1.9053 | PDE Loss:  -3.9274 | Function Loss:  -1.9487\n",
            "Total loss:  -1.9058 | PDE Loss:  -3.9261 | Function Loss:  -1.9494\n",
            "Total loss:  -1.9062 | PDE Loss:  -3.93 | Function Loss:  -1.9494\n",
            "Total loss:  -1.9066 | PDE Loss:  -3.9234 | Function Loss:  -1.9506\n",
            "Total loss:  -1.9071 | PDE Loss:  -3.926 | Function Loss:  -1.9508\n",
            "Total loss:  -1.9076 | PDE Loss:  -3.9253 | Function Loss:  -1.9514\n",
            "Total loss:  -1.9082 | PDE Loss:  -3.9236 | Function Loss:  -1.9523\n",
            "Total loss:  -1.9089 | PDE Loss:  -3.9149 | Function Loss:  -1.954\n",
            "Total loss:  -1.9095 | PDE Loss:  -3.9131 | Function Loss:  -1.9548\n",
            "Total loss:  -1.9099 | PDE Loss:  -3.9092 | Function Loss:  -1.9558\n",
            "Total loss:  -1.9105 | PDE Loss:  -3.9073 | Function Loss:  -1.9567\n",
            "Total loss:  -1.9113 | PDE Loss:  -3.9111 | Function Loss:  -1.9571\n",
            "Total loss:  -1.912 | PDE Loss:  -3.9139 | Function Loss:  -1.9575\n",
            "Total loss:  -1.9125 | PDE Loss:  -3.9246 | Function Loss:  -1.957\n",
            "Total loss:  -1.913 | PDE Loss:  -3.9312 | Function Loss:  -1.9568\n",
            "Total loss:  -1.9135 | PDE Loss:  -3.942 | Function Loss:  -1.9562\n",
            "Total loss:  -1.9141 | PDE Loss:  -3.9527 | Function Loss:  -1.9558\n",
            "Total loss:  -1.9147 | PDE Loss:  -3.9649 | Function Loss:  -1.9552\n",
            "Total loss:  -1.9153 | PDE Loss:  -3.9745 | Function Loss:  -1.955\n",
            "Total loss:  -1.9163 | PDE Loss:  -3.9839 | Function Loss:  -1.9552\n",
            "Total loss:  -1.9177 | PDE Loss:  -3.993 | Function Loss:  -1.9558\n",
            "Total loss:  -1.9189 | PDE Loss:  -3.9935 | Function Loss:  -1.9571\n",
            "Total loss:  -1.9198 | PDE Loss:  -3.9866 | Function Loss:  -1.9587\n",
            "Total loss:  -1.9207 | PDE Loss:  -3.9787 | Function Loss:  -1.9605\n",
            "Total loss:  -1.9214 | PDE Loss:  -3.968 | Function Loss:  -1.9623\n",
            "Total loss:  -1.922 | PDE Loss:  -3.9618 | Function Loss:  -1.9635\n",
            "Total loss:  -1.9228 | PDE Loss:  -3.9504 | Function Loss:  -1.9655\n",
            "Total loss:  -1.9235 | PDE Loss:  -3.9492 | Function Loss:  -1.9665\n",
            "Total loss:  -1.9241 | PDE Loss:  -3.9491 | Function Loss:  -1.9671\n",
            "Total loss:  -1.9247 | PDE Loss:  -3.9494 | Function Loss:  -1.9678\n",
            "Total loss:  -1.925 | PDE Loss:  -3.9528 | Function Loss:  -1.9677\n",
            "Total loss:  -1.9253 | PDE Loss:  -3.9567 | Function Loss:  -1.9677\n",
            "Total loss:  -1.9257 | PDE Loss:  -3.9611 | Function Loss:  -1.9677\n",
            "Total loss:  -1.926 | PDE Loss:  -3.9644 | Function Loss:  -1.9677\n",
            "Total loss:  -1.9263 | PDE Loss:  -3.9666 | Function Loss:  -1.9678\n",
            "Total loss:  -1.9268 | PDE Loss:  -3.969 | Function Loss:  -1.9682\n",
            "Total loss:  -1.9274 | PDE Loss:  -3.9683 | Function Loss:  -1.9688\n",
            "Total loss:  -1.9279 | PDE Loss:  -3.9673 | Function Loss:  -1.9695\n",
            "Total loss:  -1.9285 | PDE Loss:  -3.9622 | Function Loss:  -1.9707\n",
            "Total loss:  -1.9289 | PDE Loss:  -3.9559 | Function Loss:  -1.9718\n",
            "Total loss:  -1.9295 | PDE Loss:  -3.9482 | Function Loss:  -1.9732\n",
            "Total loss:  -1.9299 | PDE Loss:  -3.9406 | Function Loss:  -1.9745\n",
            "Total loss:  -1.9303 | PDE Loss:  -3.9359 | Function Loss:  -1.9754\n",
            "Total loss:  -1.9306 | PDE Loss:  -3.93 | Function Loss:  -1.9764\n",
            "Total loss:  -1.9307 | PDE Loss:  -3.9286 | Function Loss:  -1.9767\n",
            "Total loss:  -1.9309 | PDE Loss:  -3.9281 | Function Loss:  -1.977\n",
            "Total loss:  -1.9311 | PDE Loss:  -3.9281 | Function Loss:  -1.9772\n",
            "Total loss:  -1.9312 | PDE Loss:  -3.9288 | Function Loss:  -1.9773\n",
            "Total loss:  -1.9314 | PDE Loss:  -3.9288 | Function Loss:  -1.9775\n",
            "Total loss:  -1.9316 | PDE Loss:  -3.9292 | Function Loss:  -1.9776\n",
            "Total loss:  -1.9318 | PDE Loss:  -3.9295 | Function Loss:  -1.9778\n",
            "Total loss:  -1.9319 | PDE Loss:  -3.9269 | Function Loss:  -1.9782\n",
            "Total loss:  -1.932 | PDE Loss:  -3.9261 | Function Loss:  -1.9785\n",
            "Total loss:  -1.9322 | PDE Loss:  -3.9238 | Function Loss:  -1.9789\n",
            "Total loss:  -1.9324 | PDE Loss:  -3.9243 | Function Loss:  -1.979\n",
            "Total loss:  -1.9325 | PDE Loss:  -3.9246 | Function Loss:  -1.9792\n",
            "Total loss:  -1.9327 | PDE Loss:  -3.9272 | Function Loss:  -1.979\n",
            "Total loss:  -1.9328 | PDE Loss:  -3.9302 | Function Loss:  -1.9789\n",
            "Total loss:  -1.933 | PDE Loss:  -3.9319 | Function Loss:  -1.9789\n",
            "Total loss:  -1.9332 | PDE Loss:  -3.9346 | Function Loss:  -1.9788\n",
            "Total loss:  -1.9336 | PDE Loss:  -3.9384 | Function Loss:  -1.9788\n",
            "Total loss:  -1.934 | PDE Loss:  -3.9416 | Function Loss:  -1.979\n",
            "Total loss:  -1.9346 | PDE Loss:  -3.9457 | Function Loss:  -1.9792\n",
            "Total loss:  -1.9352 | PDE Loss:  -3.9468 | Function Loss:  -1.9797\n",
            "Total loss:  -1.9359 | PDE Loss:  -3.9462 | Function Loss:  -1.9805\n",
            "Total loss:  -1.9365 | PDE Loss:  -3.9451 | Function Loss:  -1.9813\n",
            "Total loss:  -1.9372 | PDE Loss:  -3.9423 | Function Loss:  -1.9824\n",
            "Total loss:  -1.938 | PDE Loss:  -3.94 | Function Loss:  -1.9835\n",
            "Total loss:  -1.9385 | PDE Loss:  -3.9413 | Function Loss:  -1.984\n",
            "Total loss:  -1.9391 | PDE Loss:  -3.9435 | Function Loss:  -1.9844\n",
            "Total loss:  -1.9397 | PDE Loss:  -3.9482 | Function Loss:  -1.9846\n",
            "Total loss:  -1.9404 | PDE Loss:  -3.9524 | Function Loss:  -1.9848\n",
            "Total loss:  -1.9411 | PDE Loss:  -3.9572 | Function Loss:  -1.9851\n",
            "Total loss:  -1.9419 | PDE Loss:  -3.9596 | Function Loss:  -1.9857\n",
            "Total loss:  -1.9427 | PDE Loss:  -3.9609 | Function Loss:  -1.9865\n",
            "Total loss:  -1.9435 | PDE Loss:  -3.9587 | Function Loss:  -1.9876\n",
            "Total loss:  -1.9443 | PDE Loss:  -3.9551 | Function Loss:  -1.9888\n",
            "Total loss:  -1.9449 | PDE Loss:  -3.9521 | Function Loss:  -1.9899\n",
            "Total loss:  -1.9456 | PDE Loss:  -3.9479 | Function Loss:  -1.9911\n",
            "Total loss:  -1.9462 | PDE Loss:  -3.947 | Function Loss:  -1.9919\n",
            "Total loss:  -1.9469 | PDE Loss:  -3.9451 | Function Loss:  -1.9929\n",
            "Total loss:  -1.9476 | PDE Loss:  -3.9464 | Function Loss:  -1.9935\n",
            "Total loss:  -1.9482 | PDE Loss:  -3.9493 | Function Loss:  -1.9938\n",
            "Total loss:  -1.9487 | PDE Loss:  -3.9497 | Function Loss:  -1.9943\n",
            "Total loss:  -1.9491 | PDE Loss:  -3.9531 | Function Loss:  -1.9944\n",
            "Total loss:  -1.9494 | PDE Loss:  -3.9545 | Function Loss:  -1.9946\n",
            "Total loss:  -1.9497 | PDE Loss:  -3.9575 | Function Loss:  -1.9946\n",
            "Total loss:  -1.9499 | PDE Loss:  -3.9588 | Function Loss:  -1.9947\n",
            "Total loss:  -1.9501 | PDE Loss:  -3.961 | Function Loss:  -1.9947\n",
            "Total loss:  -1.9504 | PDE Loss:  -3.9542 | Function Loss:  -1.9957\n",
            "Total loss:  -1.9506 | PDE Loss:  -3.9558 | Function Loss:  -1.9957\n",
            "Total loss:  -1.9507 | PDE Loss:  -3.9533 | Function Loss:  -1.9962\n",
            "Total loss:  -1.9509 | PDE Loss:  -3.9509 | Function Loss:  -1.9966\n",
            "Total loss:  -1.951 | PDE Loss:  -3.9463 | Function Loss:  -1.9972\n",
            "Total loss:  -1.951 | PDE Loss:  -3.9431 | Function Loss:  -1.9977\n",
            "Total loss:  -1.9511 | PDE Loss:  -3.94 | Function Loss:  -1.9981\n",
            "Total loss:  -1.9513 | PDE Loss:  -3.9361 | Function Loss:  -1.9987\n",
            "Total loss:  -1.9514 | PDE Loss:  -3.9305 | Function Loss:  -1.9996\n",
            "Total loss:  -1.9517 | PDE Loss:  -3.9282 | Function Loss:  -2.0001\n",
            "Total loss:  -1.9519 | PDE Loss:  -3.9224 | Function Loss:  -2.0011\n",
            "Total loss:  -1.9522 | PDE Loss:  -3.9227 | Function Loss:  -2.0014\n",
            "Total loss:  -1.9526 | PDE Loss:  -3.9206 | Function Loss:  -2.0021\n",
            "Total loss:  -1.953 | PDE Loss:  -3.9251 | Function Loss:  -2.002\n",
            "Total loss:  -1.9534 | PDE Loss:  -3.9272 | Function Loss:  -2.0022\n",
            "Total loss:  -1.954 | PDE Loss:  -3.9328 | Function Loss:  -2.0022\n",
            "Total loss:  -1.955 | PDE Loss:  -3.9316 | Function Loss:  -2.0034\n",
            "Total loss:  -1.9561 | PDE Loss:  -3.9352 | Function Loss:  -2.0043\n",
            "Total loss:  -1.9578 | PDE Loss:  -3.9389 | Function Loss:  -2.0057\n",
            "Total loss:  -1.9597 | PDE Loss:  -3.9463 | Function Loss:  -2.007\n",
            "Total loss:  -1.962 | PDE Loss:  -3.9427 | Function Loss:  -2.0099\n",
            "Total loss:  -1.9635 | PDE Loss:  -3.9498 | Function Loss:  -2.0108\n",
            "Total loss:  -1.9649 | PDE Loss:  -3.9507 | Function Loss:  -2.0123\n",
            "Total loss:  -1.9664 | PDE Loss:  -3.9393 | Function Loss:  -2.0153\n",
            "Total loss:  -1.9673 | PDE Loss:  -3.938 | Function Loss:  -2.0164\n",
            "Total loss:  -1.9678 | PDE Loss:  -3.9359 | Function Loss:  -2.0172\n",
            "Total loss:  -1.9683 | PDE Loss:  -3.9373 | Function Loss:  -2.0176\n",
            "Total loss:  -1.9689 | PDE Loss:  -3.9406 | Function Loss:  -2.0179\n",
            "Total loss:  -1.97 | PDE Loss:  -3.9492 | Function Loss:  -2.0181\n",
            "Total loss:  -1.9714 | PDE Loss:  -3.9622 | Function Loss:  -2.0182\n",
            "Total loss:  -1.9727 | PDE Loss:  -3.9739 | Function Loss:  -2.0183\n",
            "Total loss:  -1.974 | PDE Loss:  -3.9902 | Function Loss:  -2.018\n",
            "Total loss:  -1.9755 | PDE Loss:  -4.004 | Function Loss:  -2.0182\n",
            "Total loss:  -1.9766 | PDE Loss:  -4.0029 | Function Loss:  -2.0195\n",
            "Total loss:  -1.9778 | PDE Loss:  -4.0033 | Function Loss:  -2.0209\n",
            "Total loss:  -1.9792 | PDE Loss:  -3.991 | Function Loss:  -2.0237\n",
            "Total loss:  -1.9806 | PDE Loss:  -3.9844 | Function Loss:  -2.0259\n",
            "Total loss:  -1.9816 | PDE Loss:  -3.9657 | Function Loss:  -2.0292\n",
            "Total loss:  -1.9824 | PDE Loss:  -3.9583 | Function Loss:  -2.031\n",
            "Total loss:  -1.9831 | PDE Loss:  -3.9392 | Function Loss:  -2.034\n",
            "Total loss:  -1.9836 | PDE Loss:  -3.9403 | Function Loss:  -2.0345\n",
            "Total loss:  -1.9841 | PDE Loss:  -3.9483 | Function Loss:  -2.0341\n",
            "Total loss:  -1.9846 | PDE Loss:  -3.9539 | Function Loss:  -2.034\n",
            "Total loss:  -1.9851 | PDE Loss:  -3.958 | Function Loss:  -2.034\n",
            "Total loss:  -1.9856 | PDE Loss:  -3.9617 | Function Loss:  -2.0341\n",
            "Total loss:  -1.986 | PDE Loss:  -3.9625 | Function Loss:  -2.0345\n",
            "Total loss:  -1.9864 | PDE Loss:  -3.9622 | Function Loss:  -2.0349\n",
            "Total loss:  -1.9868 | PDE Loss:  -3.9607 | Function Loss:  -2.0356\n",
            "Total loss:  -1.9872 | PDE Loss:  -3.9593 | Function Loss:  -2.0362\n",
            "Total loss:  -1.9877 | PDE Loss:  -3.9576 | Function Loss:  -2.037\n",
            "Total loss:  -1.988 | PDE Loss:  -3.9507 | Function Loss:  -2.0381\n",
            "Total loss:  -1.9886 | PDE Loss:  -3.9479 | Function Loss:  -2.0391\n",
            "Total loss:  -1.9891 | PDE Loss:  -3.953 | Function Loss:  -2.0391\n",
            "Total loss:  -1.9896 | PDE Loss:  -3.9529 | Function Loss:  -2.0396\n",
            "Total loss:  -1.9902 | PDE Loss:  -3.9535 | Function Loss:  -2.0402\n",
            "Total loss:  -1.9906 | PDE Loss:  -3.9555 | Function Loss:  -2.0404\n",
            "Total loss:  -1.991 | PDE Loss:  -3.9562 | Function Loss:  -2.0408\n",
            "Total loss:  -1.9914 | PDE Loss:  -3.9628 | Function Loss:  -2.0404\n",
            "Total loss:  -1.9917 | PDE Loss:  -3.9639 | Function Loss:  -2.0406\n",
            "Total loss:  -1.9919 | PDE Loss:  -3.9686 | Function Loss:  -2.0404\n",
            "Total loss:  -1.9921 | PDE Loss:  -3.9703 | Function Loss:  -2.0404\n",
            "Total loss:  -1.9923 | PDE Loss:  -3.973 | Function Loss:  -2.0402\n",
            "Total loss:  -1.9924 | PDE Loss:  -3.9744 | Function Loss:  -2.0402\n",
            "Total loss:  -1.9926 | PDE Loss:  -3.9756 | Function Loss:  -2.0402\n",
            "Total loss:  -1.9927 | PDE Loss:  -3.9766 | Function Loss:  -2.0403\n",
            "Total loss:  -1.9929 | PDE Loss:  -3.9787 | Function Loss:  -2.0403\n",
            "Total loss:  -1.9931 | PDE Loss:  -3.9802 | Function Loss:  -2.0403\n",
            "Total loss:  -1.9934 | PDE Loss:  -3.9834 | Function Loss:  -2.0402\n",
            "Total loss:  -1.9937 | PDE Loss:  -3.9803 | Function Loss:  -2.041\n",
            "Total loss:  -1.9939 | PDE Loss:  -3.9823 | Function Loss:  -2.041\n",
            "Total loss:  -1.9943 | PDE Loss:  -3.9843 | Function Loss:  -2.0411\n",
            "Total loss:  -1.9946 | PDE Loss:  -3.9856 | Function Loss:  -2.0413\n",
            "Total loss:  -1.9948 | PDE Loss:  -3.9871 | Function Loss:  -2.0415\n",
            "Total loss:  -1.9951 | PDE Loss:  -3.9879 | Function Loss:  -2.0416\n",
            "Total loss:  -1.9954 | PDE Loss:  -3.9888 | Function Loss:  -2.0419\n",
            "Total loss:  -1.9957 | PDE Loss:  -3.9897 | Function Loss:  -2.0422\n",
            "Total loss:  -1.9961 | PDE Loss:  -3.9891 | Function Loss:  -2.0426\n",
            "Total loss:  -1.9964 | PDE Loss:  -3.9894 | Function Loss:  -2.043\n",
            "Total loss:  -1.9968 | PDE Loss:  -3.9871 | Function Loss:  -2.0436\n",
            "Total loss:  -1.9971 | PDE Loss:  -3.9852 | Function Loss:  -2.0442\n",
            "Total loss:  -1.9976 | PDE Loss:  -3.9814 | Function Loss:  -2.0452\n",
            "Total loss:  -1.9982 | PDE Loss:  -3.9765 | Function Loss:  -2.0464\n",
            "Total loss:  -1.9989 | PDE Loss:  -3.9634 | Function Loss:  -2.0488\n",
            "Total loss:  -1.9995 | PDE Loss:  -3.9555 | Function Loss:  -2.0505\n",
            "Total loss:  -2.0004 | PDE Loss:  -3.9482 | Function Loss:  -2.0524\n",
            "Total loss:  -2.0014 | PDE Loss:  -3.9362 | Function Loss:  -2.0551\n",
            "Total loss:  -2.0023 | PDE Loss:  -3.9294 | Function Loss:  -2.0569\n",
            "Total loss:  -2.0031 | PDE Loss:  -3.9277 | Function Loss:  -2.0581\n",
            "Total loss:  -2.004 | PDE Loss:  -3.923 | Function Loss:  -2.0598\n",
            "Total loss:  -2.0046 | PDE Loss:  -3.9238 | Function Loss:  -2.0603\n",
            "Total loss:  -2.0054 | PDE Loss:  -3.9234 | Function Loss:  -2.0613\n",
            "Total loss:  -2.0067 | PDE Loss:  -3.9242 | Function Loss:  -2.0627\n",
            "Total loss:  -2.0082 | PDE Loss:  -3.9217 | Function Loss:  -2.0647\n",
            "Total loss:  -2.0092 | PDE Loss:  -3.9218 | Function Loss:  -2.0659\n",
            "Total loss:  -2.0103 | PDE Loss:  -3.9234 | Function Loss:  -2.0668\n",
            "Total loss:  -2.0112 | PDE Loss:  -3.9224 | Function Loss:  -2.0681\n",
            "Total loss:  -2.0119 | PDE Loss:  -3.9219 | Function Loss:  -2.0689\n",
            "Total loss:  -2.0125 | PDE Loss:  -3.9287 | Function Loss:  -2.0686\n",
            "Total loss:  -2.013 | PDE Loss:  -3.9283 | Function Loss:  -2.0693\n",
            "Total loss:  -2.0133 | PDE Loss:  -3.9246 | Function Loss:  -2.0701\n",
            "Total loss:  -2.0137 | PDE Loss:  -3.9295 | Function Loss:  -2.0699\n",
            "Total loss:  -2.0143 | PDE Loss:  -3.9344 | Function Loss:  -2.0699\n",
            "Total loss:  -2.0148 | PDE Loss:  -3.9414 | Function Loss:  -2.0696\n",
            "Total loss:  -2.0154 | PDE Loss:  -3.944 | Function Loss:  -2.0698\n",
            "Total loss:  -2.016 | PDE Loss:  -3.947 | Function Loss:  -2.0701\n",
            "Total loss:  -2.0165 | PDE Loss:  -3.9491 | Function Loss:  -2.0705\n",
            "Total loss:  -2.017 | PDE Loss:  -3.9467 | Function Loss:  -2.0713\n",
            "Total loss:  -2.0174 | PDE Loss:  -3.9454 | Function Loss:  -2.0719\n",
            "Total loss:  -2.0179 | PDE Loss:  -3.9401 | Function Loss:  -2.0732\n",
            "Total loss:  -2.0186 | PDE Loss:  -3.9395 | Function Loss:  -2.0741\n",
            "Total loss:  -2.0193 | PDE Loss:  -3.9365 | Function Loss:  -2.0753\n",
            "Total loss:  -2.0201 | PDE Loss:  -3.9359 | Function Loss:  -2.0763\n",
            "Total loss:  -2.0211 | PDE Loss:  -3.9392 | Function Loss:  -2.077\n",
            "Total loss:  -2.022 | PDE Loss:  -3.9419 | Function Loss:  -2.0777\n",
            "Total loss:  -2.0227 | PDE Loss:  -3.948 | Function Loss:  -2.0776\n",
            "Total loss:  -2.0232 | PDE Loss:  -3.9494 | Function Loss:  -2.078\n",
            "Total loss:  -2.0235 | PDE Loss:  -3.9543 | Function Loss:  -2.0777\n",
            "Total loss:  -2.0238 | PDE Loss:  -3.9536 | Function Loss:  -2.0781\n",
            "Total loss:  -2.024 | PDE Loss:  -3.9549 | Function Loss:  -2.0781\n",
            "Total loss:  -2.0242 | PDE Loss:  -3.9562 | Function Loss:  -2.0782\n",
            "Total loss:  -2.0244 | PDE Loss:  -3.9561 | Function Loss:  -2.0784\n",
            "Total loss:  -2.0246 | PDE Loss:  -3.9582 | Function Loss:  -2.0785\n",
            "Total loss:  -2.0248 | PDE Loss:  -3.9589 | Function Loss:  -2.0786\n",
            "Total loss:  -2.0251 | PDE Loss:  -3.9597 | Function Loss:  -2.0787\n",
            "Total loss:  -2.0254 | PDE Loss:  -3.9613 | Function Loss:  -2.0789\n",
            "Total loss:  -2.0257 | PDE Loss:  -3.9617 | Function Loss:  -2.0792\n",
            "Total loss:  -2.0261 | PDE Loss:  -3.9638 | Function Loss:  -2.0793\n",
            "Total loss:  -2.0264 | PDE Loss:  -3.9637 | Function Loss:  -2.0797\n",
            "Total loss:  -2.0268 | PDE Loss:  -3.9651 | Function Loss:  -2.08\n",
            "Total loss:  -2.0272 | PDE Loss:  -3.9619 | Function Loss:  -2.0809\n",
            "Total loss:  -2.0276 | PDE Loss:  -3.9623 | Function Loss:  -2.0813\n",
            "Total loss:  -2.028 | PDE Loss:  -3.9596 | Function Loss:  -2.082\n",
            "Total loss:  -2.0283 | PDE Loss:  -3.9594 | Function Loss:  -2.0824\n",
            "Total loss:  -2.0286 | PDE Loss:  -3.9573 | Function Loss:  -2.083\n",
            "Total loss:  -2.0289 | PDE Loss:  -3.9569 | Function Loss:  -2.0834\n",
            "Total loss:  -2.0292 | PDE Loss:  -3.9557 | Function Loss:  -2.084\n",
            "Total loss:  -2.0295 | PDE Loss:  -3.9554 | Function Loss:  -2.0844\n",
            "Total loss:  -2.0299 | PDE Loss:  -3.9558 | Function Loss:  -2.0847\n",
            "Total loss:  -2.0303 | PDE Loss:  -3.9568 | Function Loss:  -2.085\n",
            "Total loss:  -2.0308 | PDE Loss:  -3.9591 | Function Loss:  -2.0853\n",
            "Total loss:  -2.0313 | PDE Loss:  -3.9628 | Function Loss:  -2.0854\n",
            "Total loss:  -2.0318 | PDE Loss:  -3.9654 | Function Loss:  -2.0856\n",
            "Total loss:  -2.0322 | PDE Loss:  -3.9689 | Function Loss:  -2.0856\n",
            "Total loss:  -2.0328 | PDE Loss:  -3.9679 | Function Loss:  -2.0864\n",
            "Total loss:  -2.0333 | PDE Loss:  -3.9696 | Function Loss:  -2.0868\n",
            "Total loss:  -2.034 | PDE Loss:  -3.9747 | Function Loss:  -2.0869\n",
            "Total loss:  -2.0347 | PDE Loss:  -3.9751 | Function Loss:  -2.0876\n",
            "Total loss:  -2.0353 | PDE Loss:  -3.9782 | Function Loss:  -2.0879\n",
            "Total loss:  -2.036 | PDE Loss:  -3.9782 | Function Loss:  -2.0886\n",
            "Total loss:  -2.0367 | PDE Loss:  -3.9803 | Function Loss:  -2.0893\n",
            "Total loss:  -2.0376 | PDE Loss:  -3.982 | Function Loss:  -2.09\n",
            "Total loss:  -2.0387 | PDE Loss:  -3.9841 | Function Loss:  -2.091\n",
            "Total loss:  -2.0399 | PDE Loss:  -3.9896 | Function Loss:  -2.0916\n",
            "Total loss:  -2.0408 | PDE Loss:  -3.9889 | Function Loss:  -2.0927\n",
            "Total loss:  -2.0416 | PDE Loss:  -3.9905 | Function Loss:  -2.0934\n",
            "Total loss:  -2.0422 | PDE Loss:  -3.9894 | Function Loss:  -2.0943\n",
            "Total loss:  -2.0429 | PDE Loss:  -3.9896 | Function Loss:  -2.095\n",
            "Total loss:  -2.0436 | PDE Loss:  -3.99 | Function Loss:  -2.0958\n",
            "Total loss:  -2.0443 | PDE Loss:  -3.9899 | Function Loss:  -2.0965\n",
            "Total loss:  -2.0449 | PDE Loss:  -3.9893 | Function Loss:  -2.0973\n",
            "Total loss:  -2.0456 | PDE Loss:  -3.9907 | Function Loss:  -2.0979\n",
            "Total loss:  -2.0462 | PDE Loss:  -3.9904 | Function Loss:  -2.0986\n",
            "Total loss:  -2.0467 | PDE Loss:  -3.9899 | Function Loss:  -2.0993\n",
            "Total loss:  -2.0475 | PDE Loss:  -3.9906 | Function Loss:  -2.1\n",
            "Total loss:  -2.0482 | PDE Loss:  -3.9887 | Function Loss:  -2.1011\n",
            "Total loss:  -2.0488 | PDE Loss:  -3.9924 | Function Loss:  -2.1014\n",
            "Total loss:  -2.0493 | PDE Loss:  -3.9924 | Function Loss:  -2.1019\n",
            "Total loss:  -2.0498 | PDE Loss:  -3.9945 | Function Loss:  -2.1021\n",
            "Total loss:  -2.0501 | PDE Loss:  -3.9959 | Function Loss:  -2.1023\n",
            "Total loss:  -2.0504 | PDE Loss:  -3.9962 | Function Loss:  -2.1027\n",
            "Total loss:  -2.0508 | PDE Loss:  -3.997 | Function Loss:  -2.103\n",
            "Total loss:  -2.0511 | PDE Loss:  -3.9965 | Function Loss:  -2.1034\n",
            "Total loss:  -2.0514 | PDE Loss:  -3.995 | Function Loss:  -2.1039\n",
            "Total loss:  -2.0517 | PDE Loss:  -3.9931 | Function Loss:  -2.1045\n",
            "Total loss:  -2.0519 | PDE Loss:  -3.9914 | Function Loss:  -2.105\n",
            "Total loss:  -2.0522 | PDE Loss:  -3.9955 | Function Loss:  -2.1047\n",
            "Total loss:  -2.0525 | PDE Loss:  -3.9936 | Function Loss:  -2.1053\n",
            "Total loss:  -2.0527 | PDE Loss:  -3.9929 | Function Loss:  -2.1056\n",
            "Total loss:  -2.0529 | PDE Loss:  -3.9918 | Function Loss:  -2.1061\n",
            "Total loss:  -2.0531 | PDE Loss:  -3.9919 | Function Loss:  -2.1063\n",
            "Total loss:  -2.0534 | PDE Loss:  -3.9932 | Function Loss:  -2.1064\n",
            "Total loss:  -2.0537 | PDE Loss:  -3.9949 | Function Loss:  -2.1065\n",
            "Total loss:  -2.054 | PDE Loss:  -3.9967 | Function Loss:  -2.1066\n",
            "Total loss:  -2.0543 | PDE Loss:  -3.9978 | Function Loss:  -2.1068\n",
            "Total loss:  -2.0546 | PDE Loss:  -4.0007 | Function Loss:  -2.1068\n",
            "Total loss:  -2.0549 | PDE Loss:  -4.0023 | Function Loss:  -2.1069\n",
            "Total loss:  -2.0552 | PDE Loss:  -4.0052 | Function Loss:  -2.1069\n",
            "Total loss:  -2.0554 | PDE Loss:  -4.0063 | Function Loss:  -2.107\n",
            "Total loss:  -2.0557 | PDE Loss:  -4.0076 | Function Loss:  -2.1071\n",
            "Total loss:  -2.056 | PDE Loss:  -4.0089 | Function Loss:  -2.1073\n",
            "Total loss:  -2.0563 | PDE Loss:  -4.0113 | Function Loss:  -2.1074\n",
            "Total loss:  -2.0566 | PDE Loss:  -4.0131 | Function Loss:  -2.1075\n",
            "Total loss:  -2.0569 | PDE Loss:  -4.0143 | Function Loss:  -2.1077\n",
            "Total loss:  -2.0572 | PDE Loss:  -4.0159 | Function Loss:  -2.1078\n",
            "Total loss:  -2.0575 | PDE Loss:  -4.0153 | Function Loss:  -2.1082\n",
            "Total loss:  -2.0578 | PDE Loss:  -4.016 | Function Loss:  -2.1084\n",
            "Total loss:  -2.058 | PDE Loss:  -4.0148 | Function Loss:  -2.1089\n",
            "Total loss:  -2.0583 | PDE Loss:  -4.0134 | Function Loss:  -2.1093\n",
            "Total loss:  -2.0586 | PDE Loss:  -4.015 | Function Loss:  -2.1094\n",
            "Total loss:  -2.0589 | PDE Loss:  -4.0153 | Function Loss:  -2.1098\n",
            "Total loss:  -2.0591 | PDE Loss:  -4.0144 | Function Loss:  -2.1102\n",
            "Total loss:  -2.0594 | PDE Loss:  -4.0149 | Function Loss:  -2.1103\n",
            "Total loss:  -2.0595 | PDE Loss:  -4.015 | Function Loss:  -2.1105\n",
            "Total loss:  -2.0596 | PDE Loss:  -4.0155 | Function Loss:  -2.1106\n",
            "Total loss:  -2.0598 | PDE Loss:  -4.0158 | Function Loss:  -2.1108\n",
            "Total loss:  -2.0601 | PDE Loss:  -4.0154 | Function Loss:  -2.1111\n",
            "Total loss:  -2.0605 | PDE Loss:  -4.0147 | Function Loss:  -2.1116\n",
            "Total loss:  -2.0609 | PDE Loss:  -4.0131 | Function Loss:  -2.1123\n",
            "Total loss:  -2.0613 | PDE Loss:  -4.0129 | Function Loss:  -2.1128\n",
            "Total loss:  -2.0619 | PDE Loss:  -4.0094 | Function Loss:  -2.1139\n",
            "Total loss:  -2.0626 | PDE Loss:  -4.0097 | Function Loss:  -2.1147\n",
            "Total loss:  -2.0632 | PDE Loss:  -4.0069 | Function Loss:  -2.1157\n",
            "Total loss:  -2.0638 | PDE Loss:  -4.0076 | Function Loss:  -2.1162\n",
            "Total loss:  -2.0644 | PDE Loss:  -4.0052 | Function Loss:  -2.1172\n",
            "Total loss:  -2.0649 | PDE Loss:  -4.0055 | Function Loss:  -2.1178\n",
            "Total loss:  -2.0655 | PDE Loss:  -4.0035 | Function Loss:  -2.1188\n",
            "Total loss:  -2.0663 | PDE Loss:  -4.0051 | Function Loss:  -2.1194\n",
            "Total loss:  -2.0672 | PDE Loss:  -4.0034 | Function Loss:  -2.1206\n",
            "Total loss:  -2.0681 | PDE Loss:  -4.0035 | Function Loss:  -2.1217\n",
            "Total loss:  -2.0692 | PDE Loss:  -4.001 | Function Loss:  -2.1232\n",
            "Total loss:  -2.0702 | PDE Loss:  -3.9991 | Function Loss:  -2.1246\n",
            "Total loss:  -2.0714 | PDE Loss:  -3.9952 | Function Loss:  -2.1265\n",
            "Total loss:  -2.0725 | PDE Loss:  -3.9884 | Function Loss:  -2.1287\n",
            "Total loss:  -2.0736 | PDE Loss:  -3.9834 | Function Loss:  -2.1306\n",
            "Total loss:  -2.0743 | PDE Loss:  -3.9789 | Function Loss:  -2.1321\n",
            "Total loss:  -2.0749 | PDE Loss:  -3.9764 | Function Loss:  -2.1332\n",
            "Total loss:  -2.0754 | PDE Loss:  -3.9725 | Function Loss:  -2.1343\n",
            "Total loss:  -2.0758 | PDE Loss:  -3.9734 | Function Loss:  -2.1346\n",
            "Total loss:  -2.0761 | PDE Loss:  -3.971 | Function Loss:  -2.1353\n",
            "Total loss:  -2.0766 | PDE Loss:  -3.9743 | Function Loss:  -2.1353\n",
            "Total loss:  -2.077 | PDE Loss:  -3.9735 | Function Loss:  -2.136\n",
            "Total loss:  -2.0776 | PDE Loss:  -3.9763 | Function Loss:  -2.1362\n",
            "Total loss:  -2.0782 | PDE Loss:  -3.9813 | Function Loss:  -2.1362\n",
            "Total loss:  -2.079 | PDE Loss:  -3.9855 | Function Loss:  -2.1365\n",
            "Total loss:  -2.0797 | PDE Loss:  -3.9922 | Function Loss:  -2.1364\n",
            "Total loss:  -2.0804 | PDE Loss:  -3.9962 | Function Loss:  -2.1366\n",
            "Total loss:  -2.0809 | PDE Loss:  -3.996 | Function Loss:  -2.1372\n",
            "Total loss:  -2.0814 | PDE Loss:  -3.9994 | Function Loss:  -2.1373\n",
            "Total loss:  -2.0818 | PDE Loss:  -3.9996 | Function Loss:  -2.1377\n",
            "Total loss:  -2.0822 | PDE Loss:  -4.0021 | Function Loss:  -2.1379\n",
            "Total loss:  -2.0828 | PDE Loss:  -4.0005 | Function Loss:  -2.1388\n",
            "Total loss:  -2.0835 | PDE Loss:  -4.0042 | Function Loss:  -2.139\n",
            "Total loss:  -2.0841 | PDE Loss:  -3.9978 | Function Loss:  -2.1406\n",
            "Total loss:  -2.0847 | PDE Loss:  -4.0022 | Function Loss:  -2.1407\n",
            "Total loss:  -2.0852 | PDE Loss:  -4.0028 | Function Loss:  -2.1412\n",
            "Total loss:  -2.0858 | PDE Loss:  -4.0058 | Function Loss:  -2.1414\n",
            "Total loss:  -2.0864 | PDE Loss:  -4.0098 | Function Loss:  -2.1416\n",
            "Total loss:  -2.0872 | PDE Loss:  -4.0151 | Function Loss:  -2.1417\n",
            "Total loss:  -2.0878 | PDE Loss:  -4.022 | Function Loss:  -2.1416\n",
            "Total loss:  -2.0884 | PDE Loss:  -4.0242 | Function Loss:  -2.1419\n",
            "Total loss:  -2.089 | PDE Loss:  -4.0307 | Function Loss:  -2.1417\n",
            "Total loss:  -2.0894 | PDE Loss:  -4.0278 | Function Loss:  -2.1426\n",
            "Total loss:  -2.0897 | PDE Loss:  -4.0301 | Function Loss:  -2.1426\n",
            "Total loss:  -2.09 | PDE Loss:  -4.0296 | Function Loss:  -2.143\n",
            "Total loss:  -2.0904 | PDE Loss:  -4.0308 | Function Loss:  -2.1433\n",
            "Total loss:  -2.0908 | PDE Loss:  -4.0313 | Function Loss:  -2.1437\n",
            "Total loss:  -2.0911 | PDE Loss:  -4.035 | Function Loss:  -2.1436\n",
            "Total loss:  -2.0914 | PDE Loss:  -4.035 | Function Loss:  -2.1439\n",
            "Total loss:  -2.0918 | PDE Loss:  -4.0381 | Function Loss:  -2.144\n",
            "Total loss:  -2.0922 | PDE Loss:  -4.0402 | Function Loss:  -2.1442\n",
            "Total loss:  -2.0926 | PDE Loss:  -4.0457 | Function Loss:  -2.1439\n",
            "Total loss:  -2.093 | PDE Loss:  -4.0474 | Function Loss:  -2.1441\n",
            "Total loss:  -2.0934 | PDE Loss:  -4.0518 | Function Loss:  -2.144\n",
            "Total loss:  -2.0937 | PDE Loss:  -4.0533 | Function Loss:  -2.1442\n",
            "Total loss:  -2.0939 | PDE Loss:  -4.0556 | Function Loss:  -2.1442\n",
            "Total loss:  -2.0941 | PDE Loss:  -4.0559 | Function Loss:  -2.1444\n",
            "Total loss:  -2.0943 | PDE Loss:  -4.0577 | Function Loss:  -2.1443\n",
            "Total loss:  -2.0944 | PDE Loss:  -4.0567 | Function Loss:  -2.1446\n",
            "Total loss:  -2.0946 | PDE Loss:  -4.0573 | Function Loss:  -2.1447\n",
            "Total loss:  -2.0948 | PDE Loss:  -4.0564 | Function Loss:  -2.145\n",
            "Total loss:  -2.095 | PDE Loss:  -4.0559 | Function Loss:  -2.1454\n",
            "Total loss:  -2.0953 | PDE Loss:  -4.0549 | Function Loss:  -2.1458\n",
            "Total loss:  -2.0954 | PDE Loss:  -4.0593 | Function Loss:  -2.1453\n",
            "Total loss:  -2.0958 | PDE Loss:  -4.0593 | Function Loss:  -2.1458\n",
            "Total loss:  -2.0961 | PDE Loss:  -4.0584 | Function Loss:  -2.1462\n",
            "Total loss:  -2.0963 | PDE Loss:  -4.0592 | Function Loss:  -2.1464\n",
            "Total loss:  -2.0967 | PDE Loss:  -4.0585 | Function Loss:  -2.1469\n",
            "Total loss:  -2.0971 | PDE Loss:  -4.0603 | Function Loss:  -2.1471\n",
            "Total loss:  -2.0975 | PDE Loss:  -4.0595 | Function Loss:  -2.1477\n",
            "Total loss:  -2.098 | PDE Loss:  -4.0586 | Function Loss:  -2.1483\n",
            "Total loss:  -2.0984 | PDE Loss:  -4.0562 | Function Loss:  -2.1491\n",
            "Total loss:  -2.0988 | PDE Loss:  -4.0536 | Function Loss:  -2.1499\n",
            "Total loss:  -2.0991 | PDE Loss:  -4.0515 | Function Loss:  -2.1505\n",
            "Total loss:  -2.0993 | PDE Loss:  -4.0504 | Function Loss:  -2.1509\n",
            "Total loss:  -2.0995 | PDE Loss:  -4.049 | Function Loss:  -2.1513\n",
            "Total loss:  -2.0997 | PDE Loss:  -4.0495 | Function Loss:  -2.1514\n",
            "Total loss:  -2.0998 | PDE Loss:  -4.0493 | Function Loss:  -2.1515\n",
            "Total loss:  -2.0999 | PDE Loss:  -4.0513 | Function Loss:  -2.1515\n",
            "Total loss:  -2.1001 | PDE Loss:  -4.0506 | Function Loss:  -2.1518\n",
            "Total loss:  -2.1003 | PDE Loss:  -4.0539 | Function Loss:  -2.1515\n",
            "Total loss:  -2.1004 | PDE Loss:  -4.054 | Function Loss:  -2.1517\n",
            "Total loss:  -2.1006 | PDE Loss:  -4.0545 | Function Loss:  -2.1518\n",
            "Total loss:  -2.1007 | PDE Loss:  -4.0554 | Function Loss:  -2.1518\n",
            "Total loss:  -2.1009 | PDE Loss:  -4.0568 | Function Loss:  -2.1519\n",
            "Total loss:  -2.1012 | PDE Loss:  -4.0577 | Function Loss:  -2.152\n",
            "Total loss:  -2.1014 | PDE Loss:  -4.0594 | Function Loss:  -2.1521\n",
            "Total loss:  -2.1016 | PDE Loss:  -4.0606 | Function Loss:  -2.1522\n",
            "Total loss:  -2.1019 | PDE Loss:  -4.0634 | Function Loss:  -2.1521\n",
            "Total loss:  -2.1021 | PDE Loss:  -4.0631 | Function Loss:  -2.1524\n",
            "Total loss:  -2.1022 | PDE Loss:  -4.0656 | Function Loss:  -2.1523\n",
            "Total loss:  -2.1024 | PDE Loss:  -4.0677 | Function Loss:  -2.1522\n",
            "Total loss:  -2.1026 | PDE Loss:  -4.0697 | Function Loss:  -2.1522\n",
            "Total loss:  -2.1028 | PDE Loss:  -4.0716 | Function Loss:  -2.1522\n",
            "Total loss:  -2.1031 | PDE Loss:  -4.0731 | Function Loss:  -2.1523\n",
            "Total loss:  -2.1032 | PDE Loss:  -4.0743 | Function Loss:  -2.1523\n",
            "Total loss:  -2.1034 | PDE Loss:  -4.0747 | Function Loss:  -2.1525\n",
            "Total loss:  -2.1036 | PDE Loss:  -4.075 | Function Loss:  -2.1527\n",
            "Total loss:  -2.1038 | PDE Loss:  -4.0749 | Function Loss:  -2.1529\n",
            "Total loss:  -2.104 | PDE Loss:  -4.074 | Function Loss:  -2.1532\n",
            "Total loss:  -2.1042 | PDE Loss:  -4.0744 | Function Loss:  -2.1534\n",
            "Total loss:  -2.1044 | PDE Loss:  -4.0715 | Function Loss:  -2.154\n",
            "Total loss:  -2.1047 | PDE Loss:  -4.0725 | Function Loss:  -2.1542\n",
            "Total loss:  -2.1052 | PDE Loss:  -4.0745 | Function Loss:  -2.1545\n",
            "Total loss:  -2.1058 | PDE Loss:  -4.0775 | Function Loss:  -2.1548\n",
            "Total loss:  -2.1064 | PDE Loss:  -4.0807 | Function Loss:  -2.1551\n",
            "Total loss:  -2.1071 | PDE Loss:  -4.0844 | Function Loss:  -2.1554\n",
            "Total loss:  -2.1079 | PDE Loss:  -4.0893 | Function Loss:  -2.1557\n",
            "Total loss:  -2.1087 | PDE Loss:  -4.0929 | Function Loss:  -2.1562\n",
            "Total loss:  -2.1094 | PDE Loss:  -4.0956 | Function Loss:  -2.1567\n",
            "Total loss:  -2.1101 | PDE Loss:  -4.0986 | Function Loss:  -2.1572\n",
            "Total loss:  -2.1108 | PDE Loss:  -4.0951 | Function Loss:  -2.1584\n",
            "Total loss:  -2.1114 | PDE Loss:  -4.1006 | Function Loss:  -2.1583\n",
            "Total loss:  -2.1117 | PDE Loss:  -4.0976 | Function Loss:  -2.159\n",
            "Total loss:  -2.1119 | PDE Loss:  -4.096 | Function Loss:  -2.1594\n",
            "Total loss:  -2.1121 | PDE Loss:  -4.0961 | Function Loss:  -2.1597\n",
            "Total loss:  -2.1124 | PDE Loss:  -4.0963 | Function Loss:  -2.16\n",
            "Total loss:  -2.1128 | PDE Loss:  -4.0974 | Function Loss:  -2.1603\n",
            "Total loss:  -2.1133 | PDE Loss:  -4.0966 | Function Loss:  -2.1609\n",
            "Total loss:  -2.1136 | PDE Loss:  -4.1044 | Function Loss:  -2.1604\n",
            "Total loss:  -2.114 | PDE Loss:  -4.1017 | Function Loss:  -2.1611\n",
            "Total loss:  -2.1143 | PDE Loss:  -4.1013 | Function Loss:  -2.1615\n",
            "Total loss:  -2.1148 | PDE Loss:  -4.1009 | Function Loss:  -2.1621\n",
            "Total loss:  -2.1153 | PDE Loss:  -4.1049 | Function Loss:  -2.1622\n",
            "Total loss:  -2.1157 | PDE Loss:  -4.1053 | Function Loss:  -2.1626\n",
            "Total loss:  -2.116 | PDE Loss:  -4.1069 | Function Loss:  -2.1627\n",
            "Total loss:  -2.1163 | PDE Loss:  -4.1086 | Function Loss:  -2.1629\n",
            "Total loss:  -2.1165 | PDE Loss:  -4.1094 | Function Loss:  -2.1631\n",
            "Total loss:  -2.1168 | PDE Loss:  -4.1102 | Function Loss:  -2.1632\n",
            "Total loss:  -2.117 | PDE Loss:  -4.1105 | Function Loss:  -2.1635\n",
            "Total loss:  -2.1173 | PDE Loss:  -4.1098 | Function Loss:  -2.1639\n",
            "Total loss:  -2.1176 | PDE Loss:  -4.1133 | Function Loss:  -2.1639\n",
            "Total loss:  -2.118 | PDE Loss:  -4.1107 | Function Loss:  -2.1645\n",
            "Total loss:  -2.1184 | PDE Loss:  -4.1099 | Function Loss:  -2.1651\n",
            "Total loss:  -2.1189 | PDE Loss:  -4.1064 | Function Loss:  -2.166\n",
            "Total loss:  -2.1193 | PDE Loss:  -4.1063 | Function Loss:  -2.1665\n",
            "Total loss:  -2.1196 | PDE Loss:  -4.105 | Function Loss:  -2.167\n",
            "Total loss:  -2.1198 | PDE Loss:  -4.1051 | Function Loss:  -2.1672\n",
            "Total loss:  -2.12 | PDE Loss:  -4.1055 | Function Loss:  -2.1674\n",
            "Total loss:  -2.1203 | PDE Loss:  -4.1057 | Function Loss:  -2.1677\n",
            "Total loss:  -2.1205 | PDE Loss:  -4.1068 | Function Loss:  -2.1678\n",
            "Total loss:  -2.1206 | PDE Loss:  -4.1075 | Function Loss:  -2.1679\n",
            "Total loss:  -2.1209 | PDE Loss:  -4.1084 | Function Loss:  -2.1681\n",
            "Total loss:  -2.1212 | PDE Loss:  -4.1081 | Function Loss:  -2.1685\n",
            "Total loss:  -2.1215 | PDE Loss:  -4.109 | Function Loss:  -2.1687\n",
            "Total loss:  -2.1218 | PDE Loss:  -4.1086 | Function Loss:  -2.1691\n",
            "Total loss:  -2.1221 | PDE Loss:  -4.1103 | Function Loss:  -2.1692\n",
            "Total loss:  -2.1223 | PDE Loss:  -4.1094 | Function Loss:  -2.1696\n",
            "Total loss:  -2.1225 | PDE Loss:  -4.1102 | Function Loss:  -2.1697\n",
            "Total loss:  -2.1227 | PDE Loss:  -4.1097 | Function Loss:  -2.1699\n",
            "Total loss:  -2.1228 | PDE Loss:  -4.1102 | Function Loss:  -2.17\n",
            "Total loss:  -2.123 | PDE Loss:  -4.11 | Function Loss:  -2.1702\n",
            "Total loss:  -2.1232 | PDE Loss:  -4.1109 | Function Loss:  -2.1703\n",
            "Total loss:  -2.1233 | PDE Loss:  -4.111 | Function Loss:  -2.1705\n",
            "Total loss:  -2.1235 | PDE Loss:  -4.1127 | Function Loss:  -2.1705\n",
            "Total loss:  -2.1237 | PDE Loss:  -4.1126 | Function Loss:  -2.1708\n",
            "Total loss:  -2.1239 | PDE Loss:  -4.1137 | Function Loss:  -2.1708\n",
            "Total loss:  -2.1242 | PDE Loss:  -4.1139 | Function Loss:  -2.1711\n",
            "Total loss:  -2.1244 | PDE Loss:  -4.1153 | Function Loss:  -2.1712\n",
            "Total loss:  -2.1247 | PDE Loss:  -4.1149 | Function Loss:  -2.1716\n",
            "Total loss:  -2.125 | PDE Loss:  -4.1169 | Function Loss:  -2.1717\n",
            "Total loss:  -2.1252 | PDE Loss:  -4.1157 | Function Loss:  -2.1721\n",
            "Total loss:  -2.1255 | PDE Loss:  -4.1168 | Function Loss:  -2.1722\n",
            "Total loss:  -2.1257 | PDE Loss:  -4.1171 | Function Loss:  -2.1724\n",
            "Total loss:  -2.126 | PDE Loss:  -4.1182 | Function Loss:  -2.1727\n",
            "Total loss:  -2.1264 | PDE Loss:  -4.1202 | Function Loss:  -2.1729\n",
            "Total loss:  -2.1269 | PDE Loss:  -4.1245 | Function Loss:  -2.1729\n",
            "Total loss:  -2.1274 | PDE Loss:  -4.13 | Function Loss:  -2.1729\n",
            "Total loss:  -2.1279 | PDE Loss:  -4.1338 | Function Loss:  -2.173\n",
            "Total loss:  -2.1285 | PDE Loss:  -4.1376 | Function Loss:  -2.1732\n",
            "Total loss:  -2.129 | PDE Loss:  -4.1421 | Function Loss:  -2.1733\n",
            "Total loss:  -2.1296 | PDE Loss:  -4.1445 | Function Loss:  -2.1737\n",
            "Total loss:  -2.1301 | PDE Loss:  -4.146 | Function Loss:  -2.1741\n",
            "Total loss:  -2.1306 | PDE Loss:  -4.1461 | Function Loss:  -2.1747\n",
            "Total loss:  -2.1311 | PDE Loss:  -4.1442 | Function Loss:  -2.1754\n",
            "Total loss:  -2.1316 | PDE Loss:  -4.1444 | Function Loss:  -2.176\n",
            "Total loss:  -2.1321 | PDE Loss:  -4.1414 | Function Loss:  -2.1768\n",
            "Total loss:  -2.1324 | PDE Loss:  -4.142 | Function Loss:  -2.1772\n",
            "Total loss:  -2.1328 | PDE Loss:  -4.1424 | Function Loss:  -2.1775\n",
            "Total loss:  -2.1332 | PDE Loss:  -4.1427 | Function Loss:  -2.1779\n",
            "Total loss:  -2.1336 | PDE Loss:  -4.1456 | Function Loss:  -2.178\n",
            "Total loss:  -2.134 | PDE Loss:  -4.1466 | Function Loss:  -2.1784\n",
            "Total loss:  -2.1345 | PDE Loss:  -4.15 | Function Loss:  -2.1786\n",
            "Total loss:  -2.135 | PDE Loss:  -4.1529 | Function Loss:  -2.1788\n",
            "Total loss:  -2.1354 | PDE Loss:  -4.1524 | Function Loss:  -2.1793\n",
            "Total loss:  -2.1358 | PDE Loss:  -4.1559 | Function Loss:  -2.1793\n",
            "Total loss:  -2.136 | PDE Loss:  -4.1553 | Function Loss:  -2.1797\n",
            "Total loss:  -2.1362 | PDE Loss:  -4.1543 | Function Loss:  -2.18\n",
            "Total loss:  -2.1364 | PDE Loss:  -4.1518 | Function Loss:  -2.1805\n",
            "Total loss:  -2.1366 | PDE Loss:  -4.1495 | Function Loss:  -2.181\n",
            "Total loss:  -2.1368 | PDE Loss:  -4.1471 | Function Loss:  -2.1814\n",
            "Total loss:  -2.137 | PDE Loss:  -4.145 | Function Loss:  -2.1818\n",
            "Total loss:  -2.1372 | PDE Loss:  -4.1431 | Function Loss:  -2.1823\n",
            "Total loss:  -2.1374 | PDE Loss:  -4.1424 | Function Loss:  -2.1826\n",
            "Total loss:  -2.1377 | PDE Loss:  -4.1412 | Function Loss:  -2.183\n",
            "Total loss:  -2.1379 | PDE Loss:  -4.1462 | Function Loss:  -2.1827\n",
            "Total loss:  -2.1382 | PDE Loss:  -4.1449 | Function Loss:  -2.1832\n",
            "Total loss:  -2.1384 | PDE Loss:  -4.1448 | Function Loss:  -2.1834\n",
            "Total loss:  -2.1386 | PDE Loss:  -4.1462 | Function Loss:  -2.1835\n",
            "Total loss:  -2.1388 | PDE Loss:  -4.1476 | Function Loss:  -2.1836\n",
            "Total loss:  -2.139 | PDE Loss:  -4.15 | Function Loss:  -2.1836\n",
            "Total loss:  -2.1392 | PDE Loss:  -4.151 | Function Loss:  -2.1836\n",
            "Total loss:  -2.1394 | PDE Loss:  -4.1543 | Function Loss:  -2.1835\n",
            "Total loss:  -2.1396 | PDE Loss:  -4.1548 | Function Loss:  -2.1837\n",
            "Total loss:  -2.1398 | PDE Loss:  -4.1568 | Function Loss:  -2.1837\n",
            "Total loss:  -2.14 | PDE Loss:  -4.1569 | Function Loss:  -2.1839\n",
            "Total loss:  -2.1403 | PDE Loss:  -4.1576 | Function Loss:  -2.1842\n",
            "Total loss:  -2.1407 | PDE Loss:  -4.1604 | Function Loss:  -2.1844\n",
            "Total loss:  -2.1413 | PDE Loss:  -4.1617 | Function Loss:  -2.1848\n",
            "Total loss:  -2.1419 | PDE Loss:  -4.1639 | Function Loss:  -2.1852\n",
            "Total loss:  -2.1424 | PDE Loss:  -4.1638 | Function Loss:  -2.1859\n",
            "Total loss:  -2.1429 | PDE Loss:  -4.1633 | Function Loss:  -2.1865\n",
            "Total loss:  -2.1433 | PDE Loss:  -4.1644 | Function Loss:  -2.1867\n",
            "Total loss:  -2.1436 | PDE Loss:  -4.1668 | Function Loss:  -2.1868\n",
            "Total loss:  -2.1439 | PDE Loss:  -4.1661 | Function Loss:  -2.1873\n",
            "Total loss:  -2.1443 | PDE Loss:  -4.173 | Function Loss:  -2.187\n",
            "Total loss:  -2.1445 | PDE Loss:  -4.1731 | Function Loss:  -2.1872\n",
            "Total loss:  -2.1448 | PDE Loss:  -4.1739 | Function Loss:  -2.1874\n",
            "Total loss:  -2.1453 | PDE Loss:  -4.1744 | Function Loss:  -2.1879\n",
            "Total loss:  -2.1458 | PDE Loss:  -4.1737 | Function Loss:  -2.1886\n",
            "Total loss:  -2.1463 | PDE Loss:  -4.1749 | Function Loss:  -2.189\n",
            "Total loss:  -2.1467 | PDE Loss:  -4.1687 | Function Loss:  -2.1901\n",
            "Total loss:  -2.1471 | PDE Loss:  -4.1713 | Function Loss:  -2.1902\n",
            "Total loss:  -2.1474 | PDE Loss:  -4.1738 | Function Loss:  -2.1903\n",
            "Total loss:  -2.1477 | PDE Loss:  -4.1754 | Function Loss:  -2.1905\n",
            "Total loss:  -2.1481 | PDE Loss:  -4.1777 | Function Loss:  -2.1906\n",
            "Total loss:  -2.1484 | PDE Loss:  -4.1785 | Function Loss:  -2.191\n",
            "Total loss:  -2.1488 | PDE Loss:  -4.1812 | Function Loss:  -2.1911\n",
            "Total loss:  -2.1493 | PDE Loss:  -4.1825 | Function Loss:  -2.1915\n",
            "Total loss:  -2.1498 | PDE Loss:  -4.1863 | Function Loss:  -2.1917\n",
            "Total loss:  -2.1503 | PDE Loss:  -4.1871 | Function Loss:  -2.1921\n",
            "Total loss:  -2.1507 | PDE Loss:  -4.1907 | Function Loss:  -2.1922\n",
            "Total loss:  -2.151 | PDE Loss:  -4.1912 | Function Loss:  -2.1925\n",
            "Total loss:  -2.1514 | PDE Loss:  -4.1919 | Function Loss:  -2.1928\n",
            "Total loss:  -2.1518 | PDE Loss:  -4.1938 | Function Loss:  -2.1931\n",
            "Total loss:  -2.1522 | PDE Loss:  -4.1926 | Function Loss:  -2.1937\n",
            "Total loss:  -2.1527 | PDE Loss:  -4.1945 | Function Loss:  -2.194\n",
            "Total loss:  -2.153 | PDE Loss:  -4.1946 | Function Loss:  -2.1944\n",
            "Total loss:  -2.1534 | PDE Loss:  -4.1941 | Function Loss:  -2.1948\n",
            "Total loss:  -2.1538 | PDE Loss:  -4.1931 | Function Loss:  -2.1954\n",
            "Total loss:  -2.1541 | PDE Loss:  -4.1903 | Function Loss:  -2.196\n",
            "Total loss:  -2.1544 | PDE Loss:  -4.1912 | Function Loss:  -2.1963\n",
            "Total loss:  -2.1548 | PDE Loss:  -4.1874 | Function Loss:  -2.1971\n",
            "Total loss:  -2.1552 | PDE Loss:  -4.1881 | Function Loss:  -2.1974\n",
            "Total loss:  -2.1556 | PDE Loss:  -4.1857 | Function Loss:  -2.1981\n",
            "Total loss:  -2.156 | PDE Loss:  -4.1865 | Function Loss:  -2.1985\n",
            "Total loss:  -2.1564 | PDE Loss:  -4.1844 | Function Loss:  -2.1992\n",
            "Total loss:  -2.1568 | PDE Loss:  -4.1862 | Function Loss:  -2.1994\n",
            "Total loss:  -2.1572 | PDE Loss:  -4.1858 | Function Loss:  -2.1999\n",
            "Total loss:  -2.1576 | PDE Loss:  -4.1885 | Function Loss:  -2.2\n",
            "Total loss:  -2.1581 | PDE Loss:  -4.189 | Function Loss:  -2.2005\n",
            "Total loss:  -2.1586 | PDE Loss:  -4.1916 | Function Loss:  -2.2008\n",
            "Total loss:  -2.1592 | PDE Loss:  -4.1926 | Function Loss:  -2.2014\n",
            "Total loss:  -2.1597 | PDE Loss:  -4.195 | Function Loss:  -2.2017\n",
            "Total loss:  -2.1602 | PDE Loss:  -4.1964 | Function Loss:  -2.2022\n",
            "Total loss:  -2.1607 | PDE Loss:  -4.1968 | Function Loss:  -2.2026\n",
            "Total loss:  -2.1612 | PDE Loss:  -4.1975 | Function Loss:  -2.2031\n",
            "Total loss:  -2.1618 | PDE Loss:  -4.1969 | Function Loss:  -2.2039\n",
            "Total loss:  -2.1625 | PDE Loss:  -4.1929 | Function Loss:  -2.205\n",
            "Total loss:  -2.163 | PDE Loss:  -4.1924 | Function Loss:  -2.2056\n",
            "Total loss:  -2.1636 | PDE Loss:  -4.1888 | Function Loss:  -2.2066\n",
            "Total loss:  -2.1641 | PDE Loss:  -4.1877 | Function Loss:  -2.2073\n",
            "Total loss:  -2.1647 | PDE Loss:  -4.1845 | Function Loss:  -2.2084\n",
            "Total loss:  -2.1654 | PDE Loss:  -4.1801 | Function Loss:  -2.2095\n",
            "Total loss:  -2.1659 | PDE Loss:  -4.1796 | Function Loss:  -2.2102\n",
            "Total loss:  -2.1664 | PDE Loss:  -4.18 | Function Loss:  -2.2107\n",
            "Total loss:  -2.167 | PDE Loss:  -4.1774 | Function Loss:  -2.2116\n",
            "Total loss:  -2.1675 | PDE Loss:  -4.1816 | Function Loss:  -2.2117\n",
            "Total loss:  -2.1678 | PDE Loss:  -4.1847 | Function Loss:  -2.2118\n",
            "Total loss:  -2.1681 | PDE Loss:  -4.1826 | Function Loss:  -2.2123\n",
            "Total loss:  -2.1685 | PDE Loss:  -4.1839 | Function Loss:  -2.2126\n",
            "Total loss:  -2.1689 | PDE Loss:  -4.1788 | Function Loss:  -2.2136\n",
            "Total loss:  -2.1694 | PDE Loss:  -4.1765 | Function Loss:  -2.2144\n",
            "Total loss:  -2.17 | PDE Loss:  -4.1637 | Function Loss:  -2.2164\n",
            "Total loss:  -2.1703 | PDE Loss:  -4.1567 | Function Loss:  -2.2176\n",
            "Total loss:  -2.1707 | PDE Loss:  -4.1504 | Function Loss:  -2.2188\n",
            "Total loss:  -2.1711 | PDE Loss:  -4.146 | Function Loss:  -2.2197\n",
            "Total loss:  -2.1715 | PDE Loss:  -4.1419 | Function Loss:  -2.2206\n",
            "Total loss:  -2.1719 | PDE Loss:  -4.1387 | Function Loss:  -2.2215\n",
            "Total loss:  -2.1724 | PDE Loss:  -4.135 | Function Loss:  -2.2225\n",
            "Total loss:  -2.1728 | PDE Loss:  -4.1273 | Function Loss:  -2.224\n",
            "Total loss:  -2.1733 | PDE Loss:  -4.1305 | Function Loss:  -2.224\n",
            "Total loss:  -2.1736 | PDE Loss:  -4.1335 | Function Loss:  -2.2241\n",
            "Total loss:  -2.174 | PDE Loss:  -4.1359 | Function Loss:  -2.2242\n",
            "Total loss:  -2.1742 | PDE Loss:  -4.1385 | Function Loss:  -2.2241\n",
            "Total loss:  -2.1744 | PDE Loss:  -4.139 | Function Loss:  -2.2243\n",
            "Total loss:  -2.1745 | PDE Loss:  -4.1392 | Function Loss:  -2.2244\n",
            "Total loss:  -2.1747 | PDE Loss:  -4.1393 | Function Loss:  -2.2245\n",
            "Total loss:  -2.1749 | PDE Loss:  -4.138 | Function Loss:  -2.2249\n",
            "Total loss:  -2.1751 | PDE Loss:  -4.1374 | Function Loss:  -2.2253\n",
            "Total loss:  -2.1754 | PDE Loss:  -4.1384 | Function Loss:  -2.2254\n",
            "Total loss:  -2.1757 | PDE Loss:  -4.1375 | Function Loss:  -2.2259\n",
            "Total loss:  -2.1759 | PDE Loss:  -4.1398 | Function Loss:  -2.2259\n",
            "Total loss:  -2.1762 | PDE Loss:  -4.1421 | Function Loss:  -2.2259\n",
            "Total loss:  -2.1764 | PDE Loss:  -4.146 | Function Loss:  -2.2256\n",
            "Total loss:  -2.1765 | PDE Loss:  -4.1485 | Function Loss:  -2.2255\n",
            "Total loss:  -2.1766 | PDE Loss:  -4.152 | Function Loss:  -2.2252\n",
            "Total loss:  -2.1767 | PDE Loss:  -4.1542 | Function Loss:  -2.2251\n",
            "Total loss:  -2.1769 | PDE Loss:  -4.157 | Function Loss:  -2.2249\n",
            "Total loss:  -2.1771 | PDE Loss:  -4.1605 | Function Loss:  -2.2247\n",
            "Total loss:  -2.1771 | PDE Loss:  -4.1623 | Function Loss:  -2.2245\n",
            "Total loss:  -2.1773 | PDE Loss:  -4.1656 | Function Loss:  -2.2244\n",
            "Total loss:  -2.1775 | PDE Loss:  -4.1673 | Function Loss:  -2.2245\n",
            "Total loss:  -2.1778 | PDE Loss:  -4.1697 | Function Loss:  -2.2245\n",
            "Total loss:  -2.1781 | PDE Loss:  -4.1724 | Function Loss:  -2.2245\n",
            "Total loss:  -2.1784 | PDE Loss:  -4.1758 | Function Loss:  -2.2245\n",
            "Total loss:  -2.1788 | PDE Loss:  -4.1798 | Function Loss:  -2.2244\n",
            "Total loss:  -2.1791 | PDE Loss:  -4.1839 | Function Loss:  -2.2243\n",
            "Total loss:  -2.1794 | PDE Loss:  -4.1873 | Function Loss:  -2.2243\n",
            "Total loss:  -2.1796 | PDE Loss:  -4.1892 | Function Loss:  -2.2243\n",
            "Total loss:  -2.1797 | PDE Loss:  -4.1899 | Function Loss:  -2.2244\n",
            "Total loss:  -2.1798 | PDE Loss:  -4.1898 | Function Loss:  -2.2245\n",
            "Total loss:  -2.1799 | PDE Loss:  -4.1889 | Function Loss:  -2.2247\n",
            "Total loss:  -2.18 | PDE Loss:  -4.1884 | Function Loss:  -2.2248\n",
            "Total loss:  -2.1801 | PDE Loss:  -4.1872 | Function Loss:  -2.225\n",
            "Total loss:  -2.1801 | PDE Loss:  -4.1857 | Function Loss:  -2.2253\n",
            "Total loss:  -2.1803 | PDE Loss:  -4.1855 | Function Loss:  -2.2255\n",
            "Total loss:  -2.1804 | PDE Loss:  -4.1856 | Function Loss:  -2.2256\n",
            "Total loss:  -2.1806 | PDE Loss:  -4.1868 | Function Loss:  -2.2257\n",
            "Total loss:  -2.1808 | PDE Loss:  -4.1886 | Function Loss:  -2.2257\n",
            "Total loss:  -2.181 | PDE Loss:  -4.1905 | Function Loss:  -2.2257\n",
            "Total loss:  -2.1812 | PDE Loss:  -4.1937 | Function Loss:  -2.2256\n",
            "Total loss:  -2.1814 | PDE Loss:  -4.1969 | Function Loss:  -2.2255\n",
            "Total loss:  -2.1817 | PDE Loss:  -4.2015 | Function Loss:  -2.2253\n",
            "Total loss:  -2.1821 | PDE Loss:  -4.2066 | Function Loss:  -2.2253\n",
            "Total loss:  -2.1826 | PDE Loss:  -4.2126 | Function Loss:  -2.2252\n",
            "Total loss:  -2.1831 | PDE Loss:  -4.2185 | Function Loss:  -2.2251\n",
            "Total loss:  -2.1836 | PDE Loss:  -4.2214 | Function Loss:  -2.2253\n",
            "Total loss:  -2.1841 | PDE Loss:  -4.2219 | Function Loss:  -2.2259\n",
            "Total loss:  -2.1845 | PDE Loss:  -4.2218 | Function Loss:  -2.2263\n",
            "Total loss:  -2.1849 | PDE Loss:  -4.2192 | Function Loss:  -2.227\n",
            "Total loss:  -2.1852 | PDE Loss:  -4.2147 | Function Loss:  -2.2279\n",
            "Total loss:  -2.1857 | PDE Loss:  -4.2119 | Function Loss:  -2.2286\n",
            "Total loss:  -2.1862 | PDE Loss:  -4.2028 | Function Loss:  -2.2301\n",
            "Total loss:  -2.1867 | PDE Loss:  -4.2014 | Function Loss:  -2.2308\n",
            "Total loss:  -2.1872 | PDE Loss:  -4.1925 | Function Loss:  -2.2323\n",
            "Total loss:  -2.1875 | PDE Loss:  -4.1915 | Function Loss:  -2.2329\n",
            "Total loss:  -2.1879 | PDE Loss:  -4.1913 | Function Loss:  -2.2333\n",
            "Total loss:  -2.1885 | PDE Loss:  -4.1924 | Function Loss:  -2.2338\n",
            "Total loss:  -2.1892 | PDE Loss:  -4.1929 | Function Loss:  -2.2346\n",
            "Total loss:  -2.1899 | PDE Loss:  -4.1927 | Function Loss:  -2.2354\n",
            "Total loss:  -2.1906 | PDE Loss:  -4.1913 | Function Loss:  -2.2363\n",
            "Total loss:  -2.1913 | PDE Loss:  -4.1878 | Function Loss:  -2.2375\n",
            "Total loss:  -2.192 | PDE Loss:  -4.1854 | Function Loss:  -2.2385\n",
            "Total loss:  -2.1928 | PDE Loss:  -4.1862 | Function Loss:  -2.2393\n",
            "Total loss:  -2.1935 | PDE Loss:  -4.1863 | Function Loss:  -2.2401\n",
            "Total loss:  -2.1946 | PDE Loss:  -4.187 | Function Loss:  -2.2412\n",
            "Total loss:  -2.1955 | PDE Loss:  -4.1903 | Function Loss:  -2.2418\n",
            "Total loss:  -2.1962 | PDE Loss:  -4.1905 | Function Loss:  -2.2426\n",
            "Total loss:  -2.1967 | PDE Loss:  -4.197 | Function Loss:  -2.2425\n",
            "Total loss:  -2.1971 | PDE Loss:  -4.1958 | Function Loss:  -2.243\n",
            "Total loss:  -2.1973 | PDE Loss:  -4.1987 | Function Loss:  -2.2429\n",
            "Total loss:  -2.1975 | PDE Loss:  -4.1987 | Function Loss:  -2.2432\n",
            "Total loss:  -2.1977 | PDE Loss:  -4.2002 | Function Loss:  -2.2432\n",
            "Total loss:  -2.1979 | PDE Loss:  -4.2015 | Function Loss:  -2.2433\n",
            "Total loss:  -2.1983 | PDE Loss:  -4.2011 | Function Loss:  -2.2437\n",
            "Total loss:  -2.1987 | PDE Loss:  -4.2024 | Function Loss:  -2.244\n",
            "Total loss:  -2.199 | PDE Loss:  -4.1998 | Function Loss:  -2.2447\n",
            "Total loss:  -2.1993 | PDE Loss:  -4.198 | Function Loss:  -2.2452\n",
            "Total loss:  -2.1997 | PDE Loss:  -4.1951 | Function Loss:  -2.246\n",
            "Total loss:  -2.2 | PDE Loss:  -4.1922 | Function Loss:  -2.2467\n",
            "Total loss:  -2.2005 | PDE Loss:  -4.1901 | Function Loss:  -2.2474\n",
            "Total loss:  -2.201 | PDE Loss:  -4.1861 | Function Loss:  -2.2485\n",
            "Total loss:  -2.2011 | PDE Loss:  -4.1817 | Function Loss:  -2.249\n",
            "Total loss:  -2.2014 | PDE Loss:  -4.1851 | Function Loss:  -2.249\n",
            "Total loss:  -2.2018 | PDE Loss:  -4.1839 | Function Loss:  -2.2496\n",
            "Total loss:  -2.2022 | PDE Loss:  -4.1813 | Function Loss:  -2.2503\n",
            "Total loss:  -2.2024 | PDE Loss:  -4.1816 | Function Loss:  -2.2506\n",
            "Total loss:  -2.2027 | PDE Loss:  -4.182 | Function Loss:  -2.2508\n",
            "Total loss:  -2.2029 | PDE Loss:  -4.183 | Function Loss:  -2.2509\n",
            "Total loss:  -2.2032 | PDE Loss:  -4.1847 | Function Loss:  -2.251\n",
            "Total loss:  -2.2035 | PDE Loss:  -4.187 | Function Loss:  -2.2511\n",
            "Total loss:  -2.2038 | PDE Loss:  -4.1881 | Function Loss:  -2.2513\n",
            "Total loss:  -2.2041 | PDE Loss:  -4.19 | Function Loss:  -2.2514\n",
            "Total loss:  -2.2044 | PDE Loss:  -4.1898 | Function Loss:  -2.2518\n",
            "Total loss:  -2.2047 | PDE Loss:  -4.1902 | Function Loss:  -2.2521\n",
            "Total loss:  -2.2051 | PDE Loss:  -4.1869 | Function Loss:  -2.2529\n",
            "Total loss:  -2.2054 | PDE Loss:  -4.1844 | Function Loss:  -2.2535\n",
            "Total loss:  -2.2057 | PDE Loss:  -4.1803 | Function Loss:  -2.2544\n",
            "Total loss:  -2.206 | PDE Loss:  -4.1769 | Function Loss:  -2.2551\n",
            "Total loss:  -2.2062 | PDE Loss:  -4.1746 | Function Loss:  -2.2556\n",
            "Total loss:  -2.2065 | PDE Loss:  -4.1725 | Function Loss:  -2.2562\n",
            "Total loss:  -2.2067 | PDE Loss:  -4.1706 | Function Loss:  -2.2567\n",
            "Total loss:  -2.207 | PDE Loss:  -4.1698 | Function Loss:  -2.2571\n",
            "Total loss:  -2.2072 | PDE Loss:  -4.1696 | Function Loss:  -2.2574\n",
            "Total loss:  -2.2075 | PDE Loss:  -4.1694 | Function Loss:  -2.2578\n",
            "Total loss:  -2.2079 | PDE Loss:  -4.1707 | Function Loss:  -2.258\n",
            "Total loss:  -2.2084 | PDE Loss:  -4.1691 | Function Loss:  -2.2587\n",
            "Total loss:  -2.2088 | PDE Loss:  -4.1719 | Function Loss:  -2.2589\n",
            "Total loss:  -2.2093 | PDE Loss:  -4.1724 | Function Loss:  -2.2593\n",
            "Total loss:  -2.2097 | PDE Loss:  -4.1757 | Function Loss:  -2.2594\n",
            "Total loss:  -2.2102 | PDE Loss:  -4.1769 | Function Loss:  -2.2599\n",
            "Total loss:  -2.2107 | PDE Loss:  -4.18 | Function Loss:  -2.26\n",
            "Total loss:  -2.2113 | PDE Loss:  -4.183 | Function Loss:  -2.2603\n",
            "Total loss:  -2.2119 | PDE Loss:  -4.186 | Function Loss:  -2.2606\n",
            "Total loss:  -2.2125 | PDE Loss:  -4.1882 | Function Loss:  -2.261\n",
            "Total loss:  -2.2129 | PDE Loss:  -4.1907 | Function Loss:  -2.2612\n",
            "Total loss:  -2.2133 | PDE Loss:  -4.1898 | Function Loss:  -2.2617\n",
            "Total loss:  -2.2135 | PDE Loss:  -4.1905 | Function Loss:  -2.2619\n",
            "Total loss:  -2.2137 | PDE Loss:  -4.1898 | Function Loss:  -2.2622\n",
            "Total loss:  -2.214 | PDE Loss:  -4.1885 | Function Loss:  -2.2627\n",
            "Total loss:  -2.2146 | PDE Loss:  -4.1861 | Function Loss:  -2.2636\n",
            "Total loss:  -2.2151 | PDE Loss:  -4.1833 | Function Loss:  -2.2645\n",
            "Total loss:  -2.2154 | PDE Loss:  -4.1796 | Function Loss:  -2.2654\n",
            "Total loss:  -2.2158 | PDE Loss:  -4.1781 | Function Loss:  -2.266\n",
            "Total loss:  -2.2162 | PDE Loss:  -4.1741 | Function Loss:  -2.2669\n",
            "Total loss:  -2.2165 | PDE Loss:  -4.1727 | Function Loss:  -2.2674\n",
            "Total loss:  -2.2169 | PDE Loss:  -4.1716 | Function Loss:  -2.2679\n",
            "Total loss:  -2.2174 | PDE Loss:  -4.1708 | Function Loss:  -2.2687\n",
            "Total loss:  -2.2181 | PDE Loss:  -4.1707 | Function Loss:  -2.2695\n",
            "Total loss:  -2.2192 | PDE Loss:  -4.1727 | Function Loss:  -2.2704\n",
            "Total loss:  -2.2205 | PDE Loss:  -4.173 | Function Loss:  -2.2718\n",
            "Total loss:  -2.2218 | PDE Loss:  -4.1783 | Function Loss:  -2.2727\n",
            "Total loss:  -2.2229 | PDE Loss:  -4.1792 | Function Loss:  -2.2738\n",
            "Total loss:  -2.2238 | PDE Loss:  -4.1842 | Function Loss:  -2.2742\n",
            "Total loss:  -2.2246 | PDE Loss:  -4.1845 | Function Loss:  -2.275\n",
            "Total loss:  -2.2253 | PDE Loss:  -4.1917 | Function Loss:  -2.2749\n",
            "Total loss:  -2.2258 | PDE Loss:  -4.1901 | Function Loss:  -2.2757\n",
            "Total loss:  -2.2263 | PDE Loss:  -4.1917 | Function Loss:  -2.276\n",
            "Total loss:  -2.2269 | PDE Loss:  -4.1932 | Function Loss:  -2.2765\n",
            "Total loss:  -2.2276 | PDE Loss:  -4.1944 | Function Loss:  -2.2772\n",
            "Total loss:  -2.2282 | PDE Loss:  -4.1982 | Function Loss:  -2.2775\n",
            "Total loss:  -2.2288 | PDE Loss:  -4.1991 | Function Loss:  -2.278\n",
            "Total loss:  -2.2293 | PDE Loss:  -4.1998 | Function Loss:  -2.2785\n",
            "Total loss:  -2.2298 | PDE Loss:  -4.2023 | Function Loss:  -2.2787\n",
            "Total loss:  -2.2302 | PDE Loss:  -4.2021 | Function Loss:  -2.2792\n",
            "Total loss:  -2.2307 | PDE Loss:  -4.2031 | Function Loss:  -2.2796\n",
            "Total loss:  -2.2312 | PDE Loss:  -4.2028 | Function Loss:  -2.2803\n",
            "Total loss:  -2.2318 | PDE Loss:  -4.2009 | Function Loss:  -2.2811\n",
            "Total loss:  -2.2322 | PDE Loss:  -4.2007 | Function Loss:  -2.2816\n",
            "Total loss:  -2.2327 | PDE Loss:  -4.1999 | Function Loss:  -2.2823\n",
            "Total loss:  -2.2333 | PDE Loss:  -4.2 | Function Loss:  -2.2829\n",
            "Total loss:  -2.2338 | PDE Loss:  -4.1989 | Function Loss:  -2.2837\n",
            "Total loss:  -2.2344 | PDE Loss:  -4.1954 | Function Loss:  -2.2847\n",
            "Total loss:  -2.2349 | PDE Loss:  -4.1955 | Function Loss:  -2.2852\n",
            "Total loss:  -2.2354 | PDE Loss:  -4.1942 | Function Loss:  -2.286\n",
            "Total loss:  -2.2361 | PDE Loss:  -4.1919 | Function Loss:  -2.287\n",
            "Total loss:  -2.2366 | PDE Loss:  -4.1901 | Function Loss:  -2.2879\n",
            "Total loss:  -2.2371 | PDE Loss:  -4.1868 | Function Loss:  -2.2888\n",
            "Total loss:  -2.2375 | PDE Loss:  -4.1849 | Function Loss:  -2.2895\n",
            "Total loss:  -2.2379 | PDE Loss:  -4.1812 | Function Loss:  -2.2904\n",
            "Total loss:  -2.2383 | PDE Loss:  -4.1811 | Function Loss:  -2.2909\n",
            "Total loss:  -2.2387 | PDE Loss:  -4.1782 | Function Loss:  -2.2917\n",
            "Total loss:  -2.239 | PDE Loss:  -4.1786 | Function Loss:  -2.2921\n",
            "Total loss:  -2.2394 | PDE Loss:  -4.1784 | Function Loss:  -2.2924\n",
            "Total loss:  -2.2397 | PDE Loss:  -4.1786 | Function Loss:  -2.2928\n",
            "Total loss:  -2.24 | PDE Loss:  -4.1788 | Function Loss:  -2.2931\n",
            "Total loss:  -2.2402 | PDE Loss:  -4.1789 | Function Loss:  -2.2934\n",
            "Total loss:  -2.2405 | PDE Loss:  -4.1783 | Function Loss:  -2.2937\n",
            "Total loss:  -2.2407 | PDE Loss:  -4.179 | Function Loss:  -2.2939\n",
            "Total loss:  -2.241 | PDE Loss:  -4.1768 | Function Loss:  -2.2945\n",
            "Total loss:  -2.2413 | PDE Loss:  -4.178 | Function Loss:  -2.2947\n",
            "Total loss:  -2.2415 | PDE Loss:  -4.1758 | Function Loss:  -2.2952\n",
            "Total loss:  -2.2417 | PDE Loss:  -4.1775 | Function Loss:  -2.2952\n",
            "Total loss:  -2.2419 | PDE Loss:  -4.1773 | Function Loss:  -2.2954\n",
            "Total loss:  -2.242 | PDE Loss:  -4.1788 | Function Loss:  -2.2954\n",
            "Total loss:  -2.2421 | PDE Loss:  -4.1792 | Function Loss:  -2.2955\n",
            "Total loss:  -2.2422 | PDE Loss:  -4.1812 | Function Loss:  -2.2954\n",
            "Total loss:  -2.2424 | PDE Loss:  -4.182 | Function Loss:  -2.2954\n",
            "Total loss:  -2.2425 | PDE Loss:  -4.1836 | Function Loss:  -2.2953\n",
            "Total loss:  -2.2426 | PDE Loss:  -4.1849 | Function Loss:  -2.2953\n",
            "Total loss:  -2.2428 | PDE Loss:  -4.1858 | Function Loss:  -2.2954\n",
            "Total loss:  -2.243 | PDE Loss:  -4.1866 | Function Loss:  -2.2955\n",
            "Total loss:  -2.2432 | PDE Loss:  -4.1861 | Function Loss:  -2.2958\n",
            "Total loss:  -2.2435 | PDE Loss:  -4.1854 | Function Loss:  -2.2962\n",
            "Total loss:  -2.2439 | PDE Loss:  -4.1828 | Function Loss:  -2.297\n",
            "Total loss:  -2.2444 | PDE Loss:  -4.1788 | Function Loss:  -2.2981\n",
            "Total loss:  -2.2448 | PDE Loss:  -4.1745 | Function Loss:  -2.2991\n",
            "Total loss:  -2.2453 | PDE Loss:  -4.171 | Function Loss:  -2.3002\n",
            "Total loss:  -2.2458 | PDE Loss:  -4.1667 | Function Loss:  -2.3013\n",
            "Total loss:  -2.2463 | PDE Loss:  -4.1672 | Function Loss:  -2.3018\n",
            "Total loss:  -2.2467 | PDE Loss:  -4.1614 | Function Loss:  -2.3031\n",
            "Total loss:  -2.2471 | PDE Loss:  -4.1657 | Function Loss:  -2.3029\n",
            "Total loss:  -2.2474 | PDE Loss:  -4.1672 | Function Loss:  -2.3031\n",
            "Total loss:  -2.248 | PDE Loss:  -4.1717 | Function Loss:  -2.3031\n",
            "Total loss:  -2.2488 | PDE Loss:  -4.1767 | Function Loss:  -2.3033\n",
            "Total loss:  -2.2493 | PDE Loss:  -4.1798 | Function Loss:  -2.3036\n",
            "Total loss:  -2.25 | PDE Loss:  -4.1812 | Function Loss:  -2.3041\n",
            "Total loss:  -2.2506 | PDE Loss:  -4.18 | Function Loss:  -2.305\n",
            "Total loss:  -2.2511 | PDE Loss:  -4.1788 | Function Loss:  -2.3057\n",
            "Total loss:  -2.2515 | PDE Loss:  -4.1739 | Function Loss:  -2.3068\n",
            "Total loss:  -2.2518 | PDE Loss:  -4.172 | Function Loss:  -2.3074\n",
            "Total loss:  -2.2521 | PDE Loss:  -4.1722 | Function Loss:  -2.3077\n",
            "Total loss:  -2.2523 | PDE Loss:  -4.1697 | Function Loss:  -2.3083\n",
            "Total loss:  -2.2525 | PDE Loss:  -4.1708 | Function Loss:  -2.3084\n",
            "Total loss:  -2.2527 | PDE Loss:  -4.1713 | Function Loss:  -2.3085\n",
            "Total loss:  -2.253 | PDE Loss:  -4.1739 | Function Loss:  -2.3085\n",
            "Total loss:  -2.2533 | PDE Loss:  -4.1777 | Function Loss:  -2.3084\n",
            "Total loss:  -2.2537 | PDE Loss:  -4.1806 | Function Loss:  -2.3084\n",
            "Total loss:  -2.2541 | PDE Loss:  -4.1846 | Function Loss:  -2.3083\n",
            "Total loss:  -2.2546 | PDE Loss:  -4.1882 | Function Loss:  -2.3084\n",
            "Total loss:  -2.2548 | PDE Loss:  -4.19 | Function Loss:  -2.3084\n",
            "Total loss:  -2.2553 | PDE Loss:  -4.191 | Function Loss:  -2.3088\n",
            "Total loss:  -2.2557 | PDE Loss:  -4.1909 | Function Loss:  -2.3093\n",
            "Total loss:  -2.2561 | PDE Loss:  -4.19 | Function Loss:  -2.3099\n",
            "Total loss:  -2.2565 | PDE Loss:  -4.1891 | Function Loss:  -2.3104\n",
            "Total loss:  -2.257 | PDE Loss:  -4.1874 | Function Loss:  -2.3113\n",
            "Total loss:  -2.2576 | PDE Loss:  -4.1889 | Function Loss:  -2.3117\n",
            "Total loss:  -2.2583 | PDE Loss:  -4.1848 | Function Loss:  -2.313\n",
            "Total loss:  -2.2589 | PDE Loss:  -4.1897 | Function Loss:  -2.3131\n",
            "Total loss:  -2.2595 | PDE Loss:  -4.1889 | Function Loss:  -2.3138\n",
            "Total loss:  -2.2601 | PDE Loss:  -4.1942 | Function Loss:  -2.3138\n",
            "Total loss:  -2.2606 | PDE Loss:  -4.1929 | Function Loss:  -2.3146\n",
            "Total loss:  -2.261 | PDE Loss:  -4.1956 | Function Loss:  -2.3146\n",
            "Total loss:  -2.2613 | PDE Loss:  -4.197 | Function Loss:  -2.3148\n",
            "Total loss:  -2.2616 | PDE Loss:  -4.1948 | Function Loss:  -2.3155\n",
            "Total loss:  -2.2621 | PDE Loss:  -4.1926 | Function Loss:  -2.3163\n",
            "Total loss:  -2.2623 | PDE Loss:  -4.193 | Function Loss:  -2.3165\n",
            "Total loss:  -2.2628 | PDE Loss:  -4.1885 | Function Loss:  -2.3176\n",
            "Total loss:  -2.2631 | PDE Loss:  -4.1874 | Function Loss:  -2.3182\n",
            "Total loss:  -2.2633 | PDE Loss:  -4.1841 | Function Loss:  -2.3189\n",
            "Total loss:  -2.2636 | PDE Loss:  -4.1822 | Function Loss:  -2.3194\n",
            "Total loss:  -2.2639 | PDE Loss:  -4.18 | Function Loss:  -2.32\n",
            "Total loss:  -2.2643 | PDE Loss:  -4.177 | Function Loss:  -2.3209\n",
            "Total loss:  -2.2647 | PDE Loss:  -4.1763 | Function Loss:  -2.3215\n",
            "Total loss:  -2.265 | PDE Loss:  -4.1749 | Function Loss:  -2.3221\n",
            "Total loss:  -2.2655 | PDE Loss:  -4.1749 | Function Loss:  -2.3226\n",
            "Total loss:  -2.2659 | PDE Loss:  -4.1756 | Function Loss:  -2.3229\n",
            "Total loss:  -2.2661 | PDE Loss:  -4.1753 | Function Loss:  -2.3233\n",
            "Total loss:  -2.2664 | PDE Loss:  -4.177 | Function Loss:  -2.3233\n",
            "Total loss:  -2.2666 | PDE Loss:  -4.1777 | Function Loss:  -2.3235\n",
            "Total loss:  -2.2669 | PDE Loss:  -4.1788 | Function Loss:  -2.3236\n",
            "Total loss:  -2.2672 | PDE Loss:  -4.18 | Function Loss:  -2.3238\n",
            "Total loss:  -2.2675 | PDE Loss:  -4.1778 | Function Loss:  -2.3245\n",
            "Total loss:  -2.2678 | PDE Loss:  -4.1785 | Function Loss:  -2.3248\n",
            "Total loss:  -2.2682 | PDE Loss:  -4.1791 | Function Loss:  -2.3251\n",
            "Total loss:  -2.2686 | PDE Loss:  -4.1748 | Function Loss:  -2.3262\n",
            "Total loss:  -2.269 | PDE Loss:  -4.1761 | Function Loss:  -2.3264\n",
            "Total loss:  -2.2693 | PDE Loss:  -4.1737 | Function Loss:  -2.3271\n",
            "Total loss:  -2.2696 | PDE Loss:  -4.1744 | Function Loss:  -2.3274\n",
            "Total loss:  -2.2699 | PDE Loss:  -4.1722 | Function Loss:  -2.328\n",
            "Total loss:  -2.27 | PDE Loss:  -4.1711 | Function Loss:  -2.3283\n",
            "Total loss:  -2.2703 | PDE Loss:  -4.1703 | Function Loss:  -2.3287\n",
            "Total loss:  -2.2706 | PDE Loss:  -4.1685 | Function Loss:  -2.3293\n",
            "Total loss:  -2.271 | PDE Loss:  -4.1668 | Function Loss:  -2.33\n",
            "Total loss:  -2.2714 | PDE Loss:  -4.1633 | Function Loss:  -2.331\n",
            "Total loss:  -2.2718 | PDE Loss:  -4.1603 | Function Loss:  -2.332\n",
            "Total loss:  -2.2721 | PDE Loss:  -4.1588 | Function Loss:  -2.3325\n",
            "Total loss:  -2.2726 | PDE Loss:  -4.158 | Function Loss:  -2.3331\n",
            "Total loss:  -2.273 | PDE Loss:  -4.1565 | Function Loss:  -2.3338\n",
            "Total loss:  -2.2732 | PDE Loss:  -4.1578 | Function Loss:  -2.3339\n",
            "Total loss:  -2.2735 | PDE Loss:  -4.1568 | Function Loss:  -2.3344\n",
            "Total loss:  -2.2737 | PDE Loss:  -4.1612 | Function Loss:  -2.334\n",
            "Total loss:  -2.2739 | PDE Loss:  -4.161 | Function Loss:  -2.3343\n",
            "Total loss:  -2.2741 | PDE Loss:  -4.1626 | Function Loss:  -2.3343\n",
            "Total loss:  -2.2744 | PDE Loss:  -4.1638 | Function Loss:  -2.3344\n",
            "Total loss:  -2.2748 | PDE Loss:  -4.1639 | Function Loss:  -2.3348\n",
            "Total loss:  -2.2752 | PDE Loss:  -4.1642 | Function Loss:  -2.3353\n",
            "Total loss:  -2.2756 | PDE Loss:  -4.1621 | Function Loss:  -2.336\n",
            "Total loss:  -2.276 | PDE Loss:  -4.1606 | Function Loss:  -2.3367\n",
            "Total loss:  -2.2764 | PDE Loss:  -4.1577 | Function Loss:  -2.3376\n",
            "Total loss:  -2.2768 | PDE Loss:  -4.156 | Function Loss:  -2.3383\n",
            "Total loss:  -2.277 | PDE Loss:  -4.1543 | Function Loss:  -2.3388\n",
            "Total loss:  -2.2772 | PDE Loss:  -4.1532 | Function Loss:  -2.3392\n",
            "Total loss:  -2.2773 | PDE Loss:  -4.1533 | Function Loss:  -2.3394\n",
            "Total loss:  -2.2775 | PDE Loss:  -4.153 | Function Loss:  -2.3396\n",
            "Total loss:  -2.2777 | PDE Loss:  -4.1541 | Function Loss:  -2.3396\n",
            "Total loss:  -2.2778 | PDE Loss:  -4.1543 | Function Loss:  -2.3397\n",
            "Total loss:  -2.2779 | PDE Loss:  -4.156 | Function Loss:  -2.3396\n",
            "Total loss:  -2.278 | PDE Loss:  -4.1566 | Function Loss:  -2.3396\n",
            "Total loss:  -2.2782 | PDE Loss:  -4.1579 | Function Loss:  -2.3396\n",
            "Total loss:  -2.2783 | PDE Loss:  -4.1587 | Function Loss:  -2.3397\n",
            "Total loss:  -2.2786 | PDE Loss:  -4.1594 | Function Loss:  -2.3398\n",
            "Total loss:  -2.2788 | PDE Loss:  -4.1586 | Function Loss:  -2.3402\n",
            "Total loss:  -2.2791 | PDE Loss:  -4.1601 | Function Loss:  -2.3403\n",
            "Total loss:  -2.2793 | PDE Loss:  -4.16 | Function Loss:  -2.3406\n",
            "Total loss:  -2.2796 | PDE Loss:  -4.1608 | Function Loss:  -2.3408\n",
            "Total loss:  -2.2799 | PDE Loss:  -4.1618 | Function Loss:  -2.341\n",
            "Total loss:  -2.2802 | PDE Loss:  -4.1634 | Function Loss:  -2.3411\n",
            "Total loss:  -2.2804 | PDE Loss:  -4.1653 | Function Loss:  -2.3411\n",
            "Total loss:  -2.2807 | PDE Loss:  -4.1682 | Function Loss:  -2.341\n",
            "Total loss:  -2.2811 | PDE Loss:  -4.1711 | Function Loss:  -2.3409\n",
            "Total loss:  -2.2814 | PDE Loss:  -4.1752 | Function Loss:  -2.3408\n",
            "Total loss:  -2.2818 | PDE Loss:  -4.1799 | Function Loss:  -2.3405\n",
            "Total loss:  -2.282 | PDE Loss:  -4.181 | Function Loss:  -2.3406\n",
            "Total loss:  -2.2823 | PDE Loss:  -4.1832 | Function Loss:  -2.3407\n",
            "Total loss:  -2.2826 | PDE Loss:  -4.184 | Function Loss:  -2.3408\n",
            "Total loss:  -2.2828 | PDE Loss:  -4.1831 | Function Loss:  -2.3412\n",
            "Total loss:  -2.2831 | PDE Loss:  -4.1852 | Function Loss:  -2.3412\n",
            "Total loss:  -2.2832 | PDE Loss:  -4.184 | Function Loss:  -2.3415\n",
            "Total loss:  -2.2834 | PDE Loss:  -4.1838 | Function Loss:  -2.3418\n",
            "Total loss:  -2.2837 | PDE Loss:  -4.1832 | Function Loss:  -2.3422\n",
            "Total loss:  -2.284 | PDE Loss:  -4.1842 | Function Loss:  -2.3424\n",
            "Total loss:  -2.2843 | PDE Loss:  -4.1856 | Function Loss:  -2.3426\n",
            "Total loss:  -2.2847 | PDE Loss:  -4.1882 | Function Loss:  -2.3426\n",
            "Total loss:  -2.285 | PDE Loss:  -4.1916 | Function Loss:  -2.3425\n",
            "Total loss:  -2.2853 | PDE Loss:  -4.1949 | Function Loss:  -2.3424\n",
            "Total loss:  -2.2857 | PDE Loss:  -4.1992 | Function Loss:  -2.3422\n",
            "Total loss:  -2.2861 | PDE Loss:  -4.2025 | Function Loss:  -2.3422\n",
            "Total loss:  -2.2866 | PDE Loss:  -4.2088 | Function Loss:  -2.342\n",
            "Total loss:  -2.2872 | PDE Loss:  -4.212 | Function Loss:  -2.3422\n",
            "Total loss:  -2.2878 | PDE Loss:  -4.2151 | Function Loss:  -2.3425\n",
            "Total loss:  -2.2885 | PDE Loss:  -4.2215 | Function Loss:  -2.3423\n",
            "Total loss:  -2.2892 | PDE Loss:  -4.2227 | Function Loss:  -2.343\n",
            "Total loss:  -2.2901 | PDE Loss:  -4.2241 | Function Loss:  -2.3438\n",
            "Total loss:  -2.2909 | PDE Loss:  -4.2232 | Function Loss:  -2.3449\n",
            "Total loss:  -2.2916 | PDE Loss:  -4.2265 | Function Loss:  -2.3452\n",
            "Total loss:  -2.2922 | PDE Loss:  -4.2271 | Function Loss:  -2.3459\n",
            "Total loss:  -2.2933 | PDE Loss:  -4.2313 | Function Loss:  -2.3465\n",
            "Total loss:  -2.2947 | PDE Loss:  -4.2332 | Function Loss:  -2.3479\n",
            "Total loss:  -2.2958 | PDE Loss:  -4.236 | Function Loss:  -2.3488\n",
            "Total loss:  -2.2969 | PDE Loss:  -4.2378 | Function Loss:  -2.3497\n",
            "Total loss:  -2.2976 | PDE Loss:  -4.2383 | Function Loss:  -2.3505\n",
            "Total loss:  -2.298 | PDE Loss:  -4.2384 | Function Loss:  -2.3509\n",
            "Total loss:  -2.2984 | PDE Loss:  -4.2386 | Function Loss:  -2.3514\n",
            "Total loss:  -2.299 | PDE Loss:  -4.2395 | Function Loss:  -2.3518\n",
            "Total loss:  -2.2995 | PDE Loss:  -4.2399 | Function Loss:  -2.3524\n",
            "Total loss:  -2.3001 | PDE Loss:  -4.2407 | Function Loss:  -2.353\n",
            "Total loss:  -2.3011 | PDE Loss:  -4.243 | Function Loss:  -2.3538\n",
            "Total loss:  -2.3021 | PDE Loss:  -4.2411 | Function Loss:  -2.3552\n",
            "Total loss:  -2.3027 | PDE Loss:  -4.2458 | Function Loss:  -2.3553\n",
            "Total loss:  -2.3031 | PDE Loss:  -4.2446 | Function Loss:  -2.3559\n",
            "Total loss:  -2.3035 | PDE Loss:  -4.2433 | Function Loss:  -2.3565\n",
            "Total loss:  -2.3038 | PDE Loss:  -4.2429 | Function Loss:  -2.3569\n",
            "Total loss:  -2.3042 | PDE Loss:  -4.2423 | Function Loss:  -2.3574\n",
            "Total loss:  -2.3046 | PDE Loss:  -4.2435 | Function Loss:  -2.3577\n",
            "Total loss:  -2.305 | PDE Loss:  -4.2435 | Function Loss:  -2.3581\n",
            "Total loss:  -2.3053 | PDE Loss:  -4.2449 | Function Loss:  -2.3583\n",
            "Total loss:  -2.3057 | PDE Loss:  -4.2462 | Function Loss:  -2.3586\n",
            "Total loss:  -2.306 | PDE Loss:  -4.2472 | Function Loss:  -2.3588\n",
            "Total loss:  -2.3065 | PDE Loss:  -4.2451 | Function Loss:  -2.3597\n",
            "Total loss:  -2.3072 | PDE Loss:  -4.2459 | Function Loss:  -2.3603\n",
            "Total loss:  -2.3078 | PDE Loss:  -4.2443 | Function Loss:  -2.3612\n",
            "Total loss:  -2.3087 | PDE Loss:  -4.2421 | Function Loss:  -2.3625\n",
            "Total loss:  -2.3098 | PDE Loss:  -4.2407 | Function Loss:  -2.364\n",
            "Total loss:  -2.3108 | PDE Loss:  -4.2423 | Function Loss:  -2.3649\n",
            "Total loss:  -2.3114 | PDE Loss:  -4.2446 | Function Loss:  -2.3652\n",
            "Total loss:  -2.3118 | PDE Loss:  -4.2453 | Function Loss:  -2.3657\n",
            "Total loss:  -2.3121 | PDE Loss:  -4.2462 | Function Loss:  -2.3659\n",
            "Total loss:  -2.3124 | PDE Loss:  -4.2486 | Function Loss:  -2.3659\n",
            "Total loss:  -2.3128 | PDE Loss:  -4.2468 | Function Loss:  -2.3665\n",
            "Total loss:  -2.3132 | PDE Loss:  -4.2508 | Function Loss:  -2.3664\n",
            "Total loss:  -2.3136 | PDE Loss:  -4.2524 | Function Loss:  -2.3667\n",
            "Total loss:  -2.3141 | PDE Loss:  -4.2543 | Function Loss:  -2.367\n",
            "Total loss:  -2.3146 | PDE Loss:  -4.2564 | Function Loss:  -2.3673\n",
            "Total loss:  -2.3151 | PDE Loss:  -4.2583 | Function Loss:  -2.3677\n",
            "Total loss:  -2.3156 | PDE Loss:  -4.2584 | Function Loss:  -2.3682\n",
            "Total loss:  -2.316 | PDE Loss:  -4.2595 | Function Loss:  -2.3685\n",
            "Total loss:  -2.3164 | PDE Loss:  -4.2598 | Function Loss:  -2.3689\n",
            "Total loss:  -2.3168 | PDE Loss:  -4.2599 | Function Loss:  -2.3694\n",
            "Total loss:  -2.3172 | PDE Loss:  -4.261 | Function Loss:  -2.3697\n",
            "Total loss:  -2.3175 | PDE Loss:  -4.2627 | Function Loss:  -2.3698\n",
            "Total loss:  -2.3179 | PDE Loss:  -4.2632 | Function Loss:  -2.3702\n",
            "Total loss:  -2.3182 | PDE Loss:  -4.2668 | Function Loss:  -2.37\n",
            "Total loss:  -2.3185 | PDE Loss:  -4.2693 | Function Loss:  -2.3701\n",
            "Total loss:  -2.319 | PDE Loss:  -4.2716 | Function Loss:  -2.3703\n",
            "Total loss:  -2.3195 | PDE Loss:  -4.2747 | Function Loss:  -2.3706\n",
            "Total loss:  -2.3201 | PDE Loss:  -4.2742 | Function Loss:  -2.3713\n",
            "Total loss:  -2.3208 | PDE Loss:  -4.2777 | Function Loss:  -2.3716\n",
            "Total loss:  -2.3213 | PDE Loss:  -4.2766 | Function Loss:  -2.3724\n",
            "Total loss:  -2.3221 | PDE Loss:  -4.2765 | Function Loss:  -2.3732\n",
            "Total loss:  -2.3229 | PDE Loss:  -4.2728 | Function Loss:  -2.3746\n",
            "Total loss:  -2.3235 | PDE Loss:  -4.2742 | Function Loss:  -2.3751\n",
            "Total loss:  -2.3239 | PDE Loss:  -4.2738 | Function Loss:  -2.3756\n",
            "Total loss:  -2.3245 | PDE Loss:  -4.2791 | Function Loss:  -2.3756\n",
            "Total loss:  -2.3249 | PDE Loss:  -4.2799 | Function Loss:  -2.376\n",
            "Total loss:  -2.3253 | PDE Loss:  -4.2811 | Function Loss:  -2.3763\n",
            "Total loss:  -2.3257 | PDE Loss:  -4.2837 | Function Loss:  -2.3764\n",
            "Total loss:  -2.3261 | PDE Loss:  -4.2847 | Function Loss:  -2.3767\n",
            "Total loss:  -2.3265 | PDE Loss:  -4.2858 | Function Loss:  -2.377\n",
            "Total loss:  -2.3269 | PDE Loss:  -4.2872 | Function Loss:  -2.3773\n",
            "Total loss:  -2.3273 | PDE Loss:  -4.2875 | Function Loss:  -2.3777\n",
            "Total loss:  -2.3278 | PDE Loss:  -4.2885 | Function Loss:  -2.3781\n",
            "Total loss:  -2.3283 | PDE Loss:  -4.2869 | Function Loss:  -2.3789\n",
            "Total loss:  -2.3286 | PDE Loss:  -4.2882 | Function Loss:  -2.3791\n",
            "Total loss:  -2.3289 | PDE Loss:  -4.2878 | Function Loss:  -2.3795\n",
            "Total loss:  -2.3292 | PDE Loss:  -4.2896 | Function Loss:  -2.3796\n",
            "Total loss:  -2.3294 | PDE Loss:  -4.2901 | Function Loss:  -2.3797\n",
            "Total loss:  -2.3296 | PDE Loss:  -4.2926 | Function Loss:  -2.3797\n",
            "Total loss:  -2.3299 | PDE Loss:  -4.2943 | Function Loss:  -2.3798\n",
            "Total loss:  -2.3303 | PDE Loss:  -4.297 | Function Loss:  -2.3799\n",
            "Total loss:  -2.3309 | PDE Loss:  -4.299 | Function Loss:  -2.3804\n",
            "Total loss:  -2.3314 | PDE Loss:  -4.3073 | Function Loss:  -2.3799\n",
            "Total loss:  -2.3318 | PDE Loss:  -4.3082 | Function Loss:  -2.3803\n",
            "Total loss:  -2.3323 | PDE Loss:  -4.3067 | Function Loss:  -2.381\n",
            "Total loss:  -2.3327 | PDE Loss:  -4.3082 | Function Loss:  -2.3813\n",
            "Total loss:  -2.333 | PDE Loss:  -4.3067 | Function Loss:  -2.3818\n",
            "Total loss:  -2.3333 | PDE Loss:  -4.3062 | Function Loss:  -2.3822\n",
            "Total loss:  -2.3335 | PDE Loss:  -4.3103 | Function Loss:  -2.3819\n",
            "Total loss:  -2.3339 | PDE Loss:  -4.3096 | Function Loss:  -2.3825\n",
            "Total loss:  -2.3343 | PDE Loss:  -4.3101 | Function Loss:  -2.3829\n",
            "Total loss:  -2.3346 | PDE Loss:  -4.3079 | Function Loss:  -2.3834\n",
            "Total loss:  -2.3348 | PDE Loss:  -4.308 | Function Loss:  -2.3837\n",
            "Total loss:  -2.335 | PDE Loss:  -4.306 | Function Loss:  -2.3842\n",
            "Total loss:  -2.3353 | PDE Loss:  -4.3051 | Function Loss:  -2.3845\n",
            "Total loss:  -2.3355 | PDE Loss:  -4.3031 | Function Loss:  -2.385\n",
            "Total loss:  -2.3358 | PDE Loss:  -4.3017 | Function Loss:  -2.3855\n",
            "Total loss:  -2.3361 | PDE Loss:  -4.2997 | Function Loss:  -2.3861\n",
            "Total loss:  -2.3364 | PDE Loss:  -4.2994 | Function Loss:  -2.3865\n",
            "Total loss:  -2.3368 | PDE Loss:  -4.2983 | Function Loss:  -2.387\n",
            "Total loss:  -2.3372 | PDE Loss:  -4.3002 | Function Loss:  -2.3872\n",
            "Total loss:  -2.3375 | PDE Loss:  -4.3002 | Function Loss:  -2.3876\n",
            "Total loss:  -2.3378 | PDE Loss:  -4.3033 | Function Loss:  -2.3876\n",
            "Total loss:  -2.3381 | PDE Loss:  -4.304 | Function Loss:  -2.3878\n",
            "Total loss:  -2.3383 | PDE Loss:  -4.3069 | Function Loss:  -2.3877\n",
            "Total loss:  -2.3386 | PDE Loss:  -4.3078 | Function Loss:  -2.3879\n",
            "Total loss:  -2.339 | PDE Loss:  -4.3098 | Function Loss:  -2.3881\n",
            "Total loss:  -2.3394 | PDE Loss:  -4.312 | Function Loss:  -2.3883\n",
            "Total loss:  -2.3399 | PDE Loss:  -4.3109 | Function Loss:  -2.389\n",
            "Total loss:  -2.3403 | PDE Loss:  -4.3139 | Function Loss:  -2.3891\n",
            "Total loss:  -2.3406 | PDE Loss:  -4.3123 | Function Loss:  -2.3897\n",
            "Total loss:  -2.3409 | PDE Loss:  -4.3081 | Function Loss:  -2.3905\n",
            "Total loss:  -2.3413 | PDE Loss:  -4.3067 | Function Loss:  -2.3911\n",
            "Total loss:  -2.3415 | PDE Loss:  -4.3036 | Function Loss:  -2.3917\n",
            "Total loss:  -2.3417 | PDE Loss:  -4.3004 | Function Loss:  -2.3924\n",
            "Total loss:  -2.3421 | PDE Loss:  -4.2993 | Function Loss:  -2.3929\n",
            "Total loss:  -2.3424 | PDE Loss:  -4.2981 | Function Loss:  -2.3934\n",
            "Total loss:  -2.3428 | PDE Loss:  -4.2993 | Function Loss:  -2.3937\n",
            "Total loss:  -2.3432 | PDE Loss:  -4.3013 | Function Loss:  -2.3939\n",
            "Total loss:  -2.3437 | PDE Loss:  -4.305 | Function Loss:  -2.394\n",
            "Total loss:  -2.3442 | PDE Loss:  -4.3097 | Function Loss:  -2.394\n",
            "Total loss:  -2.3446 | PDE Loss:  -4.3124 | Function Loss:  -2.394\n",
            "Total loss:  -2.3453 | PDE Loss:  -4.3173 | Function Loss:  -2.3943\n",
            "Total loss:  -2.3459 | PDE Loss:  -4.3207 | Function Loss:  -2.3945\n",
            "Total loss:  -2.3464 | PDE Loss:  -4.3218 | Function Loss:  -2.3949\n",
            "Total loss:  -2.3468 | PDE Loss:  -4.3239 | Function Loss:  -2.3952\n",
            "Total loss:  -2.3471 | PDE Loss:  -4.3237 | Function Loss:  -2.3956\n",
            "Total loss:  -2.3475 | PDE Loss:  -4.3238 | Function Loss:  -2.3959\n",
            "Total loss:  -2.3479 | PDE Loss:  -4.3253 | Function Loss:  -2.3962\n",
            "Total loss:  -2.3482 | PDE Loss:  -4.3219 | Function Loss:  -2.397\n",
            "Total loss:  -2.3484 | PDE Loss:  -4.3262 | Function Loss:  -2.3967\n",
            "Total loss:  -2.3485 | PDE Loss:  -4.3268 | Function Loss:  -2.3968\n",
            "Total loss:  -2.3488 | PDE Loss:  -4.3292 | Function Loss:  -2.3967\n",
            "Total loss:  -2.349 | PDE Loss:  -4.3329 | Function Loss:  -2.3966\n",
            "Total loss:  -2.3494 | PDE Loss:  -4.3396 | Function Loss:  -2.3962\n",
            "Total loss:  -2.3496 | PDE Loss:  -4.3429 | Function Loss:  -2.3961\n",
            "Total loss:  -2.35 | PDE Loss:  -4.3471 | Function Loss:  -2.3961\n",
            "Total loss:  -2.3505 | PDE Loss:  -4.3497 | Function Loss:  -2.3963\n",
            "Total loss:  -2.3509 | PDE Loss:  -4.3521 | Function Loss:  -2.3965\n",
            "Total loss:  -2.3512 | PDE Loss:  -4.3525 | Function Loss:  -2.3968\n",
            "Total loss:  -2.3515 | PDE Loss:  -4.3532 | Function Loss:  -2.3971\n",
            "Total loss:  -2.3518 | PDE Loss:  -4.3514 | Function Loss:  -2.3976\n",
            "Total loss:  -2.352 | PDE Loss:  -4.3511 | Function Loss:  -2.3978\n",
            "Total loss:  -2.3523 | PDE Loss:  -4.35 | Function Loss:  -2.3984\n",
            "Total loss:  -2.3527 | PDE Loss:  -4.3493 | Function Loss:  -2.3989\n",
            "Total loss:  -2.3531 | PDE Loss:  -4.3488 | Function Loss:  -2.3993\n",
            "Total loss:  -2.3534 | PDE Loss:  -4.3492 | Function Loss:  -2.3996\n",
            "Total loss:  -2.3537 | PDE Loss:  -4.3496 | Function Loss:  -2.3999\n",
            "Total loss:  -2.354 | PDE Loss:  -4.3516 | Function Loss:  -2.4\n",
            "Total loss:  -2.3542 | PDE Loss:  -4.3523 | Function Loss:  -2.4002\n",
            "Total loss:  -2.3544 | PDE Loss:  -4.3544 | Function Loss:  -2.4001\n",
            "Total loss:  -2.3546 | PDE Loss:  -4.3549 | Function Loss:  -2.4003\n",
            "Total loss:  -2.3548 | PDE Loss:  -4.3562 | Function Loss:  -2.4004\n",
            "Total loss:  -2.3551 | PDE Loss:  -4.3567 | Function Loss:  -2.4007\n",
            "Total loss:  -2.3555 | PDE Loss:  -4.3552 | Function Loss:  -2.4013\n",
            "Total loss:  -2.3557 | PDE Loss:  -4.3548 | Function Loss:  -2.4016\n",
            "Total loss:  -2.3561 | PDE Loss:  -4.3529 | Function Loss:  -2.4022\n",
            "Total loss:  -2.3565 | PDE Loss:  -4.3513 | Function Loss:  -2.4029\n",
            "Total loss:  -2.3569 | PDE Loss:  -4.349 | Function Loss:  -2.4035\n",
            "Total loss:  -2.3571 | PDE Loss:  -4.3473 | Function Loss:  -2.4039\n",
            "Total loss:  -2.3573 | PDE Loss:  -4.3462 | Function Loss:  -2.4043\n",
            "Total loss:  -2.3576 | PDE Loss:  -4.3456 | Function Loss:  -2.4047\n",
            "Total loss:  -2.3579 | PDE Loss:  -4.345 | Function Loss:  -2.4051\n",
            "Total loss:  -2.3581 | PDE Loss:  -4.3456 | Function Loss:  -2.4053\n",
            "Total loss:  -2.3583 | PDE Loss:  -4.3456 | Function Loss:  -2.4055\n",
            "Total loss:  -2.3585 | PDE Loss:  -4.3467 | Function Loss:  -2.4056\n",
            "Total loss:  -2.3587 | PDE Loss:  -4.3477 | Function Loss:  -2.4057\n",
            "Total loss:  -2.3588 | PDE Loss:  -4.3472 | Function Loss:  -2.4058\n",
            "Total loss:  -2.3589 | PDE Loss:  -4.348 | Function Loss:  -2.4059\n",
            "Total loss:  -2.359 | PDE Loss:  -4.3465 | Function Loss:  -2.4061\n",
            "Total loss:  -2.359 | PDE Loss:  -4.3461 | Function Loss:  -2.4063\n",
            "Total loss:  -2.3592 | PDE Loss:  -4.3445 | Function Loss:  -2.4066\n",
            "Total loss:  -2.3593 | PDE Loss:  -4.343 | Function Loss:  -2.4069\n",
            "Total loss:  -2.3595 | PDE Loss:  -4.3399 | Function Loss:  -2.4075\n",
            "Total loss:  -2.3596 | PDE Loss:  -4.3391 | Function Loss:  -2.4077\n",
            "Total loss:  -2.3598 | PDE Loss:  -4.3384 | Function Loss:  -2.408\n",
            "Total loss:  -2.36 | PDE Loss:  -4.3385 | Function Loss:  -2.4082\n",
            "Total loss:  -2.3601 | PDE Loss:  -4.3386 | Function Loss:  -2.4084\n",
            "Total loss:  -2.3603 | PDE Loss:  -4.3394 | Function Loss:  -2.4084\n",
            "Total loss:  -2.3605 | PDE Loss:  -4.3395 | Function Loss:  -2.4086\n",
            "Total loss:  -2.3606 | PDE Loss:  -4.3401 | Function Loss:  -2.4087\n",
            "Total loss:  -2.3608 | PDE Loss:  -4.3404 | Function Loss:  -2.4089\n",
            "Total loss:  -2.3609 | PDE Loss:  -4.3408 | Function Loss:  -2.409\n",
            "Total loss:  -2.3611 | PDE Loss:  -4.3418 | Function Loss:  -2.409\n",
            "Total loss:  -2.3612 | PDE Loss:  -4.3426 | Function Loss:  -2.4091\n",
            "Total loss:  -2.3614 | PDE Loss:  -4.3437 | Function Loss:  -2.4092\n",
            "Total loss:  -2.3616 | PDE Loss:  -4.3449 | Function Loss:  -2.4093\n",
            "Total loss:  -2.3619 | PDE Loss:  -4.3465 | Function Loss:  -2.4094\n",
            "Total loss:  -2.3623 | PDE Loss:  -4.3487 | Function Loss:  -2.4096\n",
            "Total loss:  -2.3626 | PDE Loss:  -4.3492 | Function Loss:  -2.4099\n",
            "Total loss:  -2.363 | PDE Loss:  -4.3503 | Function Loss:  -2.4101\n",
            "Total loss:  -2.3637 | PDE Loss:  -4.3528 | Function Loss:  -2.4107\n",
            "Total loss:  -2.3646 | PDE Loss:  -4.355 | Function Loss:  -2.4115\n",
            "Total loss:  -2.3655 | PDE Loss:  -4.3566 | Function Loss:  -2.4123\n",
            "Total loss:  -2.3665 | PDE Loss:  -4.3578 | Function Loss:  -2.4133\n",
            "Total loss:  -2.3677 | PDE Loss:  -4.3605 | Function Loss:  -2.4142\n",
            "Total loss:  -2.3688 | PDE Loss:  -4.3595 | Function Loss:  -2.4156\n",
            "Total loss:  -2.3696 | PDE Loss:  -4.3623 | Function Loss:  -2.4162\n",
            "Total loss:  -2.3703 | PDE Loss:  -4.3641 | Function Loss:  -2.4168\n",
            "Total loss:  -2.371 | PDE Loss:  -4.3697 | Function Loss:  -2.4169\n",
            "Total loss:  -2.3713 | PDE Loss:  -4.3713 | Function Loss:  -2.4171\n",
            "Total loss:  -2.3716 | PDE Loss:  -4.3735 | Function Loss:  -2.4172\n",
            "Total loss:  -2.3719 | PDE Loss:  -4.3754 | Function Loss:  -2.4173\n",
            "Total loss:  -2.3724 | PDE Loss:  -4.3784 | Function Loss:  -2.4175\n",
            "Total loss:  -2.3729 | PDE Loss:  -4.3815 | Function Loss:  -2.4178\n",
            "Total loss:  -2.3734 | PDE Loss:  -4.3857 | Function Loss:  -2.4178\n",
            "Total loss:  -2.3739 | PDE Loss:  -4.391 | Function Loss:  -2.4178\n",
            "Total loss:  -2.3743 | PDE Loss:  -4.3947 | Function Loss:  -2.4178\n",
            "Total loss:  -2.3748 | PDE Loss:  -4.3959 | Function Loss:  -2.4183\n",
            "Total loss:  -2.3752 | PDE Loss:  -4.4007 | Function Loss:  -2.4183\n",
            "Total loss:  -2.3757 | PDE Loss:  -4.4001 | Function Loss:  -2.4188\n",
            "Total loss:  -2.3762 | PDE Loss:  -4.4005 | Function Loss:  -2.4194\n",
            "Total loss:  -2.3769 | PDE Loss:  -4.3995 | Function Loss:  -2.4202\n",
            "Total loss:  -2.3775 | PDE Loss:  -4.3989 | Function Loss:  -2.4209\n",
            "Total loss:  -2.378 | PDE Loss:  -4.3982 | Function Loss:  -2.4215\n",
            "Total loss:  -2.3785 | PDE Loss:  -4.398 | Function Loss:  -2.4221\n",
            "Total loss:  -2.3791 | PDE Loss:  -4.3991 | Function Loss:  -2.4227\n",
            "Total loss:  -2.3797 | PDE Loss:  -4.3989 | Function Loss:  -2.4234\n",
            "Total loss:  -2.3803 | PDE Loss:  -4.4001 | Function Loss:  -2.424\n",
            "Total loss:  -2.3809 | PDE Loss:  -4.4006 | Function Loss:  -2.4245\n",
            "Total loss:  -2.3817 | PDE Loss:  -4.4018 | Function Loss:  -2.4253\n",
            "Total loss:  -2.3827 | PDE Loss:  -4.4015 | Function Loss:  -2.4264\n",
            "Total loss:  -2.3837 | PDE Loss:  -4.4047 | Function Loss:  -2.4271\n",
            "Total loss:  -2.3845 | PDE Loss:  -4.4024 | Function Loss:  -2.4283\n",
            "Total loss:  -2.3852 | PDE Loss:  -4.4022 | Function Loss:  -2.4291\n",
            "Total loss:  -2.3857 | PDE Loss:  -4.401 | Function Loss:  -2.4298\n",
            "Total loss:  -2.3864 | PDE Loss:  -4.3995 | Function Loss:  -2.4307\n",
            "Total loss:  -2.3869 | PDE Loss:  -4.3984 | Function Loss:  -2.4314\n",
            "Total loss:  -2.3875 | PDE Loss:  -4.3978 | Function Loss:  -2.4321\n",
            "Total loss:  -2.388 | PDE Loss:  -4.3964 | Function Loss:  -2.4328\n",
            "Total loss:  -2.3884 | PDE Loss:  -4.3971 | Function Loss:  -2.4332\n",
            "Total loss:  -2.3888 | PDE Loss:  -4.3974 | Function Loss:  -2.4336\n",
            "Total loss:  -2.3891 | PDE Loss:  -4.399 | Function Loss:  -2.4338\n",
            "Total loss:  -2.3896 | PDE Loss:  -4.4016 | Function Loss:  -2.434\n",
            "Total loss:  -2.3901 | PDE Loss:  -4.4033 | Function Loss:  -2.4344\n",
            "Total loss:  -2.3905 | PDE Loss:  -4.4058 | Function Loss:  -2.4346\n",
            "Total loss:  -2.3909 | PDE Loss:  -4.4093 | Function Loss:  -2.4347\n",
            "Total loss:  -2.3915 | PDE Loss:  -4.412 | Function Loss:  -2.435\n",
            "Total loss:  -2.3918 | PDE Loss:  -4.414 | Function Loss:  -2.4352\n",
            "Total loss:  -2.3921 | PDE Loss:  -4.4154 | Function Loss:  -2.4353\n",
            "Total loss:  -2.3923 | PDE Loss:  -4.4149 | Function Loss:  -2.4357\n",
            "Total loss:  -2.3926 | PDE Loss:  -4.4161 | Function Loss:  -2.4359\n",
            "Total loss:  -2.393 | PDE Loss:  -4.4106 | Function Loss:  -2.4368\n",
            "Total loss:  -2.3932 | PDE Loss:  -4.4117 | Function Loss:  -2.437\n",
            "Total loss:  -2.3936 | PDE Loss:  -4.4126 | Function Loss:  -2.4373\n",
            "Total loss:  -2.394 | PDE Loss:  -4.4132 | Function Loss:  -2.4377\n",
            "Total loss:  -2.3944 | PDE Loss:  -4.4133 | Function Loss:  -2.4381\n",
            "Total loss:  -2.3948 | PDE Loss:  -4.4128 | Function Loss:  -2.4386\n",
            "Total loss:  -2.3952 | PDE Loss:  -4.412 | Function Loss:  -2.4391\n",
            "Total loss:  -2.3957 | PDE Loss:  -4.4098 | Function Loss:  -2.4399\n",
            "Total loss:  -2.396 | PDE Loss:  -4.4087 | Function Loss:  -2.4404\n",
            "Total loss:  -2.3963 | PDE Loss:  -4.4074 | Function Loss:  -2.4408\n",
            "Total loss:  -2.3966 | PDE Loss:  -4.4062 | Function Loss:  -2.4413\n",
            "Total loss:  -2.3969 | PDE Loss:  -4.4069 | Function Loss:  -2.4416\n",
            "Total loss:  -2.3974 | PDE Loss:  -4.4067 | Function Loss:  -2.4421\n",
            "Total loss:  -2.3978 | PDE Loss:  -4.4081 | Function Loss:  -2.4424\n",
            "Total loss:  -2.3982 | PDE Loss:  -4.4103 | Function Loss:  -2.4426\n",
            "Total loss:  -2.3987 | PDE Loss:  -4.411 | Function Loss:  -2.4431\n",
            "Total loss:  -2.399 | PDE Loss:  -4.4123 | Function Loss:  -2.4433\n",
            "Total loss:  -2.3992 | PDE Loss:  -4.4114 | Function Loss:  -2.4437\n",
            "Total loss:  -2.3994 | PDE Loss:  -4.4107 | Function Loss:  -2.444\n",
            "Total loss:  -2.3997 | PDE Loss:  -4.4095 | Function Loss:  -2.4444\n",
            "Total loss:  -2.4 | PDE Loss:  -4.4068 | Function Loss:  -2.445\n",
            "Total loss:  -2.4003 | PDE Loss:  -4.4053 | Function Loss:  -2.4455\n",
            "Total loss:  -2.4006 | PDE Loss:  -4.4025 | Function Loss:  -2.4461\n",
            "Total loss:  -2.4009 | PDE Loss:  -4.4008 | Function Loss:  -2.4467\n",
            "Total loss:  -2.4013 | PDE Loss:  -4.3992 | Function Loss:  -2.4473\n",
            "Total loss:  -2.4016 | PDE Loss:  -4.3954 | Function Loss:  -2.4481\n",
            "Total loss:  -2.402 | PDE Loss:  -4.3943 | Function Loss:  -2.4486\n",
            "Total loss:  -2.4022 | PDE Loss:  -4.3942 | Function Loss:  -2.4489\n",
            "Total loss:  -2.4027 | PDE Loss:  -4.3942 | Function Loss:  -2.4494\n",
            "Total loss:  -2.4031 | PDE Loss:  -4.3942 | Function Loss:  -2.4499\n",
            "Total loss:  -2.4034 | PDE Loss:  -4.3934 | Function Loss:  -2.4503\n",
            "Total loss:  -2.4036 | PDE Loss:  -4.3935 | Function Loss:  -2.4505\n",
            "Total loss:  -2.4038 | PDE Loss:  -4.3931 | Function Loss:  -2.4507\n",
            "Total loss:  -2.404 | PDE Loss:  -4.3928 | Function Loss:  -2.451\n",
            "Total loss:  -2.4043 | PDE Loss:  -4.3925 | Function Loss:  -2.4514\n",
            "Total loss:  -2.4048 | PDE Loss:  -4.3925 | Function Loss:  -2.4519\n",
            "Total loss:  -2.4054 | PDE Loss:  -4.3954 | Function Loss:  -2.4522\n",
            "Total loss:  -2.4058 | PDE Loss:  -4.3942 | Function Loss:  -2.4529\n",
            "Total loss:  -2.4063 | PDE Loss:  -4.3973 | Function Loss:  -2.453\n",
            "Total loss:  -2.4066 | PDE Loss:  -4.3958 | Function Loss:  -2.4536\n",
            "Total loss:  -2.4069 | PDE Loss:  -4.395 | Function Loss:  -2.454\n",
            "Total loss:  -2.4071 | PDE Loss:  -4.3954 | Function Loss:  -2.4542\n",
            "Total loss:  -2.4073 | PDE Loss:  -4.3939 | Function Loss:  -2.4546\n",
            "Total loss:  -2.4077 | PDE Loss:  -4.3924 | Function Loss:  -2.4552\n",
            "Total loss:  -2.408 | PDE Loss:  -4.3907 | Function Loss:  -2.4558\n",
            "Total loss:  -2.4083 | PDE Loss:  -4.3849 | Function Loss:  -2.4567\n",
            "Total loss:  -2.4086 | PDE Loss:  -4.3855 | Function Loss:  -2.457\n",
            "Total loss:  -2.4089 | PDE Loss:  -4.3874 | Function Loss:  -2.4571\n",
            "Total loss:  -2.4092 | PDE Loss:  -4.3901 | Function Loss:  -2.4572\n",
            "Total loss:  -2.4096 | PDE Loss:  -4.3932 | Function Loss:  -2.4572\n",
            "Total loss:  -2.4099 | PDE Loss:  -4.3966 | Function Loss:  -2.4572\n",
            "Total loss:  -2.4102 | PDE Loss:  -4.3997 | Function Loss:  -2.4571\n",
            "Total loss:  -2.4106 | PDE Loss:  -4.4044 | Function Loss:  -2.4571\n",
            "Total loss:  -2.411 | PDE Loss:  -4.4077 | Function Loss:  -2.4571\n",
            "Total loss:  -2.4113 | PDE Loss:  -4.4115 | Function Loss:  -2.4571\n",
            "Total loss:  -2.4117 | PDE Loss:  -4.4144 | Function Loss:  -2.4571\n",
            "Total loss:  -2.412 | PDE Loss:  -4.4167 | Function Loss:  -2.4573\n",
            "Total loss:  -2.4125 | PDE Loss:  -4.4234 | Function Loss:  -2.457\n",
            "Total loss:  -2.4127 | PDE Loss:  -4.4236 | Function Loss:  -2.4573\n",
            "Total loss:  -2.4133 | PDE Loss:  -4.4234 | Function Loss:  -2.4579\n",
            "Total loss:  -2.4138 | PDE Loss:  -4.424 | Function Loss:  -2.4584\n",
            "Total loss:  -2.4142 | PDE Loss:  -4.4239 | Function Loss:  -2.4589\n",
            "Total loss:  -2.4146 | PDE Loss:  -4.4255 | Function Loss:  -2.4591\n",
            "Total loss:  -2.4149 | PDE Loss:  -4.4253 | Function Loss:  -2.4595\n",
            "Total loss:  -2.4152 | PDE Loss:  -4.425 | Function Loss:  -2.4599\n",
            "Total loss:  -2.4155 | PDE Loss:  -4.4241 | Function Loss:  -2.4603\n",
            "Total loss:  -2.4157 | PDE Loss:  -4.4221 | Function Loss:  -2.4608\n",
            "Total loss:  -2.416 | PDE Loss:  -4.4201 | Function Loss:  -2.4613\n",
            "Total loss:  -2.4163 | PDE Loss:  -4.4168 | Function Loss:  -2.462\n",
            "Total loss:  -2.4167 | PDE Loss:  -4.4132 | Function Loss:  -2.4629\n",
            "Total loss:  -2.4169 | PDE Loss:  -4.4006 | Function Loss:  -2.4645\n",
            "Total loss:  -2.4174 | PDE Loss:  -4.4007 | Function Loss:  -2.465\n",
            "Total loss:  -2.4178 | PDE Loss:  -4.4014 | Function Loss:  -2.4654\n",
            "Total loss:  -2.4181 | PDE Loss:  -4.401 | Function Loss:  -2.4658\n",
            "Total loss:  -2.4184 | PDE Loss:  -4.4001 | Function Loss:  -2.4662\n",
            "Total loss:  -2.4187 | PDE Loss:  -4.3984 | Function Loss:  -2.4668\n",
            "Total loss:  -2.4189 | PDE Loss:  -4.3923 | Function Loss:  -2.4677\n",
            "Total loss:  -2.4192 | PDE Loss:  -4.393 | Function Loss:  -2.468\n",
            "Total loss:  -2.4195 | PDE Loss:  -4.3928 | Function Loss:  -2.4684\n",
            "Total loss:  -2.4198 | PDE Loss:  -4.3913 | Function Loss:  -2.4689\n",
            "Total loss:  -2.4201 | PDE Loss:  -4.3902 | Function Loss:  -2.4693\n",
            "Total loss:  -2.4203 | PDE Loss:  -4.3885 | Function Loss:  -2.4697\n",
            "Total loss:  -2.4205 | PDE Loss:  -4.3867 | Function Loss:  -2.4702\n",
            "Total loss:  -2.4207 | PDE Loss:  -4.3855 | Function Loss:  -2.4706\n",
            "Total loss:  -2.4209 | PDE Loss:  -4.3836 | Function Loss:  -2.4711\n",
            "Total loss:  -2.4212 | PDE Loss:  -4.383 | Function Loss:  -2.4715\n",
            "Total loss:  -2.4216 | PDE Loss:  -4.3825 | Function Loss:  -2.472\n",
            "Total loss:  -2.4222 | PDE Loss:  -4.3815 | Function Loss:  -2.4727\n",
            "Total loss:  -2.4226 | PDE Loss:  -4.3814 | Function Loss:  -2.4732\n",
            "Total loss:  -2.4231 | PDE Loss:  -4.3808 | Function Loss:  -2.4738\n",
            "Total loss:  -2.4237 | PDE Loss:  -4.3805 | Function Loss:  -2.4745\n",
            "Total loss:  -2.4245 | PDE Loss:  -4.38 | Function Loss:  -2.4755\n",
            "Total loss:  -2.4255 | PDE Loss:  -4.3781 | Function Loss:  -2.4768\n",
            "Total loss:  -2.4265 | PDE Loss:  -4.3759 | Function Loss:  -2.4782\n",
            "Total loss:  -2.4274 | PDE Loss:  -4.3705 | Function Loss:  -2.4799\n",
            "Total loss:  -2.4282 | PDE Loss:  -4.3681 | Function Loss:  -2.4812\n",
            "Total loss:  -2.4286 | PDE Loss:  -4.3645 | Function Loss:  -2.4821\n",
            "Total loss:  -2.4291 | PDE Loss:  -4.3648 | Function Loss:  -2.4827\n",
            "Total loss:  -2.4297 | PDE Loss:  -4.363 | Function Loss:  -2.4835\n",
            "Total loss:  -2.4303 | PDE Loss:  -4.3658 | Function Loss:  -2.4839\n",
            "Total loss:  -2.4311 | PDE Loss:  -4.3626 | Function Loss:  -2.4851\n",
            "Total loss:  -2.4318 | PDE Loss:  -4.3682 | Function Loss:  -2.4852\n",
            "Total loss:  -2.4326 | PDE Loss:  -4.374 | Function Loss:  -2.4853\n",
            "Total loss:  -2.4336 | PDE Loss:  -4.3843 | Function Loss:  -2.4852\n",
            "Total loss:  -2.4344 | PDE Loss:  -4.3899 | Function Loss:  -2.4854\n",
            "Total loss:  -2.4351 | PDE Loss:  -4.3962 | Function Loss:  -2.4854\n",
            "Total loss:  -2.436 | PDE Loss:  -4.4003 | Function Loss:  -2.486\n",
            "Total loss:  -2.4371 | PDE Loss:  -4.4021 | Function Loss:  -2.4869\n",
            "Total loss:  -2.4379 | PDE Loss:  -4.4024 | Function Loss:  -2.4878\n",
            "Total loss:  -2.4387 | PDE Loss:  -4.4012 | Function Loss:  -2.4888\n",
            "Total loss:  -2.4393 | PDE Loss:  -4.3965 | Function Loss:  -2.49\n",
            "Total loss:  -2.4399 | PDE Loss:  -4.3927 | Function Loss:  -2.4912\n",
            "Total loss:  -2.4407 | PDE Loss:  -4.3893 | Function Loss:  -2.4925\n",
            "Total loss:  -2.4399 | PDE Loss:  -4.3624 | Function Loss:  -2.4952\n",
            "Total loss:  -2.4412 | PDE Loss:  -4.3808 | Function Loss:  -2.4942\n",
            "Total loss:  -2.4421 | PDE Loss:  -4.3813 | Function Loss:  -2.4952\n",
            "Total loss:  -2.443 | PDE Loss:  -4.3814 | Function Loss:  -2.4961\n",
            "Total loss:  -2.4437 | PDE Loss:  -4.3854 | Function Loss:  -2.4964\n",
            "Total loss:  -2.4442 | PDE Loss:  -4.3871 | Function Loss:  -2.4968\n",
            "Total loss:  -2.4445 | PDE Loss:  -4.3887 | Function Loss:  -2.4969\n",
            "Total loss:  -2.4448 | PDE Loss:  -4.3919 | Function Loss:  -2.4969\n",
            "Total loss:  -2.4452 | PDE Loss:  -4.3924 | Function Loss:  -2.4973\n",
            "Total loss:  -2.4456 | PDE Loss:  -4.3941 | Function Loss:  -2.4974\n",
            "Total loss:  -2.4459 | PDE Loss:  -4.3945 | Function Loss:  -2.4977\n",
            "Total loss:  -2.4461 | PDE Loss:  -4.3925 | Function Loss:  -2.4982\n",
            "Total loss:  -2.4463 | PDE Loss:  -4.3916 | Function Loss:  -2.4985\n",
            "Total loss:  -2.4464 | PDE Loss:  -4.3902 | Function Loss:  -2.4989\n",
            "Total loss:  -2.4466 | PDE Loss:  -4.3896 | Function Loss:  -2.4992\n",
            "Total loss:  -2.4469 | PDE Loss:  -4.3881 | Function Loss:  -2.4997\n",
            "Total loss:  -2.4472 | PDE Loss:  -4.3879 | Function Loss:  -2.5001\n",
            "Total loss:  -2.4476 | PDE Loss:  -4.387 | Function Loss:  -2.5006\n",
            "Total loss:  -2.4479 | PDE Loss:  -4.3879 | Function Loss:  -2.5008\n",
            "Total loss:  -2.4482 | PDE Loss:  -4.3891 | Function Loss:  -2.501\n",
            "Total loss:  -2.4486 | PDE Loss:  -4.3904 | Function Loss:  -2.5013\n",
            "Total loss:  -2.449 | PDE Loss:  -4.3933 | Function Loss:  -2.5015\n",
            "Total loss:  -2.4495 | PDE Loss:  -4.398 | Function Loss:  -2.5013\n",
            "Total loss:  -2.4499 | PDE Loss:  -4.401 | Function Loss:  -2.5015\n",
            "Total loss:  -2.4504 | PDE Loss:  -4.405 | Function Loss:  -2.5016\n",
            "Total loss:  -2.451 | PDE Loss:  -4.4085 | Function Loss:  -2.5018\n",
            "Total loss:  -2.4514 | PDE Loss:  -4.4106 | Function Loss:  -2.502\n",
            "Total loss:  -2.4518 | PDE Loss:  -4.4107 | Function Loss:  -2.5024\n",
            "Total loss:  -2.4522 | PDE Loss:  -4.4125 | Function Loss:  -2.5026\n",
            "Total loss:  -2.4525 | PDE Loss:  -4.4118 | Function Loss:  -2.503\n",
            "Total loss:  -2.4529 | PDE Loss:  -4.4133 | Function Loss:  -2.5033\n",
            "Total loss:  -2.4532 | PDE Loss:  -4.4132 | Function Loss:  -2.5036\n",
            "Total loss:  -2.4534 | PDE Loss:  -4.4146 | Function Loss:  -2.5037\n",
            "Total loss:  -2.4536 | PDE Loss:  -4.415 | Function Loss:  -2.5038\n",
            "Total loss:  -2.4538 | PDE Loss:  -4.4159 | Function Loss:  -2.504\n",
            "Total loss:  -2.4542 | PDE Loss:  -4.4162 | Function Loss:  -2.5043\n",
            "Total loss:  -2.4545 | PDE Loss:  -4.415 | Function Loss:  -2.5049\n",
            "Total loss:  -2.4549 | PDE Loss:  -4.4135 | Function Loss:  -2.5055\n",
            "Total loss:  -2.4552 | PDE Loss:  -4.4103 | Function Loss:  -2.5063\n",
            "Total loss:  -2.4555 | PDE Loss:  -4.4082 | Function Loss:  -2.5068\n",
            "Total loss:  -2.4557 | PDE Loss:  -4.4046 | Function Loss:  -2.5075\n",
            "Total loss:  -2.4559 | PDE Loss:  -4.403 | Function Loss:  -2.5079\n",
            "Total loss:  -2.456 | PDE Loss:  -4.4006 | Function Loss:  -2.5084\n",
            "Total loss:  -2.4562 | PDE Loss:  -4.3995 | Function Loss:  -2.5087\n",
            "Total loss:  -2.4563 | PDE Loss:  -4.3981 | Function Loss:  -2.5091\n",
            "Total loss:  -2.4565 | PDE Loss:  -4.3973 | Function Loss:  -2.5094\n",
            "Total loss:  -2.4567 | PDE Loss:  -4.3965 | Function Loss:  -2.5097\n",
            "Total loss:  -2.4569 | PDE Loss:  -4.3964 | Function Loss:  -2.5099\n",
            "Total loss:  -2.4571 | PDE Loss:  -4.3973 | Function Loss:  -2.5101\n",
            "Total loss:  -2.4574 | PDE Loss:  -4.3987 | Function Loss:  -2.5102\n",
            "Total loss:  -2.4577 | PDE Loss:  -4.3992 | Function Loss:  -2.5105\n",
            "Total loss:  -2.4579 | PDE Loss:  -4.3996 | Function Loss:  -2.5107\n",
            "Total loss:  -2.4583 | PDE Loss:  -4.4008 | Function Loss:  -2.5109\n",
            "Total loss:  -2.4587 | PDE Loss:  -4.3976 | Function Loss:  -2.5118\n",
            "Total loss:  -2.4591 | PDE Loss:  -4.3971 | Function Loss:  -2.5123\n",
            "Total loss:  -2.4595 | PDE Loss:  -4.3942 | Function Loss:  -2.5131\n",
            "Total loss:  -2.4598 | PDE Loss:  -4.3923 | Function Loss:  -2.5138\n",
            "Total loss:  -2.4601 | PDE Loss:  -4.3896 | Function Loss:  -2.5145\n",
            "Total loss:  -2.4603 | PDE Loss:  -4.3884 | Function Loss:  -2.5149\n",
            "Total loss:  -2.4605 | PDE Loss:  -4.3878 | Function Loss:  -2.5152\n",
            "Total loss:  -2.4607 | PDE Loss:  -4.3878 | Function Loss:  -2.5153\n",
            "Total loss:  -2.4609 | PDE Loss:  -4.3886 | Function Loss:  -2.5155\n",
            "Total loss:  -2.4611 | PDE Loss:  -4.389 | Function Loss:  -2.5157\n",
            "Total loss:  -2.4614 | PDE Loss:  -4.3896 | Function Loss:  -2.5159\n",
            "Total loss:  -2.4617 | PDE Loss:  -4.3899 | Function Loss:  -2.5162\n",
            "Total loss:  -2.462 | PDE Loss:  -4.3893 | Function Loss:  -2.5167\n",
            "Total loss:  -2.4625 | PDE Loss:  -4.388 | Function Loss:  -2.5173\n",
            "Total loss:  -2.4629 | PDE Loss:  -4.386 | Function Loss:  -2.5181\n",
            "Total loss:  -2.4632 | PDE Loss:  -4.3837 | Function Loss:  -2.5188\n",
            "Total loss:  -2.4635 | PDE Loss:  -4.3817 | Function Loss:  -2.5194\n",
            "Total loss:  -2.4638 | PDE Loss:  -4.379 | Function Loss:  -2.52\n",
            "Total loss:  -2.464 | PDE Loss:  -4.3774 | Function Loss:  -2.5205\n",
            "Total loss:  -2.4641 | PDE Loss:  -4.3764 | Function Loss:  -2.5208\n",
            "Total loss:  -2.4643 | PDE Loss:  -4.3751 | Function Loss:  -2.5212\n",
            "Total loss:  -2.4644 | PDE Loss:  -4.3758 | Function Loss:  -2.5213\n",
            "Total loss:  -2.4646 | PDE Loss:  -4.3757 | Function Loss:  -2.5214\n",
            "Total loss:  -2.4647 | PDE Loss:  -4.3765 | Function Loss:  -2.5215\n",
            "Total loss:  -2.4649 | PDE Loss:  -4.3775 | Function Loss:  -2.5215\n",
            "Total loss:  -2.4651 | PDE Loss:  -4.3787 | Function Loss:  -2.5217\n",
            "Total loss:  -2.4653 | PDE Loss:  -4.3777 | Function Loss:  -2.5219\n",
            "Total loss:  -2.4657 | PDE Loss:  -4.3797 | Function Loss:  -2.5221\n",
            "Total loss:  -2.4661 | PDE Loss:  -4.3817 | Function Loss:  -2.5223\n",
            "Total loss:  -2.4664 | PDE Loss:  -4.3834 | Function Loss:  -2.5225\n",
            "Total loss:  -2.4668 | PDE Loss:  -4.3842 | Function Loss:  -2.5228\n",
            "Total loss:  -2.4673 | PDE Loss:  -4.3864 | Function Loss:  -2.5231\n",
            "Total loss:  -2.4679 | PDE Loss:  -4.3863 | Function Loss:  -2.5238\n",
            "Total loss:  -2.4686 | PDE Loss:  -4.3892 | Function Loss:  -2.5242\n",
            "Total loss:  -2.4693 | PDE Loss:  -4.3884 | Function Loss:  -2.5251\n",
            "Total loss:  -2.4699 | PDE Loss:  -4.3874 | Function Loss:  -2.5259\n",
            "Total loss:  -2.4708 | PDE Loss:  -4.3908 | Function Loss:  -2.5264\n",
            "Total loss:  -2.4713 | PDE Loss:  -4.3933 | Function Loss:  -2.5266\n",
            "Total loss:  -2.4719 | PDE Loss:  -4.3941 | Function Loss:  -2.5273\n",
            "Total loss:  -2.4724 | PDE Loss:  -4.396 | Function Loss:  -2.5276\n",
            "Total loss:  -2.4729 | PDE Loss:  -4.3971 | Function Loss:  -2.528\n",
            "Total loss:  -2.4733 | PDE Loss:  -4.3979 | Function Loss:  -2.5283\n",
            "Total loss:  -2.4738 | PDE Loss:  -4.4021 | Function Loss:  -2.5282\n",
            "Total loss:  -2.4743 | PDE Loss:  -4.4044 | Function Loss:  -2.5285\n",
            "Total loss:  -2.4746 | PDE Loss:  -4.4068 | Function Loss:  -2.5286\n",
            "Total loss:  -2.4751 | PDE Loss:  -4.41 | Function Loss:  -2.5287\n",
            "Total loss:  -2.4756 | PDE Loss:  -4.4136 | Function Loss:  -2.5288\n",
            "Total loss:  -2.476 | PDE Loss:  -4.4174 | Function Loss:  -2.5288\n",
            "Total loss:  -2.4765 | PDE Loss:  -4.4212 | Function Loss:  -2.5288\n",
            "Total loss:  -2.4769 | PDE Loss:  -4.4255 | Function Loss:  -2.5287\n",
            "Total loss:  -2.4772 | PDE Loss:  -4.4281 | Function Loss:  -2.5287\n",
            "Total loss:  -2.4775 | PDE Loss:  -4.4295 | Function Loss:  -2.5289\n",
            "Total loss:  -2.4778 | PDE Loss:  -4.4307 | Function Loss:  -2.5291\n",
            "Total loss:  -2.4781 | PDE Loss:  -4.431 | Function Loss:  -2.5294\n",
            "Total loss:  -2.4783 | PDE Loss:  -4.4314 | Function Loss:  -2.5296\n",
            "Total loss:  -2.4787 | PDE Loss:  -4.4317 | Function Loss:  -2.53\n",
            "Total loss:  -2.4791 | PDE Loss:  -4.4319 | Function Loss:  -2.5304\n",
            "Total loss:  -2.4795 | PDE Loss:  -4.4329 | Function Loss:  -2.5308\n",
            "Total loss:  -2.48 | PDE Loss:  -4.432 | Function Loss:  -2.5315\n",
            "Total loss:  -2.4805 | PDE Loss:  -4.4319 | Function Loss:  -2.5321\n",
            "Total loss:  -2.481 | PDE Loss:  -4.4305 | Function Loss:  -2.5327\n",
            "Total loss:  -2.4814 | PDE Loss:  -4.4291 | Function Loss:  -2.5333\n",
            "Total loss:  -2.4817 | PDE Loss:  -4.4283 | Function Loss:  -2.5338\n",
            "Total loss:  -2.4821 | PDE Loss:  -4.4275 | Function Loss:  -2.5344\n",
            "Total loss:  -2.4825 | PDE Loss:  -4.4271 | Function Loss:  -2.5349\n",
            "Total loss:  -2.4829 | PDE Loss:  -4.4259 | Function Loss:  -2.5355\n",
            "Total loss:  -2.4833 | PDE Loss:  -4.4267 | Function Loss:  -2.5358\n",
            "Total loss:  -2.4836 | PDE Loss:  -4.4264 | Function Loss:  -2.5362\n",
            "Total loss:  -2.4838 | PDE Loss:  -4.427 | Function Loss:  -2.5364\n",
            "Total loss:  -2.4841 | PDE Loss:  -4.4271 | Function Loss:  -2.5366\n",
            "Total loss:  -2.4844 | PDE Loss:  -4.4274 | Function Loss:  -2.537\n",
            "Total loss:  -2.4847 | PDE Loss:  -4.428 | Function Loss:  -2.5372\n",
            "Total loss:  -2.485 | PDE Loss:  -4.4278 | Function Loss:  -2.5376\n",
            "Total loss:  -2.4854 | PDE Loss:  -4.4281 | Function Loss:  -2.538\n",
            "Total loss:  -2.4858 | PDE Loss:  -4.4275 | Function Loss:  -2.5385\n",
            "Total loss:  -2.4864 | PDE Loss:  -4.4279 | Function Loss:  -2.5391\n",
            "Total loss:  -2.4869 | PDE Loss:  -4.4266 | Function Loss:  -2.5399\n",
            "Total loss:  -2.4874 | PDE Loss:  -4.4298 | Function Loss:  -2.5401\n",
            "Total loss:  -2.4878 | PDE Loss:  -4.4281 | Function Loss:  -2.5407\n",
            "Total loss:  -2.4882 | PDE Loss:  -4.4267 | Function Loss:  -2.5413\n",
            "Total loss:  -2.4885 | PDE Loss:  -4.4246 | Function Loss:  -2.542\n",
            "Total loss:  -2.4889 | PDE Loss:  -4.4242 | Function Loss:  -2.5424\n",
            "Total loss:  -2.4891 | PDE Loss:  -4.4207 | Function Loss:  -2.5432\n",
            "Total loss:  -2.4894 | PDE Loss:  -4.4224 | Function Loss:  -2.5432\n",
            "Total loss:  -2.4896 | PDE Loss:  -4.424 | Function Loss:  -2.5433\n",
            "Total loss:  -2.49 | PDE Loss:  -4.4282 | Function Loss:  -2.5432\n",
            "Total loss:  -2.4904 | PDE Loss:  -4.431 | Function Loss:  -2.5433\n",
            "Total loss:  -2.4908 | PDE Loss:  -4.4342 | Function Loss:  -2.5433\n",
            "Total loss:  -2.4911 | PDE Loss:  -4.4365 | Function Loss:  -2.5434\n",
            "Total loss:  -2.4914 | PDE Loss:  -4.4391 | Function Loss:  -2.5434\n",
            "Total loss:  -2.4918 | PDE Loss:  -4.4415 | Function Loss:  -2.5435\n",
            "Total loss:  -2.4921 | PDE Loss:  -4.4436 | Function Loss:  -2.5436\n",
            "Total loss:  -2.4925 | PDE Loss:  -4.4462 | Function Loss:  -2.5437\n",
            "Total loss:  -2.4929 | PDE Loss:  -4.4479 | Function Loss:  -2.544\n",
            "Total loss:  -2.4933 | PDE Loss:  -4.4487 | Function Loss:  -2.5443\n",
            "Total loss:  -2.4937 | PDE Loss:  -4.4499 | Function Loss:  -2.5446\n",
            "Total loss:  -2.4942 | PDE Loss:  -4.4504 | Function Loss:  -2.5451\n",
            "Total loss:  -2.4948 | PDE Loss:  -4.4519 | Function Loss:  -2.5456\n",
            "Total loss:  -2.4953 | PDE Loss:  -4.4516 | Function Loss:  -2.5462\n",
            "Total loss:  -2.4957 | PDE Loss:  -4.4532 | Function Loss:  -2.5465\n",
            "Total loss:  -2.4961 | PDE Loss:  -4.4547 | Function Loss:  -2.5467\n",
            "Total loss:  -2.4965 | PDE Loss:  -4.456 | Function Loss:  -2.547\n",
            "Total loss:  -2.4968 | PDE Loss:  -4.4571 | Function Loss:  -2.5472\n",
            "Total loss:  -2.497 | PDE Loss:  -4.4577 | Function Loss:  -2.5474\n",
            "Total loss:  -2.4973 | PDE Loss:  -4.4581 | Function Loss:  -2.5477\n",
            "Total loss:  -2.4976 | PDE Loss:  -4.4574 | Function Loss:  -2.5481\n",
            "Total loss:  -2.4979 | PDE Loss:  -4.4575 | Function Loss:  -2.5484\n",
            "Total loss:  -2.4982 | PDE Loss:  -4.4569 | Function Loss:  -2.5488\n",
            "Total loss:  -2.4984 | PDE Loss:  -4.4553 | Function Loss:  -2.5493\n",
            "Total loss:  -2.4988 | PDE Loss:  -4.4556 | Function Loss:  -2.5497\n",
            "Total loss:  -2.4992 | PDE Loss:  -4.456 | Function Loss:  -2.55\n",
            "Total loss:  -2.4995 | PDE Loss:  -4.4549 | Function Loss:  -2.5505\n",
            "Total loss:  -2.4997 | PDE Loss:  -4.4545 | Function Loss:  -2.5508\n",
            "Total loss:  -2.5001 | PDE Loss:  -4.4536 | Function Loss:  -2.5514\n",
            "Total loss:  -2.5007 | PDE Loss:  -4.4526 | Function Loss:  -2.5522\n",
            "Total loss:  -2.5014 | PDE Loss:  -4.4532 | Function Loss:  -2.5529\n",
            "Total loss:  -2.5021 | PDE Loss:  -4.4542 | Function Loss:  -2.5535\n",
            "Total loss:  -2.5027 | PDE Loss:  -4.4574 | Function Loss:  -2.5538\n",
            "Total loss:  -2.5034 | PDE Loss:  -4.4603 | Function Loss:  -2.5542\n",
            "Total loss:  -2.5039 | PDE Loss:  -4.4632 | Function Loss:  -2.5544\n",
            "Total loss:  -2.5043 | PDE Loss:  -4.4668 | Function Loss:  -2.5544\n",
            "Total loss:  -2.5047 | PDE Loss:  -4.4678 | Function Loss:  -2.5547\n",
            "Total loss:  -2.5053 | PDE Loss:  -4.4706 | Function Loss:  -2.5551\n",
            "Total loss:  -2.506 | PDE Loss:  -4.4729 | Function Loss:  -2.5556\n",
            "Total loss:  -2.5065 | PDE Loss:  -4.4771 | Function Loss:  -2.5556\n",
            "Total loss:  -2.507 | PDE Loss:  -4.477 | Function Loss:  -2.5562\n",
            "Total loss:  -2.5074 | PDE Loss:  -4.4782 | Function Loss:  -2.5565\n",
            "Total loss:  -2.5079 | PDE Loss:  -4.4772 | Function Loss:  -2.5572\n",
            "Total loss:  -2.5084 | PDE Loss:  -4.4805 | Function Loss:  -2.5574\n",
            "Total loss:  -2.5088 | PDE Loss:  -4.4803 | Function Loss:  -2.5578\n",
            "Total loss:  -2.5092 | PDE Loss:  -4.4809 | Function Loss:  -2.5582\n",
            "Total loss:  -2.5096 | PDE Loss:  -4.4807 | Function Loss:  -2.5587\n",
            "Total loss:  -2.51 | PDE Loss:  -4.4808 | Function Loss:  -2.5591\n",
            "Total loss:  -2.5105 | PDE Loss:  -4.481 | Function Loss:  -2.5596\n",
            "Total loss:  -2.5109 | PDE Loss:  -4.4812 | Function Loss:  -2.5601\n",
            "Total loss:  -2.5112 | PDE Loss:  -4.4819 | Function Loss:  -2.5603\n",
            "Total loss:  -2.5115 | PDE Loss:  -4.482 | Function Loss:  -2.5606\n",
            "Total loss:  -2.5117 | PDE Loss:  -4.4828 | Function Loss:  -2.5608\n",
            "Total loss:  -2.5119 | PDE Loss:  -4.4831 | Function Loss:  -2.561\n",
            "Total loss:  -2.5122 | PDE Loss:  -4.4832 | Function Loss:  -2.5613\n",
            "Total loss:  -2.5126 | PDE Loss:  -4.4845 | Function Loss:  -2.5616\n",
            "Total loss:  -2.513 | PDE Loss:  -4.4839 | Function Loss:  -2.5622\n",
            "Total loss:  -2.5134 | PDE Loss:  -4.4849 | Function Loss:  -2.5625\n",
            "Total loss:  -2.514 | PDE Loss:  -4.4864 | Function Loss:  -2.563\n",
            "Total loss:  -2.5148 | PDE Loss:  -4.489 | Function Loss:  -2.5635\n",
            "Total loss:  -2.5153 | PDE Loss:  -4.4904 | Function Loss:  -2.5639\n",
            "Total loss:  -2.5157 | PDE Loss:  -4.4909 | Function Loss:  -2.5643\n",
            "Total loss:  -2.5159 | PDE Loss:  -4.4905 | Function Loss:  -2.5646\n",
            "Total loss:  -2.5162 | PDE Loss:  -4.4908 | Function Loss:  -2.5649\n",
            "Total loss:  -2.5165 | PDE Loss:  -4.4908 | Function Loss:  -2.5652\n",
            "Total loss:  -2.5168 | PDE Loss:  -4.4896 | Function Loss:  -2.5657\n",
            "Total loss:  -2.5171 | PDE Loss:  -4.4902 | Function Loss:  -2.566\n",
            "Total loss:  -2.5174 | PDE Loss:  -4.4879 | Function Loss:  -2.5666\n",
            "Total loss:  -2.5177 | PDE Loss:  -4.488 | Function Loss:  -2.5668\n",
            "Total loss:  -2.5179 | PDE Loss:  -4.4838 | Function Loss:  -2.5676\n",
            "Total loss:  -2.5181 | PDE Loss:  -4.4822 | Function Loss:  -2.5681\n",
            "Total loss:  -2.5183 | PDE Loss:  -4.4814 | Function Loss:  -2.5684\n",
            "Total loss:  -2.5185 | PDE Loss:  -4.4802 | Function Loss:  -2.5687\n",
            "Total loss:  -2.5187 | PDE Loss:  -4.4802 | Function Loss:  -2.569\n",
            "Total loss:  -2.5189 | PDE Loss:  -4.4814 | Function Loss:  -2.5691\n",
            "Total loss:  -2.5192 | PDE Loss:  -4.4826 | Function Loss:  -2.5692\n",
            "Total loss:  -2.5193 | PDE Loss:  -4.485 | Function Loss:  -2.5691\n",
            "Total loss:  -2.5195 | PDE Loss:  -4.488 | Function Loss:  -2.5689\n",
            "Total loss:  -2.5198 | PDE Loss:  -4.4904 | Function Loss:  -2.5689\n",
            "Total loss:  -2.5199 | PDE Loss:  -4.4994 | Function Loss:  -2.5679\n",
            "Total loss:  -2.5201 | PDE Loss:  -4.4994 | Function Loss:  -2.5682\n",
            "Total loss:  -2.5203 | PDE Loss:  -4.4969 | Function Loss:  -2.5687\n",
            "Total loss:  -2.5204 | PDE Loss:  -4.4967 | Function Loss:  -2.5689\n",
            "Total loss:  -2.5205 | PDE Loss:  -4.4953 | Function Loss:  -2.5691\n",
            "Total loss:  -2.5206 | PDE Loss:  -4.4931 | Function Loss:  -2.5696\n",
            "Total loss:  -2.5208 | PDE Loss:  -4.4913 | Function Loss:  -2.57\n",
            "Total loss:  -2.521 | PDE Loss:  -4.4887 | Function Loss:  -2.5705\n",
            "Total loss:  -2.5212 | PDE Loss:  -4.487 | Function Loss:  -2.5709\n",
            "Total loss:  -2.5214 | PDE Loss:  -4.4853 | Function Loss:  -2.5714\n",
            "Total loss:  -2.5217 | PDE Loss:  -4.4849 | Function Loss:  -2.5718\n",
            "Total loss:  -2.5218 | PDE Loss:  -4.4733 | Function Loss:  -2.5733\n",
            "Total loss:  -2.5222 | PDE Loss:  -4.477 | Function Loss:  -2.5733\n",
            "Total loss:  -2.5226 | PDE Loss:  -4.4815 | Function Loss:  -2.5731\n",
            "Total loss:  -2.523 | PDE Loss:  -4.484 | Function Loss:  -2.5733\n",
            "Total loss:  -2.5233 | PDE Loss:  -4.4855 | Function Loss:  -2.5735\n",
            "Total loss:  -2.5238 | PDE Loss:  -4.487 | Function Loss:  -2.5738\n",
            "Total loss:  -2.5237 | PDE Loss:  -4.4746 | Function Loss:  -2.5752\n",
            "Total loss:  -2.524 | PDE Loss:  -4.4826 | Function Loss:  -2.5746\n",
            "Total loss:  -2.5244 | PDE Loss:  -4.4843 | Function Loss:  -2.5748\n",
            "Total loss:  -2.5247 | PDE Loss:  -4.4826 | Function Loss:  -2.5754\n",
            "Total loss:  -2.525 | PDE Loss:  -4.4828 | Function Loss:  -2.5757\n",
            "Total loss:  -2.5253 | PDE Loss:  -4.48 | Function Loss:  -2.5763\n",
            "Total loss:  -2.5255 | PDE Loss:  -4.476 | Function Loss:  -2.5772\n",
            "Total loss:  -2.5258 | PDE Loss:  -4.4746 | Function Loss:  -2.5776\n",
            "Total loss:  -2.5261 | PDE Loss:  -4.4724 | Function Loss:  -2.5783\n",
            "Total loss:  -2.5262 | PDE Loss:  -4.4656 | Function Loss:  -2.5792\n",
            "Total loss:  -2.5263 | PDE Loss:  -4.4694 | Function Loss:  -2.5789\n",
            "Total loss:  -2.5266 | PDE Loss:  -4.4705 | Function Loss:  -2.5791\n",
            "Total loss:  -2.527 | PDE Loss:  -4.4732 | Function Loss:  -2.5791\n",
            "Total loss:  -2.5273 | PDE Loss:  -4.4758 | Function Loss:  -2.5792\n",
            "Total loss:  -2.5278 | PDE Loss:  -4.48 | Function Loss:  -2.5792\n",
            "Total loss:  -2.5282 | PDE Loss:  -4.482 | Function Loss:  -2.5794\n",
            "Total loss:  -2.5286 | PDE Loss:  -4.4868 | Function Loss:  -2.5793\n",
            "Total loss:  -2.5289 | PDE Loss:  -4.4876 | Function Loss:  -2.5795\n",
            "Total loss:  -2.5292 | PDE Loss:  -4.4884 | Function Loss:  -2.5797\n",
            "Total loss:  -2.5296 | PDE Loss:  -4.4885 | Function Loss:  -2.5802\n",
            "Total loss:  -2.5301 | PDE Loss:  -4.4888 | Function Loss:  -2.5807\n",
            "Total loss:  -2.5307 | PDE Loss:  -4.489 | Function Loss:  -2.5813\n",
            "Total loss:  -2.5311 | PDE Loss:  -4.4895 | Function Loss:  -2.5818\n",
            "Total loss:  -2.5315 | PDE Loss:  -4.4912 | Function Loss:  -2.582\n",
            "Total loss:  -2.5319 | PDE Loss:  -4.4893 | Function Loss:  -2.5826\n",
            "Total loss:  -2.5322 | PDE Loss:  -4.4914 | Function Loss:  -2.5827\n",
            "Total loss:  -2.5324 | PDE Loss:  -4.4948 | Function Loss:  -2.5825\n",
            "Total loss:  -2.5327 | PDE Loss:  -4.4956 | Function Loss:  -2.5827\n",
            "Total loss:  -2.533 | PDE Loss:  -4.4951 | Function Loss:  -2.5831\n",
            "Total loss:  -2.5332 | PDE Loss:  -4.4957 | Function Loss:  -2.5834\n",
            "Total loss:  -2.5336 | PDE Loss:  -4.4941 | Function Loss:  -2.584\n",
            "Total loss:  -2.5339 | PDE Loss:  -4.4933 | Function Loss:  -2.5845\n",
            "Total loss:  -2.5346 | PDE Loss:  -4.4913 | Function Loss:  -2.5855\n",
            "Total loss:  -2.5353 | PDE Loss:  -4.4887 | Function Loss:  -2.5866\n",
            "Total loss:  -2.5358 | PDE Loss:  -4.4889 | Function Loss:  -2.5871\n",
            "Total loss:  -2.5362 | PDE Loss:  -4.4902 | Function Loss:  -2.5874\n",
            "Total loss:  -2.5367 | PDE Loss:  -4.4919 | Function Loss:  -2.5877\n",
            "Total loss:  -2.5373 | PDE Loss:  -4.4965 | Function Loss:  -2.5878\n",
            "Total loss:  -2.5378 | PDE Loss:  -4.5007 | Function Loss:  -2.5879\n",
            "Total loss:  -2.5383 | PDE Loss:  -4.5055 | Function Loss:  -2.5878\n",
            "Total loss:  -2.5386 | PDE Loss:  -4.5077 | Function Loss:  -2.5879\n",
            "Total loss:  -2.5389 | PDE Loss:  -4.5099 | Function Loss:  -2.588\n",
            "Total loss:  -2.5391 | PDE Loss:  -4.5118 | Function Loss:  -2.588\n",
            "Total loss:  -2.5393 | PDE Loss:  -4.5124 | Function Loss:  -2.5882\n",
            "Total loss:  -2.5394 | PDE Loss:  -4.5137 | Function Loss:  -2.5881\n",
            "Total loss:  -2.5395 | PDE Loss:  -4.5141 | Function Loss:  -2.5882\n",
            "Total loss:  -2.5398 | PDE Loss:  -4.5147 | Function Loss:  -2.5885\n",
            "Total loss:  -2.5401 | PDE Loss:  -4.5164 | Function Loss:  -2.5886\n",
            "Total loss:  -2.5405 | PDE Loss:  -4.5192 | Function Loss:  -2.5887\n",
            "Total loss:  -2.5408 | PDE Loss:  -4.5294 | Function Loss:  -2.5878\n",
            "Total loss:  -2.5411 | PDE Loss:  -4.5291 | Function Loss:  -2.5883\n",
            "Total loss:  -2.5415 | PDE Loss:  -4.5296 | Function Loss:  -2.5886\n",
            "Total loss:  -2.5418 | PDE Loss:  -4.5293 | Function Loss:  -2.589\n",
            "Total loss:  -2.542 | PDE Loss:  -4.5295 | Function Loss:  -2.5892\n",
            "Total loss:  -2.5423 | PDE Loss:  -4.5298 | Function Loss:  -2.5894\n",
            "Total loss:  -2.5426 | PDE Loss:  -4.53 | Function Loss:  -2.5897\n",
            "Total loss:  -2.5429 | PDE Loss:  -4.5302 | Function Loss:  -2.5901\n",
            "Total loss:  -2.5433 | PDE Loss:  -4.5301 | Function Loss:  -2.5905\n",
            "Total loss:  -2.5437 | PDE Loss:  -4.5293 | Function Loss:  -2.591\n",
            "Total loss:  -2.5441 | PDE Loss:  -4.5278 | Function Loss:  -2.5917\n",
            "Total loss:  -2.5445 | PDE Loss:  -4.5256 | Function Loss:  -2.5924\n",
            "Total loss:  -2.5449 | PDE Loss:  -4.522 | Function Loss:  -2.5932\n",
            "Total loss:  -2.5452 | PDE Loss:  -4.5195 | Function Loss:  -2.594\n",
            "Total loss:  -2.5457 | PDE Loss:  -4.516 | Function Loss:  -2.5949\n",
            "Total loss:  -2.5461 | PDE Loss:  -4.5133 | Function Loss:  -2.5956\n",
            "Total loss:  -2.5463 | PDE Loss:  -4.5133 | Function Loss:  -2.5959\n",
            "Total loss:  -2.5467 | PDE Loss:  -4.5133 | Function Loss:  -2.5963\n",
            "Total loss:  -2.5472 | PDE Loss:  -4.5146 | Function Loss:  -2.5967\n",
            "Total loss:  -2.5476 | PDE Loss:  -4.5136 | Function Loss:  -2.5973\n",
            "Total loss:  -2.548 | PDE Loss:  -4.5141 | Function Loss:  -2.5977\n",
            "Total loss:  -2.5484 | PDE Loss:  -4.5137 | Function Loss:  -2.5982\n",
            "Total loss:  -2.5487 | PDE Loss:  -4.5134 | Function Loss:  -2.5986\n",
            "Total loss:  -2.5491 | PDE Loss:  -4.5142 | Function Loss:  -2.5989\n",
            "Total loss:  -2.5494 | PDE Loss:  -4.5132 | Function Loss:  -2.5994\n",
            "Total loss:  -2.5498 | PDE Loss:  -4.5146 | Function Loss:  -2.5996\n",
            "Total loss:  -2.55 | PDE Loss:  -4.5143 | Function Loss:  -2.5999\n",
            "Total loss:  -2.5502 | PDE Loss:  -4.5141 | Function Loss:  -2.6001\n",
            "Total loss:  -2.5504 | PDE Loss:  -4.5155 | Function Loss:  -2.6002\n",
            "Total loss:  -2.5507 | PDE Loss:  -4.516 | Function Loss:  -2.6005\n",
            "Total loss:  -2.551 | PDE Loss:  -4.5183 | Function Loss:  -2.6005\n",
            "Total loss:  -2.5513 | PDE Loss:  -4.5202 | Function Loss:  -2.6007\n",
            "Total loss:  -2.5516 | PDE Loss:  -4.5223 | Function Loss:  -2.6008\n",
            "Total loss:  -2.552 | PDE Loss:  -4.5261 | Function Loss:  -2.6007\n",
            "Total loss:  -2.5523 | PDE Loss:  -4.5278 | Function Loss:  -2.6009\n",
            "Total loss:  -2.5526 | PDE Loss:  -4.5316 | Function Loss:  -2.6008\n",
            "Total loss:  -2.553 | PDE Loss:  -4.5348 | Function Loss:  -2.6009\n",
            "Total loss:  -2.5535 | PDE Loss:  -4.539 | Function Loss:  -2.6009\n",
            "Total loss:  -2.5539 | PDE Loss:  -4.5436 | Function Loss:  -2.6008\n",
            "Total loss:  -2.5544 | PDE Loss:  -4.5456 | Function Loss:  -2.6011\n",
            "Total loss:  -2.5548 | PDE Loss:  -4.5496 | Function Loss:  -2.6011\n",
            "Total loss:  -2.5551 | PDE Loss:  -4.5499 | Function Loss:  -2.6014\n",
            "Total loss:  -2.5553 | PDE Loss:  -4.5504 | Function Loss:  -2.6016\n",
            "Total loss:  -2.5556 | PDE Loss:  -4.5501 | Function Loss:  -2.6019\n",
            "Total loss:  -2.5558 | PDE Loss:  -4.5491 | Function Loss:  -2.6023\n",
            "Total loss:  -2.5562 | PDE Loss:  -4.5474 | Function Loss:  -2.6029\n",
            "Total loss:  -2.5565 | PDE Loss:  -4.5468 | Function Loss:  -2.6033\n",
            "Total loss:  -2.5567 | PDE Loss:  -4.5471 | Function Loss:  -2.6035\n",
            "Total loss:  -2.557 | PDE Loss:  -4.5482 | Function Loss:  -2.6038\n",
            "Total loss:  -2.5573 | PDE Loss:  -4.549 | Function Loss:  -2.604\n",
            "Total loss:  -2.5575 | PDE Loss:  -4.5501 | Function Loss:  -2.6041\n",
            "Total loss:  -2.5576 | PDE Loss:  -4.5506 | Function Loss:  -2.6042\n",
            "Total loss:  -2.5578 | PDE Loss:  -4.5513 | Function Loss:  -2.6043\n",
            "Total loss:  -2.5579 | PDE Loss:  -4.5515 | Function Loss:  -2.6044\n",
            "Total loss:  -2.5581 | PDE Loss:  -4.5513 | Function Loss:  -2.6046\n",
            "Total loss:  -2.5583 | PDE Loss:  -4.5511 | Function Loss:  -2.6048\n",
            "Total loss:  -2.5584 | PDE Loss:  -4.5505 | Function Loss:  -2.6051\n",
            "Total loss:  -2.5586 | PDE Loss:  -4.5496 | Function Loss:  -2.6054\n",
            "Total loss:  -2.5589 | PDE Loss:  -4.5488 | Function Loss:  -2.6058\n",
            "Total loss:  -2.5591 | PDE Loss:  -4.5478 | Function Loss:  -2.6062\n",
            "Total loss:  -2.5595 | PDE Loss:  -4.5466 | Function Loss:  -2.6067\n",
            "Total loss:  -2.5599 | PDE Loss:  -4.5449 | Function Loss:  -2.6073\n",
            "Total loss:  -2.5602 | PDE Loss:  -4.5448 | Function Loss:  -2.6077\n",
            "Total loss:  -2.5606 | PDE Loss:  -4.5445 | Function Loss:  -2.6082\n",
            "Total loss:  -2.5611 | PDE Loss:  -4.5448 | Function Loss:  -2.6087\n",
            "Total loss:  -2.5614 | PDE Loss:  -4.544 | Function Loss:  -2.6092\n",
            "Total loss:  -2.5618 | PDE Loss:  -4.544 | Function Loss:  -2.6096\n",
            "Total loss:  -2.5621 | PDE Loss:  -4.5432 | Function Loss:  -2.61\n",
            "Total loss:  -2.5624 | PDE Loss:  -4.5419 | Function Loss:  -2.6105\n",
            "Total loss:  -2.5628 | PDE Loss:  -4.5414 | Function Loss:  -2.611\n",
            "Total loss:  -2.5631 | PDE Loss:  -4.5385 | Function Loss:  -2.6117\n",
            "Total loss:  -2.5634 | PDE Loss:  -4.5384 | Function Loss:  -2.612\n",
            "Total loss:  -2.5637 | PDE Loss:  -4.5366 | Function Loss:  -2.6125\n",
            "Total loss:  -2.564 | PDE Loss:  -4.5357 | Function Loss:  -2.613\n",
            "Total loss:  -2.5643 | PDE Loss:  -4.5339 | Function Loss:  -2.6135\n",
            "Total loss:  -2.5645 | PDE Loss:  -4.5328 | Function Loss:  -2.6139\n",
            "Total loss:  -2.5647 | PDE Loss:  -4.532 | Function Loss:  -2.6143\n",
            "Total loss:  -2.565 | PDE Loss:  -4.5312 | Function Loss:  -2.6146\n",
            "Total loss:  -2.5652 | PDE Loss:  -4.5311 | Function Loss:  -2.615\n",
            "Total loss:  -2.5655 | PDE Loss:  -4.5305 | Function Loss:  -2.6153\n",
            "Total loss:  -2.5658 | PDE Loss:  -4.5313 | Function Loss:  -2.6156\n",
            "Total loss:  -2.5661 | PDE Loss:  -4.5317 | Function Loss:  -2.6159\n",
            "Total loss:  -2.5666 | PDE Loss:  -4.5338 | Function Loss:  -2.6162\n",
            "Total loss:  -2.5671 | PDE Loss:  -4.5355 | Function Loss:  -2.6165\n",
            "Total loss:  -2.5676 | PDE Loss:  -4.5373 | Function Loss:  -2.6168\n",
            "Total loss:  -2.5675 | PDE Loss:  -4.5435 | Function Loss:  -2.6161\n",
            "Total loss:  -2.5679 | PDE Loss:  -4.5416 | Function Loss:  -2.6166\n",
            "Total loss:  -2.5681 | PDE Loss:  -4.5426 | Function Loss:  -2.6168\n",
            "Total loss:  -2.5684 | PDE Loss:  -4.5444 | Function Loss:  -2.617\n",
            "Total loss:  -2.5688 | PDE Loss:  -4.5434 | Function Loss:  -2.6175\n",
            "Total loss:  -2.5691 | PDE Loss:  -4.546 | Function Loss:  -2.6175\n",
            "Total loss:  -2.5694 | PDE Loss:  -4.5455 | Function Loss:  -2.6179\n",
            "Total loss:  -2.5696 | PDE Loss:  -4.5459 | Function Loss:  -2.6181\n",
            "Total loss:  -2.5698 | PDE Loss:  -4.5457 | Function Loss:  -2.6183\n",
            "Total loss:  -2.5699 | PDE Loss:  -4.5459 | Function Loss:  -2.6185\n",
            "Total loss:  -2.5701 | PDE Loss:  -4.5461 | Function Loss:  -2.6186\n",
            "Total loss:  -2.5703 | PDE Loss:  -4.5463 | Function Loss:  -2.6189\n",
            "Total loss:  -2.5705 | PDE Loss:  -4.547 | Function Loss:  -2.6189\n",
            "Total loss:  -2.5707 | PDE Loss:  -4.5482 | Function Loss:  -2.619\n",
            "Total loss:  -2.5709 | PDE Loss:  -4.5497 | Function Loss:  -2.6191\n",
            "Total loss:  -2.5712 | PDE Loss:  -4.5501 | Function Loss:  -2.6194\n",
            "Total loss:  -2.5715 | PDE Loss:  -4.5512 | Function Loss:  -2.6196\n",
            "Total loss:  -2.5718 | PDE Loss:  -4.5498 | Function Loss:  -2.62\n",
            "Total loss:  -2.572 | PDE Loss:  -4.549 | Function Loss:  -2.6204\n",
            "Total loss:  -2.5722 | PDE Loss:  -4.5474 | Function Loss:  -2.6208\n",
            "Total loss:  -2.5724 | PDE Loss:  -4.5449 | Function Loss:  -2.6214\n",
            "Total loss:  -2.5726 | PDE Loss:  -4.5444 | Function Loss:  -2.6216\n",
            "Total loss:  -2.5728 | PDE Loss:  -4.5433 | Function Loss:  -2.622\n",
            "Total loss:  -2.573 | PDE Loss:  -4.5409 | Function Loss:  -2.6225\n",
            "Total loss:  -2.5731 | PDE Loss:  -4.54 | Function Loss:  -2.6227\n",
            "Total loss:  -2.5733 | PDE Loss:  -4.5384 | Function Loss:  -2.6231\n",
            "Total loss:  -2.5734 | PDE Loss:  -4.537 | Function Loss:  -2.6234\n",
            "Total loss:  -2.5735 | PDE Loss:  -4.5372 | Function Loss:  -2.6235\n",
            "Total loss:  -2.5737 | PDE Loss:  -4.5362 | Function Loss:  -2.6238\n",
            "Total loss:  -2.5738 | PDE Loss:  -4.5364 | Function Loss:  -2.6239\n",
            "Total loss:  -2.574 | PDE Loss:  -4.5352 | Function Loss:  -2.6243\n",
            "Total loss:  -2.5742 | PDE Loss:  -4.5352 | Function Loss:  -2.6245\n",
            "Total loss:  -2.5744 | PDE Loss:  -4.5346 | Function Loss:  -2.6248\n",
            "Total loss:  -2.5746 | PDE Loss:  -4.5342 | Function Loss:  -2.6251\n",
            "Total loss:  -2.5748 | PDE Loss:  -4.5331 | Function Loss:  -2.6255\n",
            "Total loss:  -2.575 | PDE Loss:  -4.5328 | Function Loss:  -2.6257\n",
            "Total loss:  -2.5751 | PDE Loss:  -4.5343 | Function Loss:  -2.6257\n",
            "Total loss:  -2.5753 | PDE Loss:  -4.5357 | Function Loss:  -2.6256\n",
            "Total loss:  -2.5754 | PDE Loss:  -4.5368 | Function Loss:  -2.6257\n",
            "Total loss:  -2.5755 | PDE Loss:  -4.5381 | Function Loss:  -2.6257\n",
            "Total loss:  -2.5757 | PDE Loss:  -4.5394 | Function Loss:  -2.6257\n",
            "Total loss:  -2.5758 | PDE Loss:  -4.5401 | Function Loss:  -2.6257\n",
            "Total loss:  -2.5759 | PDE Loss:  -4.5405 | Function Loss:  -2.6258\n",
            "Total loss:  -2.5761 | PDE Loss:  -4.5414 | Function Loss:  -2.6259\n",
            "Total loss:  -2.5763 | PDE Loss:  -4.5423 | Function Loss:  -2.626\n",
            "Total loss:  -2.5765 | PDE Loss:  -4.5432 | Function Loss:  -2.6261\n",
            "Total loss:  -2.5766 | PDE Loss:  -4.5441 | Function Loss:  -2.6261\n",
            "Total loss:  -2.5768 | PDE Loss:  -4.5452 | Function Loss:  -2.6262\n",
            "Total loss:  -2.577 | PDE Loss:  -4.5464 | Function Loss:  -2.6262\n",
            "Total loss:  -2.5772 | PDE Loss:  -4.547 | Function Loss:  -2.6264\n",
            "Total loss:  -2.5774 | PDE Loss:  -4.5481 | Function Loss:  -2.6266\n",
            "Total loss:  -2.5777 | PDE Loss:  -4.5473 | Function Loss:  -2.627\n",
            "Total loss:  -2.578 | PDE Loss:  -4.5474 | Function Loss:  -2.6272\n",
            "Total loss:  -2.5782 | PDE Loss:  -4.5446 | Function Loss:  -2.6279\n",
            "Total loss:  -2.5785 | PDE Loss:  -4.5455 | Function Loss:  -2.628\n",
            "Total loss:  -2.5787 | PDE Loss:  -4.5445 | Function Loss:  -2.6284\n",
            "Total loss:  -2.579 | PDE Loss:  -4.5464 | Function Loss:  -2.6286\n",
            "Total loss:  -2.5794 | PDE Loss:  -4.5487 | Function Loss:  -2.6287\n",
            "Total loss:  -2.5797 | PDE Loss:  -4.5527 | Function Loss:  -2.6285\n",
            "Total loss:  -2.58 | PDE Loss:  -4.5563 | Function Loss:  -2.6285\n",
            "Total loss:  -2.5804 | PDE Loss:  -4.5606 | Function Loss:  -2.6284\n",
            "Total loss:  -2.5808 | PDE Loss:  -4.564 | Function Loss:  -2.6285\n",
            "Total loss:  -2.5812 | PDE Loss:  -4.5675 | Function Loss:  -2.6285\n",
            "Total loss:  -2.5815 | PDE Loss:  -4.5693 | Function Loss:  -2.6286\n",
            "Total loss:  -2.5818 | PDE Loss:  -4.5707 | Function Loss:  -2.6288\n",
            "Total loss:  -2.5822 | PDE Loss:  -4.5718 | Function Loss:  -2.6291\n",
            "Total loss:  -2.5826 | PDE Loss:  -4.5715 | Function Loss:  -2.6297\n",
            "Total loss:  -2.5831 | PDE Loss:  -4.5715 | Function Loss:  -2.6301\n",
            "Total loss:  -2.5836 | PDE Loss:  -4.5703 | Function Loss:  -2.6309\n",
            "Total loss:  -2.5843 | PDE Loss:  -4.5682 | Function Loss:  -2.6318\n",
            "Total loss:  -2.5848 | PDE Loss:  -4.5701 | Function Loss:  -2.6322\n",
            "Total loss:  -2.5853 | PDE Loss:  -4.5693 | Function Loss:  -2.6329\n",
            "Total loss:  -2.5861 | PDE Loss:  -4.5697 | Function Loss:  -2.6337\n",
            "Total loss:  -2.5867 | PDE Loss:  -4.5698 | Function Loss:  -2.6344\n",
            "Total loss:  -2.5873 | PDE Loss:  -4.5715 | Function Loss:  -2.6348\n",
            "Total loss:  -2.5877 | PDE Loss:  -4.5726 | Function Loss:  -2.6352\n",
            "Total loss:  -2.5881 | PDE Loss:  -4.5742 | Function Loss:  -2.6355\n",
            "Total loss:  -2.5887 | PDE Loss:  -4.576 | Function Loss:  -2.6359\n",
            "Total loss:  -2.5894 | PDE Loss:  -4.5763 | Function Loss:  -2.6367\n",
            "Total loss:  -2.59 | PDE Loss:  -4.5774 | Function Loss:  -2.6372\n",
            "Total loss:  -2.5906 | PDE Loss:  -4.5756 | Function Loss:  -2.6381\n",
            "Total loss:  -2.5911 | PDE Loss:  -4.5732 | Function Loss:  -2.6389\n",
            "Total loss:  -2.5916 | PDE Loss:  -4.5708 | Function Loss:  -2.6398\n",
            "Total loss:  -2.5921 | PDE Loss:  -4.5673 | Function Loss:  -2.6407\n",
            "Total loss:  -2.5923 | PDE Loss:  -4.565 | Function Loss:  -2.6412\n",
            "Total loss:  -2.5927 | PDE Loss:  -4.5645 | Function Loss:  -2.6417\n",
            "Total loss:  -2.593 | PDE Loss:  -4.5648 | Function Loss:  -2.642\n",
            "Total loss:  -2.5933 | PDE Loss:  -4.5657 | Function Loss:  -2.6423\n",
            "Total loss:  -2.5937 | PDE Loss:  -4.5673 | Function Loss:  -2.6424\n",
            "Total loss:  -2.594 | PDE Loss:  -4.5689 | Function Loss:  -2.6426\n",
            "Total loss:  -2.5943 | PDE Loss:  -4.5708 | Function Loss:  -2.6427\n",
            "Total loss:  -2.5946 | PDE Loss:  -4.5718 | Function Loss:  -2.643\n",
            "Total loss:  -2.595 | PDE Loss:  -4.5734 | Function Loss:  -2.6432\n",
            "Total loss:  -2.5953 | PDE Loss:  -4.5732 | Function Loss:  -2.6435\n",
            "Total loss:  -2.5955 | PDE Loss:  -4.5735 | Function Loss:  -2.6438\n",
            "Total loss:  -2.5957 | PDE Loss:  -4.5716 | Function Loss:  -2.6442\n",
            "Total loss:  -2.5959 | PDE Loss:  -4.5724 | Function Loss:  -2.6444\n",
            "Total loss:  -2.5961 | PDE Loss:  -4.5736 | Function Loss:  -2.6444\n",
            "Total loss:  -2.5963 | PDE Loss:  -4.5748 | Function Loss:  -2.6446\n",
            "Total loss:  -2.5966 | PDE Loss:  -4.5763 | Function Loss:  -2.6446\n",
            "Total loss:  -2.5967 | PDE Loss:  -4.5775 | Function Loss:  -2.6447\n",
            "Total loss:  -2.5969 | PDE Loss:  -4.5785 | Function Loss:  -2.6448\n",
            "Total loss:  -2.5971 | PDE Loss:  -4.5796 | Function Loss:  -2.6449\n",
            "Total loss:  -2.5973 | PDE Loss:  -4.5806 | Function Loss:  -2.645\n",
            "Total loss:  -2.5975 | PDE Loss:  -4.581 | Function Loss:  -2.6452\n",
            "Total loss:  -2.5977 | PDE Loss:  -4.5842 | Function Loss:  -2.645\n",
            "Total loss:  -2.5978 | PDE Loss:  -4.5835 | Function Loss:  -2.6452\n",
            "Total loss:  -2.598 | PDE Loss:  -4.5825 | Function Loss:  -2.6455\n",
            "Total loss:  -2.5982 | PDE Loss:  -4.5812 | Function Loss:  -2.6459\n",
            "Total loss:  -2.5983 | PDE Loss:  -4.5801 | Function Loss:  -2.6462\n",
            "Total loss:  -2.5984 | PDE Loss:  -4.5786 | Function Loss:  -2.6464\n",
            "Total loss:  -2.5985 | PDE Loss:  -4.5777 | Function Loss:  -2.6467\n",
            "Total loss:  -2.5986 | PDE Loss:  -4.5763 | Function Loss:  -2.647\n",
            "Total loss:  -2.5988 | PDE Loss:  -4.5754 | Function Loss:  -2.6472\n",
            "Total loss:  -2.5989 | PDE Loss:  -4.5738 | Function Loss:  -2.6475\n",
            "Total loss:  -2.599 | PDE Loss:  -4.5734 | Function Loss:  -2.6477\n",
            "Total loss:  -2.5991 | PDE Loss:  -4.5738 | Function Loss:  -2.6478\n",
            "Total loss:  -2.5993 | PDE Loss:  -4.574 | Function Loss:  -2.6479\n",
            "Total loss:  -2.5994 | PDE Loss:  -4.5748 | Function Loss:  -2.648\n",
            "Total loss:  -2.5995 | PDE Loss:  -4.5754 | Function Loss:  -2.648\n",
            "Total loss:  -2.5997 | PDE Loss:  -4.5757 | Function Loss:  -2.6482\n",
            "Total loss:  -2.5998 | PDE Loss:  -4.5762 | Function Loss:  -2.6483\n",
            "Total loss:  -2.6001 | PDE Loss:  -4.5773 | Function Loss:  -2.6484\n",
            "Total loss:  -2.6003 | PDE Loss:  -4.5771 | Function Loss:  -2.6487\n",
            "Total loss:  -2.6005 | PDE Loss:  -4.5768 | Function Loss:  -2.649\n",
            "Total loss:  -2.6007 | PDE Loss:  -4.5761 | Function Loss:  -2.6493\n",
            "Total loss:  -2.6009 | PDE Loss:  -4.5762 | Function Loss:  -2.6495\n",
            "Total loss:  -2.6011 | PDE Loss:  -4.5752 | Function Loss:  -2.6499\n",
            "Total loss:  -2.6013 | PDE Loss:  -4.5757 | Function Loss:  -2.65\n",
            "Total loss:  -2.6016 | PDE Loss:  -4.5748 | Function Loss:  -2.6504\n",
            "Total loss:  -2.6018 | PDE Loss:  -4.5754 | Function Loss:  -2.6506\n",
            "Total loss:  -2.6021 | PDE Loss:  -4.5752 | Function Loss:  -2.651\n",
            "Total loss:  -2.6024 | PDE Loss:  -4.5758 | Function Loss:  -2.6512\n",
            "Total loss:  -2.6028 | PDE Loss:  -4.5768 | Function Loss:  -2.6515\n",
            "Total loss:  -2.6032 | PDE Loss:  -4.5733 | Function Loss:  -2.6524\n",
            "Total loss:  -2.6037 | PDE Loss:  -4.5795 | Function Loss:  -2.6522\n",
            "Total loss:  -2.6039 | PDE Loss:  -4.5785 | Function Loss:  -2.6526\n",
            "Total loss:  -2.6044 | PDE Loss:  -4.5784 | Function Loss:  -2.6532\n",
            "Total loss:  -2.6051 | PDE Loss:  -4.5798 | Function Loss:  -2.6537\n",
            "Total loss:  -2.6057 | PDE Loss:  -4.5788 | Function Loss:  -2.6546\n",
            "Total loss:  -2.6062 | PDE Loss:  -4.5776 | Function Loss:  -2.6553\n",
            "Total loss:  -2.6067 | PDE Loss:  -4.5771 | Function Loss:  -2.6559\n",
            "Total loss:  -2.6072 | PDE Loss:  -4.576 | Function Loss:  -2.6566\n",
            "Total loss:  -2.6076 | PDE Loss:  -4.5769 | Function Loss:  -2.657\n",
            "Total loss:  -2.6081 | PDE Loss:  -4.579 | Function Loss:  -2.6572\n",
            "Total loss:  -2.6085 | PDE Loss:  -4.5832 | Function Loss:  -2.6572\n",
            "Total loss:  -2.6089 | PDE Loss:  -4.5866 | Function Loss:  -2.6572\n",
            "Total loss:  -2.6093 | PDE Loss:  -4.5919 | Function Loss:  -2.657\n",
            "Total loss:  -2.6097 | PDE Loss:  -4.5965 | Function Loss:  -2.6569\n",
            "Total loss:  -2.61 | PDE Loss:  -4.6019 | Function Loss:  -2.6567\n",
            "Total loss:  -2.6104 | PDE Loss:  -4.6055 | Function Loss:  -2.6567\n",
            "Total loss:  -2.6106 | PDE Loss:  -4.6084 | Function Loss:  -2.6566\n",
            "Total loss:  -2.6107 | PDE Loss:  -4.6094 | Function Loss:  -2.6566\n",
            "Total loss:  -2.6109 | PDE Loss:  -4.61 | Function Loss:  -2.6567\n",
            "Total loss:  -2.611 | PDE Loss:  -4.6097 | Function Loss:  -2.6569\n",
            "Total loss:  -2.6113 | PDE Loss:  -4.6096 | Function Loss:  -2.6572\n",
            "Total loss:  -2.6116 | PDE Loss:  -4.609 | Function Loss:  -2.6576\n",
            "Total loss:  -2.612 | PDE Loss:  -4.608 | Function Loss:  -2.6582\n",
            "Total loss:  -2.6124 | PDE Loss:  -4.6057 | Function Loss:  -2.6589\n",
            "Total loss:  -2.6126 | PDE Loss:  -4.6035 | Function Loss:  -2.6594\n",
            "Total loss:  -2.6128 | PDE Loss:  -4.6003 | Function Loss:  -2.66\n",
            "Total loss:  -2.613 | PDE Loss:  -4.5989 | Function Loss:  -2.6603\n",
            "Total loss:  -2.6131 | PDE Loss:  -4.5969 | Function Loss:  -2.6607\n",
            "Total loss:  -2.6132 | PDE Loss:  -4.5958 | Function Loss:  -2.6609\n",
            "Total loss:  -2.6133 | PDE Loss:  -4.5947 | Function Loss:  -2.6611\n",
            "Total loss:  -2.6134 | PDE Loss:  -4.594 | Function Loss:  -2.6614\n",
            "Total loss:  -2.6136 | PDE Loss:  -4.5927 | Function Loss:  -2.6618\n",
            "Total loss:  -2.6138 | PDE Loss:  -4.5921 | Function Loss:  -2.662\n",
            "Total loss:  -2.614 | PDE Loss:  -4.5912 | Function Loss:  -2.6624\n",
            "Total loss:  -2.6142 | PDE Loss:  -4.5907 | Function Loss:  -2.6627\n",
            "Total loss:  -2.6146 | PDE Loss:  -4.5913 | Function Loss:  -2.663\n",
            "Total loss:  -2.6151 | PDE Loss:  -4.5912 | Function Loss:  -2.6636\n",
            "Total loss:  -2.6157 | PDE Loss:  -4.5921 | Function Loss:  -2.6641\n",
            "Total loss:  -2.6162 | PDE Loss:  -4.5919 | Function Loss:  -2.6648\n",
            "Total loss:  -2.6169 | PDE Loss:  -4.5913 | Function Loss:  -2.6657\n",
            "Total loss:  -2.6176 | PDE Loss:  -4.5912 | Function Loss:  -2.6664\n",
            "Total loss:  -2.6181 | PDE Loss:  -4.589 | Function Loss:  -2.6672\n",
            "Total loss:  -2.6185 | PDE Loss:  -4.5887 | Function Loss:  -2.6677\n",
            "Total loss:  -2.6188 | PDE Loss:  -4.5891 | Function Loss:  -2.668\n",
            "Total loss:  -2.6193 | PDE Loss:  -4.5905 | Function Loss:  -2.6683\n",
            "Total loss:  -2.6195 | PDE Loss:  -4.5912 | Function Loss:  -2.6685\n",
            "Total loss:  -2.62 | PDE Loss:  -4.5961 | Function Loss:  -2.6685\n",
            "Total loss:  -2.6203 | PDE Loss:  -4.5987 | Function Loss:  -2.6686\n",
            "Total loss:  -2.6207 | PDE Loss:  -4.6023 | Function Loss:  -2.6685\n",
            "Total loss:  -2.621 | PDE Loss:  -4.6043 | Function Loss:  -2.6687\n",
            "Total loss:  -2.6214 | PDE Loss:  -4.6076 | Function Loss:  -2.6687\n",
            "Total loss:  -2.6217 | PDE Loss:  -4.6083 | Function Loss:  -2.6689\n",
            "Total loss:  -2.622 | PDE Loss:  -4.6088 | Function Loss:  -2.6692\n",
            "Total loss:  -2.6224 | PDE Loss:  -4.6094 | Function Loss:  -2.6696\n",
            "Total loss:  -2.6228 | PDE Loss:  -4.6065 | Function Loss:  -2.6705\n",
            "Total loss:  -2.6233 | PDE Loss:  -4.6045 | Function Loss:  -2.6712\n",
            "Total loss:  -2.6237 | PDE Loss:  -4.6045 | Function Loss:  -2.6716\n",
            "Total loss:  -2.6243 | PDE Loss:  -4.6039 | Function Loss:  -2.6723\n",
            "Total loss:  -2.6247 | PDE Loss:  -4.6024 | Function Loss:  -2.6731\n",
            "Total loss:  -2.6251 | PDE Loss:  -4.6019 | Function Loss:  -2.6735\n",
            "Total loss:  -2.6256 | PDE Loss:  -4.6014 | Function Loss:  -2.6741\n",
            "Total loss:  -2.6263 | PDE Loss:  -4.6016 | Function Loss:  -2.6749\n",
            "Total loss:  -2.6271 | PDE Loss:  -4.6011 | Function Loss:  -2.6758\n",
            "Total loss:  -2.6277 | PDE Loss:  -4.6023 | Function Loss:  -2.6764\n",
            "Total loss:  -2.6282 | PDE Loss:  -4.5995 | Function Loss:  -2.6773\n",
            "Total loss:  -2.6288 | PDE Loss:  -4.6013 | Function Loss:  -2.6777\n",
            "Total loss:  -2.6292 | PDE Loss:  -4.6042 | Function Loss:  -2.6778\n",
            "Total loss:  -2.6298 | PDE Loss:  -4.605 | Function Loss:  -2.6784\n",
            "Total loss:  -2.6304 | PDE Loss:  -4.6051 | Function Loss:  -2.679\n",
            "Total loss:  -2.6309 | PDE Loss:  -4.6049 | Function Loss:  -2.6796\n",
            "Total loss:  -2.6314 | PDE Loss:  -4.6009 | Function Loss:  -2.6806\n",
            "Total loss:  -2.6319 | PDE Loss:  -4.6015 | Function Loss:  -2.6812\n",
            "Total loss:  -2.6322 | PDE Loss:  -4.5986 | Function Loss:  -2.6819\n",
            "Total loss:  -2.6326 | PDE Loss:  -4.5968 | Function Loss:  -2.6825\n",
            "Total loss:  -2.633 | PDE Loss:  -4.595 | Function Loss:  -2.6831\n",
            "Total loss:  -2.6333 | PDE Loss:  -4.5932 | Function Loss:  -2.6838\n",
            "Total loss:  -2.6337 | PDE Loss:  -4.5922 | Function Loss:  -2.6844\n",
            "Total loss:  -2.6341 | PDE Loss:  -4.5918 | Function Loss:  -2.6849\n",
            "Total loss:  -2.6345 | PDE Loss:  -4.5907 | Function Loss:  -2.6854\n",
            "Total loss:  -2.6348 | PDE Loss:  -4.5921 | Function Loss:  -2.6855\n",
            "Total loss:  -2.6349 | PDE Loss:  -4.592 | Function Loss:  -2.6857\n",
            "Total loss:  -2.6351 | PDE Loss:  -4.5918 | Function Loss:  -2.6859\n",
            "Total loss:  -2.6352 | PDE Loss:  -4.5925 | Function Loss:  -2.686\n",
            "Total loss:  -2.6354 | PDE Loss:  -4.5918 | Function Loss:  -2.6863\n",
            "Total loss:  -2.6356 | PDE Loss:  -4.5926 | Function Loss:  -2.6864\n",
            "Total loss:  -2.6358 | PDE Loss:  -4.5918 | Function Loss:  -2.6868\n",
            "Total loss:  -2.636 | PDE Loss:  -4.5916 | Function Loss:  -2.6869\n",
            "Total loss:  -2.6362 | PDE Loss:  -4.5919 | Function Loss:  -2.6872\n",
            "Total loss:  -2.6365 | PDE Loss:  -4.5923 | Function Loss:  -2.6875\n",
            "Total loss:  -2.6368 | PDE Loss:  -4.5917 | Function Loss:  -2.6879\n",
            "Total loss:  -2.6371 | PDE Loss:  -4.5924 | Function Loss:  -2.6881\n",
            "Total loss:  -2.6373 | PDE Loss:  -4.5914 | Function Loss:  -2.6885\n",
            "Total loss:  -2.6375 | PDE Loss:  -4.5902 | Function Loss:  -2.6889\n",
            "Total loss:  -2.6378 | PDE Loss:  -4.5887 | Function Loss:  -2.6893\n",
            "Total loss:  -2.6381 | PDE Loss:  -4.5857 | Function Loss:  -2.69\n",
            "Total loss:  -2.6383 | PDE Loss:  -4.5847 | Function Loss:  -2.6905\n",
            "Total loss:  -2.6386 | PDE Loss:  -4.5824 | Function Loss:  -2.6911\n",
            "Total loss:  -2.639 | PDE Loss:  -4.5811 | Function Loss:  -2.6917\n",
            "Total loss:  -2.6394 | PDE Loss:  -4.5812 | Function Loss:  -2.6922\n",
            "Total loss:  -2.64 | PDE Loss:  -4.5773 | Function Loss:  -2.6933\n",
            "Total loss:  -2.6404 | PDE Loss:  -4.5798 | Function Loss:  -2.6934\n",
            "Total loss:  -2.6409 | PDE Loss:  -4.5812 | Function Loss:  -2.6938\n",
            "Total loss:  -2.6413 | PDE Loss:  -4.5844 | Function Loss:  -2.6939\n",
            "Total loss:  -2.6417 | PDE Loss:  -4.5878 | Function Loss:  -2.6939\n",
            "Total loss:  -2.6422 | PDE Loss:  -4.5905 | Function Loss:  -2.6941\n",
            "Total loss:  -2.6427 | PDE Loss:  -4.5927 | Function Loss:  -2.6944\n",
            "Total loss:  -2.6432 | PDE Loss:  -4.5931 | Function Loss:  -2.6949\n",
            "Total loss:  -2.6435 | PDE Loss:  -4.594 | Function Loss:  -2.6952\n",
            "Total loss:  -2.644 | PDE Loss:  -4.5947 | Function Loss:  -2.6956\n",
            "Total loss:  -2.6445 | PDE Loss:  -4.5953 | Function Loss:  -2.696\n",
            "Total loss:  -2.6449 | PDE Loss:  -4.5941 | Function Loss:  -2.6967\n",
            "Total loss:  -2.6454 | PDE Loss:  -4.5948 | Function Loss:  -2.6971\n",
            "Total loss:  -2.6458 | PDE Loss:  -4.5929 | Function Loss:  -2.6979\n",
            "Total loss:  -2.6463 | PDE Loss:  -4.5938 | Function Loss:  -2.6983\n",
            "Total loss:  -2.6467 | PDE Loss:  -4.5924 | Function Loss:  -2.699\n",
            "Total loss:  -2.6471 | PDE Loss:  -4.5925 | Function Loss:  -2.6994\n",
            "Total loss:  -2.6474 | PDE Loss:  -4.5923 | Function Loss:  -2.6998\n",
            "Total loss:  -2.6477 | PDE Loss:  -4.5921 | Function Loss:  -2.7002\n",
            "Total loss:  -2.648 | PDE Loss:  -4.5927 | Function Loss:  -2.7004\n",
            "Total loss:  -2.6484 | PDE Loss:  -4.593 | Function Loss:  -2.7008\n",
            "Total loss:  -2.6488 | PDE Loss:  -4.5935 | Function Loss:  -2.7012\n",
            "Total loss:  -2.6493 | PDE Loss:  -4.5956 | Function Loss:  -2.7014\n",
            "Total loss:  -2.6497 | PDE Loss:  -4.5946 | Function Loss:  -2.702\n",
            "Total loss:  -2.6502 | PDE Loss:  -4.5992 | Function Loss:  -2.7021\n",
            "Total loss:  -2.6506 | PDE Loss:  -4.5976 | Function Loss:  -2.7027\n",
            "Total loss:  -2.6511 | PDE Loss:  -4.5981 | Function Loss:  -2.7031\n",
            "Total loss:  -2.6514 | PDE Loss:  -4.5981 | Function Loss:  -2.7035\n",
            "Total loss:  -2.6516 | PDE Loss:  -4.5976 | Function Loss:  -2.7038\n",
            "Total loss:  -2.6518 | PDE Loss:  -4.5981 | Function Loss:  -2.704\n",
            "Total loss:  -2.652 | PDE Loss:  -4.5967 | Function Loss:  -2.7044\n",
            "Total loss:  -2.6522 | PDE Loss:  -4.5963 | Function Loss:  -2.7047\n",
            "Total loss:  -2.6525 | PDE Loss:  -4.5947 | Function Loss:  -2.7051\n",
            "Total loss:  -2.6526 | PDE Loss:  -4.5914 | Function Loss:  -2.7057\n",
            "Total loss:  -2.6529 | PDE Loss:  -4.5899 | Function Loss:  -2.7063\n",
            "Total loss:  -2.6531 | PDE Loss:  -4.5877 | Function Loss:  -2.7068\n",
            "Total loss:  -2.6534 | PDE Loss:  -4.5857 | Function Loss:  -2.7074\n",
            "Total loss:  -2.6536 | PDE Loss:  -4.5822 | Function Loss:  -2.7081\n",
            "Total loss:  -2.6539 | PDE Loss:  -4.5796 | Function Loss:  -2.7087\n",
            "Total loss:  -2.6541 | PDE Loss:  -4.5761 | Function Loss:  -2.7095\n",
            "Total loss:  -2.6543 | PDE Loss:  -4.5734 | Function Loss:  -2.7101\n",
            "Total loss:  -2.6546 | PDE Loss:  -4.5704 | Function Loss:  -2.7108\n",
            "Total loss:  -2.6548 | PDE Loss:  -4.5698 | Function Loss:  -2.7111\n",
            "Total loss:  -2.655 | PDE Loss:  -4.5694 | Function Loss:  -2.7114\n",
            "Total loss:  -2.6552 | PDE Loss:  -4.5702 | Function Loss:  -2.7115\n",
            "Total loss:  -2.6553 | PDE Loss:  -4.5717 | Function Loss:  -2.7115\n",
            "Total loss:  -2.6555 | PDE Loss:  -4.5729 | Function Loss:  -2.7115\n",
            "Total loss:  -2.6557 | PDE Loss:  -4.5744 | Function Loss:  -2.7115\n",
            "Total loss:  -2.6559 | PDE Loss:  -4.575 | Function Loss:  -2.7117\n",
            "Total loss:  -2.6561 | PDE Loss:  -4.5764 | Function Loss:  -2.7117\n",
            "Total loss:  -2.6563 | PDE Loss:  -4.5763 | Function Loss:  -2.712\n",
            "Total loss:  -2.6565 | PDE Loss:  -4.576 | Function Loss:  -2.7122\n",
            "Total loss:  -2.6567 | PDE Loss:  -4.577 | Function Loss:  -2.7123\n",
            "Total loss:  -2.6569 | PDE Loss:  -4.5753 | Function Loss:  -2.7127\n",
            "Total loss:  -2.657 | PDE Loss:  -4.5756 | Function Loss:  -2.7129\n",
            "Total loss:  -2.6572 | PDE Loss:  -4.5749 | Function Loss:  -2.7132\n",
            "Total loss:  -2.6574 | PDE Loss:  -4.5741 | Function Loss:  -2.7135\n",
            "Total loss:  -2.6576 | PDE Loss:  -4.5733 | Function Loss:  -2.7138\n",
            "Total loss:  -2.6578 | PDE Loss:  -4.5722 | Function Loss:  -2.7142\n",
            "Total loss:  -2.6581 | PDE Loss:  -4.5715 | Function Loss:  -2.7146\n",
            "Total loss:  -2.6584 | PDE Loss:  -4.57 | Function Loss:  -2.7152\n",
            "Total loss:  -2.6586 | PDE Loss:  -4.5693 | Function Loss:  -2.7156\n",
            "Total loss:  -2.6589 | PDE Loss:  -4.5681 | Function Loss:  -2.716\n",
            "Total loss:  -2.6593 | PDE Loss:  -4.5677 | Function Loss:  -2.7165\n",
            "Total loss:  -2.6597 | PDE Loss:  -4.5671 | Function Loss:  -2.7171\n",
            "Total loss:  -2.6602 | PDE Loss:  -4.5665 | Function Loss:  -2.7177\n",
            "Total loss:  -2.6607 | PDE Loss:  -4.5661 | Function Loss:  -2.7183\n",
            "Total loss:  -2.6611 | PDE Loss:  -4.565 | Function Loss:  -2.719\n",
            "Total loss:  -2.6614 | PDE Loss:  -4.5648 | Function Loss:  -2.7194\n",
            "Total loss:  -2.6617 | PDE Loss:  -4.5637 | Function Loss:  -2.7198\n",
            "Total loss:  -2.662 | PDE Loss:  -4.5632 | Function Loss:  -2.7202\n",
            "Total loss:  -2.6623 | PDE Loss:  -4.5621 | Function Loss:  -2.7207\n",
            "Total loss:  -2.6625 | PDE Loss:  -4.5618 | Function Loss:  -2.7211\n",
            "Total loss:  -2.6627 | PDE Loss:  -4.561 | Function Loss:  -2.7214\n",
            "Total loss:  -2.663 | PDE Loss:  -4.5604 | Function Loss:  -2.7218\n",
            "Total loss:  -2.6632 | PDE Loss:  -4.5586 | Function Loss:  -2.7224\n",
            "Total loss:  -2.6635 | PDE Loss:  -4.557 | Function Loss:  -2.7229\n",
            "Total loss:  -2.6637 | PDE Loss:  -4.5547 | Function Loss:  -2.7234\n",
            "Total loss:  -2.6638 | PDE Loss:  -4.5538 | Function Loss:  -2.7237\n",
            "Total loss:  -2.6641 | PDE Loss:  -4.5513 | Function Loss:  -2.7244\n",
            "Total loss:  -2.6643 | PDE Loss:  -4.5499 | Function Loss:  -2.7249\n",
            "Total loss:  -2.6645 | PDE Loss:  -4.5478 | Function Loss:  -2.7254\n",
            "Total loss:  -2.6647 | PDE Loss:  -4.546 | Function Loss:  -2.7259\n",
            "Total loss:  -2.665 | PDE Loss:  -4.5438 | Function Loss:  -2.7266\n",
            "Total loss:  -2.6653 | PDE Loss:  -4.5419 | Function Loss:  -2.7272\n",
            "Total loss:  -2.6657 | PDE Loss:  -4.5383 | Function Loss:  -2.7282\n",
            "Total loss:  -2.6661 | PDE Loss:  -4.5394 | Function Loss:  -2.7285\n",
            "Total loss:  -2.6664 | PDE Loss:  -4.5372 | Function Loss:  -2.7292\n",
            "Total loss:  -2.6668 | PDE Loss:  -4.5371 | Function Loss:  -2.7297\n",
            "Total loss:  -2.6672 | PDE Loss:  -4.5374 | Function Loss:  -2.7301\n",
            "Total loss:  -2.6675 | PDE Loss:  -4.5384 | Function Loss:  -2.7303\n",
            "Total loss:  -2.668 | PDE Loss:  -4.5387 | Function Loss:  -2.7308\n",
            "Total loss:  -2.6684 | PDE Loss:  -4.5384 | Function Loss:  -2.7314\n",
            "Total loss:  -2.6688 | PDE Loss:  -4.5376 | Function Loss:  -2.7319\n",
            "Total loss:  -2.6691 | PDE Loss:  -4.5371 | Function Loss:  -2.7323\n",
            "Total loss:  -2.6693 | PDE Loss:  -4.5359 | Function Loss:  -2.7328\n",
            "Total loss:  -2.6696 | PDE Loss:  -4.5355 | Function Loss:  -2.7332\n",
            "Total loss:  -2.67 | PDE Loss:  -4.5346 | Function Loss:  -2.7338\n",
            "Total loss:  -2.6704 | PDE Loss:  -4.535 | Function Loss:  -2.7341\n",
            "Total loss:  -2.6708 | PDE Loss:  -4.5339 | Function Loss:  -2.7349\n",
            "Total loss:  -2.6714 | PDE Loss:  -4.5354 | Function Loss:  -2.7353\n",
            "Total loss:  -2.6718 | PDE Loss:  -4.5359 | Function Loss:  -2.7356\n",
            "Total loss:  -2.6721 | PDE Loss:  -4.535 | Function Loss:  -2.7362\n",
            "Total loss:  -2.6725 | PDE Loss:  -4.5353 | Function Loss:  -2.7365\n",
            "Total loss:  -2.6728 | PDE Loss:  -4.5343 | Function Loss:  -2.7371\n",
            "Total loss:  -2.6732 | PDE Loss:  -4.5334 | Function Loss:  -2.7377\n",
            "Total loss:  -2.6736 | PDE Loss:  -4.5307 | Function Loss:  -2.7385\n",
            "Total loss:  -2.6738 | PDE Loss:  -4.5298 | Function Loss:  -2.739\n",
            "Total loss:  -2.6741 | PDE Loss:  -4.531 | Function Loss:  -2.7391\n",
            "Total loss:  -2.6744 | PDE Loss:  -4.5308 | Function Loss:  -2.7395\n",
            "Total loss:  -2.6748 | PDE Loss:  -4.5308 | Function Loss:  -2.7399\n",
            "Total loss:  -2.6752 | PDE Loss:  -4.53 | Function Loss:  -2.7406\n",
            "Total loss:  -2.6755 | PDE Loss:  -4.5301 | Function Loss:  -2.7408\n",
            "Total loss:  -2.6758 | PDE Loss:  -4.5275 | Function Loss:  -2.7417\n",
            "Total loss:  -2.6761 | PDE Loss:  -4.5264 | Function Loss:  -2.7421\n",
            "Total loss:  -2.6763 | PDE Loss:  -4.5244 | Function Loss:  -2.7427\n",
            "Total loss:  -2.6765 | PDE Loss:  -4.5207 | Function Loss:  -2.7436\n",
            "Total loss:  -2.6767 | PDE Loss:  -4.5185 | Function Loss:  -2.7442\n",
            "Total loss:  -2.6768 | PDE Loss:  -4.5152 | Function Loss:  -2.7449\n",
            "Total loss:  -2.677 | PDE Loss:  -4.5126 | Function Loss:  -2.7456\n",
            "Total loss:  -2.6773 | PDE Loss:  -4.509 | Function Loss:  -2.7465\n",
            "Total loss:  -2.6776 | PDE Loss:  -4.5061 | Function Loss:  -2.7473\n",
            "Total loss:  -2.6778 | PDE Loss:  -4.498 | Function Loss:  -2.7491\n",
            "Total loss:  -2.6782 | PDE Loss:  -4.4976 | Function Loss:  -2.7495\n",
            "Total loss:  -2.6786 | PDE Loss:  -4.4987 | Function Loss:  -2.7498\n",
            "Total loss:  -2.679 | PDE Loss:  -4.4994 | Function Loss:  -2.7502\n",
            "Total loss:  -2.6794 | PDE Loss:  -4.5024 | Function Loss:  -2.7502\n",
            "Total loss:  -2.6799 | PDE Loss:  -4.5028 | Function Loss:  -2.7506\n",
            "Total loss:  -2.6802 | PDE Loss:  -4.5067 | Function Loss:  -2.7504\n",
            "Total loss:  -2.6805 | PDE Loss:  -4.5073 | Function Loss:  -2.7506\n",
            "Total loss:  -2.6808 | PDE Loss:  -4.5084 | Function Loss:  -2.7508\n",
            "Total loss:  -2.6812 | PDE Loss:  -4.5074 | Function Loss:  -2.7514\n",
            "Total loss:  -2.6816 | PDE Loss:  -4.5087 | Function Loss:  -2.7516\n",
            "Total loss:  -2.6818 | PDE Loss:  -4.5082 | Function Loss:  -2.7519\n",
            "Total loss:  -2.682 | PDE Loss:  -4.5072 | Function Loss:  -2.7524\n",
            "Total loss:  -2.6824 | PDE Loss:  -4.507 | Function Loss:  -2.7528\n",
            "Total loss:  -2.6828 | PDE Loss:  -4.5062 | Function Loss:  -2.7534\n",
            "Total loss:  -2.6832 | PDE Loss:  -4.5053 | Function Loss:  -2.7541\n",
            "Total loss:  -2.6835 | PDE Loss:  -4.5039 | Function Loss:  -2.7547\n",
            "Total loss:  -2.6839 | PDE Loss:  -4.503 | Function Loss:  -2.7553\n",
            "Total loss:  -2.6843 | PDE Loss:  -4.5031 | Function Loss:  -2.7557\n",
            "Total loss:  -2.6847 | PDE Loss:  -4.5036 | Function Loss:  -2.7562\n",
            "Total loss:  -2.6851 | PDE Loss:  -4.5046 | Function Loss:  -2.7565\n",
            "Total loss:  -2.6855 | PDE Loss:  -4.5063 | Function Loss:  -2.7566\n",
            "Total loss:  -2.686 | PDE Loss:  -4.5088 | Function Loss:  -2.7567\n",
            "Total loss:  -2.6866 | PDE Loss:  -4.5125 | Function Loss:  -2.7568\n",
            "Total loss:  -2.6873 | PDE Loss:  -4.5181 | Function Loss:  -2.7567\n",
            "Total loss:  -2.688 | PDE Loss:  -4.5222 | Function Loss:  -2.7568\n",
            "Total loss:  -2.6888 | PDE Loss:  -4.5294 | Function Loss:  -2.7565\n",
            "Total loss:  -2.6895 | PDE Loss:  -4.5339 | Function Loss:  -2.7566\n",
            "Total loss:  -2.6902 | PDE Loss:  -4.5391 | Function Loss:  -2.7565\n",
            "Total loss:  -2.6908 | PDE Loss:  -4.5411 | Function Loss:  -2.7569\n",
            "Total loss:  -2.6914 | PDE Loss:  -4.5431 | Function Loss:  -2.7573\n",
            "Total loss:  -2.6921 | PDE Loss:  -4.5418 | Function Loss:  -2.7582\n",
            "Total loss:  -2.6927 | PDE Loss:  -4.5416 | Function Loss:  -2.759\n",
            "Total loss:  -2.6933 | PDE Loss:  -4.5399 | Function Loss:  -2.76\n",
            "Total loss:  -2.6941 | PDE Loss:  -4.5393 | Function Loss:  -2.761\n",
            "Total loss:  -2.6948 | PDE Loss:  -4.5377 | Function Loss:  -2.7621\n",
            "Total loss:  -2.6954 | PDE Loss:  -4.5374 | Function Loss:  -2.7629\n",
            "Total loss:  -2.6958 | PDE Loss:  -4.5374 | Function Loss:  -2.7634\n",
            "Total loss:  -2.6963 | PDE Loss:  -4.5366 | Function Loss:  -2.764\n",
            "Total loss:  -2.6967 | PDE Loss:  -4.5379 | Function Loss:  -2.7643\n",
            "Total loss:  -2.697 | PDE Loss:  -4.5371 | Function Loss:  -2.7648\n",
            "Total loss:  -2.6973 | PDE Loss:  -4.5379 | Function Loss:  -2.765\n",
            "Total loss:  -2.6976 | PDE Loss:  -4.5377 | Function Loss:  -2.7654\n",
            "Total loss:  -2.6979 | PDE Loss:  -4.5381 | Function Loss:  -2.7657\n",
            "Total loss:  -2.6982 | PDE Loss:  -4.5384 | Function Loss:  -2.766\n",
            "Total loss:  -2.6986 | PDE Loss:  -4.5395 | Function Loss:  -2.7662\n",
            "Total loss:  -2.6989 | PDE Loss:  -4.5398 | Function Loss:  -2.7666\n",
            "Total loss:  -2.6993 | PDE Loss:  -4.5412 | Function Loss:  -2.7668\n",
            "Total loss:  -2.6998 | PDE Loss:  -4.5411 | Function Loss:  -2.7674\n",
            "Total loss:  -2.7001 | PDE Loss:  -4.5422 | Function Loss:  -2.7676\n",
            "Total loss:  -2.7004 | PDE Loss:  -4.5419 | Function Loss:  -2.768\n",
            "Total loss:  -2.7007 | PDE Loss:  -4.5412 | Function Loss:  -2.7684\n",
            "Total loss:  -2.7011 | PDE Loss:  -4.5386 | Function Loss:  -2.7693\n",
            "Total loss:  -2.7015 | PDE Loss:  -4.5376 | Function Loss:  -2.77\n",
            "Total loss:  -2.7018 | PDE Loss:  -4.5358 | Function Loss:  -2.7706\n",
            "Total loss:  -2.7023 | PDE Loss:  -4.5348 | Function Loss:  -2.7713\n",
            "Total loss:  -2.7026 | PDE Loss:  -4.5284 | Function Loss:  -2.7729\n",
            "Total loss:  -2.7031 | PDE Loss:  -4.527 | Function Loss:  -2.7737\n",
            "Total loss:  -2.7035 | PDE Loss:  -4.5282 | Function Loss:  -2.7739\n",
            "Total loss:  -2.7039 | PDE Loss:  -4.5301 | Function Loss:  -2.7741\n",
            "Total loss:  -2.7043 | PDE Loss:  -4.5292 | Function Loss:  -2.7748\n",
            "Total loss:  -2.7047 | PDE Loss:  -4.5304 | Function Loss:  -2.7749\n",
            "Total loss:  -2.705 | PDE Loss:  -4.5291 | Function Loss:  -2.7755\n",
            "Total loss:  -2.7053 | PDE Loss:  -4.5285 | Function Loss:  -2.776\n",
            "Total loss:  -2.7056 | PDE Loss:  -4.5275 | Function Loss:  -2.7765\n",
            "Total loss:  -2.706 | PDE Loss:  -4.5267 | Function Loss:  -2.7772\n",
            "Total loss:  -2.7064 | PDE Loss:  -4.5267 | Function Loss:  -2.7776\n",
            "Total loss:  -2.7067 | PDE Loss:  -4.5265 | Function Loss:  -2.778\n",
            "Total loss:  -2.707 | PDE Loss:  -4.5272 | Function Loss:  -2.7782\n",
            "Total loss:  -2.7073 | PDE Loss:  -4.5282 | Function Loss:  -2.7785\n",
            "Total loss:  -2.7077 | PDE Loss:  -4.5293 | Function Loss:  -2.7786\n",
            "Total loss:  -2.708 | PDE Loss:  -4.5304 | Function Loss:  -2.7788\n",
            "Total loss:  -2.7083 | PDE Loss:  -4.5316 | Function Loss:  -2.7789\n",
            "Total loss:  -2.7085 | PDE Loss:  -4.5333 | Function Loss:  -2.779\n",
            "Total loss:  -2.7089 | PDE Loss:  -4.5359 | Function Loss:  -2.779\n",
            "Total loss:  -2.7086 | PDE Loss:  -4.537 | Function Loss:  -2.7784\n",
            "Total loss:  -2.7091 | PDE Loss:  -4.5371 | Function Loss:  -2.7789\n",
            "Total loss:  -2.7094 | PDE Loss:  -4.5402 | Function Loss:  -2.7788\n",
            "Total loss:  -2.7097 | PDE Loss:  -4.5426 | Function Loss:  -2.7787\n",
            "Total loss:  -2.7099 | PDE Loss:  -4.5444 | Function Loss:  -2.7787\n",
            "Total loss:  -2.7102 | PDE Loss:  -4.5467 | Function Loss:  -2.7786\n",
            "Total loss:  -2.7104 | PDE Loss:  -4.547 | Function Loss:  -2.7788\n",
            "Total loss:  -2.7107 | PDE Loss:  -4.5481 | Function Loss:  -2.7789\n",
            "Total loss:  -2.711 | PDE Loss:  -4.5469 | Function Loss:  -2.7794\n",
            "Total loss:  -2.7112 | PDE Loss:  -4.5464 | Function Loss:  -2.7798\n",
            "Total loss:  -2.7115 | PDE Loss:  -4.5445 | Function Loss:  -2.7804\n",
            "Total loss:  -2.7116 | PDE Loss:  -4.5427 | Function Loss:  -2.7809\n",
            "Total loss:  -2.7118 | PDE Loss:  -4.542 | Function Loss:  -2.7813\n",
            "Total loss:  -2.712 | PDE Loss:  -4.5414 | Function Loss:  -2.7816\n",
            "Total loss:  -2.7121 | PDE Loss:  -4.5419 | Function Loss:  -2.7817\n",
            "Total loss:  -2.7123 | PDE Loss:  -4.5426 | Function Loss:  -2.7818\n",
            "Total loss:  -2.7125 | PDE Loss:  -4.5436 | Function Loss:  -2.7818\n",
            "Total loss:  -2.7127 | PDE Loss:  -4.5451 | Function Loss:  -2.7817\n",
            "Total loss:  -2.7128 | PDE Loss:  -4.5458 | Function Loss:  -2.7818\n",
            "Total loss:  -2.713 | PDE Loss:  -4.5467 | Function Loss:  -2.7818\n",
            "Total loss:  -2.7131 | PDE Loss:  -4.5471 | Function Loss:  -2.7819\n",
            "Total loss:  -2.7133 | PDE Loss:  -4.5472 | Function Loss:  -2.7821\n",
            "Total loss:  -2.7134 | PDE Loss:  -4.5472 | Function Loss:  -2.7823\n",
            "Total loss:  -2.7136 | PDE Loss:  -4.5468 | Function Loss:  -2.7825\n",
            "Total loss:  -2.7138 | PDE Loss:  -4.5463 | Function Loss:  -2.7829\n",
            "Total loss:  -2.7141 | PDE Loss:  -4.5449 | Function Loss:  -2.7834\n",
            "Total loss:  -2.7143 | PDE Loss:  -4.544 | Function Loss:  -2.7839\n",
            "Total loss:  -2.7146 | PDE Loss:  -4.5416 | Function Loss:  -2.7846\n",
            "Total loss:  -2.7148 | PDE Loss:  -4.5407 | Function Loss:  -2.7851\n",
            "Total loss:  -2.7151 | PDE Loss:  -4.5393 | Function Loss:  -2.7857\n",
            "Total loss:  -2.7155 | PDE Loss:  -4.5393 | Function Loss:  -2.7861\n",
            "Total loss:  -2.7158 | PDE Loss:  -4.5387 | Function Loss:  -2.7866\n",
            "Total loss:  -2.7161 | PDE Loss:  -4.5384 | Function Loss:  -2.787\n",
            "Total loss:  -2.7163 | PDE Loss:  -4.5393 | Function Loss:  -2.7871\n",
            "Total loss:  -2.7166 | PDE Loss:  -4.5397 | Function Loss:  -2.7873\n",
            "Total loss:  -2.7168 | PDE Loss:  -4.5394 | Function Loss:  -2.7876\n",
            "Total loss:  -2.7168 | PDE Loss:  -4.5414 | Function Loss:  -2.7872\n",
            "Total loss:  -2.7169 | PDE Loss:  -4.5407 | Function Loss:  -2.7875\n",
            "Total loss:  -2.7173 | PDE Loss:  -4.5393 | Function Loss:  -2.7882\n",
            "Total loss:  -2.7174 | PDE Loss:  -4.5381 | Function Loss:  -2.7886\n",
            "Total loss:  -2.7178 | PDE Loss:  -4.5366 | Function Loss:  -2.7893\n",
            "Total loss:  -2.7181 | PDE Loss:  -4.5349 | Function Loss:  -2.79\n",
            "Total loss:  -2.7184 | PDE Loss:  -4.5338 | Function Loss:  -2.7905\n",
            "Total loss:  -2.7186 | PDE Loss:  -4.5331 | Function Loss:  -2.7909\n",
            "Total loss:  -2.7188 | PDE Loss:  -4.5326 | Function Loss:  -2.7912\n",
            "Total loss:  -2.719 | PDE Loss:  -4.5333 | Function Loss:  -2.7913\n",
            "Total loss:  -2.7191 | PDE Loss:  -4.5333 | Function Loss:  -2.7915\n",
            "Total loss:  -2.7193 | PDE Loss:  -4.5343 | Function Loss:  -2.7914\n",
            "Total loss:  -2.7195 | PDE Loss:  -4.5346 | Function Loss:  -2.7916\n",
            "Total loss:  -2.7198 | PDE Loss:  -4.5349 | Function Loss:  -2.792\n",
            "Total loss:  -2.7202 | PDE Loss:  -4.5339 | Function Loss:  -2.7926\n",
            "Total loss:  -2.7205 | PDE Loss:  -4.5328 | Function Loss:  -2.7931\n",
            "Total loss:  -2.7208 | PDE Loss:  -4.5317 | Function Loss:  -2.7937\n",
            "Total loss:  -2.7211 | PDE Loss:  -4.5299 | Function Loss:  -2.7944\n",
            "Total loss:  -2.7215 | PDE Loss:  -4.5286 | Function Loss:  -2.7951\n",
            "Total loss:  -2.7218 | PDE Loss:  -4.5274 | Function Loss:  -2.7957\n",
            "Total loss:  -2.7222 | PDE Loss:  -4.5281 | Function Loss:  -2.796\n",
            "Total loss:  -2.7225 | PDE Loss:  -4.5287 | Function Loss:  -2.7962\n",
            "Total loss:  -2.7229 | PDE Loss:  -4.5314 | Function Loss:  -2.7962\n",
            "Total loss:  -2.7233 | PDE Loss:  -4.5328 | Function Loss:  -2.7964\n",
            "Total loss:  -2.7237 | PDE Loss:  -4.5356 | Function Loss:  -2.7964\n",
            "Total loss:  -2.7242 | PDE Loss:  -4.5376 | Function Loss:  -2.7967\n",
            "Total loss:  -2.7246 | PDE Loss:  -4.5379 | Function Loss:  -2.7971\n",
            "Total loss:  -2.725 | PDE Loss:  -4.5375 | Function Loss:  -2.7976\n",
            "Total loss:  -2.7254 | PDE Loss:  -4.5369 | Function Loss:  -2.7982\n",
            "Total loss:  -2.7256 | PDE Loss:  -4.5348 | Function Loss:  -2.7989\n",
            "Total loss:  -2.7258 | PDE Loss:  -4.5343 | Function Loss:  -2.7992\n",
            "Total loss:  -2.726 | PDE Loss:  -4.5327 | Function Loss:  -2.7997\n",
            "Total loss:  -2.7263 | PDE Loss:  -4.5315 | Function Loss:  -2.8002\n",
            "Total loss:  -2.7265 | PDE Loss:  -4.5301 | Function Loss:  -2.8008\n",
            "Total loss:  -2.7266 | PDE Loss:  -4.5284 | Function Loss:  -2.8013\n",
            "Total loss:  -2.7269 | PDE Loss:  -4.5271 | Function Loss:  -2.8018\n",
            "Total loss:  -2.727 | PDE Loss:  -4.5276 | Function Loss:  -2.8019\n",
            "Total loss:  -2.7273 | PDE Loss:  -4.5273 | Function Loss:  -2.8022\n",
            "Total loss:  -2.7274 | PDE Loss:  -4.5272 | Function Loss:  -2.8024\n",
            "Total loss:  -2.7276 | PDE Loss:  -4.5268 | Function Loss:  -2.8027\n",
            "Total loss:  -2.7278 | PDE Loss:  -4.526 | Function Loss:  -2.803\n",
            "Total loss:  -2.728 | PDE Loss:  -4.5252 | Function Loss:  -2.8034\n",
            "Total loss:  -2.7282 | PDE Loss:  -4.5233 | Function Loss:  -2.804\n",
            "Total loss:  -2.7284 | PDE Loss:  -4.5219 | Function Loss:  -2.8046\n",
            "Total loss:  -2.7287 | PDE Loss:  -4.5201 | Function Loss:  -2.8053\n",
            "Total loss:  -2.7289 | PDE Loss:  -4.5193 | Function Loss:  -2.8057\n",
            "Total loss:  -2.7292 | PDE Loss:  -4.5156 | Function Loss:  -2.8068\n",
            "Total loss:  -2.7294 | PDE Loss:  -4.5165 | Function Loss:  -2.8068\n",
            "Total loss:  -2.7297 | PDE Loss:  -4.5178 | Function Loss:  -2.8069\n",
            "Total loss:  -2.7299 | PDE Loss:  -4.5195 | Function Loss:  -2.8068\n",
            "Total loss:  -2.7301 | PDE Loss:  -4.5214 | Function Loss:  -2.8067\n",
            "Total loss:  -2.7303 | PDE Loss:  -4.5231 | Function Loss:  -2.8066\n",
            "Total loss:  -2.7305 | PDE Loss:  -4.5244 | Function Loss:  -2.8066\n",
            "Total loss:  -2.7304 | PDE Loss:  -4.5241 | Function Loss:  -2.8066\n",
            "Total loss:  -2.7306 | PDE Loss:  -4.5245 | Function Loss:  -2.8067\n",
            "Total loss:  -2.731 | PDE Loss:  -4.527 | Function Loss:  -2.8066\n",
            "Total loss:  -2.7312 | PDE Loss:  -4.527 | Function Loss:  -2.8069\n",
            "Total loss:  -2.7315 | PDE Loss:  -4.526 | Function Loss:  -2.8075\n",
            "Total loss:  -2.7318 | PDE Loss:  -4.5245 | Function Loss:  -2.8082\n",
            "Total loss:  -2.7321 | PDE Loss:  -4.5225 | Function Loss:  -2.8089\n",
            "Total loss:  -2.7323 | PDE Loss:  -4.5212 | Function Loss:  -2.8094\n",
            "Total loss:  -2.7325 | PDE Loss:  -4.5192 | Function Loss:  -2.81\n",
            "Total loss:  -2.7327 | PDE Loss:  -4.5187 | Function Loss:  -2.8103\n",
            "Total loss:  -2.733 | PDE Loss:  -4.5179 | Function Loss:  -2.8108\n",
            "Total loss:  -2.7332 | PDE Loss:  -4.5175 | Function Loss:  -2.8112\n",
            "Total loss:  -2.7335 | PDE Loss:  -4.5189 | Function Loss:  -2.8112\n",
            "Total loss:  -2.7337 | PDE Loss:  -4.5205 | Function Loss:  -2.8112\n",
            "Total loss:  -2.734 | PDE Loss:  -4.5227 | Function Loss:  -2.8111\n",
            "Total loss:  -2.7343 | PDE Loss:  -4.525 | Function Loss:  -2.811\n",
            "Total loss:  -2.7345 | PDE Loss:  -4.5265 | Function Loss:  -2.8109\n",
            "Total loss:  -2.7347 | PDE Loss:  -4.5278 | Function Loss:  -2.8109\n",
            "Total loss:  -2.7348 | PDE Loss:  -4.5287 | Function Loss:  -2.8109\n",
            "Total loss:  -2.735 | PDE Loss:  -4.5292 | Function Loss:  -2.8111\n",
            "Total loss:  -2.7352 | PDE Loss:  -4.5288 | Function Loss:  -2.8114\n",
            "Total loss:  -2.7354 | PDE Loss:  -4.5285 | Function Loss:  -2.8117\n",
            "Total loss:  -2.7356 | PDE Loss:  -4.5278 | Function Loss:  -2.8121\n",
            "Total loss:  -2.7358 | PDE Loss:  -4.5275 | Function Loss:  -2.8123\n",
            "Total loss:  -2.7361 | PDE Loss:  -4.5277 | Function Loss:  -2.8126\n",
            "Total loss:  -2.7364 | PDE Loss:  -4.5276 | Function Loss:  -2.8131\n",
            "Total loss:  -2.7368 | PDE Loss:  -4.5329 | Function Loss:  -2.8125\n",
            "Total loss:  -2.7371 | PDE Loss:  -4.5326 | Function Loss:  -2.8129\n",
            "Total loss:  -2.7378 | PDE Loss:  -4.5349 | Function Loss:  -2.8133\n",
            "Total loss:  -2.7386 | PDE Loss:  -4.5383 | Function Loss:  -2.8136\n",
            "Total loss:  -2.7394 | PDE Loss:  -4.5446 | Function Loss:  -2.8134\n",
            "Total loss:  -2.7401 | PDE Loss:  -4.5502 | Function Loss:  -2.8132\n",
            "Total loss:  -2.7408 | PDE Loss:  -4.5549 | Function Loss:  -2.8131\n",
            "Total loss:  -2.7415 | PDE Loss:  -4.5582 | Function Loss:  -2.8134\n",
            "Total loss:  -2.7422 | PDE Loss:  -4.5608 | Function Loss:  -2.8137\n",
            "Total loss:  -2.7428 | PDE Loss:  -4.5604 | Function Loss:  -2.8145\n",
            "Total loss:  -2.7434 | PDE Loss:  -4.5602 | Function Loss:  -2.8152\n",
            "Total loss:  -2.7439 | PDE Loss:  -4.5584 | Function Loss:  -2.8162\n",
            "Total loss:  -2.7446 | PDE Loss:  -4.5573 | Function Loss:  -2.8171\n",
            "Total loss:  -2.7453 | PDE Loss:  -4.5557 | Function Loss:  -2.8183\n",
            "Total loss:  -2.746 | PDE Loss:  -4.5547 | Function Loss:  -2.8193\n",
            "Total loss:  -2.7466 | PDE Loss:  -4.5545 | Function Loss:  -2.8201\n",
            "Total loss:  -2.7471 | PDE Loss:  -4.554 | Function Loss:  -2.8208\n",
            "Total loss:  -2.7475 | PDE Loss:  -4.555 | Function Loss:  -2.8211\n",
            "Total loss:  -2.7479 | PDE Loss:  -4.5561 | Function Loss:  -2.8213\n",
            "Total loss:  -2.7483 | PDE Loss:  -4.5582 | Function Loss:  -2.8214\n",
            "Total loss:  -2.7487 | PDE Loss:  -4.5604 | Function Loss:  -2.8215\n",
            "Total loss:  -2.7491 | PDE Loss:  -4.5617 | Function Loss:  -2.8217\n",
            "Total loss:  -2.7495 | PDE Loss:  -4.5635 | Function Loss:  -2.8219\n",
            "Total loss:  -2.75 | PDE Loss:  -4.5651 | Function Loss:  -2.8221\n",
            "Total loss:  -2.7507 | PDE Loss:  -4.5664 | Function Loss:  -2.8227\n",
            "Total loss:  -2.7516 | PDE Loss:  -4.5677 | Function Loss:  -2.8236\n",
            "Total loss:  -2.7525 | PDE Loss:  -4.57 | Function Loss:  -2.8242\n",
            "Total loss:  -2.7532 | PDE Loss:  -4.57 | Function Loss:  -2.825\n",
            "Total loss:  -2.7542 | PDE Loss:  -4.5737 | Function Loss:  -2.8256\n",
            "Total loss:  -2.7551 | PDE Loss:  -4.5747 | Function Loss:  -2.8265\n",
            "Total loss:  -2.7557 | PDE Loss:  -4.5771 | Function Loss:  -2.8268\n",
            "Total loss:  -2.7562 | PDE Loss:  -4.5784 | Function Loss:  -2.8271\n",
            "Total loss:  -2.7567 | PDE Loss:  -4.5794 | Function Loss:  -2.8275\n",
            "Total loss:  -2.7572 | PDE Loss:  -4.5801 | Function Loss:  -2.828\n",
            "Total loss:  -2.7581 | PDE Loss:  -4.5799 | Function Loss:  -2.8291\n",
            "Total loss:  -2.7592 | PDE Loss:  -4.5826 | Function Loss:  -2.8299\n",
            "Total loss:  -2.7602 | PDE Loss:  -4.5818 | Function Loss:  -2.8312\n",
            "Total loss:  -2.7611 | PDE Loss:  -4.5851 | Function Loss:  -2.8317\n",
            "Total loss:  -2.7618 | PDE Loss:  -4.5855 | Function Loss:  -2.8325\n",
            "Total loss:  -2.7623 | PDE Loss:  -4.5869 | Function Loss:  -2.8327\n",
            "Total loss:  -2.7627 | PDE Loss:  -4.589 | Function Loss:  -2.8329\n",
            "Total loss:  -2.763 | PDE Loss:  -4.5897 | Function Loss:  -2.8331\n",
            "Total loss:  -2.7632 | PDE Loss:  -4.5905 | Function Loss:  -2.8332\n",
            "Total loss:  -2.7635 | PDE Loss:  -4.5907 | Function Loss:  -2.8335\n",
            "Total loss:  -2.7638 | PDE Loss:  -4.5906 | Function Loss:  -2.8338\n",
            "Total loss:  -2.764 | PDE Loss:  -4.5905 | Function Loss:  -2.8342\n",
            "Total loss:  -2.7643 | PDE Loss:  -4.5902 | Function Loss:  -2.8345\n",
            "Total loss:  -2.7644 | PDE Loss:  -4.5902 | Function Loss:  -2.8347\n",
            "Total loss:  -2.7647 | PDE Loss:  -4.5911 | Function Loss:  -2.8348\n",
            "Total loss:  -2.7649 | PDE Loss:  -4.5914 | Function Loss:  -2.835\n",
            "Total loss:  -2.7652 | PDE Loss:  -4.5921 | Function Loss:  -2.8352\n",
            "Total loss:  -2.7654 | PDE Loss:  -4.5923 | Function Loss:  -2.8355\n",
            "Total loss:  -2.7657 | PDE Loss:  -4.5935 | Function Loss:  -2.8355\n",
            "Total loss:  -2.7659 | PDE Loss:  -4.5938 | Function Loss:  -2.8358\n",
            "Total loss:  -2.7662 | PDE Loss:  -4.5952 | Function Loss:  -2.8358\n",
            "Total loss:  -2.7664 | PDE Loss:  -4.5957 | Function Loss:  -2.836\n",
            "Total loss:  -2.7666 | PDE Loss:  -4.5963 | Function Loss:  -2.8362\n",
            "Total loss:  -2.7668 | PDE Loss:  -4.5974 | Function Loss:  -2.8362\n",
            "Total loss:  -2.767 | PDE Loss:  -4.5972 | Function Loss:  -2.8364\n",
            "Total loss:  -2.7671 | PDE Loss:  -4.5976 | Function Loss:  -2.8365\n",
            "Total loss:  -2.7672 | PDE Loss:  -4.5972 | Function Loss:  -2.8367\n",
            "Total loss:  -2.7673 | PDE Loss:  -4.5971 | Function Loss:  -2.8368\n",
            "Total loss:  -2.7674 | PDE Loss:  -4.5967 | Function Loss:  -2.837\n",
            "Total loss:  -2.7675 | PDE Loss:  -4.596 | Function Loss:  -2.8373\n",
            "Total loss:  -2.7676 | PDE Loss:  -4.595 | Function Loss:  -2.8376\n",
            "Total loss:  -2.7678 | PDE Loss:  -4.5939 | Function Loss:  -2.838\n",
            "Total loss:  -2.768 | PDE Loss:  -4.5925 | Function Loss:  -2.8385\n",
            "Total loss:  -2.7683 | PDE Loss:  -4.5917 | Function Loss:  -2.839\n",
            "Total loss:  -2.7687 | PDE Loss:  -4.5919 | Function Loss:  -2.8395\n",
            "Total loss:  -2.7691 | PDE Loss:  -4.5927 | Function Loss:  -2.8397\n",
            "Total loss:  -2.7694 | PDE Loss:  -4.5937 | Function Loss:  -2.8399\n",
            "Total loss:  -2.7697 | PDE Loss:  -4.5949 | Function Loss:  -2.84\n",
            "Total loss:  -2.7698 | PDE Loss:  -4.596 | Function Loss:  -2.84\n",
            "Total loss:  -2.77 | PDE Loss:  -4.5968 | Function Loss:  -2.8401\n",
            "Total loss:  -2.7702 | PDE Loss:  -4.5973 | Function Loss:  -2.8402\n",
            "Total loss:  -2.7705 | PDE Loss:  -4.5983 | Function Loss:  -2.8404\n",
            "Total loss:  -2.7708 | PDE Loss:  -4.5988 | Function Loss:  -2.8406\n",
            "Total loss:  -2.7711 | PDE Loss:  -4.5993 | Function Loss:  -2.8409\n",
            "Total loss:  -2.7715 | PDE Loss:  -4.5998 | Function Loss:  -2.8413\n",
            "Total loss:  -2.7719 | PDE Loss:  -4.5994 | Function Loss:  -2.8419\n",
            "Total loss:  -2.7723 | PDE Loss:  -4.6006 | Function Loss:  -2.8421\n",
            "Total loss:  -2.7725 | PDE Loss:  -4.601 | Function Loss:  -2.8423\n",
            "Total loss:  -2.7728 | PDE Loss:  -4.6016 | Function Loss:  -2.8426\n",
            "Total loss:  -2.7732 | PDE Loss:  -4.6044 | Function Loss:  -2.8425\n",
            "Total loss:  -2.7735 | PDE Loss:  -4.6019 | Function Loss:  -2.8434\n",
            "Total loss:  -2.7738 | PDE Loss:  -4.604 | Function Loss:  -2.8433\n",
            "Total loss:  -2.7741 | PDE Loss:  -4.6059 | Function Loss:  -2.8433\n",
            "Total loss:  -2.7745 | PDE Loss:  -4.6074 | Function Loss:  -2.8436\n",
            "Total loss:  -2.775 | PDE Loss:  -4.6079 | Function Loss:  -2.844\n",
            "Total loss:  -2.7755 | PDE Loss:  -4.6051 | Function Loss:  -2.845\n",
            "Total loss:  -2.776 | PDE Loss:  -4.6079 | Function Loss:  -2.8452\n",
            "Total loss:  -2.7764 | PDE Loss:  -4.606 | Function Loss:  -2.846\n",
            "Total loss:  -2.7769 | PDE Loss:  -4.6048 | Function Loss:  -2.8467\n",
            "Total loss:  -2.7774 | PDE Loss:  -4.6043 | Function Loss:  -2.8474\n",
            "Total loss:  -2.7777 | PDE Loss:  -4.6047 | Function Loss:  -2.8478\n",
            "Total loss:  -2.7781 | PDE Loss:  -4.6056 | Function Loss:  -2.848\n",
            "Total loss:  -2.7784 | PDE Loss:  -4.6075 | Function Loss:  -2.848\n",
            "Total loss:  -2.7787 | PDE Loss:  -4.6101 | Function Loss:  -2.848\n",
            "Total loss:  -2.7792 | PDE Loss:  -4.613 | Function Loss:  -2.848\n",
            "Total loss:  -2.7796 | PDE Loss:  -4.6183 | Function Loss:  -2.8476\n",
            "Total loss:  -2.7801 | PDE Loss:  -4.6228 | Function Loss:  -2.8475\n",
            "Total loss:  -2.7808 | PDE Loss:  -4.6286 | Function Loss:  -2.8473\n",
            "Total loss:  -2.7814 | PDE Loss:  -4.6337 | Function Loss:  -2.8472\n",
            "Total loss:  -2.7821 | PDE Loss:  -4.6362 | Function Loss:  -2.8475\n",
            "Total loss:  -2.7827 | PDE Loss:  -4.6391 | Function Loss:  -2.8477\n",
            "Total loss:  -2.7831 | PDE Loss:  -4.6379 | Function Loss:  -2.8485\n",
            "Total loss:  -2.7835 | PDE Loss:  -4.6362 | Function Loss:  -2.8492\n",
            "Total loss:  -2.7839 | PDE Loss:  -4.6328 | Function Loss:  -2.8502\n",
            "Total loss:  -2.7842 | PDE Loss:  -4.6301 | Function Loss:  -2.851\n",
            "Total loss:  -2.7846 | PDE Loss:  -4.6271 | Function Loss:  -2.8519\n",
            "Total loss:  -2.7852 | PDE Loss:  -4.6256 | Function Loss:  -2.853\n",
            "Total loss:  -2.7862 | PDE Loss:  -4.6235 | Function Loss:  -2.8545\n",
            "Total loss:  -2.7874 | PDE Loss:  -4.6233 | Function Loss:  -2.8559\n",
            "Total loss:  -2.7888 | PDE Loss:  -4.6231 | Function Loss:  -2.8575\n",
            "Total loss:  -2.7901 | PDE Loss:  -4.6242 | Function Loss:  -2.8589\n",
            "Total loss:  -2.7913 | PDE Loss:  -4.627 | Function Loss:  -2.8598\n",
            "Total loss:  -2.7922 | PDE Loss:  -4.629 | Function Loss:  -2.8606\n",
            "Total loss:  -2.793 | PDE Loss:  -4.634 | Function Loss:  -2.8606\n",
            "Total loss:  -2.7935 | PDE Loss:  -4.6356 | Function Loss:  -2.8609\n",
            "Total loss:  -2.7939 | PDE Loss:  -4.6365 | Function Loss:  -2.8612\n",
            "Total loss:  -2.7944 | PDE Loss:  -4.6393 | Function Loss:  -2.8614\n",
            "Total loss:  -2.7949 | PDE Loss:  -4.6386 | Function Loss:  -2.8621\n",
            "Total loss:  -2.7953 | PDE Loss:  -4.6379 | Function Loss:  -2.8627\n",
            "Total loss:  -2.796 | PDE Loss:  -4.6372 | Function Loss:  -2.8636\n",
            "Total loss:  -2.7968 | PDE Loss:  -4.6327 | Function Loss:  -2.8652\n",
            "Total loss:  -2.7973 | PDE Loss:  -4.6323 | Function Loss:  -2.866\n",
            "Total loss:  -2.7978 | PDE Loss:  -4.6294 | Function Loss:  -2.867\n",
            "Total loss:  -2.7982 | PDE Loss:  -4.629 | Function Loss:  -2.8676\n",
            "Total loss:  -2.7985 | PDE Loss:  -4.6292 | Function Loss:  -2.8679\n",
            "Total loss:  -2.7988 | PDE Loss:  -4.63 | Function Loss:  -2.8681\n",
            "Total loss:  -2.7991 | PDE Loss:  -4.6312 | Function Loss:  -2.8682\n",
            "Total loss:  -2.7994 | PDE Loss:  -4.6319 | Function Loss:  -2.8685\n",
            "Total loss:  -2.8001 | PDE Loss:  -4.6358 | Function Loss:  -2.8686\n",
            "Total loss:  -2.8009 | PDE Loss:  -4.6296 | Function Loss:  -2.8706\n",
            "Total loss:  -2.8016 | PDE Loss:  -4.6348 | Function Loss:  -2.8706\n",
            "Total loss:  -2.8024 | PDE Loss:  -4.6382 | Function Loss:  -2.8709\n",
            "Total loss:  -2.8032 | PDE Loss:  -4.6401 | Function Loss:  -2.8715\n",
            "Total loss:  -2.8039 | PDE Loss:  -4.6399 | Function Loss:  -2.8724\n",
            "Total loss:  -2.8045 | PDE Loss:  -4.6402 | Function Loss:  -2.873\n",
            "Total loss:  -2.8049 | PDE Loss:  -4.6381 | Function Loss:  -2.8738\n",
            "Total loss:  -2.8052 | PDE Loss:  -4.6385 | Function Loss:  -2.8742\n",
            "Total loss:  -2.8057 | PDE Loss:  -4.6386 | Function Loss:  -2.8747\n",
            "Total loss:  -2.8063 | PDE Loss:  -4.6399 | Function Loss:  -2.8752\n",
            "Total loss:  -2.8071 | PDE Loss:  -4.6406 | Function Loss:  -2.876\n",
            "Total loss:  -2.8078 | PDE Loss:  -4.6409 | Function Loss:  -2.8768\n",
            "Total loss:  -2.8086 | PDE Loss:  -4.6411 | Function Loss:  -2.8776\n",
            "Total loss:  -2.8095 | PDE Loss:  -4.6412 | Function Loss:  -2.8788\n",
            "Total loss:  -2.8104 | PDE Loss:  -4.6399 | Function Loss:  -2.88\n",
            "Total loss:  -2.8112 | PDE Loss:  -4.6413 | Function Loss:  -2.8807\n",
            "Total loss:  -2.8121 | PDE Loss:  -4.6413 | Function Loss:  -2.8817\n",
            "Total loss:  -2.8128 | PDE Loss:  -4.6433 | Function Loss:  -2.8822\n",
            "Total loss:  -2.8133 | PDE Loss:  -4.6437 | Function Loss:  -2.8828\n",
            "Total loss:  -2.8138 | PDE Loss:  -4.6464 | Function Loss:  -2.8829\n",
            "Total loss:  -2.8143 | PDE Loss:  -4.6474 | Function Loss:  -2.8832\n",
            "Total loss:  -2.815 | PDE Loss:  -4.6512 | Function Loss:  -2.8834\n",
            "Total loss:  -2.8159 | PDE Loss:  -4.6521 | Function Loss:  -2.8843\n",
            "Total loss:  -2.8167 | PDE Loss:  -4.6567 | Function Loss:  -2.8845\n",
            "Total loss:  -2.8176 | PDE Loss:  -4.6624 | Function Loss:  -2.8846\n",
            "Total loss:  -2.819 | PDE Loss:  -4.6696 | Function Loss:  -2.885\n",
            "Total loss:  -2.8205 | PDE Loss:  -4.675 | Function Loss:  -2.8859\n",
            "Total loss:  -2.822 | PDE Loss:  -4.6806 | Function Loss:  -2.8867\n",
            "Total loss:  -2.8234 | PDE Loss:  -4.6818 | Function Loss:  -2.8881\n",
            "Total loss:  -2.8244 | PDE Loss:  -4.6806 | Function Loss:  -2.8895\n",
            "Total loss:  -2.8252 | PDE Loss:  -4.6803 | Function Loss:  -2.8905\n",
            "Total loss:  -2.826 | PDE Loss:  -4.678 | Function Loss:  -2.8919\n",
            "Total loss:  -2.827 | PDE Loss:  -4.6779 | Function Loss:  -2.893\n",
            "Total loss:  -2.8281 | PDE Loss:  -4.6785 | Function Loss:  -2.8941\n",
            "Total loss:  -2.8293 | PDE Loss:  -4.6801 | Function Loss:  -2.8953\n",
            "Total loss:  -2.8305 | PDE Loss:  -4.6836 | Function Loss:  -2.8961\n",
            "Total loss:  -2.8316 | PDE Loss:  -4.6859 | Function Loss:  -2.897\n",
            "Total loss:  -2.8322 | PDE Loss:  -4.6898 | Function Loss:  -2.8971\n",
            "Total loss:  -2.8329 | PDE Loss:  -4.6943 | Function Loss:  -2.8972\n",
            "Total loss:  -2.8336 | PDE Loss:  -4.6991 | Function Loss:  -2.8972\n",
            "Total loss:  -2.8342 | PDE Loss:  -4.7044 | Function Loss:  -2.8971\n",
            "Total loss:  -2.8347 | PDE Loss:  -4.7068 | Function Loss:  -2.8973\n",
            "Total loss:  -2.8354 | PDE Loss:  -4.7109 | Function Loss:  -2.8974\n",
            "Total loss:  -2.8362 | PDE Loss:  -4.7146 | Function Loss:  -2.8978\n",
            "Total loss:  -2.837 | PDE Loss:  -4.7151 | Function Loss:  -2.8987\n",
            "Total loss:  -2.8377 | PDE Loss:  -4.7168 | Function Loss:  -2.8992\n",
            "Total loss:  -2.8382 | PDE Loss:  -4.7157 | Function Loss:  -2.9\n",
            "Total loss:  -2.8386 | PDE Loss:  -4.7152 | Function Loss:  -2.9005\n",
            "Total loss:  -2.8391 | PDE Loss:  -4.7132 | Function Loss:  -2.9014\n",
            "Total loss:  -2.8395 | PDE Loss:  -4.7124 | Function Loss:  -2.902\n",
            "Total loss:  -2.8401 | PDE Loss:  -4.7107 | Function Loss:  -2.9029\n",
            "Total loss:  -2.8409 | PDE Loss:  -4.7088 | Function Loss:  -2.9041\n",
            "Total loss:  -2.8415 | PDE Loss:  -4.7057 | Function Loss:  -2.9054\n",
            "Total loss:  -2.8424 | PDE Loss:  -4.7034 | Function Loss:  -2.9068\n",
            "Total loss:  -2.843 | PDE Loss:  -4.7031 | Function Loss:  -2.9075\n",
            "Total loss:  -2.844 | PDE Loss:  -4.7032 | Function Loss:  -2.9087\n",
            "Total loss:  -2.8452 | PDE Loss:  -4.7014 | Function Loss:  -2.9103\n",
            "Total loss:  -2.8461 | PDE Loss:  -4.7011 | Function Loss:  -2.9114\n",
            "Total loss:  -2.8469 | PDE Loss:  -4.7006 | Function Loss:  -2.9124\n",
            "Total loss:  -2.8476 | PDE Loss:  -4.6995 | Function Loss:  -2.9134\n",
            "Total loss:  -2.8481 | PDE Loss:  -4.6987 | Function Loss:  -2.9141\n",
            "Total loss:  -2.8483 | PDE Loss:  -4.6981 | Function Loss:  -2.9145\n",
            "Total loss:  -2.8485 | PDE Loss:  -4.6968 | Function Loss:  -2.9149\n",
            "Total loss:  -2.8487 | PDE Loss:  -4.6957 | Function Loss:  -2.9153\n",
            "Total loss:  -2.8489 | PDE Loss:  -4.6955 | Function Loss:  -2.9156\n",
            "Total loss:  -2.8492 | PDE Loss:  -4.6956 | Function Loss:  -2.916\n",
            "Total loss:  -2.8497 | PDE Loss:  -4.6957 | Function Loss:  -2.9165\n",
            "Total loss:  -2.8502 | PDE Loss:  -4.697 | Function Loss:  -2.9169\n",
            "Total loss:  -2.8508 | PDE Loss:  -4.6971 | Function Loss:  -2.9175\n",
            "Total loss:  -2.8513 | PDE Loss:  -4.6979 | Function Loss:  -2.918\n",
            "Total loss:  -2.8518 | PDE Loss:  -4.6988 | Function Loss:  -2.9184\n",
            "Total loss:  -2.8522 | PDE Loss:  -4.6986 | Function Loss:  -2.919\n",
            "Total loss:  -2.8527 | PDE Loss:  -4.6993 | Function Loss:  -2.9194\n",
            "Total loss:  -2.8533 | PDE Loss:  -4.6999 | Function Loss:  -2.92\n",
            "Total loss:  -2.8541 | PDE Loss:  -4.7014 | Function Loss:  -2.9207\n",
            "Total loss:  -2.8549 | PDE Loss:  -4.7033 | Function Loss:  -2.9213\n",
            "Total loss:  -2.8554 | PDE Loss:  -4.7048 | Function Loss:  -2.9217\n",
            "Total loss:  -2.8557 | PDE Loss:  -4.7049 | Function Loss:  -2.922\n",
            "Total loss:  -2.856 | PDE Loss:  -4.7057 | Function Loss:  -2.9221\n",
            "Total loss:  -2.8561 | PDE Loss:  -4.7062 | Function Loss:  -2.9223\n",
            "Total loss:  -2.8563 | PDE Loss:  -4.7069 | Function Loss:  -2.9223\n",
            "Total loss:  -2.8565 | PDE Loss:  -4.7087 | Function Loss:  -2.9223\n",
            "Total loss:  -2.8567 | PDE Loss:  -4.7092 | Function Loss:  -2.9225\n",
            "Total loss:  -2.857 | PDE Loss:  -4.7116 | Function Loss:  -2.9224\n",
            "Total loss:  -2.8573 | PDE Loss:  -4.7129 | Function Loss:  -2.9225\n",
            "Total loss:  -2.8576 | PDE Loss:  -4.7139 | Function Loss:  -2.9228\n",
            "Total loss:  -2.8581 | PDE Loss:  -4.7162 | Function Loss:  -2.9229\n",
            "Total loss:  -2.8586 | PDE Loss:  -4.7189 | Function Loss:  -2.923\n",
            "Total loss:  -2.8589 | PDE Loss:  -4.7218 | Function Loss:  -2.9229\n",
            "Total loss:  -2.8592 | PDE Loss:  -4.725 | Function Loss:  -2.9228\n",
            "Total loss:  -2.8596 | PDE Loss:  -4.7289 | Function Loss:  -2.9226\n",
            "Total loss:  -2.8599 | PDE Loss:  -4.733 | Function Loss:  -2.9223\n",
            "Total loss:  -2.8602 | PDE Loss:  -4.7353 | Function Loss:  -2.9223\n",
            "Total loss:  -2.8604 | PDE Loss:  -4.7381 | Function Loss:  -2.9221\n",
            "Total loss:  -2.8606 | PDE Loss:  -4.7399 | Function Loss:  -2.9221\n",
            "Total loss:  -2.861 | PDE Loss:  -4.7397 | Function Loss:  -2.9226\n",
            "Total loss:  -2.8613 | PDE Loss:  -4.7398 | Function Loss:  -2.9229\n",
            "Total loss:  -2.8618 | PDE Loss:  -4.7397 | Function Loss:  -2.9235\n",
            "Total loss:  -2.8622 | PDE Loss:  -4.739 | Function Loss:  -2.9241\n",
            "Total loss:  -2.8627 | PDE Loss:  -4.7393 | Function Loss:  -2.9247\n",
            "Total loss:  -2.8632 | PDE Loss:  -4.7388 | Function Loss:  -2.9253\n",
            "Total loss:  -2.8637 | PDE Loss:  -4.7392 | Function Loss:  -2.9257\n",
            "Total loss:  -2.8641 | PDE Loss:  -4.7397 | Function Loss:  -2.9262\n",
            "Total loss:  -2.8645 | PDE Loss:  -4.7409 | Function Loss:  -2.9264\n",
            "Total loss:  -2.8648 | PDE Loss:  -4.7423 | Function Loss:  -2.9266\n",
            "Total loss:  -2.8651 | PDE Loss:  -4.7434 | Function Loss:  -2.9268\n",
            "Total loss:  -2.8654 | PDE Loss:  -4.7454 | Function Loss:  -2.9268\n",
            "Total loss:  -2.8656 | PDE Loss:  -4.7462 | Function Loss:  -2.9269\n",
            "Total loss:  -2.866 | PDE Loss:  -4.7481 | Function Loss:  -2.927\n",
            "Total loss:  -2.8663 | PDE Loss:  -4.749 | Function Loss:  -2.9273\n",
            "Total loss:  -2.8666 | PDE Loss:  -4.75 | Function Loss:  -2.9274\n",
            "Total loss:  -2.8668 | PDE Loss:  -4.7505 | Function Loss:  -2.9276\n",
            "Total loss:  -2.8669 | PDE Loss:  -4.7509 | Function Loss:  -2.9277\n",
            "Total loss:  -2.8671 | PDE Loss:  -4.7512 | Function Loss:  -2.9279\n",
            "Total loss:  -2.8672 | PDE Loss:  -4.7518 | Function Loss:  -2.9279\n",
            "Total loss:  -2.8674 | PDE Loss:  -4.7523 | Function Loss:  -2.928\n",
            "Total loss:  -2.8675 | PDE Loss:  -4.7528 | Function Loss:  -2.9282\n",
            "Total loss:  -2.8678 | PDE Loss:  -4.7538 | Function Loss:  -2.9282\n",
            "Total loss:  -2.8679 | PDE Loss:  -4.7505 | Function Loss:  -2.9289\n",
            "Total loss:  -2.8681 | PDE Loss:  -4.7513 | Function Loss:  -2.9291\n",
            "Total loss:  -2.8684 | PDE Loss:  -4.7532 | Function Loss:  -2.9291\n",
            "Total loss:  -2.8689 | PDE Loss:  -4.7563 | Function Loss:  -2.9292\n",
            "Total loss:  -2.8694 | PDE Loss:  -4.759 | Function Loss:  -2.9294\n",
            "Total loss:  -2.8698 | PDE Loss:  -4.7604 | Function Loss:  -2.9296\n",
            "Total loss:  -2.8701 | PDE Loss:  -4.7618 | Function Loss:  -2.9298\n",
            "Total loss:  -2.8704 | PDE Loss:  -4.7622 | Function Loss:  -2.9301\n",
            "Total loss:  -2.8707 | PDE Loss:  -4.7628 | Function Loss:  -2.9303\n",
            "Total loss:  -2.8711 | PDE Loss:  -4.7631 | Function Loss:  -2.9307\n",
            "Total loss:  -2.8713 | PDE Loss:  -4.7628 | Function Loss:  -2.931\n",
            "Total loss:  -2.8717 | PDE Loss:  -4.7625 | Function Loss:  -2.9314\n",
            "Total loss:  -2.8719 | PDE Loss:  -4.7618 | Function Loss:  -2.9319\n",
            "Total loss:  -2.8723 | PDE Loss:  -4.7616 | Function Loss:  -2.9323\n",
            "Total loss:  -2.8727 | PDE Loss:  -4.7616 | Function Loss:  -2.9327\n",
            "Total loss:  -2.873 | PDE Loss:  -4.7619 | Function Loss:  -2.9331\n",
            "Total loss:  -2.8735 | PDE Loss:  -4.7627 | Function Loss:  -2.9335\n",
            "Total loss:  -2.8739 | PDE Loss:  -4.7638 | Function Loss:  -2.9338\n",
            "Total loss:  -2.8745 | PDE Loss:  -4.7647 | Function Loss:  -2.9343\n",
            "Total loss:  -2.875 | PDE Loss:  -4.7653 | Function Loss:  -2.9349\n",
            "Total loss:  -2.8756 | PDE Loss:  -4.7654 | Function Loss:  -2.9356\n",
            "Total loss:  -2.8763 | PDE Loss:  -4.7643 | Function Loss:  -2.9365\n",
            "Total loss:  -2.8769 | PDE Loss:  -4.7644 | Function Loss:  -2.9372\n",
            "Total loss:  -2.8775 | PDE Loss:  -4.7612 | Function Loss:  -2.9384\n",
            "Total loss:  -2.878 | PDE Loss:  -4.7611 | Function Loss:  -2.9389\n",
            "Total loss:  -2.8785 | PDE Loss:  -4.7607 | Function Loss:  -2.9395\n",
            "Total loss:  -2.8791 | PDE Loss:  -4.7591 | Function Loss:  -2.9404\n",
            "Total loss:  -2.8796 | PDE Loss:  -4.7577 | Function Loss:  -2.9413\n",
            "Total loss:  -2.8799 | PDE Loss:  -4.7575 | Function Loss:  -2.9417\n",
            "Total loss:  -2.8804 | PDE Loss:  -4.7563 | Function Loss:  -2.9424\n",
            "Total loss:  -2.8809 | PDE Loss:  -4.7561 | Function Loss:  -2.943\n",
            "Total loss:  -2.8814 | PDE Loss:  -4.7563 | Function Loss:  -2.9435\n",
            "Total loss:  -2.882 | PDE Loss:  -4.7574 | Function Loss:  -2.944\n",
            "Total loss:  -2.8826 | PDE Loss:  -4.7552 | Function Loss:  -2.9451\n",
            "Total loss:  -2.8814 | PDE Loss:  -4.7594 | Function Loss:  -2.9431\n",
            "Total loss:  -2.8832 | PDE Loss:  -4.7593 | Function Loss:  -2.9452\n",
            "Total loss:  -2.8836 | PDE Loss:  -4.7592 | Function Loss:  -2.9457\n",
            "Total loss:  -2.8843 | PDE Loss:  -4.76 | Function Loss:  -2.9463\n",
            "Total loss:  -2.8849 | PDE Loss:  -4.7596 | Function Loss:  -2.9471\n",
            "Total loss:  -2.8854 | PDE Loss:  -4.7593 | Function Loss:  -2.9477\n",
            "Total loss:  -2.8856 | PDE Loss:  -4.76 | Function Loss:  -2.9479\n",
            "Total loss:  -2.886 | PDE Loss:  -4.7601 | Function Loss:  -2.9483\n",
            "Total loss:  -2.8864 | PDE Loss:  -4.7596 | Function Loss:  -2.9489\n",
            "Total loss:  -2.8868 | PDE Loss:  -4.7592 | Function Loss:  -2.9494\n",
            "Total loss:  -2.8871 | PDE Loss:  -4.7581 | Function Loss:  -2.9499\n",
            "Total loss:  -2.8874 | PDE Loss:  -4.757 | Function Loss:  -2.9504\n",
            "Total loss:  -2.8878 | PDE Loss:  -4.7558 | Function Loss:  -2.9511\n",
            "Total loss:  -2.8883 | PDE Loss:  -4.754 | Function Loss:  -2.9519\n",
            "Total loss:  -2.8887 | PDE Loss:  -4.753 | Function Loss:  -2.9526\n",
            "Total loss:  -2.8891 | PDE Loss:  -4.7513 | Function Loss:  -2.9533\n",
            "Total loss:  -2.8894 | PDE Loss:  -4.7512 | Function Loss:  -2.9536\n",
            "Total loss:  -2.8898 | PDE Loss:  -4.7518 | Function Loss:  -2.9541\n",
            "Total loss:  -2.8904 | PDE Loss:  -4.7527 | Function Loss:  -2.9546\n",
            "Total loss:  -2.8909 | PDE Loss:  -4.7535 | Function Loss:  -2.955\n",
            "Total loss:  -2.8914 | PDE Loss:  -4.7531 | Function Loss:  -2.9556\n",
            "Total loss:  -2.8918 | PDE Loss:  -4.7525 | Function Loss:  -2.9562\n",
            "Total loss:  -2.8922 | PDE Loss:  -4.7507 | Function Loss:  -2.9569\n",
            "Total loss:  -2.8925 | PDE Loss:  -4.7485 | Function Loss:  -2.9577\n",
            "Total loss:  -2.8929 | PDE Loss:  -4.7459 | Function Loss:  -2.9585\n",
            "Total loss:  -2.8932 | PDE Loss:  -4.7428 | Function Loss:  -2.9593\n",
            "Total loss:  -2.8935 | PDE Loss:  -4.7402 | Function Loss:  -2.9601\n",
            "Total loss:  -2.8938 | PDE Loss:  -4.7373 | Function Loss:  -2.961\n",
            "Total loss:  -2.8942 | PDE Loss:  -4.7345 | Function Loss:  -2.962\n",
            "Total loss:  -2.8945 | PDE Loss:  -4.7323 | Function Loss:  -2.9627\n",
            "Total loss:  -2.8941 | PDE Loss:  -4.723 | Function Loss:  -2.9638\n",
            "Total loss:  -2.8949 | PDE Loss:  -4.7297 | Function Loss:  -2.9636\n",
            "Total loss:  -2.8951 | PDE Loss:  -4.7307 | Function Loss:  -2.9636\n",
            "Total loss:  -2.8956 | PDE Loss:  -4.7337 | Function Loss:  -2.9637\n",
            "Total loss:  -2.8958 | PDE Loss:  -4.7353 | Function Loss:  -2.9637\n",
            "Total loss:  -2.896 | PDE Loss:  -4.7371 | Function Loss:  -2.9636\n",
            "Total loss:  -2.8961 | PDE Loss:  -4.7373 | Function Loss:  -2.9637\n",
            "Total loss:  -2.8963 | PDE Loss:  -4.7377 | Function Loss:  -2.9638\n",
            "Total loss:  -2.8965 | PDE Loss:  -4.7378 | Function Loss:  -2.9641\n",
            "Total loss:  -2.8967 | PDE Loss:  -4.7375 | Function Loss:  -2.9644\n",
            "Total loss:  -2.8969 | PDE Loss:  -4.7365 | Function Loss:  -2.9648\n",
            "Total loss:  -2.8971 | PDE Loss:  -4.7361 | Function Loss:  -2.9651\n",
            "Total loss:  -2.8973 | PDE Loss:  -4.735 | Function Loss:  -2.9655\n",
            "Total loss:  -2.8976 | PDE Loss:  -4.7336 | Function Loss:  -2.9661\n",
            "Total loss:  -2.8979 | PDE Loss:  -4.7325 | Function Loss:  -2.9666\n",
            "Total loss:  -2.8983 | PDE Loss:  -4.7313 | Function Loss:  -2.9673\n",
            "Total loss:  -2.8987 | PDE Loss:  -4.7307 | Function Loss:  -2.9679\n",
            "Total loss:  -2.899 | PDE Loss:  -4.7306 | Function Loss:  -2.9683\n",
            "Total loss:  -2.8993 | PDE Loss:  -4.7307 | Function Loss:  -2.9686\n",
            "Total loss:  -2.8998 | PDE Loss:  -4.7309 | Function Loss:  -2.9691\n",
            "Total loss:  -2.9003 | PDE Loss:  -4.7316 | Function Loss:  -2.9696\n",
            "Total loss:  -2.9007 | PDE Loss:  -4.7323 | Function Loss:  -2.9699\n",
            "Total loss:  -2.9014 | PDE Loss:  -4.7343 | Function Loss:  -2.9704\n",
            "Total loss:  -2.902 | PDE Loss:  -4.7338 | Function Loss:  -2.9712\n",
            "Total loss:  -2.9025 | PDE Loss:  -4.7359 | Function Loss:  -2.9714\n",
            "Total loss:  -2.9027 | PDE Loss:  -4.7358 | Function Loss:  -2.9717\n",
            "Total loss:  -2.9031 | PDE Loss:  -4.7353 | Function Loss:  -2.9723\n",
            "Total loss:  -2.9035 | PDE Loss:  -4.734 | Function Loss:  -2.9729\n",
            "Total loss:  -2.9039 | PDE Loss:  -4.7326 | Function Loss:  -2.9736\n",
            "Total loss:  -2.9042 | PDE Loss:  -4.731 | Function Loss:  -2.9743\n",
            "Total loss:  -2.9046 | PDE Loss:  -4.7298 | Function Loss:  -2.975\n",
            "Total loss:  -2.905 | PDE Loss:  -4.7284 | Function Loss:  -2.9757\n",
            "Total loss:  -2.9053 | PDE Loss:  -4.728 | Function Loss:  -2.9761\n",
            "Total loss:  -2.9056 | PDE Loss:  -4.7279 | Function Loss:  -2.9765\n",
            "Total loss:  -2.906 | PDE Loss:  -4.7282 | Function Loss:  -2.9768\n",
            "Total loss:  -2.9062 | PDE Loss:  -4.7295 | Function Loss:  -2.9769\n",
            "Total loss:  -2.9065 | PDE Loss:  -4.7294 | Function Loss:  -2.9773\n",
            "Total loss:  -2.9068 | PDE Loss:  -4.7307 | Function Loss:  -2.9773\n",
            "Total loss:  -2.907 | PDE Loss:  -4.7311 | Function Loss:  -2.9776\n",
            "Total loss:  -2.9074 | PDE Loss:  -4.7313 | Function Loss:  -2.9779\n",
            "Total loss:  -2.9075 | PDE Loss:  -4.7296 | Function Loss:  -2.9784\n",
            "Total loss:  -2.9077 | PDE Loss:  -4.7252 | Function Loss:  -2.9794\n",
            "Total loss:  -2.9081 | PDE Loss:  -4.7276 | Function Loss:  -2.9794\n",
            "Total loss:  -2.9084 | PDE Loss:  -4.7292 | Function Loss:  -2.9795\n",
            "Total loss:  -2.9086 | PDE Loss:  -4.7294 | Function Loss:  -2.9797\n",
            "Total loss:  -2.9088 | PDE Loss:  -4.7297 | Function Loss:  -2.9799\n",
            "Total loss:  -2.909 | PDE Loss:  -4.7297 | Function Loss:  -2.9802\n",
            "Total loss:  -2.9093 | PDE Loss:  -4.7305 | Function Loss:  -2.9803\n",
            "Total loss:  -2.9095 | PDE Loss:  -4.7296 | Function Loss:  -2.9807\n",
            "Total loss:  -2.9097 | PDE Loss:  -4.7308 | Function Loss:  -2.9807\n",
            "Total loss:  -2.9099 | PDE Loss:  -4.7309 | Function Loss:  -2.981\n",
            "Total loss:  -2.9102 | PDE Loss:  -4.7313 | Function Loss:  -2.9813\n",
            "Total loss:  -2.9106 | PDE Loss:  -4.7305 | Function Loss:  -2.9819\n",
            "Total loss:  -2.9109 | PDE Loss:  -4.73 | Function Loss:  -2.9823\n",
            "Total loss:  -2.9112 | PDE Loss:  -4.7289 | Function Loss:  -2.9828\n",
            "Total loss:  -2.9114 | PDE Loss:  -4.7283 | Function Loss:  -2.9832\n",
            "Total loss:  -2.9116 | PDE Loss:  -4.7273 | Function Loss:  -2.9837\n",
            "Total loss:  -2.9118 | PDE Loss:  -4.727 | Function Loss:  -2.9839\n",
            "Total loss:  -2.9119 | PDE Loss:  -4.7263 | Function Loss:  -2.9842\n",
            "Total loss:  -2.912 | PDE Loss:  -4.7265 | Function Loss:  -2.9843\n",
            "Total loss:  -2.9121 | PDE Loss:  -4.7262 | Function Loss:  -2.9845\n",
            "Total loss:  -2.9123 | PDE Loss:  -4.7267 | Function Loss:  -2.9846\n",
            "Total loss:  -2.9125 | PDE Loss:  -4.7276 | Function Loss:  -2.9847\n",
            "Total loss:  -2.9128 | PDE Loss:  -4.7284 | Function Loss:  -2.9848\n",
            "Total loss:  -2.9131 | PDE Loss:  -4.7301 | Function Loss:  -2.9849\n",
            "Total loss:  -2.9134 | PDE Loss:  -4.7314 | Function Loss:  -2.985\n",
            "Total loss:  -2.9136 | PDE Loss:  -4.7328 | Function Loss:  -2.9851\n",
            "Total loss:  -2.9139 | PDE Loss:  -4.7343 | Function Loss:  -2.9852\n",
            "Total loss:  -2.9143 | PDE Loss:  -4.7355 | Function Loss:  -2.9853\n",
            "Total loss:  -2.9145 | PDE Loss:  -4.7361 | Function Loss:  -2.9855\n",
            "Total loss:  -2.9149 | PDE Loss:  -4.7374 | Function Loss:  -2.9857\n",
            "Total loss:  -2.9152 | PDE Loss:  -4.738 | Function Loss:  -2.986\n",
            "Total loss:  -2.9156 | PDE Loss:  -4.7389 | Function Loss:  -2.9863\n",
            "Total loss:  -2.916 | PDE Loss:  -4.7416 | Function Loss:  -2.9863\n",
            "Total loss:  -2.9164 | PDE Loss:  -4.7424 | Function Loss:  -2.9866\n",
            "Total loss:  -2.9167 | PDE Loss:  -4.7448 | Function Loss:  -2.9865\n",
            "Total loss:  -2.9169 | PDE Loss:  -4.7464 | Function Loss:  -2.9865\n",
            "Total loss:  -2.9172 | PDE Loss:  -4.748 | Function Loss:  -2.9865\n",
            "Total loss:  -2.9174 | PDE Loss:  -4.7497 | Function Loss:  -2.9865\n",
            "Total loss:  -2.9176 | PDE Loss:  -4.7513 | Function Loss:  -2.9865\n",
            "Total loss:  -2.9178 | PDE Loss:  -4.7534 | Function Loss:  -2.9864\n",
            "Total loss:  -2.9181 | PDE Loss:  -4.7543 | Function Loss:  -2.9865\n",
            "Total loss:  -2.9183 | PDE Loss:  -4.755 | Function Loss:  -2.9866\n",
            "Total loss:  -2.9185 | PDE Loss:  -4.7556 | Function Loss:  -2.9868\n",
            "Total loss:  -2.9188 | PDE Loss:  -4.7562 | Function Loss:  -2.987\n",
            "Total loss:  -2.9191 | PDE Loss:  -4.7571 | Function Loss:  -2.9872\n",
            "Total loss:  -2.9194 | PDE Loss:  -4.7576 | Function Loss:  -2.9875\n",
            "Total loss:  -2.9196 | PDE Loss:  -4.7583 | Function Loss:  -2.9876\n",
            "Total loss:  -2.9198 | PDE Loss:  -4.7587 | Function Loss:  -2.9878\n",
            "Total loss:  -2.92 | PDE Loss:  -4.7596 | Function Loss:  -2.9879\n",
            "Total loss:  -2.9203 | PDE Loss:  -4.7597 | Function Loss:  -2.9882\n",
            "Total loss:  -2.9206 | PDE Loss:  -4.7594 | Function Loss:  -2.9886\n",
            "Total loss:  -2.9209 | PDE Loss:  -4.7585 | Function Loss:  -2.9891\n",
            "Total loss:  -2.9212 | PDE Loss:  -4.7573 | Function Loss:  -2.9896\n",
            "Total loss:  -2.9215 | PDE Loss:  -4.7551 | Function Loss:  -2.9905\n",
            "Total loss:  -2.9219 | PDE Loss:  -4.7538 | Function Loss:  -2.9911\n",
            "Total loss:  -2.9222 | PDE Loss:  -4.7515 | Function Loss:  -2.9918\n",
            "Total loss:  -2.9224 | PDE Loss:  -4.7504 | Function Loss:  -2.9922\n",
            "Total loss:  -2.9225 | PDE Loss:  -4.7492 | Function Loss:  -2.9926\n",
            "Total loss:  -2.9227 | PDE Loss:  -4.7488 | Function Loss:  -2.9928\n",
            "Total loss:  -2.9228 | PDE Loss:  -4.7487 | Function Loss:  -2.9931\n",
            "Total loss:  -2.923 | PDE Loss:  -4.749 | Function Loss:  -2.9932\n",
            "Total loss:  -2.9232 | PDE Loss:  -4.7495 | Function Loss:  -2.9933\n",
            "Total loss:  -2.9234 | PDE Loss:  -4.7507 | Function Loss:  -2.9933\n",
            "Total loss:  -2.9235 | PDE Loss:  -4.7518 | Function Loss:  -2.9933\n",
            "Total loss:  -2.9237 | PDE Loss:  -4.753 | Function Loss:  -2.9934\n",
            "Total loss:  -2.9239 | PDE Loss:  -4.7534 | Function Loss:  -2.9935\n",
            "Total loss:  -2.924 | PDE Loss:  -4.7542 | Function Loss:  -2.9935\n",
            "Total loss:  -2.9242 | PDE Loss:  -4.7549 | Function Loss:  -2.9936\n",
            "Total loss:  -2.9244 | PDE Loss:  -4.7553 | Function Loss:  -2.9938\n",
            "Total loss:  -2.9246 | PDE Loss:  -4.7564 | Function Loss:  -2.9939\n",
            "Total loss:  -2.9248 | PDE Loss:  -4.7565 | Function Loss:  -2.994\n",
            "Total loss:  -2.925 | PDE Loss:  -4.7575 | Function Loss:  -2.9941\n",
            "Total loss:  -2.9252 | PDE Loss:  -4.7574 | Function Loss:  -2.9943\n",
            "Total loss:  -2.9253 | PDE Loss:  -4.7582 | Function Loss:  -2.9943\n",
            "Total loss:  -2.9254 | PDE Loss:  -4.7585 | Function Loss:  -2.9943\n",
            "Total loss:  -2.9254 | PDE Loss:  -4.7576 | Function Loss:  -2.9946\n",
            "Total loss:  -2.9256 | PDE Loss:  -4.7586 | Function Loss:  -2.9946\n",
            "Total loss:  -2.9257 | PDE Loss:  -4.7594 | Function Loss:  -2.9946\n",
            "Total loss:  -2.9259 | PDE Loss:  -4.7603 | Function Loss:  -2.9946\n",
            "Total loss:  -2.926 | PDE Loss:  -4.761 | Function Loss:  -2.9947\n",
            "Total loss:  -2.9263 | PDE Loss:  -4.7614 | Function Loss:  -2.9949\n",
            "Total loss:  -2.9265 | PDE Loss:  -4.7619 | Function Loss:  -2.9951\n",
            "Total loss:  -2.9268 | PDE Loss:  -4.7618 | Function Loss:  -2.9955\n",
            "Total loss:  -2.9271 | PDE Loss:  -4.7612 | Function Loss:  -2.9959\n",
            "Total loss:  -2.9273 | PDE Loss:  -4.7598 | Function Loss:  -2.9964\n",
            "Total loss:  -2.9276 | PDE Loss:  -4.7587 | Function Loss:  -2.9969\n",
            "Total loss:  -2.9278 | PDE Loss:  -4.7572 | Function Loss:  -2.9974\n",
            "Total loss:  -2.9279 | PDE Loss:  -4.7557 | Function Loss:  -2.9978\n",
            "Total loss:  -2.9281 | PDE Loss:  -4.7548 | Function Loss:  -2.9982\n",
            "Total loss:  -2.9282 | PDE Loss:  -4.7542 | Function Loss:  -2.9985\n",
            "Total loss:  -2.9283 | PDE Loss:  -4.754 | Function Loss:  -2.9986\n",
            "Total loss:  -2.9285 | PDE Loss:  -4.7543 | Function Loss:  -2.9987\n",
            "Total loss:  -2.9285 | PDE Loss:  -4.7551 | Function Loss:  -2.9987\n",
            "Total loss:  -2.9287 | PDE Loss:  -4.7555 | Function Loss:  -2.9988\n",
            "Total loss:  -2.9288 | PDE Loss:  -4.7568 | Function Loss:  -2.9987\n",
            "Total loss:  -2.9291 | PDE Loss:  -4.757 | Function Loss:  -2.9989\n",
            "Total loss:  -2.9292 | PDE Loss:  -4.758 | Function Loss:  -2.999\n",
            "Total loss:  -2.9295 | PDE Loss:  -4.7593 | Function Loss:  -2.9991\n",
            "Total loss:  -2.9298 | PDE Loss:  -4.7602 | Function Loss:  -2.9992\n",
            "Total loss:  -2.9301 | PDE Loss:  -4.7613 | Function Loss:  -2.9994\n",
            "Total loss:  -2.9304 | PDE Loss:  -4.7616 | Function Loss:  -2.9997\n",
            "Total loss:  -2.9308 | PDE Loss:  -4.7629 | Function Loss:  -3.0\n",
            "Total loss:  -2.9313 | PDE Loss:  -4.764 | Function Loss:  -3.0003\n",
            "Total loss:  -2.932 | PDE Loss:  -4.7656 | Function Loss:  -3.0008\n",
            "Total loss:  -2.9326 | PDE Loss:  -4.7677 | Function Loss:  -3.0013\n",
            "Total loss:  -2.9332 | PDE Loss:  -4.7687 | Function Loss:  -3.0017\n",
            "Total loss:  -2.9336 | PDE Loss:  -4.7689 | Function Loss:  -3.0023\n",
            "Total loss:  -2.9342 | PDE Loss:  -4.7693 | Function Loss:  -3.0028\n",
            "Total loss:  -2.9346 | PDE Loss:  -4.7689 | Function Loss:  -3.0033\n",
            "Total loss:  -2.9349 | PDE Loss:  -4.7692 | Function Loss:  -3.0037\n",
            "Total loss:  -2.9352 | PDE Loss:  -4.7689 | Function Loss:  -3.0041\n",
            "Total loss:  -2.9355 | PDE Loss:  -4.7687 | Function Loss:  -3.0045\n",
            "Total loss:  -2.9358 | PDE Loss:  -4.7676 | Function Loss:  -3.005\n",
            "Total loss:  -2.9361 | PDE Loss:  -4.7661 | Function Loss:  -3.0056\n",
            "Total loss:  -2.9362 | PDE Loss:  -4.7661 | Function Loss:  -3.0057\n",
            "Total loss:  -2.9363 | PDE Loss:  -4.7658 | Function Loss:  -3.0059\n",
            "Total loss:  -2.9365 | PDE Loss:  -4.7663 | Function Loss:  -3.006\n",
            "Total loss:  -2.9367 | PDE Loss:  -4.7662 | Function Loss:  -3.0062\n",
            "Total loss:  -2.9368 | PDE Loss:  -4.7666 | Function Loss:  -3.0064\n",
            "Total loss:  -2.9371 | PDE Loss:  -4.767 | Function Loss:  -3.0066\n",
            "Total loss:  -2.9373 | PDE Loss:  -4.7669 | Function Loss:  -3.0069\n",
            "Total loss:  -2.9377 | PDE Loss:  -4.7669 | Function Loss:  -3.0074\n",
            "Total loss:  -2.9382 | PDE Loss:  -4.7652 | Function Loss:  -3.0082\n",
            "Total loss:  -2.9387 | PDE Loss:  -4.7639 | Function Loss:  -3.0091\n",
            "Total loss:  -2.9392 | PDE Loss:  -4.7637 | Function Loss:  -3.0097\n",
            "Total loss:  -2.9399 | PDE Loss:  -4.7642 | Function Loss:  -3.0104\n",
            "Total loss:  -2.9405 | PDE Loss:  -4.7627 | Function Loss:  -3.0114\n",
            "Total loss:  -2.9411 | PDE Loss:  -4.7635 | Function Loss:  -3.0119\n",
            "Total loss:  -2.9415 | PDE Loss:  -4.7641 | Function Loss:  -3.0122\n",
            "Total loss:  -2.9419 | PDE Loss:  -4.7646 | Function Loss:  -3.0127\n",
            "Total loss:  -2.9424 | PDE Loss:  -4.767 | Function Loss:  -3.0128\n",
            "Total loss:  -2.9427 | PDE Loss:  -4.7683 | Function Loss:  -3.013\n",
            "Total loss:  -2.9431 | PDE Loss:  -4.7696 | Function Loss:  -3.0133\n",
            "Total loss:  -2.9436 | PDE Loss:  -4.7723 | Function Loss:  -3.0133\n",
            "Total loss:  -2.944 | PDE Loss:  -4.7732 | Function Loss:  -3.0137\n",
            "Total loss:  -2.9444 | PDE Loss:  -4.7751 | Function Loss:  -3.0137\n",
            "Total loss:  -2.9447 | PDE Loss:  -4.7761 | Function Loss:  -3.0139\n",
            "Total loss:  -2.945 | PDE Loss:  -4.7765 | Function Loss:  -3.0142\n",
            "Total loss:  -2.9454 | PDE Loss:  -4.7789 | Function Loss:  -3.0143\n",
            "Total loss:  -2.9456 | PDE Loss:  -4.7793 | Function Loss:  -3.0145\n",
            "Total loss:  -2.946 | PDE Loss:  -4.7809 | Function Loss:  -3.0146\n",
            "Total loss:  -2.9462 | PDE Loss:  -4.7794 | Function Loss:  -3.0152\n",
            "Total loss:  -2.9464 | PDE Loss:  -4.78 | Function Loss:  -3.0153\n",
            "Total loss:  -2.9466 | PDE Loss:  -4.7795 | Function Loss:  -3.0156\n",
            "Total loss:  -2.9469 | PDE Loss:  -4.7792 | Function Loss:  -3.016\n",
            "Total loss:  -2.9473 | PDE Loss:  -4.7784 | Function Loss:  -3.0166\n",
            "Total loss:  -2.9476 | PDE Loss:  -4.7789 | Function Loss:  -3.0169\n",
            "Total loss:  -2.948 | PDE Loss:  -4.7788 | Function Loss:  -3.0173\n",
            "Total loss:  -2.9483 | PDE Loss:  -4.7792 | Function Loss:  -3.0176\n",
            "Total loss:  -2.9486 | PDE Loss:  -4.7797 | Function Loss:  -3.0179\n",
            "Total loss:  -2.9489 | PDE Loss:  -4.7798 | Function Loss:  -3.0182\n",
            "Total loss:  -2.9492 | PDE Loss:  -4.7809 | Function Loss:  -3.0184\n",
            "Total loss:  -2.9495 | PDE Loss:  -4.7811 | Function Loss:  -3.0187\n",
            "Total loss:  -2.9497 | PDE Loss:  -4.7816 | Function Loss:  -3.0189\n",
            "Total loss:  -2.95 | PDE Loss:  -4.7823 | Function Loss:  -3.0191\n",
            "Total loss:  -2.9502 | PDE Loss:  -4.779 | Function Loss:  -3.0199\n",
            "Total loss:  -2.9504 | PDE Loss:  -4.7819 | Function Loss:  -3.0197\n",
            "Total loss:  -2.9506 | PDE Loss:  -4.7824 | Function Loss:  -3.0199\n",
            "Total loss:  -2.9509 | PDE Loss:  -4.7842 | Function Loss:  -3.0199\n",
            "Total loss:  -2.9512 | PDE Loss:  -4.7845 | Function Loss:  -3.0201\n",
            "Total loss:  -2.9514 | PDE Loss:  -4.7841 | Function Loss:  -3.0204\n",
            "Total loss:  -2.9515 | PDE Loss:  -4.7839 | Function Loss:  -3.0206\n",
            "Total loss:  -2.9518 | PDE Loss:  -4.7841 | Function Loss:  -3.0209\n",
            "Total loss:  -2.952 | PDE Loss:  -4.7846 | Function Loss:  -3.021\n",
            "Total loss:  -2.9521 | PDE Loss:  -4.786 | Function Loss:  -3.021\n",
            "Total loss:  -2.9523 | PDE Loss:  -4.7867 | Function Loss:  -3.0211\n",
            "Total loss:  -2.9525 | PDE Loss:  -4.7881 | Function Loss:  -3.021\n",
            "Total loss:  -2.9526 | PDE Loss:  -4.7896 | Function Loss:  -3.0209\n",
            "Total loss:  -2.9528 | PDE Loss:  -4.7918 | Function Loss:  -3.0207\n",
            "Total loss:  -2.9529 | PDE Loss:  -4.7926 | Function Loss:  -3.0208\n",
            "Total loss:  -2.953 | PDE Loss:  -4.7943 | Function Loss:  -3.0206\n",
            "Total loss:  -2.9532 | PDE Loss:  -4.796 | Function Loss:  -3.0205\n",
            "Total loss:  -2.9534 | PDE Loss:  -4.7976 | Function Loss:  -3.0205\n",
            "Total loss:  -2.9535 | PDE Loss:  -4.7981 | Function Loss:  -3.0205\n",
            "Total loss:  -2.9536 | PDE Loss:  -4.7981 | Function Loss:  -3.0207\n",
            "Total loss:  -2.9538 | PDE Loss:  -4.798 | Function Loss:  -3.0208\n",
            "Total loss:  -2.9538 | PDE Loss:  -4.7957 | Function Loss:  -3.0213\n",
            "Total loss:  -2.9539 | PDE Loss:  -4.7966 | Function Loss:  -3.0213\n",
            "Total loss:  -2.9541 | PDE Loss:  -4.7971 | Function Loss:  -3.0214\n",
            "Total loss:  -2.9542 | PDE Loss:  -4.7973 | Function Loss:  -3.0215\n",
            "Total loss:  -2.9544 | PDE Loss:  -4.7975 | Function Loss:  -3.0217\n",
            "Total loss:  -2.9546 | PDE Loss:  -4.7973 | Function Loss:  -3.022\n",
            "Total loss:  -2.9548 | PDE Loss:  -4.7974 | Function Loss:  -3.0222\n",
            "Total loss:  -2.955 | PDE Loss:  -4.7971 | Function Loss:  -3.0225\n",
            "Total loss:  -2.9553 | PDE Loss:  -4.7971 | Function Loss:  -3.0228\n",
            "Total loss:  -2.9556 | PDE Loss:  -4.7965 | Function Loss:  -3.0232\n",
            "Total loss:  -2.9558 | PDE Loss:  -4.7967 | Function Loss:  -3.0234\n",
            "Total loss:  -2.956 | PDE Loss:  -4.7966 | Function Loss:  -3.0237\n",
            "Total loss:  -2.9562 | PDE Loss:  -4.7968 | Function Loss:  -3.0239\n",
            "Total loss:  -2.9564 | PDE Loss:  -4.7967 | Function Loss:  -3.0241\n",
            "Total loss:  -2.9566 | PDE Loss:  -4.7969 | Function Loss:  -3.0244\n",
            "Total loss:  -2.9568 | PDE Loss:  -4.7966 | Function Loss:  -3.0247\n",
            "Total loss:  -2.9571 | PDE Loss:  -4.7972 | Function Loss:  -3.0249\n",
            "Total loss:  -2.9574 | PDE Loss:  -4.7965 | Function Loss:  -3.0253\n",
            "Total loss:  -2.9576 | PDE Loss:  -4.7965 | Function Loss:  -3.0256\n",
            "Total loss:  -2.958 | PDE Loss:  -4.7965 | Function Loss:  -3.026\n",
            "Total loss:  -2.9583 | PDE Loss:  -4.797 | Function Loss:  -3.0263\n",
            "Total loss:  -2.9585 | PDE Loss:  -4.7973 | Function Loss:  -3.0265\n",
            "Total loss:  -2.9587 | PDE Loss:  -4.7975 | Function Loss:  -3.0267\n",
            "Total loss:  -2.9588 | PDE Loss:  -4.7975 | Function Loss:  -3.0268\n",
            "Total loss:  -2.959 | PDE Loss:  -4.7976 | Function Loss:  -3.027\n",
            "Total loss:  -2.9591 | PDE Loss:  -4.7976 | Function Loss:  -3.0272\n",
            "Total loss:  -2.9593 | PDE Loss:  -4.7976 | Function Loss:  -3.0274\n",
            "Total loss:  -2.9595 | PDE Loss:  -4.7974 | Function Loss:  -3.0276\n",
            "Total loss:  -2.9597 | PDE Loss:  -4.7971 | Function Loss:  -3.0279\n",
            "Total loss:  -2.9599 | PDE Loss:  -4.7964 | Function Loss:  -3.0283\n",
            "Total loss:  -2.9603 | PDE Loss:  -4.7965 | Function Loss:  -3.0287\n",
            "Total loss:  -2.9603 | PDE Loss:  -4.7917 | Function Loss:  -3.0296\n",
            "Total loss:  -2.9609 | PDE Loss:  -4.7943 | Function Loss:  -3.0298\n",
            "Total loss:  -2.961 | PDE Loss:  -4.7978 | Function Loss:  -3.0293\n",
            "Total loss:  -2.9613 | PDE Loss:  -4.7987 | Function Loss:  -3.0295\n",
            "Total loss:  -2.9617 | PDE Loss:  -4.7993 | Function Loss:  -3.0299\n",
            "Total loss:  -2.962 | PDE Loss:  -4.7996 | Function Loss:  -3.0302\n",
            "Total loss:  -2.9622 | PDE Loss:  -4.7996 | Function Loss:  -3.0304\n",
            "Total loss:  -2.9624 | PDE Loss:  -4.7999 | Function Loss:  -3.0306\n",
            "Total loss:  -2.9625 | PDE Loss:  -4.7998 | Function Loss:  -3.0307\n",
            "Total loss:  -2.9628 | PDE Loss:  -4.8014 | Function Loss:  -3.0309\n",
            "Total loss:  -2.9631 | PDE Loss:  -4.8022 | Function Loss:  -3.0311\n",
            "Total loss:  -2.9633 | PDE Loss:  -4.8028 | Function Loss:  -3.0312\n",
            "Total loss:  -2.9635 | PDE Loss:  -4.803 | Function Loss:  -3.0313\n",
            "Total loss:  -2.9636 | PDE Loss:  -4.8034 | Function Loss:  -3.0314\n",
            "Total loss:  -2.9637 | PDE Loss:  -4.8031 | Function Loss:  -3.0316\n",
            "Total loss:  -2.9638 | PDE Loss:  -4.8031 | Function Loss:  -3.0317\n",
            "Total loss:  -2.9639 | PDE Loss:  -4.8025 | Function Loss:  -3.032\n",
            "Total loss:  -2.9641 | PDE Loss:  -4.8024 | Function Loss:  -3.0322\n",
            "Total loss:  -2.9642 | PDE Loss:  -4.8019 | Function Loss:  -3.0324\n",
            "Total loss:  -2.9643 | PDE Loss:  -4.8015 | Function Loss:  -3.0326\n",
            "Total loss:  -2.9644 | PDE Loss:  -4.8011 | Function Loss:  -3.0328\n",
            "Total loss:  -2.9645 | PDE Loss:  -4.8008 | Function Loss:  -3.0329\n",
            "Total loss:  -2.9646 | PDE Loss:  -4.8003 | Function Loss:  -3.0331\n",
            "Total loss:  -2.9647 | PDE Loss:  -4.8001 | Function Loss:  -3.0332\n",
            "Total loss:  -2.9648 | PDE Loss:  -4.7996 | Function Loss:  -3.0334\n",
            "Total loss:  -2.9649 | PDE Loss:  -4.7993 | Function Loss:  -3.0337\n",
            "Total loss:  -2.9651 | PDE Loss:  -4.7986 | Function Loss:  -3.034\n",
            "Total loss:  -2.9653 | PDE Loss:  -4.7979 | Function Loss:  -3.0344\n",
            "Total loss:  -2.9656 | PDE Loss:  -4.7975 | Function Loss:  -3.0348\n",
            "Total loss:  -2.966 | PDE Loss:  -4.7959 | Function Loss:  -3.0355\n",
            "Total loss:  -2.9661 | PDE Loss:  -4.7967 | Function Loss:  -3.0355\n",
            "Total loss:  -2.9664 | PDE Loss:  -4.7977 | Function Loss:  -3.0357\n",
            "Total loss:  -2.9666 | PDE Loss:  -4.7992 | Function Loss:  -3.0357\n",
            "Total loss:  -2.9668 | PDE Loss:  -4.8004 | Function Loss:  -3.0357\n",
            "Total loss:  -2.9669 | PDE Loss:  -4.8015 | Function Loss:  -3.0357\n",
            "Total loss:  -2.9671 | PDE Loss:  -4.8025 | Function Loss:  -3.0357\n",
            "Total loss:  -2.9674 | PDE Loss:  -4.8036 | Function Loss:  -3.0359\n",
            "Total loss:  -2.9676 | PDE Loss:  -4.8035 | Function Loss:  -3.0362\n",
            "Total loss:  -2.9679 | PDE Loss:  -4.8043 | Function Loss:  -3.0363\n",
            "Total loss:  -2.9681 | PDE Loss:  -4.8042 | Function Loss:  -3.0366\n",
            "Total loss:  -2.9683 | PDE Loss:  -4.8041 | Function Loss:  -3.0369\n",
            "Total loss:  -2.9686 | PDE Loss:  -4.8043 | Function Loss:  -3.0371\n",
            "Total loss:  -2.9689 | PDE Loss:  -4.8032 | Function Loss:  -3.0377\n",
            "Total loss:  -2.9691 | PDE Loss:  -4.8041 | Function Loss:  -3.0378\n",
            "Total loss:  -2.9694 | PDE Loss:  -4.8042 | Function Loss:  -3.038\n",
            "Total loss:  -2.9697 | PDE Loss:  -4.8053 | Function Loss:  -3.0383\n",
            "Total loss:  -2.9701 | PDE Loss:  -4.806 | Function Loss:  -3.0386\n",
            "Total loss:  -2.9705 | PDE Loss:  -4.8071 | Function Loss:  -3.0388\n",
            "Total loss:  -2.9708 | PDE Loss:  -4.808 | Function Loss:  -3.0391\n",
            "Total loss:  -2.9713 | PDE Loss:  -4.8083 | Function Loss:  -3.0396\n",
            "Total loss:  -2.9717 | PDE Loss:  -4.8094 | Function Loss:  -3.0399\n",
            "Total loss:  -2.9721 | PDE Loss:  -4.8093 | Function Loss:  -3.0403\n",
            "Total loss:  -2.9724 | PDE Loss:  -4.809 | Function Loss:  -3.0408\n",
            "Total loss:  -2.9726 | PDE Loss:  -4.8087 | Function Loss:  -3.0411\n",
            "Total loss:  -2.9728 | PDE Loss:  -4.8077 | Function Loss:  -3.0415\n",
            "Total loss:  -2.973 | PDE Loss:  -4.8075 | Function Loss:  -3.0417\n",
            "Total loss:  -2.9732 | PDE Loss:  -4.8068 | Function Loss:  -3.0421\n",
            "Total loss:  -2.9735 | PDE Loss:  -4.805 | Function Loss:  -3.0428\n",
            "Total loss:  -2.974 | PDE Loss:  -4.8054 | Function Loss:  -3.0433\n",
            "Total loss:  -2.9744 | PDE Loss:  -4.8032 | Function Loss:  -3.0442\n",
            "Total loss:  -2.9749 | PDE Loss:  -4.804 | Function Loss:  -3.0446\n",
            "Total loss:  -2.9754 | PDE Loss:  -4.8049 | Function Loss:  -3.045\n",
            "Total loss:  -2.9758 | PDE Loss:  -4.8058 | Function Loss:  -3.0453\n",
            "Total loss:  -2.9762 | PDE Loss:  -4.8062 | Function Loss:  -3.0457\n",
            "Total loss:  -2.9768 | PDE Loss:  -4.8078 | Function Loss:  -3.0462\n",
            "Total loss:  -2.9775 | PDE Loss:  -4.8081 | Function Loss:  -3.0469\n",
            "Total loss:  -2.9782 | PDE Loss:  -4.8084 | Function Loss:  -3.0477\n",
            "Total loss:  -2.9786 | PDE Loss:  -4.8062 | Function Loss:  -3.0485\n",
            "Total loss:  -2.9794 | PDE Loss:  -4.8073 | Function Loss:  -3.0492\n",
            "Total loss:  -2.9801 | PDE Loss:  -4.8078 | Function Loss:  -3.05\n",
            "Total loss:  -2.9808 | PDE Loss:  -4.8077 | Function Loss:  -3.0508\n",
            "Total loss:  -2.9814 | PDE Loss:  -4.8077 | Function Loss:  -3.0516\n",
            "Total loss:  -2.982 | PDE Loss:  -4.8077 | Function Loss:  -3.0523\n",
            "Total loss:  -2.9825 | PDE Loss:  -4.8082 | Function Loss:  -3.0528\n",
            "Total loss:  -2.983 | PDE Loss:  -4.8086 | Function Loss:  -3.0533\n",
            "Total loss:  -2.9834 | PDE Loss:  -4.8091 | Function Loss:  -3.0537\n",
            "Total loss:  -2.9839 | PDE Loss:  -4.8092 | Function Loss:  -3.0542\n",
            "Total loss:  -2.9843 | PDE Loss:  -4.8105 | Function Loss:  -3.0545\n",
            "Total loss:  -2.9846 | PDE Loss:  -4.8053 | Function Loss:  -3.0557\n",
            "Total loss:  -2.9849 | PDE Loss:  -4.8071 | Function Loss:  -3.0558\n",
            "Total loss:  -2.9853 | PDE Loss:  -4.809 | Function Loss:  -3.056\n",
            "Total loss:  -2.9857 | PDE Loss:  -4.8096 | Function Loss:  -3.0563\n",
            "Total loss:  -2.986 | PDE Loss:  -4.8099 | Function Loss:  -3.0566\n",
            "Total loss:  -2.9863 | PDE Loss:  -4.81 | Function Loss:  -3.057\n",
            "Total loss:  -2.9866 | PDE Loss:  -4.8097 | Function Loss:  -3.0574\n",
            "Total loss:  -2.9869 | PDE Loss:  -4.8099 | Function Loss:  -3.0576\n",
            "Total loss:  -2.9871 | PDE Loss:  -4.8096 | Function Loss:  -3.0579\n",
            "Total loss:  -2.9872 | PDE Loss:  -4.8098 | Function Loss:  -3.058\n",
            "Total loss:  -2.9874 | PDE Loss:  -4.8101 | Function Loss:  -3.0582\n",
            "Total loss:  -2.9876 | PDE Loss:  -4.8105 | Function Loss:  -3.0583\n",
            "Total loss:  -2.9878 | PDE Loss:  -4.8105 | Function Loss:  -3.0586\n",
            "Total loss:  -2.9879 | PDE Loss:  -4.8109 | Function Loss:  -3.0587\n",
            "Total loss:  -2.9881 | PDE Loss:  -4.8112 | Function Loss:  -3.0589\n",
            "Total loss:  -2.9884 | PDE Loss:  -4.811 | Function Loss:  -3.0592\n",
            "Total loss:  -2.9885 | PDE Loss:  -4.8102 | Function Loss:  -3.0594\n",
            "Total loss:  -2.9886 | PDE Loss:  -4.8101 | Function Loss:  -3.0596\n",
            "Total loss:  -2.9887 | PDE Loss:  -4.8095 | Function Loss:  -3.0599\n",
            "Total loss:  -2.9889 | PDE Loss:  -4.8086 | Function Loss:  -3.0602\n",
            "Total loss:  -2.9891 | PDE Loss:  -4.8074 | Function Loss:  -3.0606\n",
            "Total loss:  -2.9893 | PDE Loss:  -4.8064 | Function Loss:  -3.061\n",
            "Total loss:  -2.9895 | PDE Loss:  -4.8039 | Function Loss:  -3.0617\n",
            "Total loss:  -2.9897 | PDE Loss:  -4.8034 | Function Loss:  -3.0621\n",
            "Total loss:  -2.9899 | PDE Loss:  -4.8023 | Function Loss:  -3.0626\n",
            "Total loss:  -2.9894 | PDE Loss:  -4.8004 | Function Loss:  -3.0623\n",
            "Total loss:  -2.99 | PDE Loss:  -4.8024 | Function Loss:  -3.0627\n",
            "Total loss:  -2.9903 | PDE Loss:  -4.8024 | Function Loss:  -3.063\n",
            "Total loss:  -2.9906 | PDE Loss:  -4.8024 | Function Loss:  -3.0633\n",
            "Total loss:  -2.9909 | PDE Loss:  -4.8039 | Function Loss:  -3.0634\n",
            "Total loss:  -2.9913 | PDE Loss:  -4.8031 | Function Loss:  -3.064\n",
            "Total loss:  -2.9915 | PDE Loss:  -4.8044 | Function Loss:  -3.0641\n",
            "Total loss:  -2.9919 | PDE Loss:  -4.8067 | Function Loss:  -3.0642\n",
            "Total loss:  -2.9923 | PDE Loss:  -4.8078 | Function Loss:  -3.0644\n",
            "Total loss:  -2.9926 | PDE Loss:  -4.8083 | Function Loss:  -3.0646\n",
            "Total loss:  -2.9928 | PDE Loss:  -4.8084 | Function Loss:  -3.0648\n",
            "Total loss:  -2.993 | PDE Loss:  -4.8074 | Function Loss:  -3.0653\n",
            "Total loss:  -2.9932 | PDE Loss:  -4.8073 | Function Loss:  -3.0656\n",
            "Total loss:  -2.9935 | PDE Loss:  -4.8061 | Function Loss:  -3.066\n",
            "Total loss:  -2.9937 | PDE Loss:  -4.8053 | Function Loss:  -3.0665\n",
            "Total loss:  -2.994 | PDE Loss:  -4.8043 | Function Loss:  -3.067\n",
            "Total loss:  -2.9942 | PDE Loss:  -4.8038 | Function Loss:  -3.0674\n",
            "Total loss:  -2.9945 | PDE Loss:  -4.8037 | Function Loss:  -3.0677\n",
            "Total loss:  -2.9947 | PDE Loss:  -4.8036 | Function Loss:  -3.0679\n",
            "Total loss:  -2.9948 | PDE Loss:  -4.8044 | Function Loss:  -3.068\n",
            "Total loss:  -2.995 | PDE Loss:  -4.8053 | Function Loss:  -3.0681\n",
            "Total loss:  -2.9952 | PDE Loss:  -4.807 | Function Loss:  -3.068\n",
            "Total loss:  -2.9955 | PDE Loss:  -4.8091 | Function Loss:  -3.0679\n",
            "Total loss:  -2.9958 | PDE Loss:  -4.8112 | Function Loss:  -3.0679\n",
            "Total loss:  -2.996 | PDE Loss:  -4.8132 | Function Loss:  -3.0678\n",
            "Total loss:  -2.9962 | PDE Loss:  -4.8137 | Function Loss:  -3.068\n",
            "Total loss:  -2.9965 | PDE Loss:  -4.8142 | Function Loss:  -3.0682\n",
            "Total loss:  -2.9967 | PDE Loss:  -4.8136 | Function Loss:  -3.0685\n",
            "Total loss:  -2.9969 | PDE Loss:  -4.8128 | Function Loss:  -3.069\n",
            "Total loss:  -2.9972 | PDE Loss:  -4.8112 | Function Loss:  -3.0696\n",
            "Total loss:  -2.9975 | PDE Loss:  -4.8099 | Function Loss:  -3.0702\n",
            "Total loss:  -2.9978 | PDE Loss:  -4.8084 | Function Loss:  -3.0708\n",
            "Total loss:  -2.9981 | PDE Loss:  -4.8079 | Function Loss:  -3.0712\n",
            "Total loss:  -2.9983 | PDE Loss:  -4.8073 | Function Loss:  -3.0715\n",
            "Total loss:  -2.9986 | PDE Loss:  -4.8076 | Function Loss:  -3.0719\n",
            "Total loss:  -2.9992 | PDE Loss:  -4.8084 | Function Loss:  -3.0724\n",
            "Total loss:  -2.9996 | PDE Loss:  -4.8111 | Function Loss:  -3.0724\n",
            "Total loss:  -3.0002 | PDE Loss:  -4.8126 | Function Loss:  -3.0729\n",
            "Total loss:  -3.0008 | PDE Loss:  -4.8152 | Function Loss:  -3.0731\n",
            "Total loss:  -3.0019 | PDE Loss:  -4.8197 | Function Loss:  -3.0735\n",
            "Total loss:  -3.0025 | PDE Loss:  -4.8218 | Function Loss:  -3.0739\n",
            "Total loss:  -3.0029 | PDE Loss:  -4.8246 | Function Loss:  -3.0739\n",
            "Total loss:  -3.0032 | PDE Loss:  -4.8248 | Function Loss:  -3.0742\n",
            "Total loss:  -3.0034 | PDE Loss:  -4.8256 | Function Loss:  -3.0743\n",
            "Total loss:  -3.0036 | PDE Loss:  -4.8256 | Function Loss:  -3.0745\n",
            "Total loss:  -3.0037 | PDE Loss:  -4.8258 | Function Loss:  -3.0747\n",
            "Total loss:  -3.004 | PDE Loss:  -4.8257 | Function Loss:  -3.075\n",
            "Total loss:  -3.0043 | PDE Loss:  -4.8255 | Function Loss:  -3.0754\n",
            "Total loss:  -3.0048 | PDE Loss:  -4.8251 | Function Loss:  -3.076\n",
            "Total loss:  -3.0053 | PDE Loss:  -4.825 | Function Loss:  -3.0767\n",
            "Total loss:  -3.006 | PDE Loss:  -4.8249 | Function Loss:  -3.0774\n",
            "Total loss:  -3.0066 | PDE Loss:  -4.8254 | Function Loss:  -3.0781\n",
            "Total loss:  -3.0071 | PDE Loss:  -4.8265 | Function Loss:  -3.0785\n",
            "Total loss:  -3.0076 | PDE Loss:  -4.8272 | Function Loss:  -3.0789\n",
            "Total loss:  -3.0081 | PDE Loss:  -4.8295 | Function Loss:  -3.0791\n",
            "Total loss:  -3.0086 | PDE Loss:  -4.8305 | Function Loss:  -3.0795\n",
            "Total loss:  -3.009 | PDE Loss:  -4.8335 | Function Loss:  -3.0795\n",
            "Total loss:  -3.0093 | PDE Loss:  -4.8349 | Function Loss:  -3.0796\n",
            "Total loss:  -3.0096 | PDE Loss:  -4.8366 | Function Loss:  -3.0797\n",
            "Total loss:  -3.01 | PDE Loss:  -4.8383 | Function Loss:  -3.0798\n",
            "Total loss:  -3.0102 | PDE Loss:  -4.8395 | Function Loss:  -3.0799\n",
            "Total loss:  -3.0105 | PDE Loss:  -4.8399 | Function Loss:  -3.0801\n",
            "Total loss:  -3.0108 | PDE Loss:  -4.8397 | Function Loss:  -3.0805\n",
            "Total loss:  -3.0112 | PDE Loss:  -4.8386 | Function Loss:  -3.0812\n",
            "Total loss:  -3.0116 | PDE Loss:  -4.8369 | Function Loss:  -3.082\n",
            "Total loss:  -3.0121 | PDE Loss:  -4.8352 | Function Loss:  -3.0828\n",
            "Total loss:  -3.0125 | PDE Loss:  -4.8331 | Function Loss:  -3.0837\n",
            "Total loss:  -3.0131 | PDE Loss:  -4.8315 | Function Loss:  -3.0846\n",
            "Total loss:  -3.0135 | PDE Loss:  -4.8296 | Function Loss:  -3.0855\n",
            "Total loss:  -3.0139 | PDE Loss:  -4.8286 | Function Loss:  -3.0861\n",
            "Total loss:  -3.0142 | PDE Loss:  -4.8278 | Function Loss:  -3.0867\n",
            "Total loss:  -3.0145 | PDE Loss:  -4.8271 | Function Loss:  -3.0871\n",
            "Total loss:  -3.0147 | PDE Loss:  -4.8265 | Function Loss:  -3.0874\n",
            "Total loss:  -3.0148 | PDE Loss:  -4.8266 | Function Loss:  -3.0876\n",
            "Total loss:  -3.015 | PDE Loss:  -4.8262 | Function Loss:  -3.0878\n",
            "Total loss:  -3.0151 | PDE Loss:  -4.8262 | Function Loss:  -3.088\n",
            "Total loss:  -3.0153 | PDE Loss:  -4.826 | Function Loss:  -3.0883\n",
            "Total loss:  -3.0155 | PDE Loss:  -4.8245 | Function Loss:  -3.0887\n",
            "Total loss:  -3.0156 | PDE Loss:  -4.8242 | Function Loss:  -3.089\n",
            "Total loss:  -3.0158 | PDE Loss:  -4.824 | Function Loss:  -3.0892\n",
            "Total loss:  -3.0159 | PDE Loss:  -4.8238 | Function Loss:  -3.0894\n",
            "Total loss:  -3.0161 | PDE Loss:  -4.8242 | Function Loss:  -3.0895\n",
            "Total loss:  -3.0162 | PDE Loss:  -4.8243 | Function Loss:  -3.0896\n",
            "Total loss:  -3.0163 | PDE Loss:  -4.8248 | Function Loss:  -3.0896\n",
            "Total loss:  -3.0164 | PDE Loss:  -4.8249 | Function Loss:  -3.0897\n",
            "Total loss:  -3.0164 | PDE Loss:  -4.8253 | Function Loss:  -3.0897\n",
            "Total loss:  -3.0165 | PDE Loss:  -4.8254 | Function Loss:  -3.0898\n",
            "Total loss:  -3.0166 | PDE Loss:  -4.8255 | Function Loss:  -3.0899\n",
            "Total loss:  -3.0166 | PDE Loss:  -4.8253 | Function Loss:  -3.09\n",
            "Total loss:  -3.0167 | PDE Loss:  -4.8251 | Function Loss:  -3.0901\n",
            "Total loss:  -3.0168 | PDE Loss:  -4.8242 | Function Loss:  -3.0904\n",
            "Total loss:  -3.017 | PDE Loss:  -4.8232 | Function Loss:  -3.0907\n",
            "Total loss:  -3.0171 | PDE Loss:  -4.8209 | Function Loss:  -3.0914\n",
            "Total loss:  -3.0173 | PDE Loss:  -4.8188 | Function Loss:  -3.092\n",
            "Total loss:  -3.0175 | PDE Loss:  -4.8186 | Function Loss:  -3.0922\n",
            "Total loss:  -3.0178 | PDE Loss:  -4.8182 | Function Loss:  -3.0927\n",
            "Total loss:  -3.0181 | PDE Loss:  -4.8183 | Function Loss:  -3.093\n",
            "Total loss:  -3.0184 | PDE Loss:  -4.8176 | Function Loss:  -3.0935\n",
            "Total loss:  -3.0187 | PDE Loss:  -4.8173 | Function Loss:  -3.0939\n",
            "Total loss:  -3.0189 | PDE Loss:  -4.8172 | Function Loss:  -3.0942\n",
            "Total loss:  -3.0193 | PDE Loss:  -4.8173 | Function Loss:  -3.0946\n",
            "Total loss:  -3.0196 | PDE Loss:  -4.817 | Function Loss:  -3.095\n",
            "Total loss:  -3.0199 | PDE Loss:  -4.8176 | Function Loss:  -3.0952\n",
            "Total loss:  -3.0201 | PDE Loss:  -4.8175 | Function Loss:  -3.0955\n",
            "Total loss:  -3.0204 | PDE Loss:  -4.8174 | Function Loss:  -3.096\n",
            "Total loss:  -3.0209 | PDE Loss:  -4.817 | Function Loss:  -3.0966\n",
            "Total loss:  -3.0213 | PDE Loss:  -4.816 | Function Loss:  -3.0972\n",
            "Total loss:  -3.0218 | PDE Loss:  -4.8151 | Function Loss:  -3.098\n",
            "Total loss:  -3.0224 | PDE Loss:  -4.814 | Function Loss:  -3.0989\n",
            "Total loss:  -3.023 | PDE Loss:  -4.8125 | Function Loss:  -3.1\n",
            "Total loss:  -3.0235 | PDE Loss:  -4.8122 | Function Loss:  -3.1006\n",
            "Total loss:  -3.0239 | PDE Loss:  -4.8112 | Function Loss:  -3.1013\n",
            "Total loss:  -3.0242 | PDE Loss:  -4.8113 | Function Loss:  -3.1016\n",
            "Total loss:  -3.0245 | PDE Loss:  -4.8117 | Function Loss:  -3.1019\n",
            "Total loss:  -3.0248 | PDE Loss:  -4.8122 | Function Loss:  -3.1021\n",
            "Total loss:  -3.0252 | PDE Loss:  -4.813 | Function Loss:  -3.1025\n",
            "Total loss:  -3.0257 | PDE Loss:  -4.8131 | Function Loss:  -3.103\n",
            "Total loss:  -3.0261 | PDE Loss:  -4.8141 | Function Loss:  -3.1034\n",
            "Total loss:  -3.0265 | PDE Loss:  -4.8145 | Function Loss:  -3.1037\n",
            "Total loss:  -3.0269 | PDE Loss:  -4.815 | Function Loss:  -3.1041\n",
            "Total loss:  -3.0272 | PDE Loss:  -4.8148 | Function Loss:  -3.1046\n",
            "Total loss:  -3.0276 | PDE Loss:  -4.8152 | Function Loss:  -3.105\n",
            "Total loss:  -3.0279 | PDE Loss:  -4.8153 | Function Loss:  -3.1052\n",
            "Total loss:  -3.0284 | PDE Loss:  -4.8157 | Function Loss:  -3.1058\n",
            "Total loss:  -3.0291 | PDE Loss:  -4.8185 | Function Loss:  -3.1061\n",
            "Total loss:  -3.0298 | PDE Loss:  -4.8194 | Function Loss:  -3.1067\n",
            "Total loss:  -3.0307 | PDE Loss:  -4.8214 | Function Loss:  -3.1074\n",
            "Total loss:  -3.0315 | PDE Loss:  -4.8235 | Function Loss:  -3.108\n",
            "Total loss:  -3.0322 | PDE Loss:  -4.8249 | Function Loss:  -3.1085\n",
            "Total loss:  -3.0326 | PDE Loss:  -4.8261 | Function Loss:  -3.1087\n",
            "Total loss:  -3.0329 | PDE Loss:  -4.8266 | Function Loss:  -3.1091\n",
            "Total loss:  -3.0332 | PDE Loss:  -4.8271 | Function Loss:  -3.1093\n",
            "Total loss:  -3.0335 | PDE Loss:  -4.8271 | Function Loss:  -3.1096\n",
            "Total loss:  -3.0338 | PDE Loss:  -4.8275 | Function Loss:  -3.1099\n",
            "Total loss:  -3.0341 | PDE Loss:  -4.8279 | Function Loss:  -3.1102\n",
            "Total loss:  -3.0344 | PDE Loss:  -4.8282 | Function Loss:  -3.1105\n",
            "Total loss:  -3.0346 | PDE Loss:  -4.8283 | Function Loss:  -3.1107\n",
            "Total loss:  -3.0348 | PDE Loss:  -4.8284 | Function Loss:  -3.1109\n",
            "Total loss:  -3.035 | PDE Loss:  -4.8284 | Function Loss:  -3.1111\n",
            "Total loss:  -3.0352 | PDE Loss:  -4.8284 | Function Loss:  -3.1114\n",
            "Total loss:  -3.0355 | PDE Loss:  -4.8282 | Function Loss:  -3.1118\n",
            "Total loss:  -3.0357 | PDE Loss:  -4.8281 | Function Loss:  -3.1121\n",
            "Total loss:  -3.0359 | PDE Loss:  -4.8277 | Function Loss:  -3.1124\n",
            "Total loss:  -3.0361 | PDE Loss:  -4.8273 | Function Loss:  -3.1128\n",
            "Total loss:  -3.0365 | PDE Loss:  -4.8266 | Function Loss:  -3.1134\n",
            "Total loss:  -3.037 | PDE Loss:  -4.8266 | Function Loss:  -3.1139\n",
            "Total loss:  -3.0374 | PDE Loss:  -4.8258 | Function Loss:  -3.1146\n",
            "Total loss:  -3.0379 | PDE Loss:  -4.8262 | Function Loss:  -3.1151\n",
            "Total loss:  -3.0383 | PDE Loss:  -4.826 | Function Loss:  -3.1156\n",
            "Total loss:  -3.0386 | PDE Loss:  -4.8264 | Function Loss:  -3.1159\n",
            "Total loss:  -3.0389 | PDE Loss:  -4.8261 | Function Loss:  -3.1163\n",
            "Total loss:  -3.039 | PDE Loss:  -4.8263 | Function Loss:  -3.1164\n",
            "Total loss:  -3.0391 | PDE Loss:  -4.8264 | Function Loss:  -3.1164\n",
            "Total loss:  -3.0392 | PDE Loss:  -4.8269 | Function Loss:  -3.1165\n",
            "Total loss:  -3.0393 | PDE Loss:  -4.8274 | Function Loss:  -3.1165\n",
            "Total loss:  -3.0393 | PDE Loss:  -4.8281 | Function Loss:  -3.1164\n",
            "Total loss:  -3.0394 | PDE Loss:  -4.8281 | Function Loss:  -3.1165\n",
            "Total loss:  -3.0395 | PDE Loss:  -4.8285 | Function Loss:  -3.1166\n",
            "Total loss:  -3.0396 | PDE Loss:  -4.829 | Function Loss:  -3.1166\n",
            "Total loss:  -3.0397 | PDE Loss:  -4.8301 | Function Loss:  -3.1165\n",
            "Total loss:  -3.0399 | PDE Loss:  -4.8312 | Function Loss:  -3.1164\n",
            "Total loss:  -3.04 | PDE Loss:  -4.8325 | Function Loss:  -3.1164\n",
            "Total loss:  -3.0402 | PDE Loss:  -4.8341 | Function Loss:  -3.1163\n",
            "Total loss:  -3.0404 | PDE Loss:  -4.8356 | Function Loss:  -3.1162\n",
            "Total loss:  -3.0406 | PDE Loss:  -4.8372 | Function Loss:  -3.1161\n",
            "Total loss:  -3.0407 | PDE Loss:  -4.8381 | Function Loss:  -3.1162\n",
            "Total loss:  -3.0409 | PDE Loss:  -4.8392 | Function Loss:  -3.1162\n",
            "Total loss:  -3.0411 | PDE Loss:  -4.8398 | Function Loss:  -3.1162\n",
            "Total loss:  -3.0413 | PDE Loss:  -4.8403 | Function Loss:  -3.1165\n",
            "Total loss:  -3.0416 | PDE Loss:  -4.841 | Function Loss:  -3.1166\n",
            "Total loss:  -3.0418 | PDE Loss:  -4.8413 | Function Loss:  -3.1168\n",
            "Total loss:  -3.042 | PDE Loss:  -4.8417 | Function Loss:  -3.117\n",
            "Total loss:  -3.0422 | PDE Loss:  -4.842 | Function Loss:  -3.1171\n",
            "Total loss:  -3.0423 | PDE Loss:  -4.8422 | Function Loss:  -3.1172\n",
            "Total loss:  -3.0424 | PDE Loss:  -4.8427 | Function Loss:  -3.1173\n",
            "Total loss:  -3.0426 | PDE Loss:  -4.8429 | Function Loss:  -3.1175\n",
            "Total loss:  -3.0426 | PDE Loss:  -4.8438 | Function Loss:  -3.1174\n",
            "Total loss:  -3.0428 | PDE Loss:  -4.8439 | Function Loss:  -3.1175\n",
            "Total loss:  -3.0429 | PDE Loss:  -4.8437 | Function Loss:  -3.1176\n",
            "Total loss:  -3.043 | PDE Loss:  -4.8437 | Function Loss:  -3.1178\n",
            "Total loss:  -3.043 | PDE Loss:  -4.8437 | Function Loss:  -3.1178\n",
            "Total loss:  -3.0431 | PDE Loss:  -4.8437 | Function Loss:  -3.1179\n",
            "Total loss:  -3.0431 | PDE Loss:  -4.844 | Function Loss:  -3.1179\n",
            "Total loss:  -3.0432 | PDE Loss:  -4.8443 | Function Loss:  -3.1179\n",
            "Total loss:  -3.0433 | PDE Loss:  -4.845 | Function Loss:  -3.1179\n",
            "Total loss:  -3.0434 | PDE Loss:  -4.8454 | Function Loss:  -3.1179\n",
            "Total loss:  -3.0434 | PDE Loss:  -4.8458 | Function Loss:  -3.1179\n",
            "Total loss:  -3.0435 | PDE Loss:  -4.8462 | Function Loss:  -3.1179\n",
            "Total loss:  -3.0436 | PDE Loss:  -4.8465 | Function Loss:  -3.1179\n",
            "Total loss:  -3.0436 | PDE Loss:  -4.847 | Function Loss:  -3.118\n",
            "Total loss:  -3.0437 | PDE Loss:  -4.847 | Function Loss:  -3.118\n",
            "Total loss:  -3.0437 | PDE Loss:  -4.8473 | Function Loss:  -3.118\n",
            "Total loss:  -3.0438 | PDE Loss:  -4.8475 | Function Loss:  -3.118\n",
            "Total loss:  -3.0438 | PDE Loss:  -4.8476 | Function Loss:  -3.118\n",
            "Total loss:  -3.0438 | PDE Loss:  -4.8477 | Function Loss:  -3.1181\n",
            "Total loss:  -3.0439 | PDE Loss:  -4.8481 | Function Loss:  -3.118\n",
            "Total loss:  -3.0439 | PDE Loss:  -4.8484 | Function Loss:  -3.118\n",
            "Total loss:  -3.044 | PDE Loss:  -4.8484 | Function Loss:  -3.1181\n",
            "Total loss:  -3.044 | PDE Loss:  -4.8487 | Function Loss:  -3.1181\n",
            "Total loss:  -3.0441 | PDE Loss:  -4.8492 | Function Loss:  -3.1181\n",
            "Total loss:  -3.0442 | PDE Loss:  -4.8496 | Function Loss:  -3.1181\n",
            "Total loss:  -3.0443 | PDE Loss:  -4.8499 | Function Loss:  -3.1182\n",
            "Total loss:  -3.0444 | PDE Loss:  -4.8506 | Function Loss:  -3.1182\n",
            "Total loss:  -3.0445 | PDE Loss:  -4.8508 | Function Loss:  -3.1182\n",
            "Total loss:  -3.0446 | PDE Loss:  -4.8509 | Function Loss:  -3.1183\n",
            "Total loss:  -3.0447 | PDE Loss:  -4.8507 | Function Loss:  -3.1185\n",
            "Total loss:  -3.0447 | PDE Loss:  -4.8505 | Function Loss:  -3.1186\n",
            "Total loss:  -3.0448 | PDE Loss:  -4.8502 | Function Loss:  -3.1187\n",
            "Total loss:  -3.0448 | PDE Loss:  -4.8494 | Function Loss:  -3.1189\n",
            "Total loss:  -3.0449 | PDE Loss:  -4.8492 | Function Loss:  -3.1191\n",
            "Total loss:  -3.045 | PDE Loss:  -4.8489 | Function Loss:  -3.1191\n",
            "Total loss:  -3.045 | PDE Loss:  -4.8487 | Function Loss:  -3.1193\n",
            "Total loss:  -3.0452 | PDE Loss:  -4.8488 | Function Loss:  -3.1194\n",
            "Total loss:  -3.0452 | PDE Loss:  -4.8486 | Function Loss:  -3.1196\n",
            "Total loss:  -3.0453 | PDE Loss:  -4.8488 | Function Loss:  -3.1196\n",
            "Total loss:  -3.0454 | PDE Loss:  -4.8491 | Function Loss:  -3.1197\n",
            "Total loss:  -3.0456 | PDE Loss:  -4.8494 | Function Loss:  -3.1198\n",
            "Total loss:  -3.0456 | PDE Loss:  -4.8501 | Function Loss:  -3.1198\n",
            "Total loss:  -3.0457 | PDE Loss:  -4.8503 | Function Loss:  -3.1198\n",
            "Total loss:  -3.0458 | PDE Loss:  -4.8505 | Function Loss:  -3.1199\n",
            "Total loss:  -3.0459 | PDE Loss:  -4.8504 | Function Loss:  -3.12\n",
            "Total loss:  -3.046 | PDE Loss:  -4.8505 | Function Loss:  -3.1201\n",
            "Total loss:  -3.0462 | PDE Loss:  -4.85 | Function Loss:  -3.1204\n",
            "Total loss:  -3.0463 | PDE Loss:  -4.849 | Function Loss:  -3.1207\n",
            "Total loss:  -3.0464 | PDE Loss:  -4.8487 | Function Loss:  -3.1209\n",
            "Total loss:  -3.0466 | PDE Loss:  -4.8482 | Function Loss:  -3.1212\n",
            "Total loss:  -3.0467 | PDE Loss:  -4.8477 | Function Loss:  -3.1215\n",
            "Total loss:  -3.0469 | PDE Loss:  -4.8469 | Function Loss:  -3.1218\n",
            "Total loss:  -3.0471 | PDE Loss:  -4.8464 | Function Loss:  -3.1221\n",
            "Total loss:  -3.0473 | PDE Loss:  -4.8457 | Function Loss:  -3.1225\n",
            "Total loss:  -3.0474 | PDE Loss:  -4.8448 | Function Loss:  -3.1229\n",
            "Total loss:  -3.0476 | PDE Loss:  -4.8441 | Function Loss:  -3.1232\n",
            "Total loss:  -3.0478 | PDE Loss:  -4.8437 | Function Loss:  -3.1235\n",
            "Total loss:  -3.0479 | PDE Loss:  -4.8425 | Function Loss:  -3.1238\n",
            "Total loss:  -3.048 | PDE Loss:  -4.8417 | Function Loss:  -3.1241\n",
            "Total loss:  -3.048 | PDE Loss:  -4.8415 | Function Loss:  -3.1242\n",
            "Total loss:  -3.0481 | PDE Loss:  -4.8412 | Function Loss:  -3.1244\n",
            "Total loss:  -3.0482 | PDE Loss:  -4.841 | Function Loss:  -3.1245\n",
            "Total loss:  -3.0483 | PDE Loss:  -4.8406 | Function Loss:  -3.1246\n",
            "Total loss:  -3.0483 | PDE Loss:  -4.8399 | Function Loss:  -3.1248\n",
            "Total loss:  -3.0484 | PDE Loss:  -4.8392 | Function Loss:  -3.125\n",
            "Total loss:  -3.0484 | PDE Loss:  -4.8386 | Function Loss:  -3.1252\n",
            "Total loss:  -3.0485 | PDE Loss:  -4.8374 | Function Loss:  -3.1256\n",
            "Total loss:  -3.0486 | PDE Loss:  -4.8365 | Function Loss:  -3.1259\n",
            "Total loss:  -3.0488 | PDE Loss:  -4.8352 | Function Loss:  -3.1264\n",
            "Total loss:  -3.0487 | PDE Loss:  -4.8312 | Function Loss:  -3.127\n",
            "Total loss:  -3.049 | PDE Loss:  -4.8339 | Function Loss:  -3.1268\n",
            "Total loss:  -3.0491 | PDE Loss:  -4.8334 | Function Loss:  -3.1271\n",
            "Total loss:  -3.0494 | PDE Loss:  -4.833 | Function Loss:  -3.1275\n",
            "Total loss:  -3.0496 | PDE Loss:  -4.8328 | Function Loss:  -3.1278\n",
            "Total loss:  -3.0499 | PDE Loss:  -4.8325 | Function Loss:  -3.1282\n",
            "Total loss:  -3.0501 | PDE Loss:  -4.8323 | Function Loss:  -3.1285\n",
            "Total loss:  -3.0503 | PDE Loss:  -4.8324 | Function Loss:  -3.1286\n",
            "Total loss:  -3.0505 | PDE Loss:  -4.8325 | Function Loss:  -3.1289\n",
            "Total loss:  -3.0507 | PDE Loss:  -4.8328 | Function Loss:  -3.1291\n",
            "Total loss:  -3.0509 | PDE Loss:  -4.8328 | Function Loss:  -3.1293\n",
            "Total loss:  -3.051 | PDE Loss:  -4.8326 | Function Loss:  -3.1295\n",
            "Total loss:  -3.0511 | PDE Loss:  -4.8323 | Function Loss:  -3.1297\n",
            "Total loss:  -3.0513 | PDE Loss:  -4.8322 | Function Loss:  -3.13\n",
            "Total loss:  -3.0515 | PDE Loss:  -4.8318 | Function Loss:  -3.1303\n",
            "Total loss:  -3.0517 | PDE Loss:  -4.8306 | Function Loss:  -3.1307\n",
            "Total loss:  -3.0519 | PDE Loss:  -4.8305 | Function Loss:  -3.131\n",
            "Total loss:  -3.0521 | PDE Loss:  -4.8309 | Function Loss:  -3.1311\n",
            "Total loss:  -3.0522 | PDE Loss:  -4.8309 | Function Loss:  -3.1313\n",
            "Total loss:  -3.0523 | PDE Loss:  -4.8312 | Function Loss:  -3.1313\n",
            "Total loss:  -3.0524 | PDE Loss:  -4.8315 | Function Loss:  -3.1314\n",
            "Total loss:  -3.0525 | PDE Loss:  -4.8318 | Function Loss:  -3.1314\n",
            "Total loss:  -3.0526 | PDE Loss:  -4.8319 | Function Loss:  -3.1315\n",
            "Total loss:  -3.0527 | PDE Loss:  -4.832 | Function Loss:  -3.1316\n",
            "Total loss:  -3.0528 | PDE Loss:  -4.832 | Function Loss:  -3.1318\n",
            "Total loss:  -3.0531 | PDE Loss:  -4.832 | Function Loss:  -3.1321\n",
            "Total loss:  -3.0532 | PDE Loss:  -4.8316 | Function Loss:  -3.1323\n",
            "Total loss:  -3.0533 | PDE Loss:  -4.8315 | Function Loss:  -3.1325\n",
            "Total loss:  -3.0534 | PDE Loss:  -4.8313 | Function Loss:  -3.1326\n",
            "Total loss:  -3.0535 | PDE Loss:  -4.8313 | Function Loss:  -3.1328\n",
            "Total loss:  -3.0536 | PDE Loss:  -4.8315 | Function Loss:  -3.1329\n",
            "Total loss:  -3.0537 | PDE Loss:  -4.8313 | Function Loss:  -3.133\n",
            "Total loss:  -3.0538 | PDE Loss:  -4.8312 | Function Loss:  -3.1331\n",
            "Total loss:  -3.0539 | PDE Loss:  -4.831 | Function Loss:  -3.1332\n",
            "Total loss:  -3.054 | PDE Loss:  -4.8305 | Function Loss:  -3.1335\n",
            "Total loss:  -3.0541 | PDE Loss:  -4.8301 | Function Loss:  -3.1337\n",
            "Total loss:  -3.0543 | PDE Loss:  -4.8298 | Function Loss:  -3.134\n",
            "Total loss:  -3.0544 | PDE Loss:  -4.8295 | Function Loss:  -3.1342\n",
            "Total loss:  -3.0544 | PDE Loss:  -4.8279 | Function Loss:  -3.1346\n",
            "Total loss:  -3.0546 | PDE Loss:  -4.8285 | Function Loss:  -3.1346\n",
            "Total loss:  -3.0547 | PDE Loss:  -4.829 | Function Loss:  -3.1347\n",
            "Total loss:  -3.0549 | PDE Loss:  -4.8294 | Function Loss:  -3.1348\n",
            "Total loss:  -3.055 | PDE Loss:  -4.8299 | Function Loss:  -3.1348\n",
            "Total loss:  -3.0551 | PDE Loss:  -4.8301 | Function Loss:  -3.135\n",
            "Total loss:  -3.0553 | PDE Loss:  -4.8305 | Function Loss:  -3.1351\n",
            "Total loss:  -3.0555 | PDE Loss:  -4.8304 | Function Loss:  -3.1353\n",
            "Total loss:  -3.0557 | PDE Loss:  -4.8302 | Function Loss:  -3.1356\n",
            "Total loss:  -3.0558 | PDE Loss:  -4.8296 | Function Loss:  -3.1359\n",
            "Total loss:  -3.056 | PDE Loss:  -4.8301 | Function Loss:  -3.1359\n",
            "Total loss:  -3.0561 | PDE Loss:  -4.8297 | Function Loss:  -3.1361\n",
            "Total loss:  -3.0562 | PDE Loss:  -4.8289 | Function Loss:  -3.1364\n",
            "Total loss:  -3.0563 | PDE Loss:  -4.8287 | Function Loss:  -3.1367\n",
            "Total loss:  -3.0564 | PDE Loss:  -4.8284 | Function Loss:  -3.1369\n",
            "Total loss:  -3.0566 | PDE Loss:  -4.828 | Function Loss:  -3.1372\n",
            "Total loss:  -3.0569 | PDE Loss:  -4.8278 | Function Loss:  -3.1375\n",
            "Total loss:  -3.0571 | PDE Loss:  -4.8281 | Function Loss:  -3.1378\n",
            "Total loss:  -3.0574 | PDE Loss:  -4.828 | Function Loss:  -3.1381\n",
            "Total loss:  -3.0576 | PDE Loss:  -4.8284 | Function Loss:  -3.1382\n",
            "Total loss:  -3.0577 | PDE Loss:  -4.8289 | Function Loss:  -3.1383\n",
            "Total loss:  -3.0579 | PDE Loss:  -4.8286 | Function Loss:  -3.1386\n",
            "Total loss:  -3.0581 | PDE Loss:  -4.8291 | Function Loss:  -3.1387\n",
            "Total loss:  -3.0583 | PDE Loss:  -4.8298 | Function Loss:  -3.1388\n",
            "Total loss:  -3.0585 | PDE Loss:  -4.8302 | Function Loss:  -3.1389\n",
            "Total loss:  -3.0586 | PDE Loss:  -4.8306 | Function Loss:  -3.1391\n",
            "Total loss:  -3.0588 | PDE Loss:  -4.8304 | Function Loss:  -3.1393\n",
            "Total loss:  -3.059 | PDE Loss:  -4.8304 | Function Loss:  -3.1395\n",
            "Total loss:  -3.0589 | PDE Loss:  -4.8292 | Function Loss:  -3.1397\n",
            "Total loss:  -3.059 | PDE Loss:  -4.8301 | Function Loss:  -3.1396\n",
            "Total loss:  -3.0591 | PDE Loss:  -4.83 | Function Loss:  -3.1398\n",
            "Total loss:  -3.0593 | PDE Loss:  -4.8296 | Function Loss:  -3.1401\n",
            "Total loss:  -3.0595 | PDE Loss:  -4.8292 | Function Loss:  -3.1404\n",
            "Total loss:  -3.0598 | PDE Loss:  -4.8281 | Function Loss:  -3.141\n",
            "Total loss:  -3.06 | PDE Loss:  -4.8275 | Function Loss:  -3.1413\n",
            "Total loss:  -3.0601 | PDE Loss:  -4.8266 | Function Loss:  -3.1417\n",
            "Total loss:  -3.0603 | PDE Loss:  -4.8262 | Function Loss:  -3.1419\n",
            "Total loss:  -3.0604 | PDE Loss:  -4.8257 | Function Loss:  -3.1422\n",
            "Total loss:  -3.0605 | PDE Loss:  -4.8253 | Function Loss:  -3.1424\n",
            "Total loss:  -3.0606 | PDE Loss:  -4.825 | Function Loss:  -3.1426\n",
            "Total loss:  -3.0607 | PDE Loss:  -4.8248 | Function Loss:  -3.1427\n",
            "Total loss:  -3.0608 | PDE Loss:  -4.824 | Function Loss:  -3.1431\n",
            "Total loss:  -3.0609 | PDE Loss:  -4.824 | Function Loss:  -3.1432\n",
            "Total loss:  -3.0611 | PDE Loss:  -4.8241 | Function Loss:  -3.1433\n",
            "Total loss:  -3.0612 | PDE Loss:  -4.8243 | Function Loss:  -3.1435\n",
            "Total loss:  -3.0614 | PDE Loss:  -4.8242 | Function Loss:  -3.1437\n",
            "Total loss:  -3.0615 | PDE Loss:  -4.8241 | Function Loss:  -3.1438\n",
            "Total loss:  -3.0616 | PDE Loss:  -4.8241 | Function Loss:  -3.1439\n",
            "Total loss:  -3.0617 | PDE Loss:  -4.8242 | Function Loss:  -3.144\n",
            "Total loss:  -3.0618 | PDE Loss:  -4.8243 | Function Loss:  -3.1442\n",
            "Total loss:  -3.0619 | PDE Loss:  -4.8248 | Function Loss:  -3.1442\n",
            "Total loss:  -3.062 | PDE Loss:  -4.8255 | Function Loss:  -3.1442\n",
            "Total loss:  -3.0621 | PDE Loss:  -4.8265 | Function Loss:  -3.1441\n",
            "Total loss:  -3.0622 | PDE Loss:  -4.8272 | Function Loss:  -3.144\n",
            "Total loss:  -3.0623 | PDE Loss:  -4.8282 | Function Loss:  -3.1439\n",
            "Total loss:  -3.0624 | PDE Loss:  -4.8292 | Function Loss:  -3.1439\n",
            "Total loss:  -3.0625 | PDE Loss:  -4.8309 | Function Loss:  -3.1436\n",
            "Total loss:  -3.0626 | PDE Loss:  -4.8315 | Function Loss:  -3.1436\n",
            "Total loss:  -3.0626 | PDE Loss:  -4.832 | Function Loss:  -3.1436\n",
            "Total loss:  -3.0627 | PDE Loss:  -4.8324 | Function Loss:  -3.1436\n",
            "Total loss:  -3.0628 | PDE Loss:  -4.8325 | Function Loss:  -3.1437\n",
            "Total loss:  -3.0629 | PDE Loss:  -4.8335 | Function Loss:  -3.1436\n",
            "Total loss:  -3.063 | PDE Loss:  -4.8327 | Function Loss:  -3.1439\n",
            "Total loss:  -3.0631 | PDE Loss:  -4.8319 | Function Loss:  -3.1442\n",
            "Total loss:  -3.0632 | PDE Loss:  -4.8309 | Function Loss:  -3.1445\n",
            "Total loss:  -3.0632 | PDE Loss:  -4.8301 | Function Loss:  -3.1447\n",
            "Total loss:  -3.0633 | PDE Loss:  -4.8293 | Function Loss:  -3.145\n",
            "Total loss:  -3.0634 | PDE Loss:  -4.8286 | Function Loss:  -3.1452\n",
            "Total loss:  -3.0635 | PDE Loss:  -4.8281 | Function Loss:  -3.1454\n",
            "Total loss:  -3.0636 | PDE Loss:  -4.8278 | Function Loss:  -3.1456\n",
            "Total loss:  -3.0637 | PDE Loss:  -4.8278 | Function Loss:  -3.1457\n",
            "Total loss:  -3.0637 | PDE Loss:  -4.8278 | Function Loss:  -3.1458\n",
            "Total loss:  -3.0638 | PDE Loss:  -4.8278 | Function Loss:  -3.1459\n",
            "Total loss:  -3.0639 | PDE Loss:  -4.8279 | Function Loss:  -3.1459\n",
            "Total loss:  -3.064 | PDE Loss:  -4.8281 | Function Loss:  -3.146\n",
            "Total loss:  -3.064 | PDE Loss:  -4.828 | Function Loss:  -3.1461\n",
            "Total loss:  -3.0641 | PDE Loss:  -4.828 | Function Loss:  -3.1462\n",
            "Total loss:  -3.0642 | PDE Loss:  -4.8278 | Function Loss:  -3.1463\n",
            "Total loss:  -3.0643 | PDE Loss:  -4.8277 | Function Loss:  -3.1465\n",
            "Total loss:  -3.0644 | PDE Loss:  -4.827 | Function Loss:  -3.1468\n",
            "Total loss:  -3.0645 | PDE Loss:  -4.8277 | Function Loss:  -3.1468\n",
            "Total loss:  -3.0646 | PDE Loss:  -4.8282 | Function Loss:  -3.1468\n",
            "Total loss:  -3.0647 | PDE Loss:  -4.8289 | Function Loss:  -3.1467\n",
            "Total loss:  -3.0648 | PDE Loss:  -4.8301 | Function Loss:  -3.1466\n",
            "Total loss:  -3.065 | PDE Loss:  -4.8314 | Function Loss:  -3.1465\n",
            "Total loss:  -3.0651 | PDE Loss:  -4.8327 | Function Loss:  -3.1464\n",
            "Total loss:  -3.0652 | PDE Loss:  -4.8338 | Function Loss:  -3.1463\n",
            "Total loss:  -3.0652 | PDE Loss:  -4.8349 | Function Loss:  -3.1461\n",
            "Total loss:  -3.0654 | PDE Loss:  -4.8365 | Function Loss:  -3.146\n",
            "Total loss:  -3.0655 | PDE Loss:  -4.8373 | Function Loss:  -3.146\n",
            "Total loss:  -3.0657 | PDE Loss:  -4.8381 | Function Loss:  -3.146\n",
            "Total loss:  -3.0659 | PDE Loss:  -4.8385 | Function Loss:  -3.1462\n",
            "Total loss:  -3.0661 | PDE Loss:  -4.8387 | Function Loss:  -3.1464\n",
            "Total loss:  -3.0663 | PDE Loss:  -4.8387 | Function Loss:  -3.1466\n",
            "Total loss:  -3.0664 | PDE Loss:  -4.8387 | Function Loss:  -3.1467\n",
            "Total loss:  -3.0665 | PDE Loss:  -4.8388 | Function Loss:  -3.1469\n",
            "Total loss:  -3.0666 | PDE Loss:  -4.8391 | Function Loss:  -3.147\n",
            "Total loss:  -3.0669 | PDE Loss:  -4.8397 | Function Loss:  -3.1471\n",
            "Total loss:  -3.067 | PDE Loss:  -4.8405 | Function Loss:  -3.1472\n",
            "Total loss:  -3.0673 | PDE Loss:  -4.842 | Function Loss:  -3.1472\n",
            "Total loss:  -3.0676 | PDE Loss:  -4.8433 | Function Loss:  -3.1472\n",
            "Total loss:  -3.0678 | PDE Loss:  -4.8445 | Function Loss:  -3.1473\n",
            "Total loss:  -3.0679 | PDE Loss:  -4.8449 | Function Loss:  -3.1474\n",
            "Total loss:  -3.0681 | PDE Loss:  -4.845 | Function Loss:  -3.1475\n",
            "Total loss:  -3.0682 | PDE Loss:  -4.845 | Function Loss:  -3.1477\n",
            "Total loss:  -3.0684 | PDE Loss:  -4.8454 | Function Loss:  -3.1478\n",
            "Total loss:  -3.0686 | PDE Loss:  -4.8458 | Function Loss:  -3.1479\n",
            "Total loss:  -3.0687 | PDE Loss:  -4.8464 | Function Loss:  -3.148\n",
            "Total loss:  -3.0689 | PDE Loss:  -4.8472 | Function Loss:  -3.148\n",
            "Total loss:  -3.069 | PDE Loss:  -4.8485 | Function Loss:  -3.1479\n",
            "Total loss:  -3.0691 | PDE Loss:  -4.8499 | Function Loss:  -3.1478\n",
            "Total loss:  -3.0692 | PDE Loss:  -4.851 | Function Loss:  -3.1477\n",
            "Total loss:  -3.0693 | PDE Loss:  -4.8524 | Function Loss:  -3.1475\n",
            "Total loss:  -3.0694 | PDE Loss:  -4.8534 | Function Loss:  -3.1474\n",
            "Total loss:  -3.0695 | PDE Loss:  -4.8547 | Function Loss:  -3.1473\n",
            "Total loss:  -3.0696 | PDE Loss:  -4.8555 | Function Loss:  -3.1472\n",
            "Total loss:  -3.0696 | PDE Loss:  -4.8559 | Function Loss:  -3.1472\n",
            "Total loss:  -3.0698 | PDE Loss:  -4.8573 | Function Loss:  -3.1471\n",
            "Total loss:  -3.0699 | PDE Loss:  -4.8576 | Function Loss:  -3.1472\n",
            "Total loss:  -3.07 | PDE Loss:  -4.8577 | Function Loss:  -3.1473\n",
            "Total loss:  -3.0702 | PDE Loss:  -4.8581 | Function Loss:  -3.1474\n",
            "Total loss:  -3.0703 | PDE Loss:  -4.858 | Function Loss:  -3.1476\n",
            "Total loss:  -3.0705 | PDE Loss:  -4.8579 | Function Loss:  -3.1479\n",
            "Total loss:  -3.0707 | PDE Loss:  -4.8579 | Function Loss:  -3.1481\n",
            "Total loss:  -3.0708 | PDE Loss:  -4.8577 | Function Loss:  -3.1483\n",
            "Total loss:  -3.071 | PDE Loss:  -4.8583 | Function Loss:  -3.1483\n",
            "Total loss:  -3.0711 | PDE Loss:  -4.8586 | Function Loss:  -3.1484\n",
            "Total loss:  -3.0711 | PDE Loss:  -4.859 | Function Loss:  -3.1484\n",
            "Total loss:  -3.0713 | PDE Loss:  -4.8597 | Function Loss:  -3.1484\n",
            "Total loss:  -3.0714 | PDE Loss:  -4.8603 | Function Loss:  -3.1485\n",
            "Total loss:  -3.0715 | PDE Loss:  -4.8604 | Function Loss:  -3.1486\n",
            "Total loss:  -3.0717 | PDE Loss:  -4.8609 | Function Loss:  -3.1487\n",
            "Total loss:  -3.0719 | PDE Loss:  -4.8597 | Function Loss:  -3.1491\n",
            "Total loss:  -3.072 | PDE Loss:  -4.8603 | Function Loss:  -3.1492\n",
            "Total loss:  -3.0723 | PDE Loss:  -4.86 | Function Loss:  -3.1496\n",
            "Total loss:  -3.0726 | PDE Loss:  -4.8611 | Function Loss:  -3.1497\n",
            "Total loss:  -3.0729 | PDE Loss:  -4.8625 | Function Loss:  -3.1498\n",
            "Total loss:  -3.0732 | PDE Loss:  -4.8644 | Function Loss:  -3.1498\n",
            "Total loss:  -3.0734 | PDE Loss:  -4.866 | Function Loss:  -3.1498\n",
            "Total loss:  -3.0736 | PDE Loss:  -4.8679 | Function Loss:  -3.1496\n",
            "Total loss:  -3.0738 | PDE Loss:  -4.869 | Function Loss:  -3.1496\n",
            "Total loss:  -3.0739 | PDE Loss:  -4.8704 | Function Loss:  -3.1495\n",
            "Total loss:  -3.074 | PDE Loss:  -4.8712 | Function Loss:  -3.1495\n",
            "Total loss:  -3.0741 | PDE Loss:  -4.8724 | Function Loss:  -3.1494\n",
            "Total loss:  -3.0742 | PDE Loss:  -4.8732 | Function Loss:  -3.1494\n",
            "Total loss:  -3.0744 | PDE Loss:  -4.8741 | Function Loss:  -3.1494\n",
            "Total loss:  -3.0746 | PDE Loss:  -4.8743 | Function Loss:  -3.1496\n",
            "Total loss:  -3.0747 | PDE Loss:  -4.8751 | Function Loss:  -3.1496\n",
            "Total loss:  -3.0748 | PDE Loss:  -4.8755 | Function Loss:  -3.1496\n",
            "Total loss:  -3.0749 | PDE Loss:  -4.8759 | Function Loss:  -3.1497\n",
            "Total loss:  -3.075 | PDE Loss:  -4.8765 | Function Loss:  -3.1497\n",
            "Total loss:  -3.0751 | PDE Loss:  -4.8769 | Function Loss:  -3.1497\n",
            "Total loss:  -3.0752 | PDE Loss:  -4.8774 | Function Loss:  -3.1497\n",
            "Total loss:  -3.0753 | PDE Loss:  -4.8778 | Function Loss:  -3.1497\n",
            "Total loss:  -3.0754 | PDE Loss:  -4.8781 | Function Loss:  -3.1498\n",
            "Total loss:  -3.0754 | PDE Loss:  -4.8788 | Function Loss:  -3.1498\n",
            "Total loss:  -3.0755 | PDE Loss:  -4.8792 | Function Loss:  -3.1498\n",
            "Total loss:  -3.0756 | PDE Loss:  -4.8794 | Function Loss:  -3.1498\n",
            "Total loss:  -3.0757 | PDE Loss:  -4.8797 | Function Loss:  -3.1499\n",
            "Total loss:  -3.0759 | PDE Loss:  -4.8802 | Function Loss:  -3.15\n",
            "Total loss:  -3.076 | PDE Loss:  -4.8808 | Function Loss:  -3.1501\n",
            "Total loss:  -3.0762 | PDE Loss:  -4.8811 | Function Loss:  -3.1502\n",
            "Total loss:  -3.0764 | PDE Loss:  -4.8818 | Function Loss:  -3.1503\n",
            "Total loss:  -3.0766 | PDE Loss:  -4.8811 | Function Loss:  -3.1507\n",
            "Total loss:  -3.0768 | PDE Loss:  -4.8819 | Function Loss:  -3.1508\n",
            "Total loss:  -3.0771 | PDE Loss:  -4.8823 | Function Loss:  -3.151\n",
            "Total loss:  -3.0774 | PDE Loss:  -4.8828 | Function Loss:  -3.1513\n",
            "Total loss:  -3.0777 | PDE Loss:  -4.8824 | Function Loss:  -3.1518\n",
            "Total loss:  -3.078 | PDE Loss:  -4.8829 | Function Loss:  -3.152\n",
            "Total loss:  -3.0783 | PDE Loss:  -4.8833 | Function Loss:  -3.1523\n",
            "Total loss:  -3.0787 | PDE Loss:  -4.8846 | Function Loss:  -3.1525\n",
            "Total loss:  -3.079 | PDE Loss:  -4.8853 | Function Loss:  -3.1528\n",
            "Total loss:  -3.0793 | PDE Loss:  -4.8862 | Function Loss:  -3.1529\n",
            "Total loss:  -3.0795 | PDE Loss:  -4.8868 | Function Loss:  -3.1531\n",
            "Total loss:  -3.0797 | PDE Loss:  -4.8874 | Function Loss:  -3.1532\n",
            "Total loss:  -3.0799 | PDE Loss:  -4.8877 | Function Loss:  -3.1534\n",
            "Total loss:  -3.08 | PDE Loss:  -4.8879 | Function Loss:  -3.1535\n",
            "Total loss:  -3.0802 | PDE Loss:  -4.8881 | Function Loss:  -3.1537\n",
            "Total loss:  -3.0803 | PDE Loss:  -4.888 | Function Loss:  -3.1538\n",
            "Total loss:  -3.0804 | PDE Loss:  -4.8881 | Function Loss:  -3.1539\n",
            "Total loss:  -3.0806 | PDE Loss:  -4.8878 | Function Loss:  -3.1542\n",
            "Total loss:  -3.0807 | PDE Loss:  -4.8879 | Function Loss:  -3.1543\n",
            "Total loss:  -3.0808 | PDE Loss:  -4.8874 | Function Loss:  -3.1545\n",
            "Total loss:  -3.0809 | PDE Loss:  -4.8874 | Function Loss:  -3.1546\n",
            "Total loss:  -3.081 | PDE Loss:  -4.887 | Function Loss:  -3.1548\n",
            "Total loss:  -3.081 | PDE Loss:  -4.8869 | Function Loss:  -3.1549\n",
            "Total loss:  -3.0811 | PDE Loss:  -4.8869 | Function Loss:  -3.1549\n",
            "Total loss:  -3.0812 | PDE Loss:  -4.8872 | Function Loss:  -3.155\n",
            "Total loss:  -3.0812 | PDE Loss:  -4.8874 | Function Loss:  -3.155\n",
            "Total loss:  -3.0814 | PDE Loss:  -4.8884 | Function Loss:  -3.155\n",
            "Total loss:  -3.0815 | PDE Loss:  -4.8889 | Function Loss:  -3.1551\n",
            "Total loss:  -3.0817 | PDE Loss:  -4.8905 | Function Loss:  -3.155\n",
            "Total loss:  -3.0818 | PDE Loss:  -4.8914 | Function Loss:  -3.155\n",
            "Total loss:  -3.082 | PDE Loss:  -4.8925 | Function Loss:  -3.155\n",
            "Total loss:  -3.0822 | PDE Loss:  -4.8932 | Function Loss:  -3.155\n",
            "Total loss:  -3.0823 | PDE Loss:  -4.8943 | Function Loss:  -3.155\n",
            "Total loss:  -3.0824 | PDE Loss:  -4.8946 | Function Loss:  -3.1551\n",
            "Total loss:  -3.0825 | PDE Loss:  -4.8949 | Function Loss:  -3.1551\n",
            "Total loss:  -3.0826 | PDE Loss:  -4.8946 | Function Loss:  -3.1553\n",
            "Total loss:  -3.0827 | PDE Loss:  -4.8944 | Function Loss:  -3.1554\n",
            "Total loss:  -3.0828 | PDE Loss:  -4.8938 | Function Loss:  -3.1557\n",
            "Total loss:  -3.0828 | PDE Loss:  -4.8933 | Function Loss:  -3.1558\n",
            "Total loss:  -3.0829 | PDE Loss:  -4.8927 | Function Loss:  -3.156\n",
            "Total loss:  -3.083 | PDE Loss:  -4.8921 | Function Loss:  -3.1562\n",
            "Total loss:  -3.0831 | PDE Loss:  -4.8917 | Function Loss:  -3.1564\n",
            "Total loss:  -3.0829 | PDE Loss:  -4.8888 | Function Loss:  -3.1568\n",
            "Total loss:  -3.0831 | PDE Loss:  -4.891 | Function Loss:  -3.1566\n",
            "Total loss:  -3.0832 | PDE Loss:  -4.8907 | Function Loss:  -3.1568\n",
            "Total loss:  -3.0834 | PDE Loss:  -4.8909 | Function Loss:  -3.1569\n",
            "Total loss:  -3.0835 | PDE Loss:  -4.8911 | Function Loss:  -3.157\n",
            "Total loss:  -3.0837 | PDE Loss:  -4.8924 | Function Loss:  -3.157\n",
            "Total loss:  -3.0838 | PDE Loss:  -4.8932 | Function Loss:  -3.157\n",
            "Total loss:  -3.084 | PDE Loss:  -4.894 | Function Loss:  -3.1571\n",
            "Total loss:  -3.0841 | PDE Loss:  -4.8955 | Function Loss:  -3.157\n",
            "Total loss:  -3.0843 | PDE Loss:  -4.8953 | Function Loss:  -3.1572\n",
            "Total loss:  -3.0845 | PDE Loss:  -4.8956 | Function Loss:  -3.1574\n",
            "Total loss:  -3.0846 | PDE Loss:  -4.8947 | Function Loss:  -3.1577\n",
            "Total loss:  -3.0848 | PDE Loss:  -4.8934 | Function Loss:  -3.1582\n",
            "Total loss:  -3.085 | PDE Loss:  -4.8921 | Function Loss:  -3.1587\n",
            "Total loss:  -3.0853 | PDE Loss:  -4.8904 | Function Loss:  -3.1593\n",
            "Total loss:  -3.0857 | PDE Loss:  -4.8887 | Function Loss:  -3.16\n",
            "Total loss:  -3.086 | PDE Loss:  -4.8872 | Function Loss:  -3.1608\n",
            "Total loss:  -3.0864 | PDE Loss:  -4.8867 | Function Loss:  -3.1613\n",
            "Total loss:  -3.0868 | PDE Loss:  -4.8845 | Function Loss:  -3.1622\n",
            "Total loss:  -3.0871 | PDE Loss:  -4.8856 | Function Loss:  -3.1624\n",
            "Total loss:  -3.0874 | PDE Loss:  -4.8853 | Function Loss:  -3.1628\n",
            "Total loss:  -3.0878 | PDE Loss:  -4.8871 | Function Loss:  -3.1628\n",
            "Total loss:  -3.0881 | PDE Loss:  -4.8879 | Function Loss:  -3.1631\n",
            "Total loss:  -3.0884 | PDE Loss:  -4.8893 | Function Loss:  -3.1631\n",
            "Total loss:  -3.0886 | PDE Loss:  -4.89 | Function Loss:  -3.1632\n",
            "Total loss:  -3.0887 | PDE Loss:  -4.8906 | Function Loss:  -3.1633\n",
            "Total loss:  -3.0888 | PDE Loss:  -4.8912 | Function Loss:  -3.1633\n",
            "Total loss:  -3.089 | PDE Loss:  -4.8901 | Function Loss:  -3.1637\n",
            "Total loss:  -3.0891 | PDE Loss:  -4.8907 | Function Loss:  -3.1637\n",
            "Total loss:  -3.0892 | PDE Loss:  -4.8908 | Function Loss:  -3.1639\n",
            "Total loss:  -3.0894 | PDE Loss:  -4.8915 | Function Loss:  -3.1639\n",
            "Total loss:  -3.0895 | PDE Loss:  -4.8904 | Function Loss:  -3.1643\n",
            "Total loss:  -3.0897 | PDE Loss:  -4.8905 | Function Loss:  -3.1644\n",
            "Total loss:  -3.0898 | PDE Loss:  -4.8905 | Function Loss:  -3.1646\n",
            "Total loss:  -3.09 | PDE Loss:  -4.8908 | Function Loss:  -3.1648\n",
            "Total loss:  -3.0902 | PDE Loss:  -4.8909 | Function Loss:  -3.165\n",
            "Total loss:  -3.0903 | PDE Loss:  -4.8916 | Function Loss:  -3.165\n",
            "Total loss:  -3.0906 | PDE Loss:  -4.8922 | Function Loss:  -3.1652\n",
            "Total loss:  -3.0908 | PDE Loss:  -4.8933 | Function Loss:  -3.1653\n",
            "Total loss:  -3.091 | PDE Loss:  -4.8933 | Function Loss:  -3.1656\n",
            "Total loss:  -3.0912 | PDE Loss:  -4.8935 | Function Loss:  -3.1658\n",
            "Total loss:  -3.0915 | PDE Loss:  -4.8929 | Function Loss:  -3.1661\n",
            "Total loss:  -3.0917 | PDE Loss:  -4.8933 | Function Loss:  -3.1663\n",
            "Total loss:  -3.092 | PDE Loss:  -4.8913 | Function Loss:  -3.1671\n",
            "Total loss:  -3.0923 | PDE Loss:  -4.8921 | Function Loss:  -3.1672\n",
            "Total loss:  -3.0925 | PDE Loss:  -4.8924 | Function Loss:  -3.1674\n",
            "Total loss:  -3.0928 | PDE Loss:  -4.8932 | Function Loss:  -3.1676\n",
            "Total loss:  -3.0931 | PDE Loss:  -4.8937 | Function Loss:  -3.1679\n",
            "Total loss:  -3.0933 | PDE Loss:  -4.8948 | Function Loss:  -3.168\n",
            "Total loss:  -3.0936 | PDE Loss:  -4.8954 | Function Loss:  -3.1682\n",
            "Total loss:  -3.0941 | PDE Loss:  -4.8966 | Function Loss:  -3.1685\n",
            "Total loss:  -3.0945 | PDE Loss:  -4.8976 | Function Loss:  -3.1688\n",
            "Total loss:  -3.095 | PDE Loss:  -4.8984 | Function Loss:  -3.1693\n",
            "Total loss:  -3.0954 | PDE Loss:  -4.8987 | Function Loss:  -3.1697\n",
            "Total loss:  -3.0959 | PDE Loss:  -4.8991 | Function Loss:  -3.1702\n",
            "Total loss:  -3.0962 | PDE Loss:  -4.8983 | Function Loss:  -3.1708\n",
            "Total loss:  -3.0965 | PDE Loss:  -4.8984 | Function Loss:  -3.1711\n",
            "Total loss:  -3.0968 | PDE Loss:  -4.8984 | Function Loss:  -3.1714\n",
            "Total loss:  -3.0972 | PDE Loss:  -4.8985 | Function Loss:  -3.1719\n",
            "Total loss:  -3.0976 | PDE Loss:  -4.8991 | Function Loss:  -3.1723\n",
            "Total loss:  -3.0981 | PDE Loss:  -4.8995 | Function Loss:  -3.1728\n",
            "Total loss:  -3.0986 | PDE Loss:  -4.901 | Function Loss:  -3.1731\n",
            "Total loss:  -3.0991 | PDE Loss:  -4.9024 | Function Loss:  -3.1735\n",
            "Total loss:  -3.0997 | PDE Loss:  -4.9042 | Function Loss:  -3.1738\n",
            "Total loss:  -3.1002 | PDE Loss:  -4.9073 | Function Loss:  -3.1738\n",
            "Total loss:  -3.1005 | PDE Loss:  -4.9094 | Function Loss:  -3.1738\n",
            "Total loss:  -3.1009 | PDE Loss:  -4.9123 | Function Loss:  -3.1737\n",
            "Total loss:  -3.1015 | PDE Loss:  -4.9137 | Function Loss:  -3.1742\n",
            "Total loss:  -3.1021 | PDE Loss:  -4.9177 | Function Loss:  -3.1742\n",
            "Total loss:  -3.1028 | PDE Loss:  -4.9195 | Function Loss:  -3.1747\n",
            "Total loss:  -3.1033 | PDE Loss:  -4.9223 | Function Loss:  -3.1748\n",
            "Total loss:  -3.1038 | PDE Loss:  -4.9233 | Function Loss:  -3.1752\n",
            "Total loss:  -3.1044 | PDE Loss:  -4.9225 | Function Loss:  -3.176\n",
            "Total loss:  -3.1048 | PDE Loss:  -4.9221 | Function Loss:  -3.1765\n",
            "Total loss:  -3.1049 | PDE Loss:  -4.9189 | Function Loss:  -3.1772\n",
            "Total loss:  -3.1052 | PDE Loss:  -4.9188 | Function Loss:  -3.1776\n",
            "Total loss:  -3.1054 | PDE Loss:  -4.9194 | Function Loss:  -3.1777\n",
            "Total loss:  -3.1056 | PDE Loss:  -4.9188 | Function Loss:  -3.1781\n",
            "Total loss:  -3.1057 | PDE Loss:  -4.9184 | Function Loss:  -3.1783\n",
            "Total loss:  -3.1059 | PDE Loss:  -4.9182 | Function Loss:  -3.1786\n",
            "Total loss:  -3.1061 | PDE Loss:  -4.9178 | Function Loss:  -3.1789\n",
            "Total loss:  -3.1062 | PDE Loss:  -4.9179 | Function Loss:  -3.179\n",
            "Total loss:  -3.1064 | PDE Loss:  -4.9178 | Function Loss:  -3.1792\n",
            "Total loss:  -3.1065 | PDE Loss:  -4.9178 | Function Loss:  -3.1793\n",
            "Total loss:  -3.1066 | PDE Loss:  -4.9178 | Function Loss:  -3.1795\n",
            "Total loss:  -3.1067 | PDE Loss:  -4.9178 | Function Loss:  -3.1796\n",
            "Total loss:  -3.1069 | PDE Loss:  -4.9177 | Function Loss:  -3.1799\n",
            "Total loss:  -3.1071 | PDE Loss:  -4.9176 | Function Loss:  -3.1801\n",
            "Total loss:  -3.1074 | PDE Loss:  -4.9174 | Function Loss:  -3.1805\n",
            "Total loss:  -3.1077 | PDE Loss:  -4.9171 | Function Loss:  -3.1808\n",
            "Total loss:  -3.1079 | PDE Loss:  -4.9171 | Function Loss:  -3.1811\n",
            "Total loss:  -3.1081 | PDE Loss:  -4.9171 | Function Loss:  -3.1814\n",
            "Total loss:  -3.1083 | PDE Loss:  -4.9177 | Function Loss:  -3.1815\n",
            "Total loss:  -3.1085 | PDE Loss:  -4.9183 | Function Loss:  -3.1816\n",
            "Total loss:  -3.1087 | PDE Loss:  -4.9197 | Function Loss:  -3.1816\n",
            "Total loss:  -3.1089 | PDE Loss:  -4.9206 | Function Loss:  -3.1816\n",
            "Total loss:  -3.109 | PDE Loss:  -4.9222 | Function Loss:  -3.1815\n",
            "Total loss:  -3.1093 | PDE Loss:  -4.924 | Function Loss:  -3.1815\n",
            "Total loss:  -3.1096 | PDE Loss:  -4.9269 | Function Loss:  -3.1813\n",
            "Total loss:  -3.1098 | PDE Loss:  -4.9309 | Function Loss:  -3.1808\n",
            "Total loss:  -3.11 | PDE Loss:  -4.9316 | Function Loss:  -3.181\n",
            "Total loss:  -3.1102 | PDE Loss:  -4.9331 | Function Loss:  -3.181\n",
            "Total loss:  -3.1105 | PDE Loss:  -4.9345 | Function Loss:  -3.1811\n",
            "Total loss:  -3.1108 | PDE Loss:  -4.9358 | Function Loss:  -3.1812\n",
            "Total loss:  -3.1111 | PDE Loss:  -4.938 | Function Loss:  -3.1812\n",
            "Total loss:  -3.1113 | PDE Loss:  -4.9397 | Function Loss:  -3.1811\n",
            "Total loss:  -3.1116 | PDE Loss:  -4.9413 | Function Loss:  -3.1812\n",
            "Total loss:  -3.1119 | PDE Loss:  -4.9428 | Function Loss:  -3.1813\n",
            "Total loss:  -3.1122 | PDE Loss:  -4.9432 | Function Loss:  -3.1816\n",
            "Total loss:  -3.1125 | PDE Loss:  -4.9433 | Function Loss:  -3.1819\n",
            "Total loss:  -3.1127 | PDE Loss:  -4.9432 | Function Loss:  -3.1822\n",
            "Total loss:  -3.113 | PDE Loss:  -4.9428 | Function Loss:  -3.1825\n",
            "Total loss:  -3.1132 | PDE Loss:  -4.9429 | Function Loss:  -3.1828\n",
            "Total loss:  -3.1134 | PDE Loss:  -4.9428 | Function Loss:  -3.183\n",
            "Total loss:  -3.1136 | PDE Loss:  -4.9436 | Function Loss:  -3.1831\n",
            "Total loss:  -3.1138 | PDE Loss:  -4.9442 | Function Loss:  -3.1833\n",
            "Total loss:  -3.114 | PDE Loss:  -4.9473 | Function Loss:  -3.1829\n",
            "Total loss:  -3.1141 | PDE Loss:  -4.9472 | Function Loss:  -3.1831\n",
            "Total loss:  -3.1143 | PDE Loss:  -4.9465 | Function Loss:  -3.1834\n",
            "Total loss:  -3.1144 | PDE Loss:  -4.9465 | Function Loss:  -3.1836\n",
            "Total loss:  -3.1146 | PDE Loss:  -4.9455 | Function Loss:  -3.1839\n",
            "Total loss:  -3.1147 | PDE Loss:  -4.9455 | Function Loss:  -3.1841\n",
            "Total loss:  -3.1149 | PDE Loss:  -4.9455 | Function Loss:  -3.1843\n",
            "Total loss:  -3.115 | PDE Loss:  -4.946 | Function Loss:  -3.1844\n",
            "Total loss:  -3.1152 | PDE Loss:  -4.9455 | Function Loss:  -3.1847\n",
            "Total loss:  -3.1153 | PDE Loss:  -4.9459 | Function Loss:  -3.1847\n",
            "Total loss:  -3.1154 | PDE Loss:  -4.9465 | Function Loss:  -3.1847\n",
            "Total loss:  -3.1155 | PDE Loss:  -4.947 | Function Loss:  -3.1848\n",
            "Total loss:  -3.1156 | PDE Loss:  -4.9478 | Function Loss:  -3.1848\n",
            "Total loss:  -3.1157 | PDE Loss:  -4.9483 | Function Loss:  -3.1848\n",
            "Total loss:  -3.1158 | PDE Loss:  -4.9492 | Function Loss:  -3.1847\n",
            "Total loss:  -3.1159 | PDE Loss:  -4.9503 | Function Loss:  -3.1847\n",
            "Total loss:  -3.116 | PDE Loss:  -4.9508 | Function Loss:  -3.1847\n",
            "Total loss:  -3.1162 | PDE Loss:  -4.954 | Function Loss:  -3.1844\n",
            "Total loss:  -3.1163 | PDE Loss:  -4.9541 | Function Loss:  -3.1845\n",
            "Total loss:  -3.1166 | PDE Loss:  -4.9543 | Function Loss:  -3.1848\n",
            "Total loss:  -3.1169 | PDE Loss:  -4.9542 | Function Loss:  -3.1852\n",
            "Total loss:  -3.1172 | PDE Loss:  -4.9545 | Function Loss:  -3.1855\n",
            "Total loss:  -3.1175 | PDE Loss:  -4.9544 | Function Loss:  -3.1859\n",
            "Total loss:  -3.1179 | PDE Loss:  -4.9556 | Function Loss:  -3.1861\n",
            "Total loss:  -3.1182 | PDE Loss:  -4.9559 | Function Loss:  -3.1863\n",
            "Total loss:  -3.1184 | PDE Loss:  -4.9572 | Function Loss:  -3.1864\n",
            "Total loss:  -3.1186 | PDE Loss:  -4.9582 | Function Loss:  -3.1865\n",
            "Total loss:  -3.1188 | PDE Loss:  -4.9595 | Function Loss:  -3.1865\n",
            "Total loss:  -3.119 | PDE Loss:  -4.9599 | Function Loss:  -3.1867\n",
            "Total loss:  -3.1192 | PDE Loss:  -4.9606 | Function Loss:  -3.1867\n",
            "Total loss:  -3.1194 | PDE Loss:  -4.9608 | Function Loss:  -3.187\n",
            "Total loss:  -3.1197 | PDE Loss:  -4.9614 | Function Loss:  -3.1872\n",
            "Total loss:  -3.12 | PDE Loss:  -4.9616 | Function Loss:  -3.1876\n",
            "Total loss:  -3.1204 | PDE Loss:  -4.9619 | Function Loss:  -3.1879\n",
            "Total loss:  -3.1207 | PDE Loss:  -4.9618 | Function Loss:  -3.1883\n",
            "Total loss:  -3.1209 | PDE Loss:  -4.962 | Function Loss:  -3.1885\n",
            "Total loss:  -3.1212 | PDE Loss:  -4.9618 | Function Loss:  -3.1889\n",
            "Total loss:  -3.1215 | PDE Loss:  -4.9622 | Function Loss:  -3.1892\n",
            "Total loss:  -3.1219 | PDE Loss:  -4.9631 | Function Loss:  -3.1895\n",
            "Total loss:  -3.1222 | PDE Loss:  -4.9632 | Function Loss:  -3.1898\n",
            "Total loss:  -3.1226 | PDE Loss:  -4.9642 | Function Loss:  -3.1901\n",
            "Total loss:  -3.123 | PDE Loss:  -4.9638 | Function Loss:  -3.1906\n",
            "Total loss:  -3.1233 | PDE Loss:  -4.9646 | Function Loss:  -3.1908\n",
            "Total loss:  -3.1235 | PDE Loss:  -4.9645 | Function Loss:  -3.1911\n",
            "Total loss:  -3.1238 | PDE Loss:  -4.9647 | Function Loss:  -3.1914\n",
            "Total loss:  -3.1241 | PDE Loss:  -4.9651 | Function Loss:  -3.1917\n",
            "Total loss:  -3.1244 | PDE Loss:  -4.9648 | Function Loss:  -3.1921\n",
            "Total loss:  -3.1247 | PDE Loss:  -4.9648 | Function Loss:  -3.1925\n",
            "Total loss:  -3.1251 | PDE Loss:  -4.9652 | Function Loss:  -3.193\n",
            "Total loss:  -3.1256 | PDE Loss:  -4.9658 | Function Loss:  -3.1934\n",
            "Total loss:  -3.126 | PDE Loss:  -4.9668 | Function Loss:  -3.1937\n",
            "Total loss:  -3.1265 | PDE Loss:  -4.9678 | Function Loss:  -3.1941\n",
            "Total loss:  -3.1269 | PDE Loss:  -4.9696 | Function Loss:  -3.1943\n",
            "Total loss:  -3.1274 | PDE Loss:  -4.9703 | Function Loss:  -3.1947\n",
            "Total loss:  -3.1278 | PDE Loss:  -4.9722 | Function Loss:  -3.1949\n",
            "Total loss:  -3.1281 | PDE Loss:  -4.9727 | Function Loss:  -3.1952\n",
            "Total loss:  -3.1284 | PDE Loss:  -4.9739 | Function Loss:  -3.1953\n",
            "Total loss:  -3.1286 | PDE Loss:  -4.9736 | Function Loss:  -3.1956\n",
            "Total loss:  -3.1287 | PDE Loss:  -4.9742 | Function Loss:  -3.1956\n",
            "Total loss:  -3.1288 | PDE Loss:  -4.9743 | Function Loss:  -3.1957\n",
            "Total loss:  -3.129 | PDE Loss:  -4.9742 | Function Loss:  -3.1959\n",
            "Total loss:  -3.1291 | PDE Loss:  -4.9737 | Function Loss:  -3.1961\n",
            "Total loss:  -3.1292 | PDE Loss:  -4.9738 | Function Loss:  -3.1963\n",
            "Total loss:  -3.1294 | PDE Loss:  -4.9708 | Function Loss:  -3.197\n",
            "Total loss:  -3.1295 | PDE Loss:  -4.9703 | Function Loss:  -3.1972\n",
            "Total loss:  -3.1297 | PDE Loss:  -4.9695 | Function Loss:  -3.1975\n",
            "Total loss:  -3.1299 | PDE Loss:  -4.9691 | Function Loss:  -3.1979\n",
            "Total loss:  -3.1301 | PDE Loss:  -4.9682 | Function Loss:  -3.1983\n",
            "Total loss:  -3.1303 | PDE Loss:  -4.9676 | Function Loss:  -3.1986\n",
            "Total loss:  -3.1305 | PDE Loss:  -4.9674 | Function Loss:  -3.1988\n",
            "Total loss:  -3.1307 | PDE Loss:  -4.9672 | Function Loss:  -3.1991\n",
            "Total loss:  -3.1309 | PDE Loss:  -4.9668 | Function Loss:  -3.1994\n",
            "Total loss:  -3.131 | PDE Loss:  -4.9666 | Function Loss:  -3.1996\n",
            "Total loss:  -3.1312 | PDE Loss:  -4.9666 | Function Loss:  -3.1998\n",
            "Total loss:  -3.1313 | PDE Loss:  -4.9667 | Function Loss:  -3.1999\n",
            "Total loss:  -3.1315 | PDE Loss:  -4.9665 | Function Loss:  -3.2002\n",
            "Total loss:  -3.1316 | PDE Loss:  -4.9664 | Function Loss:  -3.2003\n",
            "Total loss:  -3.1318 | PDE Loss:  -4.9654 | Function Loss:  -3.2007\n",
            "Total loss:  -3.1319 | PDE Loss:  -4.9653 | Function Loss:  -3.2009\n",
            "Total loss:  -3.132 | PDE Loss:  -4.964 | Function Loss:  -3.2012\n",
            "Total loss:  -3.1321 | PDE Loss:  -4.9635 | Function Loss:  -3.2014\n",
            "Total loss:  -3.1322 | PDE Loss:  -4.9626 | Function Loss:  -3.2017\n",
            "Total loss:  -3.1324 | PDE Loss:  -4.961 | Function Loss:  -3.2021\n",
            "Total loss:  -3.1325 | PDE Loss:  -4.9592 | Function Loss:  -3.2026\n",
            "Total loss:  -3.1326 | PDE Loss:  -4.9568 | Function Loss:  -3.2032\n",
            "Total loss:  -3.1328 | PDE Loss:  -4.9561 | Function Loss:  -3.2035\n",
            "Total loss:  -3.1329 | PDE Loss:  -4.9556 | Function Loss:  -3.2037\n",
            "Total loss:  -3.133 | PDE Loss:  -4.9559 | Function Loss:  -3.2038\n",
            "Total loss:  -3.1331 | PDE Loss:  -4.9566 | Function Loss:  -3.2038\n",
            "Total loss:  -3.1332 | PDE Loss:  -4.9575 | Function Loss:  -3.2037\n",
            "Total loss:  -3.1333 | PDE Loss:  -4.9589 | Function Loss:  -3.2036\n",
            "Total loss:  -3.1334 | PDE Loss:  -4.9609 | Function Loss:  -3.2034\n",
            "Total loss:  -3.1336 | PDE Loss:  -4.9631 | Function Loss:  -3.2032\n",
            "Total loss:  -3.1338 | PDE Loss:  -4.9647 | Function Loss:  -3.2031\n",
            "Total loss:  -3.1339 | PDE Loss:  -4.9681 | Function Loss:  -3.2027\n",
            "Total loss:  -3.1342 | PDE Loss:  -4.9694 | Function Loss:  -3.2028\n",
            "Total loss:  -3.1343 | PDE Loss:  -4.9694 | Function Loss:  -3.2029\n",
            "Total loss:  -3.1345 | PDE Loss:  -4.9688 | Function Loss:  -3.2033\n",
            "Total loss:  -3.1348 | PDE Loss:  -4.9681 | Function Loss:  -3.2037\n",
            "Total loss:  -3.135 | PDE Loss:  -4.9669 | Function Loss:  -3.2041\n",
            "Total loss:  -3.1351 | PDE Loss:  -4.9662 | Function Loss:  -3.2044\n",
            "Total loss:  -3.1352 | PDE Loss:  -4.9654 | Function Loss:  -3.2047\n",
            "Total loss:  -3.1354 | PDE Loss:  -4.965 | Function Loss:  -3.2049\n",
            "Total loss:  -3.1356 | PDE Loss:  -4.9645 | Function Loss:  -3.2053\n",
            "Total loss:  -3.1357 | PDE Loss:  -4.9621 | Function Loss:  -3.2059\n",
            "Total loss:  -3.1359 | PDE Loss:  -4.9631 | Function Loss:  -3.206\n",
            "Total loss:  -3.1361 | PDE Loss:  -4.9641 | Function Loss:  -3.206\n",
            "Total loss:  -3.1363 | PDE Loss:  -4.9655 | Function Loss:  -3.206\n",
            "Total loss:  -3.1365 | PDE Loss:  -4.9665 | Function Loss:  -3.206\n",
            "Total loss:  -3.1366 | PDE Loss:  -4.9675 | Function Loss:  -3.2059\n",
            "Total loss:  -3.1367 | PDE Loss:  -4.9682 | Function Loss:  -3.2059\n",
            "Total loss:  -3.1368 | PDE Loss:  -4.9691 | Function Loss:  -3.206\n",
            "Total loss:  -3.137 | PDE Loss:  -4.9704 | Function Loss:  -3.206\n",
            "Total loss:  -3.1373 | PDE Loss:  -4.9702 | Function Loss:  -3.2063\n",
            "Total loss:  -3.1375 | PDE Loss:  -4.9716 | Function Loss:  -3.2063\n",
            "Total loss:  -3.1377 | PDE Loss:  -4.9717 | Function Loss:  -3.2066\n",
            "Total loss:  -3.1379 | PDE Loss:  -4.9712 | Function Loss:  -3.2069\n",
            "Total loss:  -3.1381 | PDE Loss:  -4.9708 | Function Loss:  -3.2072\n",
            "Total loss:  -3.1383 | PDE Loss:  -4.9706 | Function Loss:  -3.2074\n",
            "Total loss:  -3.1384 | PDE Loss:  -4.9702 | Function Loss:  -3.2076\n",
            "Total loss:  -3.1385 | PDE Loss:  -4.9703 | Function Loss:  -3.2078\n",
            "Total loss:  -3.1387 | PDE Loss:  -4.9701 | Function Loss:  -3.208\n",
            "Total loss:  -3.139 | PDE Loss:  -4.971 | Function Loss:  -3.2082\n",
            "Total loss:  -3.1392 | PDE Loss:  -4.9712 | Function Loss:  -3.2084\n",
            "Total loss:  -3.1395 | PDE Loss:  -4.9718 | Function Loss:  -3.2086\n",
            "Total loss:  -3.1398 | PDE Loss:  -4.9721 | Function Loss:  -3.2089\n",
            "Total loss:  -3.1401 | PDE Loss:  -4.9729 | Function Loss:  -3.2092\n",
            "Total loss:  -3.1404 | PDE Loss:  -4.9733 | Function Loss:  -3.2095\n",
            "Total loss:  -3.1407 | PDE Loss:  -4.9733 | Function Loss:  -3.2098\n",
            "Total loss:  -3.1411 | PDE Loss:  -4.9738 | Function Loss:  -3.2102\n",
            "Total loss:  -3.1414 | PDE Loss:  -4.9727 | Function Loss:  -3.2107\n",
            "Total loss:  -3.1417 | PDE Loss:  -4.9731 | Function Loss:  -3.2109\n",
            "Total loss:  -3.142 | PDE Loss:  -4.973 | Function Loss:  -3.2113\n",
            "Total loss:  -3.1424 | PDE Loss:  -4.9734 | Function Loss:  -3.2117\n",
            "Total loss:  -3.1426 | PDE Loss:  -4.9731 | Function Loss:  -3.2121\n",
            "Total loss:  -3.143 | PDE Loss:  -4.9733 | Function Loss:  -3.2125\n",
            "Total loss:  -3.1435 | PDE Loss:  -4.9739 | Function Loss:  -3.2129\n",
            "Total loss:  -3.1441 | PDE Loss:  -4.9742 | Function Loss:  -3.2136\n",
            "Total loss:  -3.1446 | PDE Loss:  -4.9761 | Function Loss:  -3.2139\n",
            "Total loss:  -3.1451 | PDE Loss:  -4.9735 | Function Loss:  -3.2149\n",
            "Total loss:  -3.1454 | PDE Loss:  -4.9733 | Function Loss:  -3.2153\n",
            "Total loss:  -3.1458 | PDE Loss:  -4.9757 | Function Loss:  -3.2153\n",
            "Total loss:  -3.1462 | PDE Loss:  -4.9773 | Function Loss:  -3.2155\n",
            "Total loss:  -3.1466 | PDE Loss:  -4.978 | Function Loss:  -3.2159\n",
            "Total loss:  -3.1469 | PDE Loss:  -4.9785 | Function Loss:  -3.2162\n",
            "Total loss:  -3.1474 | PDE Loss:  -4.979 | Function Loss:  -3.2166\n",
            "Total loss:  -3.1478 | PDE Loss:  -4.9789 | Function Loss:  -3.2171\n",
            "Total loss:  -3.1484 | PDE Loss:  -4.979 | Function Loss:  -3.2178\n",
            "Total loss:  -3.1481 | PDE Loss:  -4.9774 | Function Loss:  -3.2177\n",
            "Total loss:  -3.1489 | PDE Loss:  -4.98 | Function Loss:  -3.2182\n",
            "Total loss:  -3.1497 | PDE Loss:  -4.981 | Function Loss:  -3.219\n",
            "Total loss:  -3.1505 | PDE Loss:  -4.981 | Function Loss:  -3.2199\n",
            "Total loss:  -3.1511 | PDE Loss:  -4.982 | Function Loss:  -3.2205\n",
            "Total loss:  -3.1517 | PDE Loss:  -4.9815 | Function Loss:  -3.2213\n",
            "Total loss:  -3.1522 | PDE Loss:  -4.9834 | Function Loss:  -3.2215\n",
            "Total loss:  -3.1526 | PDE Loss:  -4.9854 | Function Loss:  -3.2217\n",
            "Total loss:  -3.1534 | PDE Loss:  -4.9884 | Function Loss:  -3.222\n",
            "Total loss:  -3.1542 | PDE Loss:  -4.9932 | Function Loss:  -3.2222\n",
            "Total loss:  -3.1549 | PDE Loss:  -4.994 | Function Loss:  -3.2229\n",
            "Total loss:  -3.1556 | PDE Loss:  -4.9984 | Function Loss:  -3.2229\n",
            "Total loss:  -3.1561 | PDE Loss:  -5.0005 | Function Loss:  -3.2231\n",
            "Total loss:  -3.1566 | PDE Loss:  -5.0029 | Function Loss:  -3.2233\n",
            "Total loss:  -3.1569 | PDE Loss:  -5.0027 | Function Loss:  -3.2238\n",
            "Total loss:  -3.1573 | PDE Loss:  -5.0028 | Function Loss:  -3.2242\n",
            "Total loss:  -3.1578 | PDE Loss:  -5.003 | Function Loss:  -3.2247\n",
            "Total loss:  -3.1583 | PDE Loss:  -5.0031 | Function Loss:  -3.2253\n",
            "Total loss:  -3.1587 | PDE Loss:  -5.0047 | Function Loss:  -3.2255\n",
            "Total loss:  -3.1592 | PDE Loss:  -5.0064 | Function Loss:  -3.2258\n",
            "Total loss:  -3.1597 | PDE Loss:  -5.0089 | Function Loss:  -3.2259\n",
            "Total loss:  -3.1603 | PDE Loss:  -5.0119 | Function Loss:  -3.2262\n",
            "Total loss:  -3.1609 | PDE Loss:  -5.0142 | Function Loss:  -3.2265\n",
            "Total loss:  -3.1615 | PDE Loss:  -5.0154 | Function Loss:  -3.227\n",
            "Total loss:  -3.162 | PDE Loss:  -5.0155 | Function Loss:  -3.2275\n",
            "Total loss:  -3.1623 | PDE Loss:  -5.0153 | Function Loss:  -3.228\n",
            "Total loss:  -3.1626 | PDE Loss:  -5.0136 | Function Loss:  -3.2286\n",
            "Total loss:  -3.1628 | PDE Loss:  -5.0126 | Function Loss:  -3.229\n",
            "Total loss:  -3.163 | PDE Loss:  -5.0117 | Function Loss:  -3.2294\n",
            "Total loss:  -3.1633 | PDE Loss:  -5.0105 | Function Loss:  -3.2299\n",
            "Total loss:  -3.1635 | PDE Loss:  -5.0096 | Function Loss:  -3.2303\n",
            "Total loss:  -3.164 | PDE Loss:  -5.0073 | Function Loss:  -3.2312\n",
            "Total loss:  -3.1643 | PDE Loss:  -5.0066 | Function Loss:  -3.2318\n",
            "Total loss:  -3.1648 | PDE Loss:  -5.0061 | Function Loss:  -3.2324\n",
            "Total loss:  -3.1655 | PDE Loss:  -5.0071 | Function Loss:  -3.233\n",
            "Total loss:  -3.166 | PDE Loss:  -5.0036 | Function Loss:  -3.2342\n",
            "Total loss:  -3.1665 | PDE Loss:  -5.0057 | Function Loss:  -3.2345\n",
            "Total loss:  -3.167 | PDE Loss:  -5.0073 | Function Loss:  -3.2348\n",
            "Total loss:  -3.1674 | PDE Loss:  -5.009 | Function Loss:  -3.235\n",
            "Total loss:  -3.1677 | PDE Loss:  -5.0089 | Function Loss:  -3.2353\n",
            "Total loss:  -3.1679 | PDE Loss:  -5.0085 | Function Loss:  -3.2356\n",
            "Total loss:  -3.1682 | PDE Loss:  -5.0112 | Function Loss:  -3.2355\n",
            "Total loss:  -3.1686 | PDE Loss:  -5.0118 | Function Loss:  -3.2359\n",
            "Total loss:  -3.1689 | PDE Loss:  -5.0113 | Function Loss:  -3.2363\n",
            "Total loss:  -3.1693 | PDE Loss:  -5.0109 | Function Loss:  -3.2369\n",
            "Total loss:  -3.1697 | PDE Loss:  -5.0103 | Function Loss:  -3.2374\n",
            "Total loss:  -3.1701 | PDE Loss:  -5.0085 | Function Loss:  -3.2382\n",
            "Total loss:  -3.1705 | PDE Loss:  -5.0081 | Function Loss:  -3.2387\n",
            "Total loss:  -3.1707 | PDE Loss:  -5.0078 | Function Loss:  -3.239\n",
            "Total loss:  -3.171 | PDE Loss:  -5.0074 | Function Loss:  -3.2394\n",
            "Total loss:  -3.1714 | PDE Loss:  -5.0072 | Function Loss:  -3.2399\n",
            "Total loss:  -3.1717 | PDE Loss:  -5.0069 | Function Loss:  -3.2403\n",
            "Total loss:  -3.172 | PDE Loss:  -5.0062 | Function Loss:  -3.2408\n",
            "Total loss:  -3.1724 | PDE Loss:  -5.0057 | Function Loss:  -3.2413\n",
            "Total loss:  -3.1727 | PDE Loss:  -5.0031 | Function Loss:  -3.2422\n",
            "Total loss:  -3.1731 | PDE Loss:  -5.0001 | Function Loss:  -3.2432\n",
            "Total loss:  -3.1734 | PDE Loss:  -5.0005 | Function Loss:  -3.2434\n",
            "Total loss:  -3.1739 | PDE Loss:  -5.0003 | Function Loss:  -3.244\n",
            "Total loss:  -3.1743 | PDE Loss:  -5.0008 | Function Loss:  -3.2445\n",
            "Total loss:  -3.1746 | PDE Loss:  -5.0002 | Function Loss:  -3.2449\n",
            "Total loss:  -3.1749 | PDE Loss:  -4.9998 | Function Loss:  -3.2453\n",
            "Total loss:  -3.1751 | PDE Loss:  -4.9991 | Function Loss:  -3.2457\n",
            "Total loss:  -3.1753 | PDE Loss:  -4.9982 | Function Loss:  -3.2461\n",
            "Total loss:  -3.1756 | PDE Loss:  -4.997 | Function Loss:  -3.2466\n",
            "Total loss:  -3.1757 | PDE Loss:  -4.9966 | Function Loss:  -3.2468\n",
            "Total loss:  -3.1759 | PDE Loss:  -4.9961 | Function Loss:  -3.2471\n",
            "Total loss:  -3.1761 | PDE Loss:  -4.9958 | Function Loss:  -3.2475\n",
            "Total loss:  -3.1764 | PDE Loss:  -4.9943 | Function Loss:  -3.248\n",
            "Total loss:  -3.1766 | PDE Loss:  -4.9948 | Function Loss:  -3.2482\n",
            "Total loss:  -3.1769 | PDE Loss:  -4.9937 | Function Loss:  -3.2487\n",
            "Total loss:  -3.1771 | PDE Loss:  -4.9928 | Function Loss:  -3.2492\n",
            "Total loss:  -3.1775 | PDE Loss:  -4.9919 | Function Loss:  -3.2498\n",
            "Total loss:  -3.1778 | PDE Loss:  -4.9911 | Function Loss:  -3.2503\n",
            "Total loss:  -3.178 | PDE Loss:  -4.9899 | Function Loss:  -3.2508\n",
            "Total loss:  -3.1783 | PDE Loss:  -4.9891 | Function Loss:  -3.2513\n",
            "Total loss:  -3.1786 | PDE Loss:  -4.9877 | Function Loss:  -3.2518\n",
            "Total loss:  -3.1789 | PDE Loss:  -4.9865 | Function Loss:  -3.2524\n",
            "Total loss:  -3.1791 | PDE Loss:  -4.9854 | Function Loss:  -3.2528\n",
            "Total loss:  -3.1793 | PDE Loss:  -4.9847 | Function Loss:  -3.2533\n",
            "Total loss:  -3.1796 | PDE Loss:  -4.9841 | Function Loss:  -3.2538\n",
            "Total loss:  -3.18 | PDE Loss:  -4.9835 | Function Loss:  -3.2543\n",
            "Total loss:  -3.1804 | PDE Loss:  -4.9844 | Function Loss:  -3.2546\n",
            "Total loss:  -3.1807 | PDE Loss:  -4.9831 | Function Loss:  -3.2552\n",
            "Total loss:  -3.181 | PDE Loss:  -4.9833 | Function Loss:  -3.2555\n",
            "Total loss:  -3.1813 | PDE Loss:  -4.9831 | Function Loss:  -3.2559\n",
            "Total loss:  -3.1815 | PDE Loss:  -4.9832 | Function Loss:  -3.2561\n",
            "Total loss:  -3.1817 | PDE Loss:  -4.983 | Function Loss:  -3.2564\n",
            "Total loss:  -3.1819 | PDE Loss:  -4.9829 | Function Loss:  -3.2567\n",
            "Total loss:  -3.1822 | PDE Loss:  -4.9825 | Function Loss:  -3.2571\n",
            "Total loss:  -3.1824 | PDE Loss:  -4.9818 | Function Loss:  -3.2575\n",
            "Total loss:  -3.1828 | PDE Loss:  -4.9805 | Function Loss:  -3.2582\n",
            "Total loss:  -3.1832 | PDE Loss:  -4.9795 | Function Loss:  -3.2589\n",
            "Total loss:  -3.1837 | PDE Loss:  -4.9785 | Function Loss:  -3.2596\n",
            "Total loss:  -3.184 | PDE Loss:  -4.9784 | Function Loss:  -3.26\n",
            "Total loss:  -3.1843 | PDE Loss:  -4.9777 | Function Loss:  -3.2605\n",
            "Total loss:  -3.1846 | PDE Loss:  -4.9786 | Function Loss:  -3.2607\n",
            "Total loss:  -3.1849 | PDE Loss:  -4.9782 | Function Loss:  -3.2611\n",
            "Total loss:  -3.1852 | PDE Loss:  -4.9784 | Function Loss:  -3.2614\n",
            "Total loss:  -3.1857 | PDE Loss:  -4.9779 | Function Loss:  -3.2621\n",
            "Total loss:  -3.1863 | PDE Loss:  -4.9759 | Function Loss:  -3.2632\n",
            "Total loss:  -3.1868 | PDE Loss:  -4.9729 | Function Loss:  -3.2644\n",
            "Total loss:  -3.1874 | PDE Loss:  -4.97 | Function Loss:  -3.2656\n",
            "Total loss:  -3.1877 | PDE Loss:  -4.9661 | Function Loss:  -3.2669\n",
            "Total loss:  -3.1881 | PDE Loss:  -4.9634 | Function Loss:  -3.2679\n",
            "Total loss:  -3.1885 | PDE Loss:  -4.9597 | Function Loss:  -3.2691\n",
            "Total loss:  -3.1888 | PDE Loss:  -4.9573 | Function Loss:  -3.27\n",
            "Total loss:  -3.1891 | PDE Loss:  -4.9551 | Function Loss:  -3.2708\n",
            "Total loss:  -3.1894 | PDE Loss:  -4.9533 | Function Loss:  -3.2715\n",
            "Total loss:  -3.1899 | PDE Loss:  -4.9513 | Function Loss:  -3.2725\n",
            "Total loss:  -3.1903 | PDE Loss:  -4.9494 | Function Loss:  -3.2734\n",
            "Total loss:  -3.1905 | PDE Loss:  -4.9484 | Function Loss:  -3.2739\n",
            "Total loss:  -3.1912 | PDE Loss:  -4.9464 | Function Loss:  -3.2751\n",
            "Total loss:  -3.1916 | PDE Loss:  -4.9465 | Function Loss:  -3.2755\n",
            "Total loss:  -3.1922 | PDE Loss:  -4.9471 | Function Loss:  -3.2762\n",
            "Total loss:  -3.193 | PDE Loss:  -4.9479 | Function Loss:  -3.277\n",
            "Total loss:  -3.1936 | PDE Loss:  -4.9485 | Function Loss:  -3.2776\n",
            "Total loss:  -3.1941 | PDE Loss:  -4.9488 | Function Loss:  -3.2781\n",
            "Total loss:  -3.1945 | PDE Loss:  -4.9495 | Function Loss:  -3.2785\n",
            "Total loss:  -3.1949 | PDE Loss:  -4.9495 | Function Loss:  -3.2789\n",
            "Total loss:  -3.1953 | PDE Loss:  -4.9499 | Function Loss:  -3.2793\n",
            "Total loss:  -3.1957 | PDE Loss:  -4.9501 | Function Loss:  -3.2798\n",
            "Total loss:  -3.1961 | PDE Loss:  -4.9482 | Function Loss:  -3.2807\n",
            "Total loss:  -3.1964 | PDE Loss:  -4.9483 | Function Loss:  -3.281\n",
            "Total loss:  -3.1968 | PDE Loss:  -4.9468 | Function Loss:  -3.2818\n",
            "Total loss:  -3.1972 | PDE Loss:  -4.9457 | Function Loss:  -3.2825\n",
            "Total loss:  -3.1974 | PDE Loss:  -4.9455 | Function Loss:  -3.2829\n",
            "Total loss:  -3.1977 | PDE Loss:  -4.9447 | Function Loss:  -3.2834\n",
            "Total loss:  -3.198 | PDE Loss:  -4.9444 | Function Loss:  -3.2839\n",
            "Total loss:  -3.1985 | PDE Loss:  -4.9429 | Function Loss:  -3.2848\n",
            "Total loss:  -3.1989 | PDE Loss:  -4.9432 | Function Loss:  -3.2852\n",
            "Total loss:  -3.1993 | PDE Loss:  -4.9435 | Function Loss:  -3.2856\n",
            "Total loss:  -3.1996 | PDE Loss:  -4.9453 | Function Loss:  -3.2856\n",
            "Total loss:  -3.2 | PDE Loss:  -4.9466 | Function Loss:  -3.2858\n",
            "Total loss:  -3.2002 | PDE Loss:  -4.9478 | Function Loss:  -3.2857\n",
            "Total loss:  -3.2004 | PDE Loss:  -4.9478 | Function Loss:  -3.286\n",
            "Total loss:  -3.2006 | PDE Loss:  -4.9493 | Function Loss:  -3.2859\n",
            "Total loss:  -3.2008 | PDE Loss:  -4.9501 | Function Loss:  -3.286\n",
            "Total loss:  -3.2011 | PDE Loss:  -4.9508 | Function Loss:  -3.2862\n",
            "Total loss:  -3.2013 | PDE Loss:  -4.95 | Function Loss:  -3.2866\n",
            "Total loss:  -3.2015 | PDE Loss:  -4.9495 | Function Loss:  -3.287\n",
            "Total loss:  -3.2017 | PDE Loss:  -4.948 | Function Loss:  -3.2875\n",
            "Total loss:  -3.2018 | PDE Loss:  -4.9467 | Function Loss:  -3.288\n",
            "Total loss:  -3.2021 | PDE Loss:  -4.9457 | Function Loss:  -3.2885\n",
            "Total loss:  -3.2022 | PDE Loss:  -4.9447 | Function Loss:  -3.2889\n",
            "Total loss:  -3.2024 | PDE Loss:  -4.944 | Function Loss:  -3.2892\n",
            "Total loss:  -3.2026 | PDE Loss:  -4.9437 | Function Loss:  -3.2895\n",
            "Total loss:  -3.2028 | PDE Loss:  -4.9432 | Function Loss:  -3.2899\n",
            "Total loss:  -3.2029 | PDE Loss:  -4.9433 | Function Loss:  -3.2901\n",
            "Total loss:  -3.2031 | PDE Loss:  -4.9438 | Function Loss:  -3.2902\n",
            "Total loss:  -3.2033 | PDE Loss:  -4.9439 | Function Loss:  -3.2904\n",
            "Total loss:  -3.2035 | PDE Loss:  -4.9447 | Function Loss:  -3.2905\n",
            "Total loss:  -3.2037 | PDE Loss:  -4.9451 | Function Loss:  -3.2906\n",
            "Total loss:  -3.2038 | PDE Loss:  -4.9452 | Function Loss:  -3.2907\n",
            "Total loss:  -3.2039 | PDE Loss:  -4.9448 | Function Loss:  -3.291\n",
            "Total loss:  -3.2041 | PDE Loss:  -4.9448 | Function Loss:  -3.2911\n",
            "Total loss:  -3.2041 | PDE Loss:  -4.9441 | Function Loss:  -3.2914\n",
            "Total loss:  -3.2043 | PDE Loss:  -4.9443 | Function Loss:  -3.2915\n",
            "Total loss:  -3.2044 | PDE Loss:  -4.9439 | Function Loss:  -3.2918\n",
            "Total loss:  -3.2046 | PDE Loss:  -4.9444 | Function Loss:  -3.2918\n",
            "Total loss:  -3.2048 | PDE Loss:  -4.9446 | Function Loss:  -3.2921\n",
            "Total loss:  -3.205 | PDE Loss:  -4.9455 | Function Loss:  -3.2921\n",
            "Total loss:  -3.2052 | PDE Loss:  -4.9468 | Function Loss:  -3.292\n",
            "Total loss:  -3.2055 | PDE Loss:  -4.9487 | Function Loss:  -3.292\n",
            "Total loss:  -3.2058 | PDE Loss:  -4.9493 | Function Loss:  -3.2923\n",
            "Total loss:  -3.2061 | PDE Loss:  -4.9508 | Function Loss:  -3.2923\n",
            "Total loss:  -3.2062 | PDE Loss:  -4.9514 | Function Loss:  -3.2923\n",
            "Total loss:  -3.2065 | PDE Loss:  -4.9517 | Function Loss:  -3.2926\n",
            "Total loss:  -3.2067 | PDE Loss:  -4.9519 | Function Loss:  -3.2927\n",
            "Total loss:  -3.2069 | PDE Loss:  -4.9516 | Function Loss:  -3.2931\n",
            "Total loss:  -3.2071 | PDE Loss:  -4.9519 | Function Loss:  -3.2933\n",
            "Total loss:  -3.2074 | PDE Loss:  -4.9521 | Function Loss:  -3.2936\n",
            "Total loss:  -3.2076 | PDE Loss:  -4.9529 | Function Loss:  -3.2937\n",
            "Total loss:  -3.2078 | PDE Loss:  -4.953 | Function Loss:  -3.2939\n",
            "Total loss:  -3.2083 | PDE Loss:  -4.9543 | Function Loss:  -3.2942\n",
            "Total loss:  -3.2088 | PDE Loss:  -4.9545 | Function Loss:  -3.2948\n",
            "Total loss:  -3.2093 | PDE Loss:  -4.9558 | Function Loss:  -3.2951\n",
            "Total loss:  -3.2097 | PDE Loss:  -4.9564 | Function Loss:  -3.2955\n",
            "Total loss:  -3.2102 | PDE Loss:  -4.9573 | Function Loss:  -3.2959\n",
            "Total loss:  -3.2106 | PDE Loss:  -4.9582 | Function Loss:  -3.2962\n",
            "Total loss:  -3.2109 | PDE Loss:  -4.9598 | Function Loss:  -3.2962\n",
            "Total loss:  -3.2111 | PDE Loss:  -4.96 | Function Loss:  -3.2964\n",
            "Total loss:  -3.2113 | PDE Loss:  -4.9612 | Function Loss:  -3.2964\n",
            "Total loss:  -3.2115 | PDE Loss:  -4.9622 | Function Loss:  -3.2964\n",
            "Total loss:  -3.2117 | PDE Loss:  -4.9629 | Function Loss:  -3.2964\n",
            "Total loss:  -3.2119 | PDE Loss:  -4.9643 | Function Loss:  -3.2964\n",
            "Total loss:  -3.212 | PDE Loss:  -4.9649 | Function Loss:  -3.2964\n",
            "Total loss:  -3.2122 | PDE Loss:  -4.9655 | Function Loss:  -3.2965\n",
            "Total loss:  -3.2124 | PDE Loss:  -4.9663 | Function Loss:  -3.2966\n",
            "Total loss:  -3.2127 | PDE Loss:  -4.9668 | Function Loss:  -3.2968\n",
            "Total loss:  -3.2129 | PDE Loss:  -4.9673 | Function Loss:  -3.2969\n",
            "Total loss:  -3.213 | PDE Loss:  -4.9675 | Function Loss:  -3.2971\n",
            "Total loss:  -3.2132 | PDE Loss:  -4.9678 | Function Loss:  -3.2972\n",
            "Total loss:  -3.2134 | PDE Loss:  -4.9685 | Function Loss:  -3.2974\n",
            "Total loss:  -3.2136 | PDE Loss:  -4.9683 | Function Loss:  -3.2976\n",
            "Total loss:  -3.2142 | PDE Loss:  -4.9672 | Function Loss:  -3.2986\n",
            "Total loss:  -3.2147 | PDE Loss:  -4.9666 | Function Loss:  -3.2993\n",
            "Total loss:  -3.2149 | PDE Loss:  -4.9657 | Function Loss:  -3.2998\n",
            "Total loss:  -3.2152 | PDE Loss:  -4.9656 | Function Loss:  -3.3002\n",
            "Total loss:  -3.2156 | PDE Loss:  -4.9655 | Function Loss:  -3.3007\n",
            "Total loss:  -3.2161 | PDE Loss:  -4.966 | Function Loss:  -3.3012\n",
            "Total loss:  -3.2166 | PDE Loss:  -4.9663 | Function Loss:  -3.3018\n",
            "Total loss:  -3.2172 | PDE Loss:  -4.9676 | Function Loss:  -3.3021\n",
            "Total loss:  -3.2177 | PDE Loss:  -4.9675 | Function Loss:  -3.3028\n",
            "Total loss:  -3.2183 | PDE Loss:  -4.9683 | Function Loss:  -3.3033\n",
            "Total loss:  -3.219 | PDE Loss:  -4.9699 | Function Loss:  -3.3038\n",
            "Total loss:  -3.22 | PDE Loss:  -4.969 | Function Loss:  -3.3052\n",
            "Total loss:  -3.2207 | PDE Loss:  -4.9692 | Function Loss:  -3.306\n",
            "Total loss:  -3.2213 | PDE Loss:  -4.9675 | Function Loss:  -3.3072\n",
            "Total loss:  -3.2217 | PDE Loss:  -4.9651 | Function Loss:  -3.3082\n",
            "Total loss:  -3.2221 | PDE Loss:  -4.9639 | Function Loss:  -3.3089\n",
            "Total loss:  -3.2226 | PDE Loss:  -4.9623 | Function Loss:  -3.3099\n",
            "Total loss:  -3.2231 | PDE Loss:  -4.9596 | Function Loss:  -3.3111\n",
            "Total loss:  -3.2235 | PDE Loss:  -4.9583 | Function Loss:  -3.3119\n",
            "Total loss:  -3.2239 | PDE Loss:  -4.9561 | Function Loss:  -3.3129\n",
            "Total loss:  -3.2245 | PDE Loss:  -4.9546 | Function Loss:  -3.3139\n",
            "Total loss:  -3.2253 | PDE Loss:  -4.9535 | Function Loss:  -3.3152\n",
            "Total loss:  -3.2259 | PDE Loss:  -4.9515 | Function Loss:  -3.3164\n",
            "Total loss:  -3.2263 | PDE Loss:  -4.9498 | Function Loss:  -3.3173\n",
            "Total loss:  -3.2267 | PDE Loss:  -4.9484 | Function Loss:  -3.3181\n",
            "Total loss:  -3.227 | PDE Loss:  -4.9462 | Function Loss:  -3.319\n",
            "Total loss:  -3.2274 | PDE Loss:  -4.9442 | Function Loss:  -3.32\n",
            "Total loss:  -3.2279 | PDE Loss:  -4.9419 | Function Loss:  -3.3211\n",
            "Total loss:  -3.2284 | PDE Loss:  -4.9398 | Function Loss:  -3.3222\n",
            "Total loss:  -3.229 | PDE Loss:  -4.9371 | Function Loss:  -3.3237\n",
            "Total loss:  -3.2296 | PDE Loss:  -4.9353 | Function Loss:  -3.3249\n",
            "Total loss:  -3.2301 | PDE Loss:  -4.9346 | Function Loss:  -3.3256\n",
            "Total loss:  -3.2306 | PDE Loss:  -4.9347 | Function Loss:  -3.3263\n",
            "Total loss:  -3.2313 | PDE Loss:  -4.9366 | Function Loss:  -3.3266\n",
            "Total loss:  -3.2318 | PDE Loss:  -4.9377 | Function Loss:  -3.327\n",
            "Total loss:  -3.2322 | PDE Loss:  -4.9397 | Function Loss:  -3.327\n",
            "Total loss:  -3.2325 | PDE Loss:  -4.9419 | Function Loss:  -3.3269\n",
            "Total loss:  -3.2329 | PDE Loss:  -4.9433 | Function Loss:  -3.327\n",
            "Total loss:  -3.2333 | PDE Loss:  -4.9463 | Function Loss:  -3.3268\n",
            "Total loss:  -3.2337 | PDE Loss:  -4.9482 | Function Loss:  -3.3268\n",
            "Total loss:  -3.2341 | PDE Loss:  -4.9513 | Function Loss:  -3.3266\n",
            "Total loss:  -3.2346 | PDE Loss:  -4.9552 | Function Loss:  -3.3263\n",
            "Total loss:  -3.235 | PDE Loss:  -4.9574 | Function Loss:  -3.3262\n",
            "Total loss:  -3.2353 | PDE Loss:  -4.9599 | Function Loss:  -3.326\n",
            "Total loss:  -3.2357 | PDE Loss:  -4.9626 | Function Loss:  -3.3258\n",
            "Total loss:  -3.236 | PDE Loss:  -4.9656 | Function Loss:  -3.3256\n",
            "Total loss:  -3.2364 | PDE Loss:  -4.9685 | Function Loss:  -3.3254\n",
            "Total loss:  -3.2367 | PDE Loss:  -4.9708 | Function Loss:  -3.3253\n",
            "Total loss:  -3.2373 | PDE Loss:  -4.9733 | Function Loss:  -3.3254\n",
            "Total loss:  -3.238 | PDE Loss:  -4.9752 | Function Loss:  -3.3259\n",
            "Total loss:  -3.2388 | PDE Loss:  -4.9796 | Function Loss:  -3.3258\n",
            "Total loss:  -3.2393 | PDE Loss:  -4.9795 | Function Loss:  -3.3264\n",
            "Total loss:  -3.2402 | PDE Loss:  -4.9799 | Function Loss:  -3.3275\n",
            "Total loss:  -3.2409 | PDE Loss:  -4.9791 | Function Loss:  -3.3285\n",
            "Total loss:  -3.2414 | PDE Loss:  -4.979 | Function Loss:  -3.3292\n",
            "Total loss:  -3.2417 | PDE Loss:  -4.9769 | Function Loss:  -3.33\n",
            "Total loss:  -3.2423 | PDE Loss:  -4.9779 | Function Loss:  -3.3305\n",
            "Total loss:  -3.2427 | PDE Loss:  -4.979 | Function Loss:  -3.3307\n",
            "Total loss:  -3.2432 | PDE Loss:  -4.9807 | Function Loss:  -3.331\n",
            "Total loss:  -3.2438 | PDE Loss:  -4.982 | Function Loss:  -3.3314\n",
            "Total loss:  -3.2442 | PDE Loss:  -4.9831 | Function Loss:  -3.3316\n",
            "Total loss:  -3.2444 | PDE Loss:  -4.9834 | Function Loss:  -3.3318\n",
            "Total loss:  -3.2446 | PDE Loss:  -4.9842 | Function Loss:  -3.3319\n",
            "Total loss:  -3.2448 | PDE Loss:  -4.9851 | Function Loss:  -3.332\n",
            "Total loss:  -3.245 | PDE Loss:  -4.9864 | Function Loss:  -3.332\n",
            "Total loss:  -3.2453 | PDE Loss:  -4.9873 | Function Loss:  -3.332\n",
            "Total loss:  -3.2454 | PDE Loss:  -4.9881 | Function Loss:  -3.3321\n",
            "Total loss:  -3.2456 | PDE Loss:  -4.9889 | Function Loss:  -3.3321\n",
            "Total loss:  -3.2459 | PDE Loss:  -4.9899 | Function Loss:  -3.3323\n",
            "Total loss:  -3.2462 | PDE Loss:  -4.9907 | Function Loss:  -3.3324\n",
            "Total loss:  -3.2464 | PDE Loss:  -4.9915 | Function Loss:  -3.3325\n",
            "Total loss:  -3.2466 | PDE Loss:  -4.9922 | Function Loss:  -3.3326\n",
            "Total loss:  -3.2469 | PDE Loss:  -4.9929 | Function Loss:  -3.3328\n",
            "Total loss:  -3.2471 | PDE Loss:  -4.994 | Function Loss:  -3.3328\n",
            "Total loss:  -3.2473 | PDE Loss:  -4.9942 | Function Loss:  -3.333\n",
            "Total loss:  -3.2475 | PDE Loss:  -4.995 | Function Loss:  -3.3331\n",
            "Total loss:  -3.2477 | PDE Loss:  -4.9945 | Function Loss:  -3.3334\n",
            "Total loss:  -3.2478 | PDE Loss:  -4.9949 | Function Loss:  -3.3335\n",
            "Total loss:  -3.2479 | PDE Loss:  -4.9944 | Function Loss:  -3.3338\n",
            "Total loss:  -3.2481 | PDE Loss:  -4.9945 | Function Loss:  -3.3339\n",
            "Total loss:  -3.2483 | PDE Loss:  -4.9946 | Function Loss:  -3.3341\n",
            "Total loss:  -3.2485 | PDE Loss:  -4.9954 | Function Loss:  -3.3342\n",
            "Total loss:  -3.2487 | PDE Loss:  -4.9968 | Function Loss:  -3.3341\n",
            "Total loss:  -3.2488 | PDE Loss:  -4.9983 | Function Loss:  -3.3339\n",
            "Total loss:  -3.249 | PDE Loss:  -5.0005 | Function Loss:  -3.3337\n",
            "Total loss:  -3.2492 | PDE Loss:  -5.0024 | Function Loss:  -3.3335\n",
            "Total loss:  -3.2494 | PDE Loss:  -5.0047 | Function Loss:  -3.3333\n",
            "Total loss:  -3.2495 | PDE Loss:  -5.0069 | Function Loss:  -3.333\n",
            "Total loss:  -3.2496 | PDE Loss:  -5.0088 | Function Loss:  -3.3327\n",
            "Total loss:  -3.2497 | PDE Loss:  -5.0107 | Function Loss:  -3.3325\n",
            "Total loss:  -3.2499 | PDE Loss:  -5.0127 | Function Loss:  -3.3322\n",
            "Total loss:  -3.2501 | PDE Loss:  -5.0154 | Function Loss:  -3.3319\n",
            "Total loss:  -3.2504 | PDE Loss:  -5.0177 | Function Loss:  -3.3317\n",
            "Total loss:  -3.2503 | PDE Loss:  -5.0217 | Function Loss:  -3.3309\n",
            "Total loss:  -3.2505 | PDE Loss:  -5.02 | Function Loss:  -3.3315\n",
            "Total loss:  -3.2508 | PDE Loss:  -5.022 | Function Loss:  -3.3314\n",
            "Total loss:  -3.2512 | PDE Loss:  -5.0241 | Function Loss:  -3.3314\n",
            "Total loss:  -3.2515 | PDE Loss:  -5.0268 | Function Loss:  -3.3313\n",
            "Total loss:  -3.2519 | PDE Loss:  -5.0284 | Function Loss:  -3.3314\n",
            "Total loss:  -3.2522 | PDE Loss:  -5.0301 | Function Loss:  -3.3314\n",
            "Total loss:  -3.2523 | PDE Loss:  -5.0316 | Function Loss:  -3.3313\n",
            "Total loss:  -3.2525 | PDE Loss:  -5.0323 | Function Loss:  -3.3314\n",
            "Total loss:  -3.2527 | PDE Loss:  -5.0333 | Function Loss:  -3.3314\n",
            "Total loss:  -3.2529 | PDE Loss:  -5.0341 | Function Loss:  -3.3315\n",
            "Total loss:  -3.2532 | PDE Loss:  -5.035 | Function Loss:  -3.3317\n",
            "Total loss:  -3.2535 | PDE Loss:  -5.0355 | Function Loss:  -3.3319\n",
            "Total loss:  -3.2538 | PDE Loss:  -5.0354 | Function Loss:  -3.3323\n",
            "Total loss:  -3.2541 | PDE Loss:  -5.0357 | Function Loss:  -3.3326\n",
            "Total loss:  -3.2544 | PDE Loss:  -5.0349 | Function Loss:  -3.3331\n",
            "Total loss:  -3.2548 | PDE Loss:  -5.0343 | Function Loss:  -3.3337\n",
            "Total loss:  -3.2551 | PDE Loss:  -5.0341 | Function Loss:  -3.3341\n",
            "Total loss:  -3.2555 | PDE Loss:  -5.0339 | Function Loss:  -3.3346\n",
            "Total loss:  -3.2558 | PDE Loss:  -5.0338 | Function Loss:  -3.335\n",
            "Total loss:  -3.256 | PDE Loss:  -5.0346 | Function Loss:  -3.3351\n",
            "Total loss:  -3.2562 | PDE Loss:  -5.0357 | Function Loss:  -3.3351\n",
            "Total loss:  -3.2564 | PDE Loss:  -5.0377 | Function Loss:  -3.335\n",
            "Total loss:  -3.2567 | PDE Loss:  -5.0401 | Function Loss:  -3.3349\n",
            "Total loss:  -3.2571 | PDE Loss:  -5.044 | Function Loss:  -3.3346\n",
            "Total loss:  -3.2576 | PDE Loss:  -5.0466 | Function Loss:  -3.3346\n",
            "Total loss:  -3.258 | PDE Loss:  -5.0547 | Function Loss:  -3.3335\n",
            "Total loss:  -3.2583 | PDE Loss:  -5.0616 | Function Loss:  -3.3326\n",
            "Total loss:  -3.2587 | PDE Loss:  -5.0588 | Function Loss:  -3.3337\n",
            "Total loss:  -3.2592 | PDE Loss:  -5.0568 | Function Loss:  -3.3347\n",
            "Total loss:  -3.2596 | PDE Loss:  -5.0545 | Function Loss:  -3.3356\n",
            "Total loss:  -3.26 | PDE Loss:  -5.0519 | Function Loss:  -3.3365\n",
            "Total loss:  -3.2604 | PDE Loss:  -5.05 | Function Loss:  -3.3373\n",
            "Total loss:  -3.2607 | PDE Loss:  -5.0489 | Function Loss:  -3.3379\n",
            "Total loss:  -3.2611 | PDE Loss:  -5.0486 | Function Loss:  -3.3384\n",
            "Total loss:  -3.2615 | PDE Loss:  -5.049 | Function Loss:  -3.3388\n",
            "Total loss:  -3.2619 | PDE Loss:  -5.0512 | Function Loss:  -3.3389\n",
            "Total loss:  -3.2623 | PDE Loss:  -5.0538 | Function Loss:  -3.3389\n",
            "Total loss:  -3.2626 | PDE Loss:  -5.0557 | Function Loss:  -3.3388\n",
            "Total loss:  -3.2628 | PDE Loss:  -5.0579 | Function Loss:  -3.3387\n",
            "Total loss:  -3.2631 | PDE Loss:  -5.0623 | Function Loss:  -3.3382\n",
            "Total loss:  -3.2635 | PDE Loss:  -5.0666 | Function Loss:  -3.3378\n",
            "Total loss:  -3.2638 | PDE Loss:  -5.0702 | Function Loss:  -3.3375\n",
            "Total loss:  -3.264 | PDE Loss:  -5.0733 | Function Loss:  -3.3372\n",
            "Total loss:  -3.2643 | PDE Loss:  -5.0746 | Function Loss:  -3.3373\n",
            "Total loss:  -3.2645 | PDE Loss:  -5.0755 | Function Loss:  -3.3373\n",
            "Total loss:  -3.2646 | PDE Loss:  -5.0754 | Function Loss:  -3.3376\n",
            "Total loss:  -3.2649 | PDE Loss:  -5.0756 | Function Loss:  -3.3378\n",
            "Total loss:  -3.2651 | PDE Loss:  -5.0757 | Function Loss:  -3.338\n",
            "Total loss:  -3.2653 | PDE Loss:  -5.0762 | Function Loss:  -3.3382\n",
            "Total loss:  -3.2655 | PDE Loss:  -5.0766 | Function Loss:  -3.3384\n",
            "Total loss:  -3.2657 | PDE Loss:  -5.0769 | Function Loss:  -3.3386\n",
            "Total loss:  -3.2658 | PDE Loss:  -5.0775 | Function Loss:  -3.3386\n",
            "Total loss:  -3.266 | PDE Loss:  -5.0784 | Function Loss:  -3.3386\n",
            "Total loss:  -3.2661 | PDE Loss:  -5.0789 | Function Loss:  -3.3387\n",
            "Total loss:  -3.2663 | PDE Loss:  -5.0801 | Function Loss:  -3.3387\n",
            "Total loss:  -3.2664 | PDE Loss:  -5.0821 | Function Loss:  -3.3385\n",
            "Total loss:  -3.2667 | PDE Loss:  -5.0844 | Function Loss:  -3.3383\n",
            "Total loss:  -3.2669 | PDE Loss:  -5.0871 | Function Loss:  -3.3381\n",
            "Total loss:  -3.2671 | PDE Loss:  -5.0891 | Function Loss:  -3.338\n",
            "Total loss:  -3.2674 | PDE Loss:  -5.0909 | Function Loss:  -3.338\n",
            "Total loss:  -3.2663 | PDE Loss:  -5.0939 | Function Loss:  -3.3362\n",
            "Total loss:  -3.2676 | PDE Loss:  -5.0934 | Function Loss:  -3.3378\n",
            "Total loss:  -3.2679 | PDE Loss:  -5.0944 | Function Loss:  -3.338\n",
            "Total loss:  -3.2683 | PDE Loss:  -5.0946 | Function Loss:  -3.3385\n",
            "Total loss:  -3.2687 | PDE Loss:  -5.0953 | Function Loss:  -3.3388\n",
            "Total loss:  -3.269 | PDE Loss:  -5.0937 | Function Loss:  -3.3394\n",
            "Total loss:  -3.2692 | PDE Loss:  -5.0934 | Function Loss:  -3.3397\n",
            "Total loss:  -3.2694 | PDE Loss:  -5.093 | Function Loss:  -3.3401\n",
            "Total loss:  -3.2698 | PDE Loss:  -5.0943 | Function Loss:  -3.3403\n",
            "Total loss:  -3.2702 | PDE Loss:  -5.0949 | Function Loss:  -3.3406\n",
            "Total loss:  -3.2707 | PDE Loss:  -5.0967 | Function Loss:  -3.3409\n",
            "Total loss:  -3.2712 | PDE Loss:  -5.0987 | Function Loss:  -3.3412\n",
            "Total loss:  -3.2717 | PDE Loss:  -5.1007 | Function Loss:  -3.3415\n",
            "Total loss:  -3.2723 | PDE Loss:  -5.1045 | Function Loss:  -3.3415\n",
            "Total loss:  -3.2729 | PDE Loss:  -5.1018 | Function Loss:  -3.3426\n",
            "Total loss:  -3.2733 | PDE Loss:  -5.1035 | Function Loss:  -3.3428\n",
            "Total loss:  -3.2738 | PDE Loss:  -5.1047 | Function Loss:  -3.3432\n",
            "Total loss:  -3.2743 | PDE Loss:  -5.1038 | Function Loss:  -3.3439\n",
            "Total loss:  -3.2747 | PDE Loss:  -5.1036 | Function Loss:  -3.3444\n",
            "Total loss:  -3.2751 | PDE Loss:  -5.1027 | Function Loss:  -3.3451\n",
            "Total loss:  -3.2757 | PDE Loss:  -5.1017 | Function Loss:  -3.3459\n",
            "Total loss:  -3.2763 | PDE Loss:  -5.101 | Function Loss:  -3.3468\n",
            "Total loss:  -3.2769 | PDE Loss:  -5.1001 | Function Loss:  -3.3476\n",
            "Total loss:  -3.2774 | PDE Loss:  -5.0968 | Function Loss:  -3.3488\n",
            "Total loss:  -3.2778 | PDE Loss:  -5.0956 | Function Loss:  -3.3495\n",
            "Total loss:  -3.2782 | PDE Loss:  -5.0947 | Function Loss:  -3.35\n",
            "Total loss:  -3.2786 | PDE Loss:  -5.0937 | Function Loss:  -3.3508\n",
            "Total loss:  -3.2792 | PDE Loss:  -5.0933 | Function Loss:  -3.3515\n",
            "Total loss:  -3.2797 | PDE Loss:  -5.0931 | Function Loss:  -3.3522\n",
            "Total loss:  -3.2802 | PDE Loss:  -5.0922 | Function Loss:  -3.3529\n",
            "Total loss:  -3.2805 | PDE Loss:  -5.0913 | Function Loss:  -3.3535\n",
            "Total loss:  -3.2809 | PDE Loss:  -5.0898 | Function Loss:  -3.3542\n",
            "Total loss:  -3.2814 | PDE Loss:  -5.0899 | Function Loss:  -3.3548\n",
            "Total loss:  -3.2818 | PDE Loss:  -5.0873 | Function Loss:  -3.3558\n",
            "Total loss:  -3.2821 | PDE Loss:  -5.0882 | Function Loss:  -3.3559\n",
            "Total loss:  -3.2823 | PDE Loss:  -5.0887 | Function Loss:  -3.3561\n",
            "Total loss:  -3.2826 | PDE Loss:  -5.0902 | Function Loss:  -3.3561\n",
            "Total loss:  -3.2827 | PDE Loss:  -5.0914 | Function Loss:  -3.3561\n",
            "Total loss:  -3.2829 | PDE Loss:  -5.0925 | Function Loss:  -3.356\n",
            "Total loss:  -3.2831 | PDE Loss:  -5.0937 | Function Loss:  -3.356\n",
            "Total loss:  -3.2833 | PDE Loss:  -5.095 | Function Loss:  -3.3561\n",
            "Total loss:  -3.2836 | PDE Loss:  -5.0957 | Function Loss:  -3.3563\n",
            "Total loss:  -3.2838 | PDE Loss:  -5.0967 | Function Loss:  -3.3563\n",
            "Total loss:  -3.284 | PDE Loss:  -5.0966 | Function Loss:  -3.3566\n",
            "Total loss:  -3.2841 | PDE Loss:  -5.0964 | Function Loss:  -3.3568\n",
            "Total loss:  -3.2842 | PDE Loss:  -5.0945 | Function Loss:  -3.3572\n",
            "Total loss:  -3.2843 | PDE Loss:  -5.0946 | Function Loss:  -3.3574\n",
            "Total loss:  -3.2845 | PDE Loss:  -5.0947 | Function Loss:  -3.3575\n",
            "Total loss:  -3.2846 | PDE Loss:  -5.0947 | Function Loss:  -3.3577\n",
            "Total loss:  -3.2848 | PDE Loss:  -5.0949 | Function Loss:  -3.3579\n",
            "Total loss:  -3.285 | PDE Loss:  -5.0952 | Function Loss:  -3.358\n",
            "Total loss:  -3.2851 | PDE Loss:  -5.0953 | Function Loss:  -3.3582\n",
            "Total loss:  -3.2854 | PDE Loss:  -5.0956 | Function Loss:  -3.3584\n",
            "Total loss:  -3.2857 | PDE Loss:  -5.095 | Function Loss:  -3.3589\n",
            "Total loss:  -3.2859 | PDE Loss:  -5.0951 | Function Loss:  -3.3592\n",
            "Total loss:  -3.2862 | PDE Loss:  -5.0949 | Function Loss:  -3.3595\n",
            "Total loss:  -3.2864 | PDE Loss:  -5.0951 | Function Loss:  -3.3597\n",
            "Total loss:  -3.2866 | PDE Loss:  -5.0955 | Function Loss:  -3.3599\n",
            "Total loss:  -3.2868 | PDE Loss:  -5.0958 | Function Loss:  -3.3601\n",
            "Total loss:  -3.287 | PDE Loss:  -5.0961 | Function Loss:  -3.3603\n",
            "Total loss:  -3.2872 | PDE Loss:  -5.0957 | Function Loss:  -3.3605\n",
            "Total loss:  -3.2874 | PDE Loss:  -5.0958 | Function Loss:  -3.3608\n",
            "Total loss:  -3.2876 | PDE Loss:  -5.0952 | Function Loss:  -3.3611\n",
            "Total loss:  -3.2874 | PDE Loss:  -5.0912 | Function Loss:  -3.3617\n",
            "Total loss:  -3.2877 | PDE Loss:  -5.0941 | Function Loss:  -3.3614\n",
            "Total loss:  -3.2878 | PDE Loss:  -5.0944 | Function Loss:  -3.3615\n",
            "Total loss:  -3.288 | PDE Loss:  -5.0945 | Function Loss:  -3.3617\n",
            "Total loss:  -3.2881 | PDE Loss:  -5.0947 | Function Loss:  -3.3618\n",
            "Total loss:  -3.2882 | PDE Loss:  -5.0948 | Function Loss:  -3.3619\n",
            "Total loss:  -3.2882 | PDE Loss:  -5.0945 | Function Loss:  -3.362\n",
            "Total loss:  -3.2883 | PDE Loss:  -5.0949 | Function Loss:  -3.362\n",
            "Total loss:  -3.2884 | PDE Loss:  -5.0944 | Function Loss:  -3.3622\n",
            "Total loss:  -3.2885 | PDE Loss:  -5.0947 | Function Loss:  -3.3623\n",
            "Total loss:  -3.2886 | PDE Loss:  -5.094 | Function Loss:  -3.3626\n",
            "Total loss:  -3.2888 | PDE Loss:  -5.0943 | Function Loss:  -3.3627\n",
            "Total loss:  -3.2889 | PDE Loss:  -5.0938 | Function Loss:  -3.363\n",
            "Total loss:  -3.2891 | PDE Loss:  -5.0937 | Function Loss:  -3.3631\n",
            "Total loss:  -3.2892 | PDE Loss:  -5.0936 | Function Loss:  -3.3633\n",
            "Total loss:  -3.2894 | PDE Loss:  -5.0946 | Function Loss:  -3.3633\n",
            "Total loss:  -3.2896 | PDE Loss:  -5.0944 | Function Loss:  -3.3636\n",
            "Total loss:  -3.2897 | PDE Loss:  -5.0946 | Function Loss:  -3.3638\n",
            "Total loss:  -3.2897 | PDE Loss:  -5.0885 | Function Loss:  -3.3649\n",
            "Total loss:  -3.2898 | PDE Loss:  -5.092 | Function Loss:  -3.3644\n",
            "Total loss:  -3.29 | PDE Loss:  -5.0924 | Function Loss:  -3.3645\n",
            "Total loss:  -3.2901 | PDE Loss:  -5.0932 | Function Loss:  -3.3645\n",
            "Total loss:  -3.2904 | PDE Loss:  -5.0927 | Function Loss:  -3.3649\n",
            "Total loss:  -3.2905 | PDE Loss:  -5.093 | Function Loss:  -3.365\n",
            "Total loss:  -3.2907 | PDE Loss:  -5.0938 | Function Loss:  -3.365\n",
            "Total loss:  -3.2909 | PDE Loss:  -5.0943 | Function Loss:  -3.3652\n",
            "Total loss:  -3.2911 | PDE Loss:  -5.0947 | Function Loss:  -3.3653\n",
            "Total loss:  -3.2913 | PDE Loss:  -5.095 | Function Loss:  -3.3656\n",
            "Total loss:  -3.2915 | PDE Loss:  -5.0939 | Function Loss:  -3.366\n",
            "Total loss:  -3.2917 | PDE Loss:  -5.0936 | Function Loss:  -3.3663\n",
            "Total loss:  -3.2919 | PDE Loss:  -5.0932 | Function Loss:  -3.3666\n",
            "Total loss:  -3.2921 | PDE Loss:  -5.0927 | Function Loss:  -3.367\n",
            "Total loss:  -3.2917 | PDE Loss:  -5.0856 | Function Loss:  -3.3677\n",
            "Total loss:  -3.2923 | PDE Loss:  -5.0909 | Function Loss:  -3.3675\n",
            "Total loss:  -3.2925 | PDE Loss:  -5.0911 | Function Loss:  -3.3677\n",
            "Total loss:  -3.2927 | PDE Loss:  -5.0908 | Function Loss:  -3.368\n",
            "Total loss:  -3.293 | PDE Loss:  -5.0909 | Function Loss:  -3.3683\n",
            "Total loss:  -3.2932 | PDE Loss:  -5.0909 | Function Loss:  -3.3686\n",
            "Total loss:  -3.2935 | PDE Loss:  -5.0918 | Function Loss:  -3.3688\n",
            "Total loss:  -3.2937 | PDE Loss:  -5.0921 | Function Loss:  -3.369\n",
            "Total loss:  -3.2939 | PDE Loss:  -5.0937 | Function Loss:  -3.3689\n",
            "Total loss:  -3.2941 | PDE Loss:  -5.0933 | Function Loss:  -3.3692\n",
            "Total loss:  -3.2943 | PDE Loss:  -5.0946 | Function Loss:  -3.3691\n",
            "Total loss:  -3.2944 | PDE Loss:  -5.0949 | Function Loss:  -3.3693\n",
            "Total loss:  -3.2946 | PDE Loss:  -5.0956 | Function Loss:  -3.3693\n",
            "Total loss:  -3.2947 | PDE Loss:  -5.0951 | Function Loss:  -3.3696\n",
            "Total loss:  -3.2948 | PDE Loss:  -5.0948 | Function Loss:  -3.3697\n",
            "Total loss:  -3.2949 | PDE Loss:  -5.0944 | Function Loss:  -3.37\n",
            "Total loss:  -3.295 | PDE Loss:  -5.0939 | Function Loss:  -3.3701\n",
            "Total loss:  -3.2951 | PDE Loss:  -5.094 | Function Loss:  -3.3703\n",
            "Total loss:  -3.2951 | PDE Loss:  -5.094 | Function Loss:  -3.3703\n",
            "Total loss:  -3.2952 | PDE Loss:  -5.0946 | Function Loss:  -3.3703\n",
            "Total loss:  -3.2953 | PDE Loss:  -5.0956 | Function Loss:  -3.3702\n",
            "Total loss:  -3.2954 | PDE Loss:  -5.097 | Function Loss:  -3.37\n",
            "Total loss:  -3.2955 | PDE Loss:  -5.0998 | Function Loss:  -3.3696\n",
            "Total loss:  -3.2955 | PDE Loss:  -5.1004 | Function Loss:  -3.3695\n",
            "Total loss:  -3.2955 | PDE Loss:  -5.1013 | Function Loss:  -3.3694\n",
            "Total loss:  -3.2956 | PDE Loss:  -5.1017 | Function Loss:  -3.3694\n",
            "Total loss:  -3.2956 | PDE Loss:  -5.1024 | Function Loss:  -3.3693\n",
            "Total loss:  -3.2957 | PDE Loss:  -5.103 | Function Loss:  -3.3692\n",
            "Total loss:  -3.2957 | PDE Loss:  -5.104 | Function Loss:  -3.3691\n",
            "Total loss:  -3.2959 | PDE Loss:  -5.1047 | Function Loss:  -3.3692\n",
            "Total loss:  -3.2961 | PDE Loss:  -5.1065 | Function Loss:  -3.3692\n",
            "Total loss:  -3.2963 | PDE Loss:  -5.1076 | Function Loss:  -3.3691\n",
            "Total loss:  -3.2966 | PDE Loss:  -5.1086 | Function Loss:  -3.3693\n",
            "Total loss:  -3.2969 | PDE Loss:  -5.1093 | Function Loss:  -3.3695\n",
            "Total loss:  -3.2971 | PDE Loss:  -5.1102 | Function Loss:  -3.3696\n",
            "Total loss:  -3.2973 | PDE Loss:  -5.1107 | Function Loss:  -3.3697\n",
            "Total loss:  -3.2974 | PDE Loss:  -5.1111 | Function Loss:  -3.3698\n",
            "Total loss:  -3.2975 | PDE Loss:  -5.1115 | Function Loss:  -3.3698\n",
            "Total loss:  -3.2976 | PDE Loss:  -5.1119 | Function Loss:  -3.3699\n",
            "Total loss:  -3.2977 | PDE Loss:  -5.1125 | Function Loss:  -3.3699\n",
            "Total loss:  -3.298 | PDE Loss:  -5.1141 | Function Loss:  -3.3699\n",
            "Total loss:  -3.2982 | PDE Loss:  -5.1151 | Function Loss:  -3.3701\n",
            "Total loss:  -3.2984 | PDE Loss:  -5.1161 | Function Loss:  -3.3701\n",
            "Total loss:  -3.2987 | PDE Loss:  -5.1171 | Function Loss:  -3.3703\n",
            "Total loss:  -3.299 | PDE Loss:  -5.118 | Function Loss:  -3.3705\n",
            "Total loss:  -3.2991 | PDE Loss:  -5.1171 | Function Loss:  -3.3708\n",
            "Total loss:  -3.2993 | PDE Loss:  -5.117 | Function Loss:  -3.3709\n",
            "Total loss:  -3.2994 | PDE Loss:  -5.116 | Function Loss:  -3.3713\n",
            "Total loss:  -3.2995 | PDE Loss:  -5.1155 | Function Loss:  -3.3715\n",
            "Total loss:  -3.2996 | PDE Loss:  -5.1142 | Function Loss:  -3.3719\n",
            "Total loss:  -3.2998 | PDE Loss:  -5.1136 | Function Loss:  -3.3722\n",
            "Total loss:  -3.2998 | PDE Loss:  -5.1146 | Function Loss:  -3.3721\n",
            "Total loss:  -3.3 | PDE Loss:  -5.1145 | Function Loss:  -3.3722\n",
            "Total loss:  -3.3001 | PDE Loss:  -5.1149 | Function Loss:  -3.3723\n",
            "Total loss:  -3.3002 | PDE Loss:  -5.1153 | Function Loss:  -3.3724\n",
            "Total loss:  -3.3003 | PDE Loss:  -5.1157 | Function Loss:  -3.3724\n",
            "Total loss:  -3.3004 | PDE Loss:  -5.1163 | Function Loss:  -3.3724\n",
            "Total loss:  -3.3005 | PDE Loss:  -5.1172 | Function Loss:  -3.3723\n",
            "Total loss:  -3.3006 | PDE Loss:  -5.118 | Function Loss:  -3.3723\n",
            "Total loss:  -3.3007 | PDE Loss:  -5.1193 | Function Loss:  -3.3722\n",
            "Total loss:  -3.3009 | PDE Loss:  -5.1209 | Function Loss:  -3.3721\n",
            "Total loss:  -3.3011 | PDE Loss:  -5.1239 | Function Loss:  -3.3719\n",
            "Total loss:  -3.3014 | PDE Loss:  -5.1274 | Function Loss:  -3.3716\n",
            "Total loss:  -3.3017 | PDE Loss:  -5.1307 | Function Loss:  -3.3714\n",
            "Total loss:  -3.3021 | PDE Loss:  -5.1349 | Function Loss:  -3.3711\n",
            "Total loss:  -3.3021 | PDE Loss:  -5.1419 | Function Loss:  -3.3699\n",
            "Total loss:  -3.3023 | PDE Loss:  -5.139 | Function Loss:  -3.3707\n",
            "Total loss:  -3.3026 | PDE Loss:  -5.1408 | Function Loss:  -3.3707\n",
            "Total loss:  -3.3029 | PDE Loss:  -5.1443 | Function Loss:  -3.3705\n",
            "Total loss:  -3.3032 | PDE Loss:  -5.1455 | Function Loss:  -3.3706\n",
            "Total loss:  -3.3034 | PDE Loss:  -5.146 | Function Loss:  -3.3707\n",
            "Total loss:  -3.3035 | PDE Loss:  -5.1479 | Function Loss:  -3.3706\n",
            "Total loss:  -3.3038 | PDE Loss:  -5.149 | Function Loss:  -3.3707\n",
            "Total loss:  -3.3041 | PDE Loss:  -5.1515 | Function Loss:  -3.3707\n",
            "Total loss:  -3.3045 | PDE Loss:  -5.1525 | Function Loss:  -3.371\n",
            "Total loss:  -3.305 | PDE Loss:  -5.1567 | Function Loss:  -3.3709\n",
            "Total loss:  -3.3052 | PDE Loss:  -5.1569 | Function Loss:  -3.3711\n",
            "Total loss:  -3.3057 | PDE Loss:  -5.1567 | Function Loss:  -3.3717\n",
            "Total loss:  -3.3062 | PDE Loss:  -5.1562 | Function Loss:  -3.3723\n",
            "Total loss:  -3.3067 | PDE Loss:  -5.1539 | Function Loss:  -3.3733\n",
            "Total loss:  -3.307 | PDE Loss:  -5.1533 | Function Loss:  -3.3737\n",
            "Total loss:  -3.3075 | PDE Loss:  -5.152 | Function Loss:  -3.3745\n",
            "Total loss:  -3.3081 | PDE Loss:  -5.1536 | Function Loss:  -3.375\n",
            "Total loss:  -3.3087 | PDE Loss:  -5.1557 | Function Loss:  -3.3753\n",
            "Total loss:  -3.3091 | PDE Loss:  -5.1603 | Function Loss:  -3.375\n",
            "Total loss:  -3.3093 | PDE Loss:  -5.1618 | Function Loss:  -3.375\n",
            "Total loss:  -3.3095 | PDE Loss:  -5.1643 | Function Loss:  -3.3749\n",
            "Total loss:  -3.3098 | PDE Loss:  -5.1671 | Function Loss:  -3.3748\n",
            "Total loss:  -3.3101 | PDE Loss:  -5.17 | Function Loss:  -3.3747\n",
            "Total loss:  -3.3103 | PDE Loss:  -5.1717 | Function Loss:  -3.3746\n",
            "Total loss:  -3.3107 | PDE Loss:  -5.1747 | Function Loss:  -3.3746\n",
            "Total loss:  -3.3111 | PDE Loss:  -5.1766 | Function Loss:  -3.3748\n",
            "Total loss:  -3.3115 | PDE Loss:  -5.1788 | Function Loss:  -3.3748\n",
            "Total loss:  -3.3118 | PDE Loss:  -5.1802 | Function Loss:  -3.375\n",
            "Total loss:  -3.3121 | PDE Loss:  -5.1831 | Function Loss:  -3.3749\n",
            "Total loss:  -3.3124 | PDE Loss:  -5.1855 | Function Loss:  -3.3748\n",
            "Total loss:  -3.3127 | PDE Loss:  -5.1883 | Function Loss:  -3.3747\n",
            "Total loss:  -3.313 | PDE Loss:  -5.1904 | Function Loss:  -3.3748\n",
            "Total loss:  -3.3134 | PDE Loss:  -5.1943 | Function Loss:  -3.3746\n",
            "Total loss:  -3.3135 | PDE Loss:  -5.1954 | Function Loss:  -3.3746\n",
            "Total loss:  -3.3138 | PDE Loss:  -5.1971 | Function Loss:  -3.3747\n",
            "Total loss:  -3.314 | PDE Loss:  -5.1971 | Function Loss:  -3.3749\n",
            "Total loss:  -3.3141 | PDE Loss:  -5.1979 | Function Loss:  -3.375\n",
            "Total loss:  -3.3142 | PDE Loss:  -5.1974 | Function Loss:  -3.3752\n",
            "Total loss:  -3.3144 | PDE Loss:  -5.1976 | Function Loss:  -3.3753\n",
            "Total loss:  -3.3145 | PDE Loss:  -5.1971 | Function Loss:  -3.3755\n",
            "Total loss:  -3.3147 | PDE Loss:  -5.1966 | Function Loss:  -3.3758\n",
            "Total loss:  -3.3149 | PDE Loss:  -5.1956 | Function Loss:  -3.3762\n",
            "Total loss:  -3.3149 | PDE Loss:  -5.1967 | Function Loss:  -3.3761\n",
            "Total loss:  -3.3151 | PDE Loss:  -5.1965 | Function Loss:  -3.3762\n",
            "Total loss:  -3.3152 | PDE Loss:  -5.1967 | Function Loss:  -3.3764\n",
            "Total loss:  -3.3154 | PDE Loss:  -5.1986 | Function Loss:  -3.3763\n",
            "Total loss:  -3.3156 | PDE Loss:  -5.1996 | Function Loss:  -3.3764\n",
            "Total loss:  -3.3158 | PDE Loss:  -5.2017 | Function Loss:  -3.3763\n",
            "Total loss:  -3.316 | PDE Loss:  -5.2036 | Function Loss:  -3.3763\n",
            "Total loss:  -3.3163 | PDE Loss:  -5.2054 | Function Loss:  -3.3763\n",
            "Total loss:  -3.3166 | PDE Loss:  -5.2085 | Function Loss:  -3.3763\n",
            "Total loss:  -3.317 | PDE Loss:  -5.2101 | Function Loss:  -3.3764\n",
            "Total loss:  -3.3173 | PDE Loss:  -5.2116 | Function Loss:  -3.3766\n",
            "Total loss:  -3.3176 | PDE Loss:  -5.2124 | Function Loss:  -3.3768\n",
            "Total loss:  -3.318 | PDE Loss:  -5.2127 | Function Loss:  -3.3772\n",
            "Total loss:  -3.3183 | PDE Loss:  -5.2121 | Function Loss:  -3.3776\n",
            "Total loss:  -3.3186 | PDE Loss:  -5.2111 | Function Loss:  -3.3781\n",
            "Total loss:  -3.3188 | PDE Loss:  -5.2095 | Function Loss:  -3.3786\n",
            "Total loss:  -3.319 | PDE Loss:  -5.2078 | Function Loss:  -3.3791\n",
            "Total loss:  -3.3193 | PDE Loss:  -5.2055 | Function Loss:  -3.3798\n",
            "Total loss:  -3.3197 | PDE Loss:  -5.2025 | Function Loss:  -3.3807\n",
            "Total loss:  -3.3199 | PDE Loss:  -5.2012 | Function Loss:  -3.3811\n",
            "Total loss:  -3.3202 | PDE Loss:  -5.1994 | Function Loss:  -3.3817\n",
            "Total loss:  -3.3205 | PDE Loss:  -5.1977 | Function Loss:  -3.3824\n",
            "Total loss:  -3.3209 | PDE Loss:  -5.1966 | Function Loss:  -3.383\n",
            "Total loss:  -3.3212 | PDE Loss:  -5.1936 | Function Loss:  -3.3838\n",
            "Total loss:  -3.3215 | PDE Loss:  -5.195 | Function Loss:  -3.3839\n",
            "Total loss:  -3.3219 | PDE Loss:  -5.1966 | Function Loss:  -3.3841\n",
            "Total loss:  -3.3223 | PDE Loss:  -5.2 | Function Loss:  -3.3841\n",
            "Total loss:  -3.3228 | PDE Loss:  -5.2036 | Function Loss:  -3.384\n",
            "Total loss:  -3.3231 | PDE Loss:  -5.2073 | Function Loss:  -3.3839\n",
            "Total loss:  -3.3235 | PDE Loss:  -5.2107 | Function Loss:  -3.3838\n",
            "Total loss:  -3.3238 | PDE Loss:  -5.2133 | Function Loss:  -3.3837\n",
            "Total loss:  -3.3242 | PDE Loss:  -5.217 | Function Loss:  -3.3837\n",
            "Total loss:  -3.3246 | PDE Loss:  -5.2202 | Function Loss:  -3.3836\n",
            "Total loss:  -3.3249 | PDE Loss:  -5.2213 | Function Loss:  -3.3838\n",
            "Total loss:  -3.3251 | PDE Loss:  -5.2226 | Function Loss:  -3.3839\n",
            "Total loss:  -3.3253 | PDE Loss:  -5.2243 | Function Loss:  -3.3839\n",
            "Total loss:  -3.3254 | PDE Loss:  -5.2285 | Function Loss:  -3.3834\n",
            "Total loss:  -3.3257 | PDE Loss:  -5.2278 | Function Loss:  -3.3838\n",
            "Total loss:  -3.3259 | PDE Loss:  -5.227 | Function Loss:  -3.3841\n",
            "Total loss:  -3.3261 | PDE Loss:  -5.2247 | Function Loss:  -3.3847\n",
            "Total loss:  -3.3263 | PDE Loss:  -5.2238 | Function Loss:  -3.385\n",
            "Total loss:  -3.3264 | PDE Loss:  -5.2217 | Function Loss:  -3.3856\n",
            "Total loss:  -3.3266 | PDE Loss:  -5.2202 | Function Loss:  -3.386\n",
            "Total loss:  -3.3269 | PDE Loss:  -5.22 | Function Loss:  -3.3863\n",
            "Total loss:  -3.3271 | PDE Loss:  -5.2193 | Function Loss:  -3.3866\n",
            "Total loss:  -3.3273 | PDE Loss:  -5.2193 | Function Loss:  -3.3869\n",
            "Total loss:  -3.3275 | PDE Loss:  -5.2208 | Function Loss:  -3.3869\n",
            "Total loss:  -3.3277 | PDE Loss:  -5.222 | Function Loss:  -3.3869\n",
            "Total loss:  -3.3279 | PDE Loss:  -5.2246 | Function Loss:  -3.3868\n",
            "Total loss:  -3.3281 | PDE Loss:  -5.2265 | Function Loss:  -3.3867\n",
            "Total loss:  -3.3283 | PDE Loss:  -5.2282 | Function Loss:  -3.3867\n",
            "Total loss:  -3.3285 | PDE Loss:  -5.2297 | Function Loss:  -3.3867\n",
            "Total loss:  -3.3287 | PDE Loss:  -5.2312 | Function Loss:  -3.3868\n",
            "Total loss:  -3.329 | PDE Loss:  -5.2327 | Function Loss:  -3.3869\n",
            "Total loss:  -3.3293 | PDE Loss:  -5.2333 | Function Loss:  -3.3872\n",
            "Total loss:  -3.3297 | PDE Loss:  -5.2338 | Function Loss:  -3.3875\n",
            "Total loss:  -3.33 | PDE Loss:  -5.2338 | Function Loss:  -3.3879\n",
            "Total loss:  -3.3304 | PDE Loss:  -5.2349 | Function Loss:  -3.3882\n",
            "Total loss:  -3.3307 | PDE Loss:  -5.2348 | Function Loss:  -3.3886\n",
            "Total loss:  -3.331 | PDE Loss:  -5.2356 | Function Loss:  -3.3888\n",
            "Total loss:  -3.3312 | PDE Loss:  -5.2359 | Function Loss:  -3.3889\n",
            "Total loss:  -3.3313 | PDE Loss:  -5.2362 | Function Loss:  -3.3891\n",
            "Total loss:  -3.3315 | PDE Loss:  -5.2369 | Function Loss:  -3.3892\n",
            "Total loss:  -3.3317 | PDE Loss:  -5.2366 | Function Loss:  -3.3894\n",
            "Total loss:  -3.3318 | PDE Loss:  -5.2377 | Function Loss:  -3.3894\n",
            "Total loss:  -3.3321 | PDE Loss:  -5.2372 | Function Loss:  -3.3898\n",
            "Total loss:  -3.3324 | PDE Loss:  -5.24 | Function Loss:  -3.3897\n",
            "Total loss:  -3.3326 | PDE Loss:  -5.237 | Function Loss:  -3.3903\n",
            "Total loss:  -3.3327 | PDE Loss:  -5.2383 | Function Loss:  -3.3904\n",
            "Total loss:  -3.333 | PDE Loss:  -5.2401 | Function Loss:  -3.3905\n",
            "Total loss:  -3.3334 | PDE Loss:  -5.2406 | Function Loss:  -3.3908\n",
            "Total loss:  -3.3336 | PDE Loss:  -5.241 | Function Loss:  -3.391\n",
            "Total loss:  -3.3338 | PDE Loss:  -5.241 | Function Loss:  -3.3912\n",
            "Total loss:  -3.334 | PDE Loss:  -5.2413 | Function Loss:  -3.3914\n",
            "Total loss:  -3.3343 | PDE Loss:  -5.242 | Function Loss:  -3.3916\n",
            "Total loss:  -3.3345 | PDE Loss:  -5.2424 | Function Loss:  -3.3918\n",
            "Total loss:  -3.3348 | PDE Loss:  -5.2454 | Function Loss:  -3.3918\n",
            "Total loss:  -3.3351 | PDE Loss:  -5.2482 | Function Loss:  -3.3917\n",
            "Total loss:  -3.3354 | PDE Loss:  -5.2509 | Function Loss:  -3.3917\n",
            "Total loss:  -3.3357 | PDE Loss:  -5.2542 | Function Loss:  -3.3915\n",
            "Total loss:  -3.336 | PDE Loss:  -5.2556 | Function Loss:  -3.3917\n",
            "Total loss:  -3.3362 | PDE Loss:  -5.2577 | Function Loss:  -3.3916\n",
            "Total loss:  -3.3365 | PDE Loss:  -5.2593 | Function Loss:  -3.3917\n",
            "Total loss:  -3.3366 | PDE Loss:  -5.259 | Function Loss:  -3.3919\n",
            "Total loss:  -3.3368 | PDE Loss:  -5.2599 | Function Loss:  -3.392\n",
            "Total loss:  -3.3369 | PDE Loss:  -5.2594 | Function Loss:  -3.3922\n",
            "Total loss:  -3.337 | PDE Loss:  -5.2595 | Function Loss:  -3.3923\n",
            "Total loss:  -3.3372 | PDE Loss:  -5.2597 | Function Loss:  -3.3925\n",
            "Total loss:  -3.3374 | PDE Loss:  -5.2599 | Function Loss:  -3.3927\n",
            "Total loss:  -3.3376 | PDE Loss:  -5.2614 | Function Loss:  -3.3927\n",
            "Total loss:  -3.3378 | PDE Loss:  -5.2596 | Function Loss:  -3.3932\n",
            "Total loss:  -3.338 | PDE Loss:  -5.2607 | Function Loss:  -3.3932\n",
            "Total loss:  -3.3382 | PDE Loss:  -5.2617 | Function Loss:  -3.3934\n",
            "Total loss:  -3.3385 | PDE Loss:  -5.2626 | Function Loss:  -3.3935\n",
            "Total loss:  -3.3387 | PDE Loss:  -5.2622 | Function Loss:  -3.3939\n",
            "Total loss:  -3.339 | PDE Loss:  -5.2622 | Function Loss:  -3.3942\n",
            "Total loss:  -3.3393 | PDE Loss:  -5.261 | Function Loss:  -3.3947\n",
            "Total loss:  -3.3397 | PDE Loss:  -5.2594 | Function Loss:  -3.3954\n",
            "Total loss:  -3.3401 | PDE Loss:  -5.2579 | Function Loss:  -3.396\n",
            "Total loss:  -3.3405 | PDE Loss:  -5.2558 | Function Loss:  -3.3968\n",
            "Total loss:  -3.341 | PDE Loss:  -5.2537 | Function Loss:  -3.3976\n",
            "Total loss:  -3.3413 | PDE Loss:  -5.2518 | Function Loss:  -3.3983\n",
            "Total loss:  -3.3416 | PDE Loss:  -5.2508 | Function Loss:  -3.3988\n",
            "Total loss:  -3.3419 | PDE Loss:  -5.2511 | Function Loss:  -3.399\n",
            "Total loss:  -3.3421 | PDE Loss:  -5.2522 | Function Loss:  -3.3992\n",
            "Total loss:  -3.3423 | PDE Loss:  -5.2543 | Function Loss:  -3.3991\n",
            "Total loss:  -3.3425 | PDE Loss:  -5.2559 | Function Loss:  -3.399\n",
            "Total loss:  -3.3426 | PDE Loss:  -5.2574 | Function Loss:  -3.3989\n",
            "Total loss:  -3.3428 | PDE Loss:  -5.2584 | Function Loss:  -3.399\n",
            "Total loss:  -3.3429 | PDE Loss:  -5.2592 | Function Loss:  -3.3991\n",
            "Total loss:  -3.343 | PDE Loss:  -5.2588 | Function Loss:  -3.3992\n",
            "Total loss:  -3.3432 | PDE Loss:  -5.2584 | Function Loss:  -3.3995\n",
            "Total loss:  -3.3433 | PDE Loss:  -5.2581 | Function Loss:  -3.3996\n",
            "Total loss:  -3.3434 | PDE Loss:  -5.2551 | Function Loss:  -3.4001\n",
            "Total loss:  -3.3435 | PDE Loss:  -5.255 | Function Loss:  -3.4003\n",
            "Total loss:  -3.3436 | PDE Loss:  -5.2549 | Function Loss:  -3.4004\n",
            "Total loss:  -3.3437 | PDE Loss:  -5.2535 | Function Loss:  -3.4008\n",
            "Total loss:  -3.3439 | PDE Loss:  -5.2525 | Function Loss:  -3.4011\n",
            "Total loss:  -3.344 | PDE Loss:  -5.2514 | Function Loss:  -3.4014\n",
            "Total loss:  -3.3441 | PDE Loss:  -5.2512 | Function Loss:  -3.4016\n",
            "Total loss:  -3.3443 | PDE Loss:  -5.2509 | Function Loss:  -3.4018\n",
            "Total loss:  -3.3445 | PDE Loss:  -5.2514 | Function Loss:  -3.402\n",
            "Total loss:  -3.3448 | PDE Loss:  -5.2522 | Function Loss:  -3.4022\n",
            "Total loss:  -3.345 | PDE Loss:  -5.2543 | Function Loss:  -3.4021\n",
            "Total loss:  -3.3453 | PDE Loss:  -5.2565 | Function Loss:  -3.4022\n",
            "Total loss:  -3.3456 | PDE Loss:  -5.2594 | Function Loss:  -3.4021\n",
            "Total loss:  -3.346 | PDE Loss:  -5.2624 | Function Loss:  -3.4021\n",
            "Total loss:  -3.3463 | PDE Loss:  -5.265 | Function Loss:  -3.4021\n",
            "Total loss:  -3.3466 | PDE Loss:  -5.2676 | Function Loss:  -3.4021\n",
            "Total loss:  -3.3468 | PDE Loss:  -5.2692 | Function Loss:  -3.4021\n",
            "Total loss:  -3.347 | PDE Loss:  -5.2704 | Function Loss:  -3.4022\n",
            "Total loss:  -3.3473 | PDE Loss:  -5.271 | Function Loss:  -3.4024\n",
            "Total loss:  -3.3476 | PDE Loss:  -5.2715 | Function Loss:  -3.4027\n",
            "Total loss:  -3.3481 | PDE Loss:  -5.2716 | Function Loss:  -3.4032\n",
            "Total loss:  -3.3483 | PDE Loss:  -5.2724 | Function Loss:  -3.4034\n",
            "Total loss:  -3.3489 | PDE Loss:  -5.274 | Function Loss:  -3.4039\n",
            "Total loss:  -3.3496 | PDE Loss:  -5.2765 | Function Loss:  -3.4043\n",
            "Total loss:  -3.3503 | PDE Loss:  -5.2768 | Function Loss:  -3.405\n",
            "Total loss:  -3.3508 | PDE Loss:  -5.2802 | Function Loss:  -3.4051\n",
            "Total loss:  -3.3512 | PDE Loss:  -5.2828 | Function Loss:  -3.4053\n",
            "Total loss:  -3.3519 | PDE Loss:  -5.2863 | Function Loss:  -3.4056\n",
            "Total loss:  -3.3525 | PDE Loss:  -5.2905 | Function Loss:  -3.4057\n",
            "Total loss:  -3.353 | PDE Loss:  -5.293 | Function Loss:  -3.406\n",
            "Total loss:  -3.3537 | PDE Loss:  -5.296 | Function Loss:  -3.4064\n",
            "Total loss:  -3.3544 | PDE Loss:  -5.2986 | Function Loss:  -3.4068\n",
            "Total loss:  -3.3548 | PDE Loss:  -5.2995 | Function Loss:  -3.4072\n",
            "Total loss:  -3.3552 | PDE Loss:  -5.3009 | Function Loss:  -3.4074\n",
            "Total loss:  -3.3555 | PDE Loss:  -5.3016 | Function Loss:  -3.4077\n",
            "Total loss:  -3.3558 | PDE Loss:  -5.3028 | Function Loss:  -3.4079\n",
            "Total loss:  -3.3561 | PDE Loss:  -5.3045 | Function Loss:  -3.408\n",
            "Total loss:  -3.3564 | PDE Loss:  -5.3057 | Function Loss:  -3.4082\n",
            "Total loss:  -3.3568 | PDE Loss:  -5.3081 | Function Loss:  -3.4083\n",
            "Total loss:  -3.3572 | PDE Loss:  -5.3093 | Function Loss:  -3.4086\n",
            "Total loss:  -3.3575 | PDE Loss:  -5.3116 | Function Loss:  -3.4087\n",
            "Total loss:  -3.3579 | PDE Loss:  -5.3131 | Function Loss:  -3.4089\n",
            "Total loss:  -3.3583 | PDE Loss:  -5.3121 | Function Loss:  -3.4095\n",
            "Total loss:  -3.3589 | PDE Loss:  -5.3142 | Function Loss:  -3.4099\n",
            "Total loss:  -3.3594 | PDE Loss:  -5.3156 | Function Loss:  -3.4104\n",
            "Total loss:  -3.36 | PDE Loss:  -5.3151 | Function Loss:  -3.4111\n",
            "Total loss:  -3.3607 | PDE Loss:  -5.3159 | Function Loss:  -3.4117\n",
            "Total loss:  -3.3612 | PDE Loss:  -5.3165 | Function Loss:  -3.4123\n",
            "Total loss:  -3.3618 | PDE Loss:  -5.3171 | Function Loss:  -3.4128\n",
            "Total loss:  -3.3624 | PDE Loss:  -5.3191 | Function Loss:  -3.4132\n",
            "Total loss:  -3.3629 | PDE Loss:  -5.3213 | Function Loss:  -3.4136\n",
            "Total loss:  -3.3637 | PDE Loss:  -5.3247 | Function Loss:  -3.414\n",
            "Total loss:  -3.3645 | PDE Loss:  -5.3314 | Function Loss:  -3.4141\n",
            "Total loss:  -3.3643 | PDE Loss:  -5.3276 | Function Loss:  -3.4143\n",
            "Total loss:  -3.365 | PDE Loss:  -5.3322 | Function Loss:  -3.4145\n",
            "Total loss:  -3.3654 | PDE Loss:  -5.3359 | Function Loss:  -3.4146\n",
            "Total loss:  -3.3659 | PDE Loss:  -5.3393 | Function Loss:  -3.4147\n",
            "Total loss:  -3.3665 | PDE Loss:  -5.3431 | Function Loss:  -3.4149\n",
            "Total loss:  -3.367 | PDE Loss:  -5.3451 | Function Loss:  -3.4153\n",
            "Total loss:  -3.3676 | PDE Loss:  -5.3472 | Function Loss:  -3.4157\n",
            "Total loss:  -3.3682 | PDE Loss:  -5.3485 | Function Loss:  -3.4162\n",
            "Total loss:  -3.3688 | PDE Loss:  -5.3498 | Function Loss:  -3.4167\n",
            "Total loss:  -3.3694 | PDE Loss:  -5.3514 | Function Loss:  -3.4172\n",
            "Total loss:  -3.37 | PDE Loss:  -5.3546 | Function Loss:  -3.4175\n",
            "Total loss:  -3.3704 | PDE Loss:  -5.3565 | Function Loss:  -3.4178\n",
            "Total loss:  -3.3707 | PDE Loss:  -5.3588 | Function Loss:  -3.4178\n",
            "Total loss:  -3.3709 | PDE Loss:  -5.3604 | Function Loss:  -3.4178\n",
            "Total loss:  -3.3709 | PDE Loss:  -5.3617 | Function Loss:  -3.4177\n",
            "Total loss:  -3.3711 | PDE Loss:  -5.3635 | Function Loss:  -3.4177\n",
            "Total loss:  -3.3713 | PDE Loss:  -5.3653 | Function Loss:  -3.4177\n",
            "Total loss:  -3.3714 | PDE Loss:  -5.3676 | Function Loss:  -3.4176\n",
            "Total loss:  -3.3718 | PDE Loss:  -5.3698 | Function Loss:  -3.4177\n",
            "Total loss:  -3.3719 | PDE Loss:  -5.3693 | Function Loss:  -3.418\n",
            "Total loss:  -3.3724 | PDE Loss:  -5.3681 | Function Loss:  -3.4186\n",
            "Total loss:  -3.3727 | PDE Loss:  -5.3653 | Function Loss:  -3.4193\n",
            "Total loss:  -3.373 | PDE Loss:  -5.3622 | Function Loss:  -3.42\n",
            "Total loss:  -3.3733 | PDE Loss:  -5.3609 | Function Loss:  -3.4205\n",
            "Total loss:  -3.3737 | PDE Loss:  -5.357 | Function Loss:  -3.4213\n",
            "Total loss:  -3.374 | PDE Loss:  -5.3578 | Function Loss:  -3.4216\n",
            "Total loss:  -3.3744 | PDE Loss:  -5.3596 | Function Loss:  -3.4218\n",
            "Total loss:  -3.3749 | PDE Loss:  -5.3619 | Function Loss:  -3.4221\n",
            "Total loss:  -3.3753 | PDE Loss:  -5.3652 | Function Loss:  -3.4222\n",
            "Total loss:  -3.3756 | PDE Loss:  -5.3689 | Function Loss:  -3.4221\n",
            "Total loss:  -3.3758 | PDE Loss:  -5.3719 | Function Loss:  -3.422\n",
            "Total loss:  -3.376 | PDE Loss:  -5.3743 | Function Loss:  -3.422\n",
            "Total loss:  -3.3764 | PDE Loss:  -5.3764 | Function Loss:  -3.4221\n",
            "Total loss:  -3.3768 | PDE Loss:  -5.3781 | Function Loss:  -3.4224\n",
            "Total loss:  -3.3772 | PDE Loss:  -5.3758 | Function Loss:  -3.4231\n",
            "Total loss:  -3.3776 | PDE Loss:  -5.3741 | Function Loss:  -3.4237\n",
            "Total loss:  -3.3778 | PDE Loss:  -5.3733 | Function Loss:  -3.424\n",
            "Total loss:  -3.3781 | PDE Loss:  -5.3701 | Function Loss:  -3.4248\n",
            "Total loss:  -3.3784 | PDE Loss:  -5.3675 | Function Loss:  -3.4254\n",
            "Total loss:  -3.3786 | PDE Loss:  -5.365 | Function Loss:  -3.4259\n",
            "Total loss:  -3.3789 | PDE Loss:  -5.3643 | Function Loss:  -3.4263\n",
            "Total loss:  -3.3792 | PDE Loss:  -5.3635 | Function Loss:  -3.4268\n",
            "Total loss:  -3.3797 | PDE Loss:  -5.3639 | Function Loss:  -3.4272\n",
            "Total loss:  -3.38 | PDE Loss:  -5.3624 | Function Loss:  -3.4278\n",
            "Total loss:  -3.3805 | PDE Loss:  -5.3642 | Function Loss:  -3.4281\n",
            "Total loss:  -3.3808 | PDE Loss:  -5.3659 | Function Loss:  -3.4283\n",
            "Total loss:  -3.3813 | PDE Loss:  -5.3687 | Function Loss:  -3.4285\n",
            "Total loss:  -3.3818 | PDE Loss:  -5.3703 | Function Loss:  -3.4288\n",
            "Total loss:  -3.3821 | PDE Loss:  -5.3722 | Function Loss:  -3.429\n",
            "Total loss:  -3.3823 | PDE Loss:  -5.3721 | Function Loss:  -3.4292\n",
            "Total loss:  -3.3826 | PDE Loss:  -5.3726 | Function Loss:  -3.4294\n",
            "Total loss:  -3.3828 | PDE Loss:  -5.3726 | Function Loss:  -3.4297\n",
            "Total loss:  -3.383 | PDE Loss:  -5.3738 | Function Loss:  -3.4298\n",
            "Total loss:  -3.3832 | PDE Loss:  -5.3741 | Function Loss:  -3.43\n",
            "Total loss:  -3.3835 | PDE Loss:  -5.375 | Function Loss:  -3.4302\n",
            "Total loss:  -3.3837 | PDE Loss:  -5.3753 | Function Loss:  -3.4304\n",
            "Total loss:  -3.3838 | PDE Loss:  -5.3763 | Function Loss:  -3.4304\n",
            "Total loss:  -3.384 | PDE Loss:  -5.3771 | Function Loss:  -3.4305\n",
            "Total loss:  -3.3836 | PDE Loss:  -5.3827 | Function Loss:  -3.4295\n",
            "Total loss:  -3.384 | PDE Loss:  -5.3793 | Function Loss:  -3.4303\n",
            "Total loss:  -3.3842 | PDE Loss:  -5.3798 | Function Loss:  -3.4304\n",
            "Total loss:  -3.3843 | PDE Loss:  -5.3801 | Function Loss:  -3.4305\n",
            "Total loss:  -3.3845 | PDE Loss:  -5.3803 | Function Loss:  -3.4307\n",
            "Total loss:  -3.3846 | PDE Loss:  -5.3793 | Function Loss:  -3.4309\n",
            "Total loss:  -3.3847 | PDE Loss:  -5.3789 | Function Loss:  -3.4311\n",
            "Total loss:  -3.3848 | PDE Loss:  -5.3778 | Function Loss:  -3.4313\n",
            "Total loss:  -3.3849 | PDE Loss:  -5.3768 | Function Loss:  -3.4315\n",
            "Total loss:  -3.385 | PDE Loss:  -5.3762 | Function Loss:  -3.4317\n",
            "Total loss:  -3.3851 | PDE Loss:  -5.3744 | Function Loss:  -3.4321\n",
            "Total loss:  -3.3852 | PDE Loss:  -5.3741 | Function Loss:  -3.4323\n",
            "Total loss:  -3.3854 | PDE Loss:  -5.3738 | Function Loss:  -3.4324\n",
            "Total loss:  -3.3856 | PDE Loss:  -5.3737 | Function Loss:  -3.4327\n",
            "Total loss:  -3.3858 | PDE Loss:  -5.3739 | Function Loss:  -3.4329\n",
            "Total loss:  -3.3859 | PDE Loss:  -5.3747 | Function Loss:  -3.433\n",
            "Total loss:  -3.3862 | PDE Loss:  -5.3751 | Function Loss:  -3.4332\n",
            "Total loss:  -3.3864 | PDE Loss:  -5.3756 | Function Loss:  -3.4334\n",
            "Total loss:  -3.3869 | PDE Loss:  -5.3791 | Function Loss:  -3.4335\n",
            "Total loss:  -3.3872 | PDE Loss:  -5.3798 | Function Loss:  -3.4338\n",
            "Total loss:  -3.3875 | PDE Loss:  -5.3805 | Function Loss:  -3.434\n",
            "Total loss:  -3.3878 | PDE Loss:  -5.3816 | Function Loss:  -3.4342\n",
            "Total loss:  -3.388 | PDE Loss:  -5.3819 | Function Loss:  -3.4344\n",
            "Total loss:  -3.3882 | PDE Loss:  -5.3825 | Function Loss:  -3.4345\n",
            "Total loss:  -3.3883 | PDE Loss:  -5.3826 | Function Loss:  -3.4347\n",
            "Total loss:  -3.3885 | PDE Loss:  -5.3831 | Function Loss:  -3.4349\n",
            "Total loss:  -3.3888 | PDE Loss:  -5.384 | Function Loss:  -3.4351\n",
            "Total loss:  -3.3891 | PDE Loss:  -5.3851 | Function Loss:  -3.4353\n",
            "Total loss:  -3.3893 | PDE Loss:  -5.3871 | Function Loss:  -3.4353\n",
            "Total loss:  -3.3894 | PDE Loss:  -5.3886 | Function Loss:  -3.4353\n",
            "Total loss:  -3.3895 | PDE Loss:  -5.3904 | Function Loss:  -3.4352\n",
            "Total loss:  -3.3897 | PDE Loss:  -5.3921 | Function Loss:  -3.4351\n",
            "Total loss:  -3.3897 | PDE Loss:  -5.394 | Function Loss:  -3.435\n",
            "Total loss:  -3.3898 | PDE Loss:  -5.3952 | Function Loss:  -3.435\n",
            "Total loss:  -3.3899 | PDE Loss:  -5.3977 | Function Loss:  -3.4348\n",
            "Total loss:  -3.39 | PDE Loss:  -5.399 | Function Loss:  -3.4348\n",
            "Total loss:  -3.3901 | PDE Loss:  -5.4002 | Function Loss:  -3.4348\n",
            "Total loss:  -3.3903 | PDE Loss:  -5.4014 | Function Loss:  -3.4348\n",
            "Total loss:  -3.3904 | PDE Loss:  -5.404 | Function Loss:  -3.4347\n",
            "Total loss:  -3.3906 | PDE Loss:  -5.4042 | Function Loss:  -3.4348\n",
            "Total loss:  -3.3907 | PDE Loss:  -5.4042 | Function Loss:  -3.435\n",
            "Total loss:  -3.3909 | PDE Loss:  -5.4047 | Function Loss:  -3.4352\n",
            "Total loss:  -3.3911 | PDE Loss:  -5.4054 | Function Loss:  -3.4353\n",
            "Total loss:  -3.3913 | PDE Loss:  -5.4065 | Function Loss:  -3.4354\n",
            "Total loss:  -3.3916 | PDE Loss:  -5.409 | Function Loss:  -3.4354\n",
            "Total loss:  -3.3919 | PDE Loss:  -5.4117 | Function Loss:  -3.4355\n",
            "Total loss:  -3.3922 | PDE Loss:  -5.4141 | Function Loss:  -3.4355\n",
            "Total loss:  -3.3925 | PDE Loss:  -5.4179 | Function Loss:  -3.4355\n",
            "Total loss:  -3.3927 | PDE Loss:  -5.4203 | Function Loss:  -3.4355\n",
            "Total loss:  -3.3929 | PDE Loss:  -5.4208 | Function Loss:  -3.4357\n",
            "Total loss:  -3.3932 | PDE Loss:  -5.4229 | Function Loss:  -3.4358\n",
            "Total loss:  -3.3933 | PDE Loss:  -5.4209 | Function Loss:  -3.4361\n",
            "Total loss:  -3.3935 | PDE Loss:  -5.4211 | Function Loss:  -3.4363\n",
            "Total loss:  -3.3936 | PDE Loss:  -5.4218 | Function Loss:  -3.4364\n",
            "Total loss:  -3.3938 | PDE Loss:  -5.4212 | Function Loss:  -3.4367\n",
            "Total loss:  -3.394 | PDE Loss:  -5.4213 | Function Loss:  -3.4368\n",
            "Total loss:  -3.3942 | PDE Loss:  -5.4209 | Function Loss:  -3.4371\n",
            "Total loss:  -3.3943 | PDE Loss:  -5.4197 | Function Loss:  -3.4373\n",
            "Total loss:  -3.3944 | PDE Loss:  -5.4205 | Function Loss:  -3.4373\n",
            "Total loss:  -3.3944 | PDE Loss:  -5.4205 | Function Loss:  -3.4374\n",
            "Total loss:  -3.3945 | PDE Loss:  -5.4205 | Function Loss:  -3.4375\n",
            "Total loss:  -3.3946 | PDE Loss:  -5.4209 | Function Loss:  -3.4376\n",
            "Total loss:  -3.3947 | PDE Loss:  -5.4208 | Function Loss:  -3.4376\n",
            "Total loss:  -3.3948 | PDE Loss:  -5.421 | Function Loss:  -3.4377\n",
            "Total loss:  -3.3948 | PDE Loss:  -5.4207 | Function Loss:  -3.4378\n",
            "Total loss:  -3.3949 | PDE Loss:  -5.4206 | Function Loss:  -3.4379\n",
            "Total loss:  -3.3949 | PDE Loss:  -5.4204 | Function Loss:  -3.438\n",
            "Total loss:  -3.395 | PDE Loss:  -5.4199 | Function Loss:  -3.4381\n",
            "Total loss:  -3.3951 | PDE Loss:  -5.4192 | Function Loss:  -3.4382\n",
            "Total loss:  -3.3951 | PDE Loss:  -5.4188 | Function Loss:  -3.4383\n",
            "Total loss:  -3.395 | PDE Loss:  -5.4139 | Function Loss:  -3.4387\n",
            "Total loss:  -3.3951 | PDE Loss:  -5.4177 | Function Loss:  -3.4385\n",
            "Total loss:  -3.3952 | PDE Loss:  -5.4171 | Function Loss:  -3.4386\n",
            "Total loss:  -3.3953 | PDE Loss:  -5.4168 | Function Loss:  -3.4387\n",
            "Total loss:  -3.3953 | PDE Loss:  -5.4158 | Function Loss:  -3.4389\n",
            "Total loss:  -3.3954 | PDE Loss:  -5.4157 | Function Loss:  -3.439\n",
            "Total loss:  -3.3955 | PDE Loss:  -5.4153 | Function Loss:  -3.4391\n",
            "Total loss:  -3.3956 | PDE Loss:  -5.415 | Function Loss:  -3.4392\n",
            "Total loss:  -3.3957 | PDE Loss:  -5.4144 | Function Loss:  -3.4394\n",
            "Total loss:  -3.3958 | PDE Loss:  -5.4143 | Function Loss:  -3.4395\n",
            "Total loss:  -3.3959 | PDE Loss:  -5.414 | Function Loss:  -3.4397\n",
            "Total loss:  -3.396 | PDE Loss:  -5.4134 | Function Loss:  -3.4399\n",
            "Total loss:  -3.3962 | PDE Loss:  -5.4138 | Function Loss:  -3.44\n",
            "Total loss:  -3.3963 | PDE Loss:  -5.4139 | Function Loss:  -3.4402\n",
            "Total loss:  -3.3965 | PDE Loss:  -5.4146 | Function Loss:  -3.4403\n",
            "Total loss:  -3.3967 | PDE Loss:  -5.4153 | Function Loss:  -3.4405\n",
            "Total loss:  -3.3969 | PDE Loss:  -5.4158 | Function Loss:  -3.4406\n",
            "Total loss:  -3.397 | PDE Loss:  -5.4164 | Function Loss:  -3.4406\n",
            "Total loss:  -3.3971 | PDE Loss:  -5.4167 | Function Loss:  -3.4408\n",
            "Total loss:  -3.3973 | PDE Loss:  -5.4154 | Function Loss:  -3.4411\n",
            "Total loss:  -3.3974 | PDE Loss:  -5.4155 | Function Loss:  -3.4412\n",
            "Total loss:  -3.3977 | PDE Loss:  -5.415 | Function Loss:  -3.4415\n",
            "Total loss:  -3.3977 | PDE Loss:  -5.4116 | Function Loss:  -3.4419\n",
            "Total loss:  -3.3981 | PDE Loss:  -5.4116 | Function Loss:  -3.4424\n",
            "Total loss:  -3.3983 | PDE Loss:  -5.4115 | Function Loss:  -3.4426\n",
            "Total loss:  -3.3985 | PDE Loss:  -5.4106 | Function Loss:  -3.4429\n",
            "Total loss:  -3.3988 | PDE Loss:  -5.4091 | Function Loss:  -3.4434\n",
            "Total loss:  -3.399 | PDE Loss:  -5.4091 | Function Loss:  -3.4437\n",
            "Total loss:  -3.3992 | PDE Loss:  -5.4081 | Function Loss:  -3.444\n",
            "Total loss:  -3.3994 | PDE Loss:  -5.407 | Function Loss:  -3.4443\n",
            "Total loss:  -3.3996 | PDE Loss:  -5.4064 | Function Loss:  -3.4446\n",
            "Total loss:  -3.3997 | PDE Loss:  -5.4059 | Function Loss:  -3.4448\n",
            "Total loss:  -3.3998 | PDE Loss:  -5.4056 | Function Loss:  -3.4449\n",
            "Total loss:  -3.3999 | PDE Loss:  -5.4053 | Function Loss:  -3.445\n",
            "Total loss:  -3.3999 | PDE Loss:  -5.4047 | Function Loss:  -3.4452\n",
            "Total loss:  -3.4 | PDE Loss:  -5.4048 | Function Loss:  -3.4453\n",
            "Total loss:  -3.4001 | PDE Loss:  -5.4021 | Function Loss:  -3.4457\n",
            "Total loss:  -3.4 | PDE Loss:  -5.4008 | Function Loss:  -3.4457\n",
            "Total loss:  -3.4002 | PDE Loss:  -5.402 | Function Loss:  -3.4458\n",
            "Total loss:  -3.4003 | PDE Loss:  -5.4022 | Function Loss:  -3.4459\n",
            "Total loss:  -3.4004 | PDE Loss:  -5.4012 | Function Loss:  -3.4461\n",
            "Total loss:  -3.4005 | PDE Loss:  -5.4013 | Function Loss:  -3.4462\n",
            "Total loss:  -3.4006 | PDE Loss:  -5.4009 | Function Loss:  -3.4464\n",
            "Total loss:  -3.4007 | PDE Loss:  -5.4005 | Function Loss:  -3.4465\n",
            "Total loss:  -3.4009 | PDE Loss:  -5.4002 | Function Loss:  -3.4467\n",
            "Total loss:  -3.4009 | PDE Loss:  -5.3997 | Function Loss:  -3.4468\n",
            "Total loss:  -3.401 | PDE Loss:  -5.3995 | Function Loss:  -3.447\n",
            "Total loss:  -3.4011 | PDE Loss:  -5.3995 | Function Loss:  -3.4471\n",
            "Total loss:  -3.4012 | PDE Loss:  -5.3994 | Function Loss:  -3.4471\n",
            "Total loss:  -3.4013 | PDE Loss:  -5.3993 | Function Loss:  -3.4473\n",
            "Total loss:  -3.4015 | PDE Loss:  -5.4023 | Function Loss:  -3.4472\n",
            "Total loss:  -3.4016 | PDE Loss:  -5.4018 | Function Loss:  -3.4474\n",
            "Total loss:  -3.4018 | PDE Loss:  -5.4014 | Function Loss:  -3.4476\n",
            "Total loss:  -3.4019 | PDE Loss:  -5.4019 | Function Loss:  -3.4477\n",
            "Total loss:  -3.4021 | PDE Loss:  -5.4024 | Function Loss:  -3.4478\n",
            "Total loss:  -3.4022 | PDE Loss:  -5.403 | Function Loss:  -3.4479\n",
            "Total loss:  -3.4023 | PDE Loss:  -5.4044 | Function Loss:  -3.4478\n",
            "Total loss:  -3.4025 | PDE Loss:  -5.406 | Function Loss:  -3.4478\n",
            "Total loss:  -3.4026 | PDE Loss:  -5.4073 | Function Loss:  -3.4478\n",
            "Total loss:  -3.4029 | PDE Loss:  -5.4098 | Function Loss:  -3.4479\n",
            "Total loss:  -3.4031 | PDE Loss:  -5.4113 | Function Loss:  -3.4479\n",
            "Total loss:  -3.4032 | PDE Loss:  -5.4117 | Function Loss:  -3.448\n",
            "Total loss:  -3.4033 | PDE Loss:  -5.4112 | Function Loss:  -3.4482\n",
            "Total loss:  -3.4034 | PDE Loss:  -5.4111 | Function Loss:  -3.4483\n",
            "Total loss:  -3.4035 | PDE Loss:  -5.4102 | Function Loss:  -3.4485\n",
            "Total loss:  -3.4037 | PDE Loss:  -5.4086 | Function Loss:  -3.4489\n",
            "Total loss:  -3.4038 | PDE Loss:  -5.4071 | Function Loss:  -3.4491\n",
            "Total loss:  -3.4039 | PDE Loss:  -5.4059 | Function Loss:  -3.4494\n",
            "Total loss:  -3.404 | PDE Loss:  -5.4057 | Function Loss:  -3.4496\n",
            "Total loss:  -3.4041 | PDE Loss:  -5.4057 | Function Loss:  -3.4497\n",
            "Total loss:  -3.4041 | PDE Loss:  -5.4062 | Function Loss:  -3.4497\n",
            "Total loss:  -3.4042 | PDE Loss:  -5.4069 | Function Loss:  -3.4497\n",
            "Total loss:  -3.4043 | PDE Loss:  -5.4081 | Function Loss:  -3.4496\n",
            "Total loss:  -3.4043 | PDE Loss:  -5.409 | Function Loss:  -3.4496\n",
            "Total loss:  -3.4044 | PDE Loss:  -5.4101 | Function Loss:  -3.4495\n",
            "Total loss:  -3.4045 | PDE Loss:  -5.4109 | Function Loss:  -3.4495\n",
            "Total loss:  -3.4046 | PDE Loss:  -5.412 | Function Loss:  -3.4495\n",
            "Total loss:  -3.4047 | PDE Loss:  -5.412 | Function Loss:  -3.4496\n",
            "Total loss:  -3.4048 | PDE Loss:  -5.4118 | Function Loss:  -3.4498\n",
            "Total loss:  -3.405 | PDE Loss:  -5.412 | Function Loss:  -3.4499\n",
            "Total loss:  -3.4052 | PDE Loss:  -5.4116 | Function Loss:  -3.4502\n",
            "Total loss:  -3.4055 | PDE Loss:  -5.4117 | Function Loss:  -3.4505\n",
            "Total loss:  -3.4058 | PDE Loss:  -5.4112 | Function Loss:  -3.451\n",
            "Total loss:  -3.406 | PDE Loss:  -5.4119 | Function Loss:  -3.4511\n",
            "Total loss:  -3.4062 | PDE Loss:  -5.4101 | Function Loss:  -3.4515\n",
            "Total loss:  -3.4062 | PDE Loss:  -5.4101 | Function Loss:  -3.4516\n",
            "Total loss:  -3.4064 | PDE Loss:  -5.4112 | Function Loss:  -3.4516\n",
            "Total loss:  -3.4065 | PDE Loss:  -5.4121 | Function Loss:  -3.4517\n",
            "Total loss:  -3.4067 | PDE Loss:  -5.4124 | Function Loss:  -3.4518\n",
            "Total loss:  -3.4067 | PDE Loss:  -5.4125 | Function Loss:  -3.4519\n",
            "Total loss:  -3.4068 | PDE Loss:  -5.4123 | Function Loss:  -3.452\n",
            "Total loss:  -3.407 | PDE Loss:  -5.411 | Function Loss:  -3.4523\n",
            "Total loss:  -3.4071 | PDE Loss:  -5.4101 | Function Loss:  -3.4525\n",
            "Total loss:  -3.4072 | PDE Loss:  -5.4089 | Function Loss:  -3.4528\n",
            "Total loss:  -3.4073 | PDE Loss:  -5.4075 | Function Loss:  -3.453\n",
            "Total loss:  -3.4074 | PDE Loss:  -5.4065 | Function Loss:  -3.4533\n",
            "Total loss:  -3.4074 | PDE Loss:  -5.406 | Function Loss:  -3.4534\n",
            "Total loss:  -3.4075 | PDE Loss:  -5.4056 | Function Loss:  -3.4535\n",
            "Total loss:  -3.4076 | PDE Loss:  -5.406 | Function Loss:  -3.4535\n",
            "Total loss:  -3.4075 | PDE Loss:  -5.4014 | Function Loss:  -3.454\n",
            "Total loss:  -3.4076 | PDE Loss:  -5.4044 | Function Loss:  -3.4538\n",
            "Total loss:  -3.4077 | PDE Loss:  -5.4057 | Function Loss:  -3.4537\n",
            "Total loss:  -3.4078 | PDE Loss:  -5.4069 | Function Loss:  -3.4537\n",
            "Total loss:  -3.4079 | PDE Loss:  -5.4085 | Function Loss:  -3.4536\n",
            "Total loss:  -3.408 | PDE Loss:  -5.41 | Function Loss:  -3.4536\n",
            "Total loss:  -3.4081 | PDE Loss:  -5.4111 | Function Loss:  -3.4535\n",
            "Total loss:  -3.4082 | PDE Loss:  -5.4124 | Function Loss:  -3.4535\n",
            "Total loss:  -3.4083 | PDE Loss:  -5.4131 | Function Loss:  -3.4535\n",
            "Total loss:  -3.4084 | PDE Loss:  -5.4143 | Function Loss:  -3.4535\n",
            "Total loss:  -3.4086 | PDE Loss:  -5.4149 | Function Loss:  -3.4536\n",
            "Total loss:  -3.4087 | PDE Loss:  -5.4149 | Function Loss:  -3.4538\n",
            "Total loss:  -3.4089 | PDE Loss:  -5.4153 | Function Loss:  -3.4539\n",
            "Total loss:  -3.409 | PDE Loss:  -5.4158 | Function Loss:  -3.4541\n",
            "Total loss:  -3.4091 | PDE Loss:  -5.4157 | Function Loss:  -3.4542\n",
            "Total loss:  -3.4093 | PDE Loss:  -5.4158 | Function Loss:  -3.4543\n",
            "Total loss:  -3.4094 | PDE Loss:  -5.4168 | Function Loss:  -3.4544\n",
            "Total loss:  -3.4095 | PDE Loss:  -5.4175 | Function Loss:  -3.4544\n",
            "Total loss:  -3.4097 | PDE Loss:  -5.419 | Function Loss:  -3.4544\n",
            "Total loss:  -3.4098 | PDE Loss:  -5.4211 | Function Loss:  -3.4544\n",
            "Total loss:  -3.41 | PDE Loss:  -5.4235 | Function Loss:  -3.4543\n",
            "Total loss:  -3.4102 | PDE Loss:  -5.4249 | Function Loss:  -3.4543\n",
            "Total loss:  -3.4104 | PDE Loss:  -5.4264 | Function Loss:  -3.4544\n",
            "Total loss:  -3.4106 | PDE Loss:  -5.4271 | Function Loss:  -3.4545\n",
            "Total loss:  -3.4104 | PDE Loss:  -5.4241 | Function Loss:  -3.4547\n",
            "Total loss:  -3.4107 | PDE Loss:  -5.4268 | Function Loss:  -3.4547\n",
            "Total loss:  -3.4108 | PDE Loss:  -5.4264 | Function Loss:  -3.4549\n",
            "Total loss:  -3.411 | PDE Loss:  -5.4259 | Function Loss:  -3.4551\n",
            "Total loss:  -3.411 | PDE Loss:  -5.4254 | Function Loss:  -3.4552\n",
            "Total loss:  -3.4112 | PDE Loss:  -5.4246 | Function Loss:  -3.4555\n",
            "Total loss:  -3.4113 | PDE Loss:  -5.424 | Function Loss:  -3.4557\n",
            "Total loss:  -3.4115 | PDE Loss:  -5.4233 | Function Loss:  -3.456\n",
            "Total loss:  -3.4118 | PDE Loss:  -5.4228 | Function Loss:  -3.4564\n",
            "Total loss:  -3.4122 | PDE Loss:  -5.4228 | Function Loss:  -3.4568\n",
            "Total loss:  -3.4124 | PDE Loss:  -5.4234 | Function Loss:  -3.457\n",
            "Total loss:  -3.4128 | PDE Loss:  -5.4251 | Function Loss:  -3.4572\n",
            "Total loss:  -3.4131 | PDE Loss:  -5.4271 | Function Loss:  -3.4574\n",
            "Total loss:  -3.4133 | PDE Loss:  -5.4282 | Function Loss:  -3.4575\n",
            "Total loss:  -3.4135 | PDE Loss:  -5.4292 | Function Loss:  -3.4575\n",
            "Total loss:  -3.4137 | PDE Loss:  -5.4298 | Function Loss:  -3.4577\n",
            "Total loss:  -3.4139 | PDE Loss:  -5.4308 | Function Loss:  -3.4579\n",
            "Total loss:  -3.4141 | PDE Loss:  -5.4317 | Function Loss:  -3.458\n",
            "Total loss:  -3.4143 | PDE Loss:  -5.4319 | Function Loss:  -3.4581\n",
            "Total loss:  -3.4144 | PDE Loss:  -5.4318 | Function Loss:  -3.4583\n",
            "Total loss:  -3.4145 | PDE Loss:  -5.4318 | Function Loss:  -3.4584\n",
            "Total loss:  -3.4146 | PDE Loss:  -5.4317 | Function Loss:  -3.4585\n",
            "Total loss:  -3.4146 | PDE Loss:  -5.4309 | Function Loss:  -3.4586\n",
            "Total loss:  -3.4147 | PDE Loss:  -5.4313 | Function Loss:  -3.4587\n",
            "Total loss:  -3.4147 | PDE Loss:  -5.4293 | Function Loss:  -3.4589\n",
            "Total loss:  -3.4149 | PDE Loss:  -5.4314 | Function Loss:  -3.4589\n",
            "Total loss:  -3.4151 | PDE Loss:  -5.4328 | Function Loss:  -3.4589\n",
            "Total loss:  -3.4153 | PDE Loss:  -5.4344 | Function Loss:  -3.459\n",
            "Total loss:  -3.4154 | PDE Loss:  -5.4355 | Function Loss:  -3.459\n",
            "Total loss:  -3.4156 | PDE Loss:  -5.4361 | Function Loss:  -3.4591\n",
            "Total loss:  -3.4157 | PDE Loss:  -5.4363 | Function Loss:  -3.4592\n",
            "Total loss:  -3.4158 | PDE Loss:  -5.4363 | Function Loss:  -3.4593\n",
            "Total loss:  -3.4159 | PDE Loss:  -5.436 | Function Loss:  -3.4594\n",
            "Total loss:  -3.416 | PDE Loss:  -5.4359 | Function Loss:  -3.4596\n",
            "Total loss:  -3.4162 | PDE Loss:  -5.4346 | Function Loss:  -3.4599\n",
            "Total loss:  -3.4163 | PDE Loss:  -5.4337 | Function Loss:  -3.4602\n",
            "Total loss:  -3.4164 | PDE Loss:  -5.4324 | Function Loss:  -3.4604\n",
            "Total loss:  -3.4166 | PDE Loss:  -5.4319 | Function Loss:  -3.4607\n",
            "Total loss:  -3.4168 | PDE Loss:  -5.4321 | Function Loss:  -3.4608\n",
            "Total loss:  -3.417 | PDE Loss:  -5.4322 | Function Loss:  -3.4611\n",
            "Total loss:  -3.4172 | PDE Loss:  -5.4321 | Function Loss:  -3.4613\n",
            "Total loss:  -3.4174 | PDE Loss:  -5.4322 | Function Loss:  -3.4615\n",
            "Total loss:  -3.4176 | PDE Loss:  -5.4323 | Function Loss:  -3.4617\n",
            "Total loss:  -3.4177 | PDE Loss:  -5.432 | Function Loss:  -3.4619\n",
            "Total loss:  -3.4179 | PDE Loss:  -5.4323 | Function Loss:  -3.4621\n",
            "Total loss:  -3.4181 | PDE Loss:  -5.4322 | Function Loss:  -3.4623\n",
            "Total loss:  -3.4183 | PDE Loss:  -5.4324 | Function Loss:  -3.4625\n",
            "Total loss:  -3.4185 | PDE Loss:  -5.4317 | Function Loss:  -3.4628\n",
            "Total loss:  -3.4187 | PDE Loss:  -5.4316 | Function Loss:  -3.463\n",
            "Total loss:  -3.4189 | PDE Loss:  -5.4317 | Function Loss:  -3.4632\n",
            "Total loss:  -3.4192 | PDE Loss:  -5.4327 | Function Loss:  -3.4634\n",
            "Total loss:  -3.4194 | PDE Loss:  -5.4321 | Function Loss:  -3.4638\n",
            "Total loss:  -3.4196 | PDE Loss:  -5.4332 | Function Loss:  -3.4639\n",
            "Total loss:  -3.4198 | PDE Loss:  -5.4343 | Function Loss:  -3.464\n",
            "Total loss:  -3.4201 | PDE Loss:  -5.4351 | Function Loss:  -3.4643\n",
            "Total loss:  -3.4203 | PDE Loss:  -5.4357 | Function Loss:  -3.4644\n",
            "Total loss:  -3.4205 | PDE Loss:  -5.4355 | Function Loss:  -3.4646\n",
            "Total loss:  -3.4207 | PDE Loss:  -5.4348 | Function Loss:  -3.4649\n",
            "Total loss:  -3.4208 | PDE Loss:  -5.4348 | Function Loss:  -3.4651\n",
            "Total loss:  -3.421 | PDE Loss:  -5.4326 | Function Loss:  -3.4654\n",
            "Total loss:  -3.4211 | PDE Loss:  -5.4328 | Function Loss:  -3.4656\n",
            "Total loss:  -3.4212 | PDE Loss:  -5.4315 | Function Loss:  -3.4658\n",
            "Total loss:  -3.4213 | PDE Loss:  -5.4317 | Function Loss:  -3.4659\n",
            "Total loss:  -3.4213 | PDE Loss:  -5.4319 | Function Loss:  -3.4659\n",
            "Total loss:  -3.4214 | PDE Loss:  -5.4314 | Function Loss:  -3.466\n",
            "Total loss:  -3.4215 | PDE Loss:  -5.4321 | Function Loss:  -3.466\n",
            "Total loss:  -3.4215 | PDE Loss:  -5.4325 | Function Loss:  -3.466\n",
            "Total loss:  -3.4216 | PDE Loss:  -5.4333 | Function Loss:  -3.4661\n",
            "Total loss:  -3.4216 | PDE Loss:  -5.4339 | Function Loss:  -3.466\n",
            "Total loss:  -3.4216 | PDE Loss:  -5.4351 | Function Loss:  -3.4659\n",
            "Total loss:  -3.4218 | PDE Loss:  -5.4355 | Function Loss:  -3.4661\n",
            "Total loss:  -3.4219 | PDE Loss:  -5.4354 | Function Loss:  -3.4662\n",
            "Total loss:  -3.422 | PDE Loss:  -5.4352 | Function Loss:  -3.4663\n",
            "Total loss:  -3.4221 | PDE Loss:  -5.4349 | Function Loss:  -3.4665\n",
            "Total loss:  -3.4222 | PDE Loss:  -5.4345 | Function Loss:  -3.4666\n",
            "Total loss:  -3.4223 | PDE Loss:  -5.434 | Function Loss:  -3.4668\n",
            "Total loss:  -3.4226 | PDE Loss:  -5.4352 | Function Loss:  -3.4669\n",
            "Total loss:  -3.4228 | PDE Loss:  -5.4349 | Function Loss:  -3.4672\n",
            "Total loss:  -3.4229 | PDE Loss:  -5.4345 | Function Loss:  -3.4674\n",
            "Total loss:  -3.4231 | PDE Loss:  -5.4352 | Function Loss:  -3.4675\n",
            "Total loss:  -3.4232 | PDE Loss:  -5.4352 | Function Loss:  -3.4676\n",
            "Total loss:  -3.4233 | PDE Loss:  -5.4357 | Function Loss:  -3.4677\n",
            "Total loss:  -3.4234 | PDE Loss:  -5.436 | Function Loss:  -3.4677\n",
            "Total loss:  -3.4235 | PDE Loss:  -5.4367 | Function Loss:  -3.4678\n",
            "Total loss:  -3.4237 | PDE Loss:  -5.436 | Function Loss:  -3.4681\n",
            "Total loss:  -3.4238 | PDE Loss:  -5.4356 | Function Loss:  -3.4683\n",
            "Total loss:  -3.424 | PDE Loss:  -5.4357 | Function Loss:  -3.4685\n",
            "Total loss:  -3.4241 | PDE Loss:  -5.435 | Function Loss:  -3.4686\n",
            "Total loss:  -3.4242 | PDE Loss:  -5.4347 | Function Loss:  -3.4688\n",
            "Total loss:  -3.4243 | PDE Loss:  -5.4343 | Function Loss:  -3.469\n",
            "Total loss:  -3.4244 | PDE Loss:  -5.4337 | Function Loss:  -3.4691\n",
            "Total loss:  -3.4245 | PDE Loss:  -5.4339 | Function Loss:  -3.4692\n",
            "Total loss:  -3.4245 | PDE Loss:  -5.4336 | Function Loss:  -3.4693\n",
            "Total loss:  -3.4246 | PDE Loss:  -5.4341 | Function Loss:  -3.4693\n",
            "Total loss:  -3.4247 | PDE Loss:  -5.4343 | Function Loss:  -3.4694\n",
            "Total loss:  -3.4248 | PDE Loss:  -5.4355 | Function Loss:  -3.4693\n",
            "Total loss:  -3.4248 | PDE Loss:  -5.4356 | Function Loss:  -3.4694\n",
            "Total loss:  -3.4249 | PDE Loss:  -5.4364 | Function Loss:  -3.4694\n",
            "Total loss:  -3.4249 | PDE Loss:  -5.4365 | Function Loss:  -3.4694\n",
            "Total loss:  -3.425 | PDE Loss:  -5.4361 | Function Loss:  -3.4696\n",
            "Total loss:  -3.4251 | PDE Loss:  -5.4366 | Function Loss:  -3.4696\n",
            "Total loss:  -3.4251 | PDE Loss:  -5.4363 | Function Loss:  -3.4697\n",
            "Total loss:  -3.4252 | PDE Loss:  -5.4359 | Function Loss:  -3.4698\n",
            "Total loss:  -3.4253 | PDE Loss:  -5.4354 | Function Loss:  -3.47\n",
            "Total loss:  -3.4254 | PDE Loss:  -5.4355 | Function Loss:  -3.4701\n",
            "Total loss:  -3.4255 | PDE Loss:  -5.4352 | Function Loss:  -3.4702\n",
            "Total loss:  -3.4256 | PDE Loss:  -5.4362 | Function Loss:  -3.4702\n",
            "Total loss:  -3.4256 | PDE Loss:  -5.4364 | Function Loss:  -3.4702\n",
            "Total loss:  -3.4258 | PDE Loss:  -5.4382 | Function Loss:  -3.4702\n",
            "Total loss:  -3.4259 | PDE Loss:  -5.4393 | Function Loss:  -3.4702\n",
            "Total loss:  -3.4261 | PDE Loss:  -5.4411 | Function Loss:  -3.4702\n",
            "Total loss:  -3.4263 | PDE Loss:  -5.4447 | Function Loss:  -3.4701\n",
            "Total loss:  -3.4264 | PDE Loss:  -5.4456 | Function Loss:  -3.4701\n",
            "Total loss:  -3.4266 | PDE Loss:  -5.4474 | Function Loss:  -3.4701\n",
            "Total loss:  -3.4268 | PDE Loss:  -5.4485 | Function Loss:  -3.4702\n",
            "Total loss:  -3.427 | PDE Loss:  -5.45 | Function Loss:  -3.4703\n",
            "Total loss:  -3.4272 | PDE Loss:  -5.4509 | Function Loss:  -3.4704\n",
            "Total loss:  -3.4275 | PDE Loss:  -5.4528 | Function Loss:  -3.4705\n",
            "Total loss:  -3.4277 | PDE Loss:  -5.4537 | Function Loss:  -3.4707\n",
            "Total loss:  -3.4278 | PDE Loss:  -5.455 | Function Loss:  -3.4707\n",
            "Total loss:  -3.428 | PDE Loss:  -5.4562 | Function Loss:  -3.4708\n",
            "Total loss:  -3.4282 | PDE Loss:  -5.4579 | Function Loss:  -3.4708\n",
            "Total loss:  -3.4284 | PDE Loss:  -5.4599 | Function Loss:  -3.4708\n",
            "Total loss:  -3.4286 | PDE Loss:  -5.4612 | Function Loss:  -3.4709\n",
            "Total loss:  -3.4288 | PDE Loss:  -5.463 | Function Loss:  -3.4709\n",
            "Total loss:  -3.429 | PDE Loss:  -5.4649 | Function Loss:  -3.471\n",
            "Total loss:  -3.4292 | PDE Loss:  -5.4655 | Function Loss:  -3.4712\n",
            "Total loss:  -3.4294 | PDE Loss:  -5.4663 | Function Loss:  -3.4713\n",
            "Total loss:  -3.4296 | PDE Loss:  -5.4663 | Function Loss:  -3.4715\n",
            "Total loss:  -3.4298 | PDE Loss:  -5.4657 | Function Loss:  -3.4717\n",
            "Total loss:  -3.4299 | PDE Loss:  -5.4649 | Function Loss:  -3.4719\n",
            "Total loss:  -3.43 | PDE Loss:  -5.4647 | Function Loss:  -3.472\n",
            "Total loss:  -3.4301 | PDE Loss:  -5.4632 | Function Loss:  -3.4723\n",
            "Total loss:  -3.4301 | PDE Loss:  -5.4633 | Function Loss:  -3.4724\n",
            "Total loss:  -3.4302 | PDE Loss:  -5.4628 | Function Loss:  -3.4725\n",
            "Total loss:  -3.4303 | PDE Loss:  -5.463 | Function Loss:  -3.4726\n",
            "Total loss:  -3.4305 | PDE Loss:  -5.4641 | Function Loss:  -3.4726\n",
            "Total loss:  -3.4305 | PDE Loss:  -5.4645 | Function Loss:  -3.4727\n",
            "Total loss:  -3.4306 | PDE Loss:  -5.4657 | Function Loss:  -3.4726\n",
            "Total loss:  -3.4307 | PDE Loss:  -5.4667 | Function Loss:  -3.4726\n",
            "Total loss:  -3.4308 | PDE Loss:  -5.468 | Function Loss:  -3.4726\n",
            "Total loss:  -3.4309 | PDE Loss:  -5.4689 | Function Loss:  -3.4726\n",
            "Total loss:  -3.4309 | PDE Loss:  -5.4697 | Function Loss:  -3.4726\n",
            "Total loss:  -3.4311 | PDE Loss:  -5.4709 | Function Loss:  -3.4726\n",
            "Total loss:  -3.4313 | PDE Loss:  -5.4726 | Function Loss:  -3.4727\n",
            "Total loss:  -3.4314 | PDE Loss:  -5.4735 | Function Loss:  -3.4727\n",
            "Total loss:  -3.4316 | PDE Loss:  -5.4747 | Function Loss:  -3.4729\n",
            "Total loss:  -3.4319 | PDE Loss:  -5.4757 | Function Loss:  -3.4731\n",
            "Total loss:  -3.4321 | PDE Loss:  -5.4765 | Function Loss:  -3.4732\n",
            "Total loss:  -3.4323 | PDE Loss:  -5.4781 | Function Loss:  -3.4733\n",
            "Total loss:  -3.4325 | PDE Loss:  -5.4798 | Function Loss:  -3.4733\n",
            "Total loss:  -3.4326 | PDE Loss:  -5.4812 | Function Loss:  -3.4733\n",
            "Total loss:  -3.4328 | PDE Loss:  -5.4828 | Function Loss:  -3.4733\n",
            "Total loss:  -3.4329 | PDE Loss:  -5.4842 | Function Loss:  -3.4734\n",
            "Total loss:  -3.4331 | PDE Loss:  -5.4864 | Function Loss:  -3.4734\n",
            "Total loss:  -3.4332 | PDE Loss:  -5.4831 | Function Loss:  -3.4738\n",
            "Total loss:  -3.4335 | PDE Loss:  -5.4851 | Function Loss:  -3.4739\n",
            "Total loss:  -3.4338 | PDE Loss:  -5.486 | Function Loss:  -3.4741\n",
            "Total loss:  -3.434 | PDE Loss:  -5.4849 | Function Loss:  -3.4745\n",
            "Total loss:  -3.4343 | PDE Loss:  -5.4834 | Function Loss:  -3.475\n",
            "Total loss:  -3.4346 | PDE Loss:  -5.4804 | Function Loss:  -3.4755\n",
            "Total loss:  -3.4348 | PDE Loss:  -5.4774 | Function Loss:  -3.4761\n",
            "Total loss:  -3.4351 | PDE Loss:  -5.4751 | Function Loss:  -3.4766\n",
            "Total loss:  -3.4353 | PDE Loss:  -5.472 | Function Loss:  -3.4772\n",
            "Total loss:  -3.4356 | PDE Loss:  -5.4713 | Function Loss:  -3.4775\n",
            "Total loss:  -3.4359 | PDE Loss:  -5.4716 | Function Loss:  -3.4779\n",
            "Total loss:  -3.4362 | PDE Loss:  -5.4739 | Function Loss:  -3.4779\n",
            "Total loss:  -3.4365 | PDE Loss:  -5.477 | Function Loss:  -3.478\n",
            "Total loss:  -3.4367 | PDE Loss:  -5.4807 | Function Loss:  -3.4778\n",
            "Total loss:  -3.4369 | PDE Loss:  -5.4846 | Function Loss:  -3.4777\n",
            "Total loss:  -3.4372 | PDE Loss:  -5.4887 | Function Loss:  -3.4776\n",
            "Total loss:  -3.4373 | PDE Loss:  -5.4911 | Function Loss:  -3.4775\n",
            "Total loss:  -3.4375 | PDE Loss:  -5.4935 | Function Loss:  -3.4775\n",
            "Total loss:  -3.4377 | PDE Loss:  -5.4935 | Function Loss:  -3.4776\n",
            "Total loss:  -3.4378 | PDE Loss:  -5.4938 | Function Loss:  -3.4778\n",
            "Total loss:  -3.4381 | PDE Loss:  -5.4923 | Function Loss:  -3.4782\n",
            "Total loss:  -3.4383 | PDE Loss:  -5.4919 | Function Loss:  -3.4785\n",
            "Total loss:  -3.4385 | PDE Loss:  -5.4906 | Function Loss:  -3.4788\n",
            "Total loss:  -3.4387 | PDE Loss:  -5.4897 | Function Loss:  -3.4791\n",
            "Total loss:  -3.4389 | PDE Loss:  -5.4895 | Function Loss:  -3.4793\n",
            "Total loss:  -3.439 | PDE Loss:  -5.4897 | Function Loss:  -3.4795\n",
            "Total loss:  -3.4392 | PDE Loss:  -5.4882 | Function Loss:  -3.4798\n",
            "Total loss:  -3.4393 | PDE Loss:  -5.4916 | Function Loss:  -3.4797\n",
            "Total loss:  -3.4394 | PDE Loss:  -5.4924 | Function Loss:  -3.4797\n",
            "Total loss:  -3.4397 | PDE Loss:  -5.4937 | Function Loss:  -3.4798\n",
            "Total loss:  -3.4399 | PDE Loss:  -5.4958 | Function Loss:  -3.4798\n",
            "Total loss:  -3.4401 | PDE Loss:  -5.4984 | Function Loss:  -3.4798\n",
            "Total loss:  -3.4403 | PDE Loss:  -5.5 | Function Loss:  -3.4799\n",
            "Total loss:  -3.4405 | PDE Loss:  -5.5019 | Function Loss:  -3.48\n",
            "Total loss:  -3.4407 | PDE Loss:  -5.5027 | Function Loss:  -3.4801\n",
            "Total loss:  -3.4409 | PDE Loss:  -5.5033 | Function Loss:  -3.4802\n",
            "Total loss:  -3.4411 | PDE Loss:  -5.5034 | Function Loss:  -3.4804\n",
            "Total loss:  -3.4413 | PDE Loss:  -5.5036 | Function Loss:  -3.4807\n",
            "Total loss:  -3.4415 | PDE Loss:  -5.5037 | Function Loss:  -3.4809\n",
            "Total loss:  -3.4418 | PDE Loss:  -5.5032 | Function Loss:  -3.4812\n",
            "Total loss:  -3.442 | PDE Loss:  -5.5041 | Function Loss:  -3.4814\n",
            "Total loss:  -3.4422 | PDE Loss:  -5.5029 | Function Loss:  -3.4817\n",
            "Total loss:  -3.4424 | PDE Loss:  -5.5031 | Function Loss:  -3.4819\n",
            "Total loss:  -3.4425 | PDE Loss:  -5.5028 | Function Loss:  -3.4821\n",
            "Total loss:  -3.4427 | PDE Loss:  -5.502 | Function Loss:  -3.4824\n",
            "Total loss:  -3.4429 | PDE Loss:  -5.5022 | Function Loss:  -3.4825\n",
            "Total loss:  -3.443 | PDE Loss:  -5.5016 | Function Loss:  -3.4828\n",
            "Total loss:  -3.4432 | PDE Loss:  -5.5014 | Function Loss:  -3.483\n",
            "Total loss:  -3.4434 | PDE Loss:  -5.5004 | Function Loss:  -3.4833\n",
            "Total loss:  -3.4437 | PDE Loss:  -5.5011 | Function Loss:  -3.4835\n",
            "Total loss:  -3.4439 | PDE Loss:  -5.499 | Function Loss:  -3.484\n",
            "Total loss:  -3.4442 | PDE Loss:  -5.5007 | Function Loss:  -3.4841\n",
            "Total loss:  -3.4444 | PDE Loss:  -5.5027 | Function Loss:  -3.4841\n",
            "Total loss:  -3.4446 | PDE Loss:  -5.5042 | Function Loss:  -3.4842\n",
            "Total loss:  -3.4448 | PDE Loss:  -5.5065 | Function Loss:  -3.4842\n",
            "Total loss:  -3.4449 | PDE Loss:  -5.5084 | Function Loss:  -3.4842\n",
            "Total loss:  -3.4451 | PDE Loss:  -5.5113 | Function Loss:  -3.4841\n",
            "Total loss:  -3.4453 | PDE Loss:  -5.5136 | Function Loss:  -3.4841\n",
            "Total loss:  -3.4454 | PDE Loss:  -5.5155 | Function Loss:  -3.484\n",
            "Total loss:  -3.4455 | PDE Loss:  -5.5166 | Function Loss:  -3.484\n",
            "Total loss:  -3.4455 | PDE Loss:  -5.5178 | Function Loss:  -3.484\n",
            "Total loss:  -3.4457 | PDE Loss:  -5.5177 | Function Loss:  -3.4841\n",
            "Total loss:  -3.4457 | PDE Loss:  -5.5175 | Function Loss:  -3.4842\n",
            "Total loss:  -3.4458 | PDE Loss:  -5.517 | Function Loss:  -3.4843\n",
            "Total loss:  -3.4459 | PDE Loss:  -5.5161 | Function Loss:  -3.4845\n",
            "Total loss:  -3.446 | PDE Loss:  -5.5152 | Function Loss:  -3.4847\n",
            "Total loss:  -3.4461 | PDE Loss:  -5.5143 | Function Loss:  -3.4849\n",
            "Total loss:  -3.4461 | PDE Loss:  -5.5131 | Function Loss:  -3.485\n",
            "Total loss:  -3.4462 | PDE Loss:  -5.5123 | Function Loss:  -3.4852\n",
            "Total loss:  -3.4453 | PDE Loss:  -5.5078 | Function Loss:  -3.4846\n",
            "Total loss:  -3.4463 | PDE Loss:  -5.5126 | Function Loss:  -3.4852\n",
            "Total loss:  -3.4464 | PDE Loss:  -5.5122 | Function Loss:  -3.4854\n",
            "Total loss:  -3.4464 | PDE Loss:  -5.5119 | Function Loss:  -3.4855\n",
            "Total loss:  -3.4466 | PDE Loss:  -5.5118 | Function Loss:  -3.4856\n",
            "Total loss:  -3.4467 | PDE Loss:  -5.5122 | Function Loss:  -3.4857\n",
            "Total loss:  -3.4468 | PDE Loss:  -5.5124 | Function Loss:  -3.4858\n",
            "Total loss:  -3.4469 | PDE Loss:  -5.5133 | Function Loss:  -3.4859\n",
            "Total loss:  -3.447 | PDE Loss:  -5.5145 | Function Loss:  -3.4859\n",
            "Total loss:  -3.4464 | PDE Loss:  -5.5043 | Function Loss:  -3.4861\n",
            "Total loss:  -3.4471 | PDE Loss:  -5.5133 | Function Loss:  -3.4861\n",
            "Total loss:  -3.4472 | PDE Loss:  -5.5145 | Function Loss:  -3.4861\n",
            "Total loss:  -3.4473 | PDE Loss:  -5.5151 | Function Loss:  -3.4861\n",
            "Total loss:  -3.4474 | PDE Loss:  -5.5156 | Function Loss:  -3.4862\n",
            "Total loss:  -3.4475 | PDE Loss:  -5.5155 | Function Loss:  -3.4863\n",
            "Total loss:  -3.4476 | PDE Loss:  -5.5156 | Function Loss:  -3.4865\n",
            "Total loss:  -3.4478 | PDE Loss:  -5.5155 | Function Loss:  -3.4866\n",
            "Total loss:  -3.448 | PDE Loss:  -5.5151 | Function Loss:  -3.4869\n",
            "Total loss:  -3.4482 | PDE Loss:  -5.5148 | Function Loss:  -3.4871\n",
            "Total loss:  -3.4484 | PDE Loss:  -5.5149 | Function Loss:  -3.4874\n",
            "Total loss:  -3.4487 | PDE Loss:  -5.5151 | Function Loss:  -3.4877\n",
            "Total loss:  -3.449 | PDE Loss:  -5.5157 | Function Loss:  -3.488\n",
            "Total loss:  -3.4493 | PDE Loss:  -5.5161 | Function Loss:  -3.4883\n",
            "Total loss:  -3.4496 | PDE Loss:  -5.5174 | Function Loss:  -3.4884\n",
            "Total loss:  -3.4494 | PDE Loss:  -5.5131 | Function Loss:  -3.4886\n",
            "Total loss:  -3.4498 | PDE Loss:  -5.5169 | Function Loss:  -3.4887\n",
            "Total loss:  -3.45 | PDE Loss:  -5.517 | Function Loss:  -3.4889\n",
            "Total loss:  -3.4503 | PDE Loss:  -5.517 | Function Loss:  -3.4893\n",
            "Total loss:  -3.4506 | PDE Loss:  -5.5159 | Function Loss:  -3.4897\n",
            "Total loss:  -3.4509 | PDE Loss:  -5.5135 | Function Loss:  -3.4902\n",
            "Total loss:  -3.451 | PDE Loss:  -5.5115 | Function Loss:  -3.4906\n",
            "Total loss:  -3.4512 | PDE Loss:  -5.5094 | Function Loss:  -3.4909\n",
            "Total loss:  -3.4513 | PDE Loss:  -5.5075 | Function Loss:  -3.4912\n",
            "Total loss:  -3.4514 | PDE Loss:  -5.5059 | Function Loss:  -3.4915\n",
            "Total loss:  -3.4515 | PDE Loss:  -5.5047 | Function Loss:  -3.4917\n",
            "Total loss:  -3.4515 | PDE Loss:  -5.5027 | Function Loss:  -3.492\n",
            "Total loss:  -3.4517 | PDE Loss:  -5.5014 | Function Loss:  -3.4922\n",
            "Total loss:  -3.4518 | PDE Loss:  -5.5002 | Function Loss:  -3.4925\n",
            "Total loss:  -3.4521 | PDE Loss:  -5.4987 | Function Loss:  -3.4929\n",
            "Total loss:  -3.4524 | PDE Loss:  -5.4955 | Function Loss:  -3.4936\n",
            "Total loss:  -3.4527 | PDE Loss:  -5.4929 | Function Loss:  -3.4942\n",
            "Total loss:  -3.4531 | PDE Loss:  -5.4883 | Function Loss:  -3.4951\n",
            "Total loss:  -3.4535 | PDE Loss:  -5.485 | Function Loss:  -3.4959\n",
            "Total loss:  -3.4538 | PDE Loss:  -5.482 | Function Loss:  -3.4965\n",
            "Total loss:  -3.4541 | PDE Loss:  -5.4805 | Function Loss:  -3.497\n",
            "Total loss:  -3.4544 | PDE Loss:  -5.4788 | Function Loss:  -3.4975\n",
            "Total loss:  -3.4547 | PDE Loss:  -5.4789 | Function Loss:  -3.4978\n",
            "Total loss:  -3.455 | PDE Loss:  -5.4779 | Function Loss:  -3.4983\n",
            "Total loss:  -3.4553 | PDE Loss:  -5.4749 | Function Loss:  -3.4989\n",
            "Total loss:  -3.4555 | PDE Loss:  -5.4765 | Function Loss:  -3.499\n",
            "Total loss:  -3.456 | PDE Loss:  -5.4797 | Function Loss:  -3.4992\n",
            "Total loss:  -3.4563 | PDE Loss:  -5.4822 | Function Loss:  -3.4993\n",
            "Total loss:  -3.4566 | PDE Loss:  -5.4835 | Function Loss:  -3.4995\n",
            "Total loss:  -3.4568 | PDE Loss:  -5.4838 | Function Loss:  -3.4996\n",
            "Total loss:  -3.4571 | PDE Loss:  -5.4835 | Function Loss:  -3.5\n",
            "Total loss:  -3.4574 | PDE Loss:  -5.4842 | Function Loss:  -3.5002\n",
            "Total loss:  -3.4575 | PDE Loss:  -5.4827 | Function Loss:  -3.5006\n",
            "Total loss:  -3.4578 | PDE Loss:  -5.4821 | Function Loss:  -3.5009\n",
            "Total loss:  -3.458 | PDE Loss:  -5.4817 | Function Loss:  -3.5012\n",
            "Total loss:  -3.4582 | PDE Loss:  -5.4775 | Function Loss:  -3.5019\n",
            "Total loss:  -3.4584 | PDE Loss:  -5.4775 | Function Loss:  -3.5021\n",
            "Total loss:  -3.4587 | PDE Loss:  -5.4775 | Function Loss:  -3.5024\n",
            "Total loss:  -3.4591 | PDE Loss:  -5.478 | Function Loss:  -3.5028\n",
            "Total loss:  -3.4594 | PDE Loss:  -5.4792 | Function Loss:  -3.503\n",
            "Total loss:  -3.4598 | PDE Loss:  -5.4803 | Function Loss:  -3.5034\n",
            "Total loss:  -3.4602 | PDE Loss:  -5.4836 | Function Loss:  -3.5034\n",
            "Total loss:  -3.4604 | PDE Loss:  -5.485 | Function Loss:  -3.5036\n",
            "Total loss:  -3.4608 | PDE Loss:  -5.4867 | Function Loss:  -3.5037\n",
            "Total loss:  -3.461 | PDE Loss:  -5.488 | Function Loss:  -3.5038\n",
            "Total loss:  -3.4611 | PDE Loss:  -5.4893 | Function Loss:  -3.5039\n",
            "Total loss:  -3.4613 | PDE Loss:  -5.4908 | Function Loss:  -3.5039\n",
            "Total loss:  -3.4615 | PDE Loss:  -5.4924 | Function Loss:  -3.504\n",
            "Total loss:  -3.4616 | PDE Loss:  -5.4944 | Function Loss:  -3.5039\n",
            "Total loss:  -3.4619 | PDE Loss:  -5.4962 | Function Loss:  -3.504\n",
            "Total loss:  -3.4621 | PDE Loss:  -5.4982 | Function Loss:  -3.504\n",
            "Total loss:  -3.4624 | PDE Loss:  -5.5008 | Function Loss:  -3.504\n",
            "Total loss:  -3.4626 | PDE Loss:  -5.5031 | Function Loss:  -3.5041\n",
            "Total loss:  -3.4628 | PDE Loss:  -5.5055 | Function Loss:  -3.504\n",
            "Total loss:  -3.463 | PDE Loss:  -5.5082 | Function Loss:  -3.504\n",
            "Total loss:  -3.4634 | PDE Loss:  -5.5116 | Function Loss:  -3.5041\n",
            "Total loss:  -3.4637 | PDE Loss:  -5.5159 | Function Loss:  -3.504\n",
            "Total loss:  -3.464 | PDE Loss:  -5.5196 | Function Loss:  -3.504\n",
            "Total loss:  -3.4643 | PDE Loss:  -5.5229 | Function Loss:  -3.504\n",
            "Total loss:  -3.4646 | PDE Loss:  -5.5258 | Function Loss:  -3.5041\n",
            "Total loss:  -3.4649 | PDE Loss:  -5.5289 | Function Loss:  -3.5041\n",
            "Total loss:  -3.4651 | PDE Loss:  -5.5313 | Function Loss:  -3.5041\n",
            "Total loss:  -3.4653 | PDE Loss:  -5.5341 | Function Loss:  -3.5041\n",
            "Total loss:  -3.4655 | PDE Loss:  -5.5358 | Function Loss:  -3.5041\n",
            "Total loss:  -3.4657 | PDE Loss:  -5.5387 | Function Loss:  -3.504\n",
            "Total loss:  -3.4659 | PDE Loss:  -5.5399 | Function Loss:  -3.5041\n",
            "Total loss:  -3.4661 | PDE Loss:  -5.5418 | Function Loss:  -3.5042\n",
            "Total loss:  -3.4663 | PDE Loss:  -5.5414 | Function Loss:  -3.5045\n",
            "Total loss:  -3.4665 | PDE Loss:  -5.539 | Function Loss:  -3.5049\n",
            "Total loss:  -3.4667 | PDE Loss:  -5.5383 | Function Loss:  -3.5052\n",
            "Total loss:  -3.467 | PDE Loss:  -5.5358 | Function Loss:  -3.5057\n",
            "Total loss:  -3.4673 | PDE Loss:  -5.535 | Function Loss:  -3.5061\n",
            "Total loss:  -3.4675 | PDE Loss:  -5.5323 | Function Loss:  -3.5067\n",
            "Total loss:  -3.4677 | PDE Loss:  -5.5307 | Function Loss:  -3.507\n",
            "Total loss:  -3.468 | PDE Loss:  -5.5294 | Function Loss:  -3.5074\n",
            "Total loss:  -3.4682 | PDE Loss:  -5.529 | Function Loss:  -3.5077\n",
            "Total loss:  -3.4685 | PDE Loss:  -5.5293 | Function Loss:  -3.5079\n",
            "Total loss:  -3.4686 | PDE Loss:  -5.5296 | Function Loss:  -3.5081\n",
            "Total loss:  -3.4688 | PDE Loss:  -5.5296 | Function Loss:  -3.5083\n",
            "Total loss:  -3.4692 | PDE Loss:  -5.5301 | Function Loss:  -3.5086\n",
            "Total loss:  -3.4694 | PDE Loss:  -5.5315 | Function Loss:  -3.5088\n",
            "Total loss:  -3.4699 | PDE Loss:  -5.5368 | Function Loss:  -3.5088\n",
            "Total loss:  -3.4702 | PDE Loss:  -5.5377 | Function Loss:  -3.509\n",
            "Total loss:  -3.4703 | PDE Loss:  -5.5388 | Function Loss:  -3.5091\n",
            "Total loss:  -3.4705 | PDE Loss:  -5.5402 | Function Loss:  -3.5092\n",
            "Total loss:  -3.4707 | PDE Loss:  -5.5389 | Function Loss:  -3.5095\n",
            "Total loss:  -3.4709 | PDE Loss:  -5.5377 | Function Loss:  -3.5098\n",
            "Total loss:  -3.471 | PDE Loss:  -5.5359 | Function Loss:  -3.5101\n",
            "Total loss:  -3.4712 | PDE Loss:  -5.5352 | Function Loss:  -3.5104\n",
            "Total loss:  -3.4713 | PDE Loss:  -5.5335 | Function Loss:  -3.5107\n",
            "Total loss:  -3.4715 | PDE Loss:  -5.5347 | Function Loss:  -3.5108\n",
            "Total loss:  -3.4716 | PDE Loss:  -5.5347 | Function Loss:  -3.5109\n",
            "Total loss:  -3.4718 | PDE Loss:  -5.536 | Function Loss:  -3.5109\n",
            "Total loss:  -3.4719 | PDE Loss:  -5.5331 | Function Loss:  -3.5113\n",
            "Total loss:  -3.4721 | PDE Loss:  -5.5337 | Function Loss:  -3.5115\n",
            "Total loss:  -3.4722 | PDE Loss:  -5.5362 | Function Loss:  -3.5114\n",
            "Total loss:  -3.4724 | PDE Loss:  -5.5376 | Function Loss:  -3.5115\n",
            "Total loss:  -3.4726 | PDE Loss:  -5.5371 | Function Loss:  -3.5117\n",
            "Total loss:  -3.4727 | PDE Loss:  -5.5364 | Function Loss:  -3.512\n",
            "Total loss:  -3.4729 | PDE Loss:  -5.5335 | Function Loss:  -3.5124\n",
            "Total loss:  -3.4731 | PDE Loss:  -5.5309 | Function Loss:  -3.5129\n",
            "Total loss:  -3.4734 | PDE Loss:  -5.5273 | Function Loss:  -3.5135\n",
            "Total loss:  -3.4736 | PDE Loss:  -5.523 | Function Loss:  -3.5142\n",
            "Total loss:  -3.4739 | PDE Loss:  -5.521 | Function Loss:  -3.5147\n",
            "Total loss:  -3.4741 | PDE Loss:  -5.5194 | Function Loss:  -3.5151\n",
            "Total loss:  -3.4743 | PDE Loss:  -5.5196 | Function Loss:  -3.5153\n",
            "Total loss:  -3.4745 | PDE Loss:  -5.5199 | Function Loss:  -3.5155\n",
            "Total loss:  -3.4746 | PDE Loss:  -5.5214 | Function Loss:  -3.5155\n",
            "Total loss:  -3.4748 | PDE Loss:  -5.5214 | Function Loss:  -3.5157\n",
            "Total loss:  -3.4749 | PDE Loss:  -5.5223 | Function Loss:  -3.5157\n",
            "Total loss:  -3.4751 | PDE Loss:  -5.5219 | Function Loss:  -3.5159\n",
            "Total loss:  -3.4752 | PDE Loss:  -5.5212 | Function Loss:  -3.5161\n",
            "Total loss:  -3.4754 | PDE Loss:  -5.5197 | Function Loss:  -3.5165\n",
            "Total loss:  -3.4756 | PDE Loss:  -5.5183 | Function Loss:  -3.5168\n",
            "Total loss:  -3.4759 | PDE Loss:  -5.5155 | Function Loss:  -3.5174\n",
            "Total loss:  -3.4761 | PDE Loss:  -5.513 | Function Loss:  -3.518\n",
            "Total loss:  -3.4764 | PDE Loss:  -5.5108 | Function Loss:  -3.5185\n",
            "Total loss:  -3.4767 | PDE Loss:  -5.5058 | Function Loss:  -3.5193\n",
            "Total loss:  -3.4769 | PDE Loss:  -5.5059 | Function Loss:  -3.5195\n",
            "Total loss:  -3.4772 | PDE Loss:  -5.5064 | Function Loss:  -3.5198\n",
            "Total loss:  -3.4774 | PDE Loss:  -5.5077 | Function Loss:  -3.5199\n",
            "Total loss:  -3.4777 | PDE Loss:  -5.5097 | Function Loss:  -3.52\n",
            "Total loss:  -3.4779 | PDE Loss:  -5.5118 | Function Loss:  -3.52\n",
            "Total loss:  -3.4781 | PDE Loss:  -5.514 | Function Loss:  -3.5201\n",
            "Total loss:  -3.4783 | PDE Loss:  -5.5155 | Function Loss:  -3.5201\n",
            "Total loss:  -3.4784 | PDE Loss:  -5.5167 | Function Loss:  -3.5201\n",
            "Total loss:  -3.4785 | PDE Loss:  -5.518 | Function Loss:  -3.5201\n",
            "Total loss:  -3.4786 | PDE Loss:  -5.5188 | Function Loss:  -3.5202\n",
            "Total loss:  -3.4788 | PDE Loss:  -5.5192 | Function Loss:  -3.5203\n",
            "Total loss:  -3.4789 | PDE Loss:  -5.5206 | Function Loss:  -3.5203\n",
            "Total loss:  -3.479 | PDE Loss:  -5.5209 | Function Loss:  -3.5204\n",
            "Total loss:  -3.4792 | PDE Loss:  -5.5219 | Function Loss:  -3.5204\n",
            "Total loss:  -3.4792 | PDE Loss:  -5.5231 | Function Loss:  -3.5204\n",
            "Total loss:  -3.4794 | PDE Loss:  -5.5215 | Function Loss:  -3.5207\n",
            "Total loss:  -3.4795 | PDE Loss:  -5.5221 | Function Loss:  -3.5208\n",
            "Total loss:  -3.4797 | PDE Loss:  -5.5223 | Function Loss:  -3.521\n",
            "Total loss:  -3.4798 | PDE Loss:  -5.5229 | Function Loss:  -3.5211\n",
            "Total loss:  -3.48 | PDE Loss:  -5.5227 | Function Loss:  -3.5212\n",
            "Total loss:  -3.4801 | PDE Loss:  -5.5225 | Function Loss:  -3.5214\n",
            "Total loss:  -3.4803 | PDE Loss:  -5.5226 | Function Loss:  -3.5216\n",
            "Total loss:  -3.4804 | PDE Loss:  -5.5228 | Function Loss:  -3.5217\n",
            "Total loss:  -3.4806 | PDE Loss:  -5.5235 | Function Loss:  -3.5219\n",
            "Total loss:  -3.4808 | PDE Loss:  -5.5244 | Function Loss:  -3.522\n",
            "Total loss:  -3.481 | PDE Loss:  -5.5248 | Function Loss:  -3.5222\n",
            "Total loss:  -3.4812 | PDE Loss:  -5.5258 | Function Loss:  -3.5222\n",
            "Total loss:  -3.4814 | PDE Loss:  -5.5263 | Function Loss:  -3.5224\n",
            "Total loss:  -3.4815 | PDE Loss:  -5.5264 | Function Loss:  -3.5225\n",
            "Total loss:  -3.4816 | PDE Loss:  -5.5261 | Function Loss:  -3.5227\n",
            "Total loss:  -3.4818 | PDE Loss:  -5.5258 | Function Loss:  -3.5229\n",
            "Total loss:  -3.4819 | PDE Loss:  -5.5253 | Function Loss:  -3.523\n",
            "Total loss:  -3.4819 | PDE Loss:  -5.5251 | Function Loss:  -3.5231\n",
            "Total loss:  -3.482 | PDE Loss:  -5.5248 | Function Loss:  -3.5233\n",
            "Total loss:  -3.4821 | PDE Loss:  -5.525 | Function Loss:  -3.5233\n",
            "Total loss:  -3.4821 | PDE Loss:  -5.5253 | Function Loss:  -3.5234\n",
            "Total loss:  -3.4823 | PDE Loss:  -5.5259 | Function Loss:  -3.5234\n",
            "Total loss:  -3.4824 | PDE Loss:  -5.5262 | Function Loss:  -3.5235\n",
            "Total loss:  -3.4824 | PDE Loss:  -5.5275 | Function Loss:  -3.5235\n",
            "Total loss:  -3.4826 | PDE Loss:  -5.5287 | Function Loss:  -3.5235\n",
            "Total loss:  -3.4826 | PDE Loss:  -5.5294 | Function Loss:  -3.5235\n",
            "Total loss:  -3.4828 | PDE Loss:  -5.5307 | Function Loss:  -3.5235\n",
            "Total loss:  -3.4828 | PDE Loss:  -5.5307 | Function Loss:  -3.5236\n",
            "Total loss:  -3.4829 | PDE Loss:  -5.5315 | Function Loss:  -3.5236\n",
            "Total loss:  -3.483 | PDE Loss:  -5.5317 | Function Loss:  -3.5237\n",
            "Total loss:  -3.4831 | PDE Loss:  -5.5321 | Function Loss:  -3.5238\n",
            "Total loss:  -3.4832 | PDE Loss:  -5.5318 | Function Loss:  -3.5239\n",
            "Total loss:  -3.4833 | PDE Loss:  -5.5319 | Function Loss:  -3.5239\n",
            "Total loss:  -3.4833 | PDE Loss:  -5.5315 | Function Loss:  -3.524\n",
            "Total loss:  -3.4834 | PDE Loss:  -5.5314 | Function Loss:  -3.5242\n",
            "Total loss:  -3.4836 | PDE Loss:  -5.5315 | Function Loss:  -3.5243\n",
            "Total loss:  -3.4837 | PDE Loss:  -5.5316 | Function Loss:  -3.5245\n",
            "Total loss:  -3.4838 | PDE Loss:  -5.5311 | Function Loss:  -3.5246\n",
            "Total loss:  -3.484 | PDE Loss:  -5.5317 | Function Loss:  -3.5248\n",
            "Total loss:  -3.4841 | PDE Loss:  -5.5305 | Function Loss:  -3.525\n",
            "Total loss:  -3.4842 | PDE Loss:  -5.5316 | Function Loss:  -3.525\n",
            "Total loss:  -3.4843 | PDE Loss:  -5.5319 | Function Loss:  -3.5251\n",
            "Total loss:  -3.4845 | PDE Loss:  -5.5321 | Function Loss:  -3.5253\n",
            "Total loss:  -3.4846 | PDE Loss:  -5.5323 | Function Loss:  -3.5254\n",
            "Total loss:  -3.4848 | PDE Loss:  -5.5314 | Function Loss:  -3.5256\n",
            "Total loss:  -3.4849 | PDE Loss:  -5.5292 | Function Loss:  -3.526\n",
            "Total loss:  -3.485 | PDE Loss:  -5.5296 | Function Loss:  -3.526\n",
            "Total loss:  -3.4851 | PDE Loss:  -5.5295 | Function Loss:  -3.5261\n",
            "Total loss:  -3.4852 | PDE Loss:  -5.529 | Function Loss:  -3.5263\n",
            "Total loss:  -3.4853 | PDE Loss:  -5.5286 | Function Loss:  -3.5265\n",
            "Total loss:  -3.4853 | PDE Loss:  -5.5275 | Function Loss:  -3.5266\n",
            "Total loss:  -3.4854 | PDE Loss:  -5.5272 | Function Loss:  -3.5268\n",
            "Total loss:  -3.4855 | PDE Loss:  -5.5264 | Function Loss:  -3.527\n",
            "Total loss:  -3.4856 | PDE Loss:  -5.5263 | Function Loss:  -3.5271\n",
            "Total loss:  -3.4857 | PDE Loss:  -5.5261 | Function Loss:  -3.5272\n",
            "Total loss:  -3.4858 | PDE Loss:  -5.5266 | Function Loss:  -3.5273\n",
            "Total loss:  -3.4859 | PDE Loss:  -5.5271 | Function Loss:  -3.5274\n",
            "Total loss:  -3.486 | PDE Loss:  -5.5269 | Function Loss:  -3.5274\n",
            "Total loss:  -3.4861 | PDE Loss:  -5.5281 | Function Loss:  -3.5275\n",
            "Total loss:  -3.4863 | PDE Loss:  -5.5293 | Function Loss:  -3.5275\n",
            "Total loss:  -3.4864 | PDE Loss:  -5.5299 | Function Loss:  -3.5275\n",
            "Total loss:  -3.4864 | PDE Loss:  -5.5301 | Function Loss:  -3.5276\n",
            "Total loss:  -3.4865 | PDE Loss:  -5.5296 | Function Loss:  -3.5277\n",
            "Total loss:  -3.4866 | PDE Loss:  -5.5291 | Function Loss:  -3.5279\n",
            "Total loss:  -3.4867 | PDE Loss:  -5.5278 | Function Loss:  -3.5281\n",
            "Total loss:  -3.4868 | PDE Loss:  -5.5261 | Function Loss:  -3.5284\n",
            "Total loss:  -3.487 | PDE Loss:  -5.5235 | Function Loss:  -3.5289\n",
            "Total loss:  -3.4872 | PDE Loss:  -5.5178 | Function Loss:  -3.5296\n",
            "Total loss:  -3.4872 | PDE Loss:  -5.5175 | Function Loss:  -3.5297\n",
            "Total loss:  -3.4874 | PDE Loss:  -5.5164 | Function Loss:  -3.5301\n",
            "Total loss:  -3.4875 | PDE Loss:  -5.5152 | Function Loss:  -3.5303\n",
            "Total loss:  -3.4876 | PDE Loss:  -5.5144 | Function Loss:  -3.5305\n",
            "Total loss:  -3.4877 | PDE Loss:  -5.5137 | Function Loss:  -3.5307\n",
            "Total loss:  -3.4878 | PDE Loss:  -5.5131 | Function Loss:  -3.5308\n",
            "Total loss:  -3.4879 | PDE Loss:  -5.5135 | Function Loss:  -3.5309\n",
            "Total loss:  -3.488 | PDE Loss:  -5.5124 | Function Loss:  -3.5311\n",
            "Total loss:  -3.4881 | PDE Loss:  -5.5123 | Function Loss:  -3.5312\n",
            "Total loss:  -3.4882 | PDE Loss:  -5.5129 | Function Loss:  -3.5313\n",
            "Total loss:  -3.4883 | PDE Loss:  -5.5128 | Function Loss:  -3.5314\n",
            "Total loss:  -3.4884 | PDE Loss:  -5.512 | Function Loss:  -3.5316\n",
            "Total loss:  -3.4885 | PDE Loss:  -5.515 | Function Loss:  -3.5314\n",
            "Total loss:  -3.4887 | PDE Loss:  -5.5113 | Function Loss:  -3.532\n",
            "Total loss:  -3.4888 | PDE Loss:  -5.5098 | Function Loss:  -3.5323\n",
            "Total loss:  -3.489 | PDE Loss:  -5.5072 | Function Loss:  -3.5328\n",
            "Total loss:  -3.4893 | PDE Loss:  -5.5058 | Function Loss:  -3.5333\n",
            "Total loss:  -3.4897 | PDE Loss:  -5.5051 | Function Loss:  -3.5337\n",
            "Total loss:  -3.4899 | PDE Loss:  -5.5036 | Function Loss:  -3.5342\n",
            "Total loss:  -3.4902 | PDE Loss:  -5.5053 | Function Loss:  -3.5343\n",
            "Total loss:  -3.4904 | PDE Loss:  -5.5072 | Function Loss:  -3.5343\n",
            "Total loss:  -3.4905 | PDE Loss:  -5.5102 | Function Loss:  -3.5342\n",
            "Total loss:  -3.4906 | PDE Loss:  -5.5114 | Function Loss:  -3.5341\n",
            "Total loss:  -3.4907 | PDE Loss:  -5.513 | Function Loss:  -3.5341\n",
            "Total loss:  -3.4908 | PDE Loss:  -5.5139 | Function Loss:  -3.5341\n",
            "Total loss:  -3.4909 | PDE Loss:  -5.5144 | Function Loss:  -3.5342\n",
            "Total loss:  -3.4911 | PDE Loss:  -5.5158 | Function Loss:  -3.5342\n",
            "Total loss:  -3.4912 | PDE Loss:  -5.5147 | Function Loss:  -3.5344\n",
            "Total loss:  -3.4914 | PDE Loss:  -5.5152 | Function Loss:  -3.5346\n",
            "Total loss:  -3.4915 | PDE Loss:  -5.5144 | Function Loss:  -3.5348\n",
            "Total loss:  -3.4917 | PDE Loss:  -5.5129 | Function Loss:  -3.5352\n",
            "Total loss:  -3.492 | PDE Loss:  -5.5111 | Function Loss:  -3.5357\n",
            "Total loss:  -3.4921 | PDE Loss:  -5.5074 | Function Loss:  -3.5362\n",
            "Total loss:  -3.4924 | PDE Loss:  -5.5037 | Function Loss:  -3.5369\n",
            "Total loss:  -3.4926 | PDE Loss:  -5.5034 | Function Loss:  -3.5371\n",
            "Total loss:  -3.4929 | PDE Loss:  -5.5029 | Function Loss:  -3.5376\n",
            "Total loss:  -3.4933 | PDE Loss:  -5.5027 | Function Loss:  -3.538\n",
            "Total loss:  -3.4937 | PDE Loss:  -5.5021 | Function Loss:  -3.5385\n",
            "Total loss:  -3.4939 | PDE Loss:  -5.5014 | Function Loss:  -3.5388\n",
            "Total loss:  -3.4941 | PDE Loss:  -5.5013 | Function Loss:  -3.539\n",
            "Total loss:  -3.4943 | PDE Loss:  -5.5008 | Function Loss:  -3.5393\n",
            "Total loss:  -3.4945 | PDE Loss:  -5.5015 | Function Loss:  -3.5395\n",
            "Total loss:  -3.4947 | PDE Loss:  -5.5009 | Function Loss:  -3.5398\n",
            "Total loss:  -3.4949 | PDE Loss:  -5.5015 | Function Loss:  -3.54\n",
            "Total loss:  -3.4952 | PDE Loss:  -5.5011 | Function Loss:  -3.5403\n",
            "Total loss:  -3.4953 | PDE Loss:  -5.5014 | Function Loss:  -3.5404\n",
            "Total loss:  -3.4955 | PDE Loss:  -5.5028 | Function Loss:  -3.5404\n",
            "Total loss:  -3.4956 | PDE Loss:  -5.5024 | Function Loss:  -3.5406\n",
            "Total loss:  -3.4957 | PDE Loss:  -5.5024 | Function Loss:  -3.5407\n",
            "Total loss:  -3.4959 | PDE Loss:  -5.503 | Function Loss:  -3.5409\n",
            "Total loss:  -3.4961 | PDE Loss:  -5.5028 | Function Loss:  -3.5412\n",
            "Total loss:  -3.4964 | PDE Loss:  -5.503 | Function Loss:  -3.5414\n",
            "Total loss:  -3.4965 | PDE Loss:  -5.5029 | Function Loss:  -3.5416\n",
            "Total loss:  -3.4967 | PDE Loss:  -5.5028 | Function Loss:  -3.5418\n",
            "Total loss:  -3.4969 | PDE Loss:  -5.5025 | Function Loss:  -3.542\n",
            "Total loss:  -3.4971 | PDE Loss:  -5.5024 | Function Loss:  -3.5423\n",
            "Total loss:  -3.4972 | PDE Loss:  -5.5027 | Function Loss:  -3.5424\n",
            "Total loss:  -3.4974 | PDE Loss:  -5.5027 | Function Loss:  -3.5426\n",
            "Total loss:  -3.4976 | PDE Loss:  -5.5032 | Function Loss:  -3.5427\n",
            "Total loss:  -3.4978 | PDE Loss:  -5.5045 | Function Loss:  -3.5428\n",
            "Total loss:  -3.4981 | PDE Loss:  -5.5081 | Function Loss:  -3.5428\n",
            "Total loss:  -3.4984 | PDE Loss:  -5.51 | Function Loss:  -3.5429\n",
            "Total loss:  -3.4986 | PDE Loss:  -5.5123 | Function Loss:  -3.5429\n",
            "Total loss:  -3.4988 | PDE Loss:  -5.5143 | Function Loss:  -3.5429\n",
            "Total loss:  -3.499 | PDE Loss:  -5.5152 | Function Loss:  -3.543\n",
            "Total loss:  -3.4991 | PDE Loss:  -5.5157 | Function Loss:  -3.543\n",
            "Total loss:  -3.4992 | PDE Loss:  -5.5155 | Function Loss:  -3.5432\n",
            "Total loss:  -3.4994 | PDE Loss:  -5.515 | Function Loss:  -3.5434\n",
            "Total loss:  -3.4996 | PDE Loss:  -5.5145 | Function Loss:  -3.5437\n",
            "Total loss:  -3.4998 | PDE Loss:  -5.5135 | Function Loss:  -3.5441\n",
            "Total loss:  -3.5 | PDE Loss:  -5.5123 | Function Loss:  -3.5444\n",
            "Total loss:  -3.5002 | PDE Loss:  -5.5109 | Function Loss:  -3.5447\n",
            "Total loss:  -3.5004 | PDE Loss:  -5.5105 | Function Loss:  -3.545\n",
            "Total loss:  -3.5006 | PDE Loss:  -5.5093 | Function Loss:  -3.5454\n",
            "Total loss:  -3.5008 | PDE Loss:  -5.5091 | Function Loss:  -3.5456\n",
            "Total loss:  -3.501 | PDE Loss:  -5.5081 | Function Loss:  -3.5459\n",
            "Total loss:  -3.5012 | PDE Loss:  -5.507 | Function Loss:  -3.5463\n",
            "Total loss:  -3.5013 | PDE Loss:  -5.5052 | Function Loss:  -3.5467\n",
            "Total loss:  -3.5015 | PDE Loss:  -5.5038 | Function Loss:  -3.547\n",
            "Total loss:  -3.5017 | PDE Loss:  -5.5024 | Function Loss:  -3.5473\n",
            "Total loss:  -3.5018 | PDE Loss:  -5.5016 | Function Loss:  -3.5475\n",
            "Total loss:  -3.5018 | PDE Loss:  -5.5008 | Function Loss:  -3.5477\n",
            "Total loss:  -3.502 | PDE Loss:  -5.4999 | Function Loss:  -3.548\n",
            "Total loss:  -3.5016 | PDE Loss:  -5.4949 | Function Loss:  -3.5481\n",
            "Total loss:  -3.5021 | PDE Loss:  -5.4994 | Function Loss:  -3.5481\n",
            "Total loss:  -3.5023 | PDE Loss:  -5.4974 | Function Loss:  -3.5486\n",
            "Total loss:  -3.5025 | PDE Loss:  -5.4957 | Function Loss:  -3.549\n",
            "Total loss:  -3.5028 | PDE Loss:  -5.4938 | Function Loss:  -3.5495\n",
            "Total loss:  -3.503 | PDE Loss:  -5.4922 | Function Loss:  -3.55\n",
            "Total loss:  -3.5032 | PDE Loss:  -5.49 | Function Loss:  -3.5505\n",
            "Total loss:  -3.5034 | PDE Loss:  -5.4886 | Function Loss:  -3.5508\n",
            "Total loss:  -3.5035 | PDE Loss:  -5.4869 | Function Loss:  -3.5512\n",
            "Total loss:  -3.5036 | PDE Loss:  -5.4858 | Function Loss:  -3.5514\n",
            "Total loss:  -3.5038 | PDE Loss:  -5.4851 | Function Loss:  -3.5516\n",
            "Total loss:  -3.5039 | PDE Loss:  -5.4842 | Function Loss:  -3.5519\n",
            "Total loss:  -3.504 | PDE Loss:  -5.4837 | Function Loss:  -3.552\n",
            "Total loss:  -3.5041 | PDE Loss:  -5.4842 | Function Loss:  -3.5521\n",
            "Total loss:  -3.5042 | PDE Loss:  -5.4843 | Function Loss:  -3.5522\n",
            "Total loss:  -3.5043 | PDE Loss:  -5.4847 | Function Loss:  -3.5522\n",
            "Total loss:  -3.5043 | PDE Loss:  -5.4853 | Function Loss:  -3.5523\n",
            "Total loss:  -3.5039 | PDE Loss:  -5.4838 | Function Loss:  -3.552\n",
            "Total loss:  -3.5044 | PDE Loss:  -5.4855 | Function Loss:  -3.5523\n",
            "Total loss:  -3.5045 | PDE Loss:  -5.4859 | Function Loss:  -3.5524\n",
            "Total loss:  -3.5046 | PDE Loss:  -5.4867 | Function Loss:  -3.5524\n",
            "Total loss:  -3.5048 | PDE Loss:  -5.4866 | Function Loss:  -3.5526\n",
            "Total loss:  -3.5049 | PDE Loss:  -5.4871 | Function Loss:  -3.5526\n",
            "Total loss:  -3.505 | PDE Loss:  -5.4873 | Function Loss:  -3.5528\n",
            "Total loss:  -3.5052 | PDE Loss:  -5.4875 | Function Loss:  -3.5529\n",
            "Total loss:  -3.5053 | PDE Loss:  -5.4877 | Function Loss:  -3.5531\n",
            "Total loss:  -3.5055 | PDE Loss:  -5.4875 | Function Loss:  -3.5532\n",
            "Total loss:  -3.5056 | PDE Loss:  -5.4878 | Function Loss:  -3.5534\n",
            "Total loss:  -3.5058 | PDE Loss:  -5.4875 | Function Loss:  -3.5537\n",
            "Total loss:  -3.506 | PDE Loss:  -5.4876 | Function Loss:  -3.5539\n",
            "Total loss:  -3.5063 | PDE Loss:  -5.4888 | Function Loss:  -3.554\n",
            "Total loss:  -3.5065 | PDE Loss:  -5.4901 | Function Loss:  -3.5541\n",
            "Total loss:  -3.5067 | PDE Loss:  -5.492 | Function Loss:  -3.5541\n",
            "Total loss:  -3.5068 | PDE Loss:  -5.4936 | Function Loss:  -3.5541\n",
            "Total loss:  -3.5069 | PDE Loss:  -5.4946 | Function Loss:  -3.5541\n",
            "Total loss:  -3.5071 | PDE Loss:  -5.4958 | Function Loss:  -3.5541\n",
            "Total loss:  -3.5072 | PDE Loss:  -5.4972 | Function Loss:  -3.5541\n",
            "Total loss:  -3.5075 | PDE Loss:  -5.4984 | Function Loss:  -3.5542\n",
            "Total loss:  -3.5077 | PDE Loss:  -5.4983 | Function Loss:  -3.5545\n",
            "Total loss:  -3.508 | PDE Loss:  -5.499 | Function Loss:  -3.5548\n",
            "Total loss:  -3.5082 | PDE Loss:  -5.4978 | Function Loss:  -3.5551\n",
            "Total loss:  -3.5083 | PDE Loss:  -5.4971 | Function Loss:  -3.5553\n",
            "Total loss:  -3.5085 | PDE Loss:  -5.4967 | Function Loss:  -3.5555\n",
            "Total loss:  -3.5086 | PDE Loss:  -5.496 | Function Loss:  -3.5558\n",
            "Total loss:  -3.5087 | PDE Loss:  -5.4958 | Function Loss:  -3.5559\n",
            "Total loss:  -3.5088 | PDE Loss:  -5.496 | Function Loss:  -3.556\n",
            "Total loss:  -3.5067 | PDE Loss:  -5.4801 | Function Loss:  -3.5555\n",
            "Total loss:  -3.5088 | PDE Loss:  -5.4951 | Function Loss:  -3.5561\n",
            "Total loss:  -3.5089 | PDE Loss:  -5.4954 | Function Loss:  -3.5562\n",
            "Total loss:  -3.509 | PDE Loss:  -5.4961 | Function Loss:  -3.5563\n",
            "Total loss:  -3.5091 | PDE Loss:  -5.4964 | Function Loss:  -3.5563\n",
            "Total loss:  -3.5092 | PDE Loss:  -5.4972 | Function Loss:  -3.5564\n",
            "Total loss:  -3.5093 | PDE Loss:  -5.4978 | Function Loss:  -3.5564\n",
            "Total loss:  -3.5094 | PDE Loss:  -5.4977 | Function Loss:  -3.5565\n",
            "Total loss:  -3.5095 | PDE Loss:  -5.498 | Function Loss:  -3.5565\n",
            "Total loss:  -3.5096 | PDE Loss:  -5.4981 | Function Loss:  -3.5566\n",
            "Total loss:  -3.5097 | PDE Loss:  -5.4981 | Function Loss:  -3.5568\n",
            "Total loss:  -3.5098 | PDE Loss:  -5.4981 | Function Loss:  -3.5569\n",
            "Total loss:  -3.5099 | PDE Loss:  -5.4977 | Function Loss:  -3.5571\n",
            "Total loss:  -3.51 | PDE Loss:  -5.4976 | Function Loss:  -3.5572\n",
            "Total loss:  -3.5101 | PDE Loss:  -5.4973 | Function Loss:  -3.5573\n",
            "Total loss:  -3.5102 | PDE Loss:  -5.4978 | Function Loss:  -3.5574\n",
            "Total loss:  -3.5104 | PDE Loss:  -5.497 | Function Loss:  -3.5577\n",
            "Total loss:  -3.5105 | PDE Loss:  -5.4971 | Function Loss:  -3.5578\n",
            "Total loss:  -3.5106 | PDE Loss:  -5.4967 | Function Loss:  -3.5579\n",
            "Total loss:  -3.5107 | PDE Loss:  -5.4963 | Function Loss:  -3.5581\n",
            "Total loss:  -3.5108 | PDE Loss:  -5.4954 | Function Loss:  -3.5583\n",
            "Total loss:  -3.511 | PDE Loss:  -5.4955 | Function Loss:  -3.5585\n",
            "Total loss:  -3.511 | PDE Loss:  -5.4944 | Function Loss:  -3.5586\n",
            "Total loss:  -3.5111 | PDE Loss:  -5.4943 | Function Loss:  -3.5588\n",
            "Total loss:  -3.5113 | PDE Loss:  -5.4948 | Function Loss:  -3.5589\n",
            "Total loss:  -3.5114 | PDE Loss:  -5.4951 | Function Loss:  -3.559\n",
            "Total loss:  -3.5114 | PDE Loss:  -5.4953 | Function Loss:  -3.559\n",
            "Total loss:  -3.5115 | PDE Loss:  -5.4962 | Function Loss:  -3.559\n",
            "Total loss:  -3.5115 | PDE Loss:  -5.4976 | Function Loss:  -3.5589\n",
            "Total loss:  -3.5116 | PDE Loss:  -5.4988 | Function Loss:  -3.5588\n",
            "Total loss:  -3.5117 | PDE Loss:  -5.5008 | Function Loss:  -3.5586\n",
            "Total loss:  -3.5117 | PDE Loss:  -5.5017 | Function Loss:  -3.5586\n",
            "Total loss:  -3.5118 | PDE Loss:  -5.5038 | Function Loss:  -3.5584\n",
            "Total loss:  -3.5119 | PDE Loss:  -5.5047 | Function Loss:  -3.5584\n",
            "Total loss:  -3.5121 | PDE Loss:  -5.5056 | Function Loss:  -3.5586\n",
            "Total loss:  -3.5122 | PDE Loss:  -5.5053 | Function Loss:  -3.5588\n",
            "Total loss:  -3.5125 | PDE Loss:  -5.5049 | Function Loss:  -3.5591\n",
            "Total loss:  -3.5126 | PDE Loss:  -5.5047 | Function Loss:  -3.5592\n",
            "Total loss:  -3.5128 | PDE Loss:  -5.5039 | Function Loss:  -3.5595\n",
            "Total loss:  -3.5129 | PDE Loss:  -5.5036 | Function Loss:  -3.5597\n",
            "Total loss:  -3.5131 | PDE Loss:  -5.5035 | Function Loss:  -3.5599\n",
            "Total loss:  -3.5131 | PDE Loss:  -5.5033 | Function Loss:  -3.56\n",
            "Total loss:  -3.5133 | PDE Loss:  -5.5042 | Function Loss:  -3.5601\n",
            "Total loss:  -3.5134 | PDE Loss:  -5.5056 | Function Loss:  -3.5601\n",
            "Total loss:  -3.5136 | PDE Loss:  -5.5069 | Function Loss:  -3.5601\n",
            "Total loss:  -3.5137 | PDE Loss:  -5.5082 | Function Loss:  -3.56\n",
            "Total loss:  -3.5137 | PDE Loss:  -5.5097 | Function Loss:  -3.5599\n",
            "Total loss:  -3.5138 | PDE Loss:  -5.5104 | Function Loss:  -3.5599\n",
            "Total loss:  -3.5139 | PDE Loss:  -5.5104 | Function Loss:  -3.56\n",
            "Total loss:  -3.5139 | PDE Loss:  -5.5102 | Function Loss:  -3.5601\n",
            "Total loss:  -3.514 | PDE Loss:  -5.5092 | Function Loss:  -3.5603\n",
            "Total loss:  -3.5141 | PDE Loss:  -5.5081 | Function Loss:  -3.5605\n",
            "Total loss:  -3.5142 | PDE Loss:  -5.5071 | Function Loss:  -3.5607\n",
            "Total loss:  -3.5142 | PDE Loss:  -5.5047 | Function Loss:  -3.5611\n",
            "Total loss:  -3.5144 | PDE Loss:  -5.5034 | Function Loss:  -3.5614\n",
            "Total loss:  -3.5145 | PDE Loss:  -5.5033 | Function Loss:  -3.5615\n",
            "Total loss:  -3.5146 | PDE Loss:  -5.504 | Function Loss:  -3.5616\n",
            "Total loss:  -3.5148 | PDE Loss:  -5.5051 | Function Loss:  -3.5616\n",
            "Total loss:  -3.5149 | PDE Loss:  -5.507 | Function Loss:  -3.5615\n",
            "Total loss:  -3.515 | PDE Loss:  -5.5093 | Function Loss:  -3.5614\n",
            "Total loss:  -3.5152 | PDE Loss:  -5.5121 | Function Loss:  -3.5613\n",
            "Total loss:  -3.5153 | PDE Loss:  -5.515 | Function Loss:  -3.5611\n",
            "Total loss:  -3.5155 | PDE Loss:  -5.5186 | Function Loss:  -3.5609\n",
            "Total loss:  -3.5157 | PDE Loss:  -5.5227 | Function Loss:  -3.5607\n",
            "Total loss:  -3.5159 | PDE Loss:  -5.5254 | Function Loss:  -3.5606\n",
            "Total loss:  -3.516 | PDE Loss:  -5.5277 | Function Loss:  -3.5605\n",
            "Total loss:  -3.5161 | PDE Loss:  -5.5283 | Function Loss:  -3.5606\n",
            "Total loss:  -3.5162 | PDE Loss:  -5.5282 | Function Loss:  -3.5607\n",
            "Total loss:  -3.5163 | PDE Loss:  -5.5271 | Function Loss:  -3.5609\n",
            "Total loss:  -3.5164 | PDE Loss:  -5.526 | Function Loss:  -3.5611\n",
            "Total loss:  -3.5165 | PDE Loss:  -5.5247 | Function Loss:  -3.5614\n",
            "Total loss:  -3.5167 | PDE Loss:  -5.523 | Function Loss:  -3.5618\n",
            "Total loss:  -3.5169 | PDE Loss:  -5.5222 | Function Loss:  -3.5621\n",
            "Total loss:  -3.5171 | PDE Loss:  -5.5207 | Function Loss:  -3.5624\n",
            "Total loss:  -3.5173 | PDE Loss:  -5.5205 | Function Loss:  -3.5627\n",
            "Total loss:  -3.5174 | PDE Loss:  -5.522 | Function Loss:  -3.5627\n",
            "Total loss:  -3.5175 | PDE Loss:  -5.5234 | Function Loss:  -3.5627\n",
            "Total loss:  -3.5177 | PDE Loss:  -5.5251 | Function Loss:  -3.5626\n",
            "Total loss:  -3.5178 | PDE Loss:  -5.5261 | Function Loss:  -3.5626\n",
            "Total loss:  -3.5179 | PDE Loss:  -5.5268 | Function Loss:  -3.5627\n",
            "Total loss:  -3.5181 | PDE Loss:  -5.5268 | Function Loss:  -3.5629\n",
            "Total loss:  -3.5182 | PDE Loss:  -5.526 | Function Loss:  -3.5631\n",
            "Total loss:  -3.5183 | PDE Loss:  -5.5246 | Function Loss:  -3.5634\n",
            "Total loss:  -3.5185 | PDE Loss:  -5.5224 | Function Loss:  -3.5638\n",
            "Total loss:  -3.5186 | PDE Loss:  -5.5189 | Function Loss:  -3.5643\n",
            "Total loss:  -3.5187 | PDE Loss:  -5.5178 | Function Loss:  -3.5646\n",
            "Total loss:  -3.5188 | PDE Loss:  -5.5174 | Function Loss:  -3.5647\n",
            "Total loss:  -3.5189 | PDE Loss:  -5.5173 | Function Loss:  -3.5649\n",
            "Total loss:  -3.519 | PDE Loss:  -5.5175 | Function Loss:  -3.5649\n",
            "Total loss:  -3.5191 | PDE Loss:  -5.5176 | Function Loss:  -3.565\n",
            "Total loss:  -3.5191 | PDE Loss:  -5.5184 | Function Loss:  -3.565\n",
            "Total loss:  -3.5192 | PDE Loss:  -5.5185 | Function Loss:  -3.5651\n",
            "Total loss:  -3.5193 | PDE Loss:  -5.5196 | Function Loss:  -3.565\n",
            "Total loss:  -3.5194 | PDE Loss:  -5.5205 | Function Loss:  -3.5651\n",
            "Total loss:  -3.5195 | PDE Loss:  -5.5211 | Function Loss:  -3.5651\n",
            "Total loss:  -3.5196 | PDE Loss:  -5.5215 | Function Loss:  -3.5651\n",
            "Total loss:  -3.5197 | PDE Loss:  -5.5225 | Function Loss:  -3.5651\n",
            "Total loss:  -3.5197 | PDE Loss:  -5.5228 | Function Loss:  -3.5652\n",
            "Total loss:  -3.5199 | PDE Loss:  -5.5239 | Function Loss:  -3.5652\n",
            "Total loss:  -3.52 | PDE Loss:  -5.5252 | Function Loss:  -3.5652\n",
            "Total loss:  -3.5201 | PDE Loss:  -5.5273 | Function Loss:  -3.5651\n",
            "Total loss:  -3.5202 | PDE Loss:  -5.529 | Function Loss:  -3.565\n",
            "Total loss:  -3.5204 | PDE Loss:  -5.5307 | Function Loss:  -3.565\n",
            "Total loss:  -3.5205 | PDE Loss:  -5.5322 | Function Loss:  -3.565\n",
            "Total loss:  -3.5206 | PDE Loss:  -5.5342 | Function Loss:  -3.5649\n",
            "Total loss:  -3.5208 | PDE Loss:  -5.5351 | Function Loss:  -3.565\n",
            "Total loss:  -3.5207 | PDE Loss:  -5.5367 | Function Loss:  -3.5647\n",
            "Total loss:  -3.5209 | PDE Loss:  -5.5363 | Function Loss:  -3.5649\n",
            "Total loss:  -3.521 | PDE Loss:  -5.5372 | Function Loss:  -3.565\n",
            "Total loss:  -3.5212 | PDE Loss:  -5.5379 | Function Loss:  -3.5651\n",
            "Total loss:  -3.5214 | PDE Loss:  -5.5372 | Function Loss:  -3.5654\n",
            "Total loss:  -3.5215 | PDE Loss:  -5.5378 | Function Loss:  -3.5655\n",
            "Total loss:  -3.5216 | PDE Loss:  -5.5379 | Function Loss:  -3.5656\n",
            "Total loss:  -3.5218 | PDE Loss:  -5.538 | Function Loss:  -3.5658\n",
            "Total loss:  -3.5219 | PDE Loss:  -5.5379 | Function Loss:  -3.5659\n",
            "Total loss:  -3.522 | PDE Loss:  -5.538 | Function Loss:  -3.566\n",
            "Total loss:  -3.5222 | PDE Loss:  -5.5382 | Function Loss:  -3.5662\n",
            "Total loss:  -3.5225 | PDE Loss:  -5.5402 | Function Loss:  -3.5663\n",
            "Total loss:  -3.5226 | PDE Loss:  -5.5372 | Function Loss:  -3.5667\n",
            "Total loss:  -3.5227 | PDE Loss:  -5.5395 | Function Loss:  -3.5667\n",
            "Total loss:  -3.5229 | PDE Loss:  -5.5417 | Function Loss:  -3.5666\n",
            "Total loss:  -3.5231 | PDE Loss:  -5.5431 | Function Loss:  -3.5667\n",
            "Total loss:  -3.5232 | PDE Loss:  -5.5441 | Function Loss:  -3.5667\n",
            "Total loss:  -3.5233 | PDE Loss:  -5.5451 | Function Loss:  -3.5667\n",
            "Total loss:  -3.5234 | PDE Loss:  -5.5443 | Function Loss:  -3.5669\n",
            "Total loss:  -3.5235 | PDE Loss:  -5.5453 | Function Loss:  -3.5669\n",
            "Total loss:  -3.5236 | PDE Loss:  -5.5461 | Function Loss:  -3.5669\n",
            "Total loss:  -3.5237 | PDE Loss:  -5.547 | Function Loss:  -3.5669\n",
            "Total loss:  -3.5238 | PDE Loss:  -5.5477 | Function Loss:  -3.567\n",
            "Total loss:  -3.5239 | PDE Loss:  -5.5483 | Function Loss:  -3.567\n",
            "Total loss:  -3.524 | PDE Loss:  -5.5491 | Function Loss:  -3.567\n",
            "Total loss:  -3.524 | PDE Loss:  -5.5499 | Function Loss:  -3.567\n",
            "Total loss:  -3.5241 | PDE Loss:  -5.5507 | Function Loss:  -3.567\n",
            "Total loss:  -3.5242 | PDE Loss:  -5.5513 | Function Loss:  -3.5671\n",
            "Total loss:  -3.5244 | PDE Loss:  -5.5519 | Function Loss:  -3.5672\n",
            "Total loss:  -3.5245 | PDE Loss:  -5.5525 | Function Loss:  -3.5673\n",
            "Total loss:  -3.5247 | PDE Loss:  -5.553 | Function Loss:  -3.5675\n",
            "Total loss:  -3.525 | PDE Loss:  -5.5545 | Function Loss:  -3.5676\n",
            "Total loss:  -3.5253 | PDE Loss:  -5.5559 | Function Loss:  -3.5677\n",
            "Total loss:  -3.5255 | PDE Loss:  -5.5573 | Function Loss:  -3.5678\n",
            "Total loss:  -3.5256 | PDE Loss:  -5.5579 | Function Loss:  -3.568\n",
            "Total loss:  -3.5258 | PDE Loss:  -5.5587 | Function Loss:  -3.568\n",
            "Total loss:  -3.5258 | PDE Loss:  -5.5585 | Function Loss:  -3.5681\n",
            "Total loss:  -3.5259 | PDE Loss:  -5.5593 | Function Loss:  -3.5682\n",
            "Total loss:  -3.526 | PDE Loss:  -5.5594 | Function Loss:  -3.5682\n",
            "Total loss:  -3.5261 | PDE Loss:  -5.5598 | Function Loss:  -3.5683\n",
            "Total loss:  -3.5262 | PDE Loss:  -5.56 | Function Loss:  -3.5684\n",
            "Total loss:  -3.5263 | PDE Loss:  -5.5605 | Function Loss:  -3.5685\n",
            "Total loss:  -3.5264 | PDE Loss:  -5.5617 | Function Loss:  -3.5685\n",
            "Total loss:  -3.5252 | PDE Loss:  -5.5504 | Function Loss:  -3.5682\n",
            "Total loss:  -3.5265 | PDE Loss:  -5.5608 | Function Loss:  -3.5686\n",
            "Total loss:  -3.5266 | PDE Loss:  -5.5608 | Function Loss:  -3.5687\n",
            "Total loss:  -3.5267 | PDE Loss:  -5.5614 | Function Loss:  -3.5687\n",
            "Total loss:  -3.5268 | PDE Loss:  -5.5625 | Function Loss:  -3.5688\n",
            "Total loss:  -3.5269 | PDE Loss:  -5.5629 | Function Loss:  -3.5688\n",
            "Total loss:  -3.5269 | PDE Loss:  -5.5626 | Function Loss:  -3.5689\n",
            "Total loss:  -3.527 | PDE Loss:  -5.5625 | Function Loss:  -3.569\n",
            "Total loss:  -3.5271 | PDE Loss:  -5.5622 | Function Loss:  -3.5691\n",
            "Total loss:  -3.5272 | PDE Loss:  -5.5616 | Function Loss:  -3.5693\n",
            "Total loss:  -3.5273 | PDE Loss:  -5.5607 | Function Loss:  -3.5695\n",
            "Total loss:  -3.5274 | PDE Loss:  -5.5597 | Function Loss:  -3.5697\n",
            "Total loss:  -3.5276 | PDE Loss:  -5.5588 | Function Loss:  -3.57\n",
            "Total loss:  -3.5277 | PDE Loss:  -5.5566 | Function Loss:  -3.5704\n",
            "Total loss:  -3.5279 | PDE Loss:  -5.5569 | Function Loss:  -3.5706\n",
            "Total loss:  -3.5281 | PDE Loss:  -5.5568 | Function Loss:  -3.5708\n",
            "Total loss:  -3.5283 | PDE Loss:  -5.5571 | Function Loss:  -3.571\n",
            "Total loss:  -3.5285 | PDE Loss:  -5.5571 | Function Loss:  -3.5712\n",
            "Total loss:  -3.5288 | PDE Loss:  -5.5568 | Function Loss:  -3.5715\n",
            "Total loss:  -3.529 | PDE Loss:  -5.5566 | Function Loss:  -3.5718\n",
            "Total loss:  -3.5291 | PDE Loss:  -5.5563 | Function Loss:  -3.572\n",
            "Total loss:  -3.5293 | PDE Loss:  -5.5543 | Function Loss:  -3.5723\n",
            "Total loss:  -3.5293 | PDE Loss:  -5.5546 | Function Loss:  -3.5724\n",
            "Total loss:  -3.5294 | PDE Loss:  -5.5547 | Function Loss:  -3.5725\n",
            "Total loss:  -3.5296 | PDE Loss:  -5.5554 | Function Loss:  -3.5725\n",
            "Total loss:  -3.5297 | PDE Loss:  -5.556 | Function Loss:  -3.5726\n",
            "Total loss:  -3.5298 | PDE Loss:  -5.5573 | Function Loss:  -3.5726\n",
            "Total loss:  -3.5299 | PDE Loss:  -5.5592 | Function Loss:  -3.5726\n",
            "Total loss:  -3.5301 | PDE Loss:  -5.5609 | Function Loss:  -3.5725\n",
            "Total loss:  -3.5302 | PDE Loss:  -5.5635 | Function Loss:  -3.5724\n",
            "Total loss:  -3.5303 | PDE Loss:  -5.5654 | Function Loss:  -3.5724\n",
            "Total loss:  -3.5305 | PDE Loss:  -5.5677 | Function Loss:  -3.5723\n",
            "Total loss:  -3.5306 | PDE Loss:  -5.5696 | Function Loss:  -3.5723\n",
            "Total loss:  -3.5308 | PDE Loss:  -5.5705 | Function Loss:  -3.5723\n",
            "Total loss:  -3.531 | PDE Loss:  -5.571 | Function Loss:  -3.5725\n",
            "Total loss:  -3.5311 | PDE Loss:  -5.571 | Function Loss:  -3.5726\n",
            "Total loss:  -3.5313 | PDE Loss:  -5.5717 | Function Loss:  -3.5728\n",
            "Total loss:  -3.5314 | PDE Loss:  -5.5715 | Function Loss:  -3.5729\n",
            "Total loss:  -3.5315 | PDE Loss:  -5.5722 | Function Loss:  -3.573\n",
            "Total loss:  -3.5317 | PDE Loss:  -5.5725 | Function Loss:  -3.5732\n",
            "Total loss:  -3.5319 | PDE Loss:  -5.5747 | Function Loss:  -3.5732\n",
            "Total loss:  -3.5321 | PDE Loss:  -5.5753 | Function Loss:  -3.5733\n",
            "Total loss:  -3.5322 | PDE Loss:  -5.5769 | Function Loss:  -3.5733\n",
            "Total loss:  -3.5324 | PDE Loss:  -5.5781 | Function Loss:  -3.5733\n",
            "Total loss:  -3.5326 | PDE Loss:  -5.5799 | Function Loss:  -3.5734\n",
            "Total loss:  -3.5327 | PDE Loss:  -5.5816 | Function Loss:  -3.5734\n",
            "Total loss:  -3.5329 | PDE Loss:  -5.5832 | Function Loss:  -3.5734\n",
            "Total loss:  -3.5331 | PDE Loss:  -5.585 | Function Loss:  -3.5734\n",
            "Total loss:  -3.5333 | PDE Loss:  -5.5864 | Function Loss:  -3.5735\n",
            "Total loss:  -3.5334 | PDE Loss:  -5.5874 | Function Loss:  -3.5736\n",
            "Total loss:  -3.5336 | PDE Loss:  -5.588 | Function Loss:  -3.5737\n",
            "Total loss:  -3.5337 | PDE Loss:  -5.5887 | Function Loss:  -3.5738\n",
            "Total loss:  -3.5338 | PDE Loss:  -5.5892 | Function Loss:  -3.5738\n",
            "Total loss:  -3.5339 | PDE Loss:  -5.5899 | Function Loss:  -3.5739\n",
            "Total loss:  -3.534 | PDE Loss:  -5.5901 | Function Loss:  -3.574\n",
            "Total loss:  -3.5341 | PDE Loss:  -5.5901 | Function Loss:  -3.5741\n",
            "Total loss:  -3.5342 | PDE Loss:  -5.5904 | Function Loss:  -3.5742\n",
            "Total loss:  -3.5343 | PDE Loss:  -5.5908 | Function Loss:  -3.5743\n",
            "Total loss:  -3.5345 | PDE Loss:  -5.5906 | Function Loss:  -3.5744\n",
            "Total loss:  -3.5346 | PDE Loss:  -5.5906 | Function Loss:  -3.5745\n",
            "Total loss:  -3.5347 | PDE Loss:  -5.5905 | Function Loss:  -3.5747\n",
            "Total loss:  -3.5348 | PDE Loss:  -5.5897 | Function Loss:  -3.5748\n",
            "Total loss:  -3.5349 | PDE Loss:  -5.5895 | Function Loss:  -3.575\n",
            "Total loss:  -3.535 | PDE Loss:  -5.5885 | Function Loss:  -3.5752\n",
            "Total loss:  -3.5351 | PDE Loss:  -5.5873 | Function Loss:  -3.5755\n",
            "Total loss:  -3.5353 | PDE Loss:  -5.587 | Function Loss:  -3.5756\n",
            "Total loss:  -3.5354 | PDE Loss:  -5.5867 | Function Loss:  -3.5758\n",
            "Total loss:  -3.5355 | PDE Loss:  -5.587 | Function Loss:  -3.5759\n",
            "Total loss:  -3.5356 | PDE Loss:  -5.5876 | Function Loss:  -3.5759\n",
            "Total loss:  -3.5357 | PDE Loss:  -5.5889 | Function Loss:  -3.5759\n",
            "Total loss:  -3.5358 | PDE Loss:  -5.5904 | Function Loss:  -3.5759\n",
            "Total loss:  -3.5359 | PDE Loss:  -5.5923 | Function Loss:  -3.5758\n",
            "Total loss:  -3.536 | PDE Loss:  -5.5953 | Function Loss:  -3.5756\n",
            "Total loss:  -3.5361 | PDE Loss:  -5.5969 | Function Loss:  -3.5757\n",
            "Total loss:  -3.5363 | PDE Loss:  -5.5993 | Function Loss:  -3.5756\n",
            "Total loss:  -3.5364 | PDE Loss:  -5.6003 | Function Loss:  -3.5756\n",
            "Total loss:  -3.5365 | PDE Loss:  -5.6004 | Function Loss:  -3.5757\n",
            "Total loss:  -3.5366 | PDE Loss:  -5.6006 | Function Loss:  -3.5757\n",
            "Total loss:  -3.5367 | PDE Loss:  -5.6009 | Function Loss:  -3.5758\n",
            "Total loss:  -3.5368 | PDE Loss:  -5.6015 | Function Loss:  -3.5759\n",
            "Total loss:  -3.5369 | PDE Loss:  -5.602 | Function Loss:  -3.5759\n",
            "Total loss:  -3.537 | PDE Loss:  -5.6033 | Function Loss:  -3.5759\n",
            "Total loss:  -3.5371 | PDE Loss:  -5.6046 | Function Loss:  -3.5759\n",
            "Total loss:  -3.5371 | PDE Loss:  -5.6056 | Function Loss:  -3.5759\n",
            "Total loss:  -3.5373 | PDE Loss:  -5.6074 | Function Loss:  -3.5759\n",
            "Total loss:  -3.5374 | PDE Loss:  -5.6082 | Function Loss:  -3.5759\n",
            "Total loss:  -3.5375 | PDE Loss:  -5.6088 | Function Loss:  -3.576\n",
            "Total loss:  -3.5376 | PDE Loss:  -5.6091 | Function Loss:  -3.576\n",
            "Total loss:  -3.5377 | PDE Loss:  -5.6095 | Function Loss:  -3.5761\n",
            "Total loss:  -3.5377 | PDE Loss:  -5.6094 | Function Loss:  -3.5762\n",
            "Total loss:  -3.5379 | PDE Loss:  -5.6093 | Function Loss:  -3.5764\n",
            "Total loss:  -3.538 | PDE Loss:  -5.6088 | Function Loss:  -3.5765\n",
            "Total loss:  -3.5381 | PDE Loss:  -5.6082 | Function Loss:  -3.5767\n",
            "Total loss:  -3.5382 | PDE Loss:  -5.6081 | Function Loss:  -3.5769\n",
            "Total loss:  -3.5384 | PDE Loss:  -5.6071 | Function Loss:  -3.5771\n",
            "Total loss:  -3.5385 | PDE Loss:  -5.607 | Function Loss:  -3.5772\n",
            "Total loss:  -3.5386 | PDE Loss:  -5.6075 | Function Loss:  -3.5773\n",
            "Total loss:  -3.5387 | PDE Loss:  -5.6083 | Function Loss:  -3.5773\n",
            "Total loss:  -3.5388 | PDE Loss:  -5.6098 | Function Loss:  -3.5773\n",
            "Total loss:  -3.5388 | PDE Loss:  -5.6106 | Function Loss:  -3.5773\n",
            "Total loss:  -3.5389 | PDE Loss:  -5.6119 | Function Loss:  -3.5773\n",
            "Total loss:  -3.539 | PDE Loss:  -5.6127 | Function Loss:  -3.5773\n",
            "Total loss:  -3.5391 | PDE Loss:  -5.6162 | Function Loss:  -3.5771\n",
            "Total loss:  -3.5392 | PDE Loss:  -5.6158 | Function Loss:  -3.5773\n",
            "Total loss:  -3.5394 | PDE Loss:  -5.6145 | Function Loss:  -3.5775\n",
            "Total loss:  -3.5395 | PDE Loss:  -5.6137 | Function Loss:  -3.5777\n",
            "Total loss:  -3.5396 | PDE Loss:  -5.6133 | Function Loss:  -3.5779\n",
            "Total loss:  -3.5398 | PDE Loss:  -5.6132 | Function Loss:  -3.5781\n",
            "Total loss:  -3.5399 | PDE Loss:  -5.6129 | Function Loss:  -3.5783\n",
            "Total loss:  -3.54 | PDE Loss:  -5.6132 | Function Loss:  -3.5783\n",
            "Total loss:  -3.5402 | PDE Loss:  -5.6138 | Function Loss:  -3.5785\n",
            "Total loss:  -3.5403 | PDE Loss:  -5.6145 | Function Loss:  -3.5785\n",
            "Total loss:  -3.5404 | PDE Loss:  -5.6157 | Function Loss:  -3.5786\n",
            "Total loss:  -3.5406 | PDE Loss:  -5.6167 | Function Loss:  -3.5786\n",
            "Total loss:  -3.5408 | PDE Loss:  -5.618 | Function Loss:  -3.5787\n",
            "Total loss:  -3.5409 | PDE Loss:  -5.6187 | Function Loss:  -3.5788\n",
            "Total loss:  -3.541 | PDE Loss:  -5.6192 | Function Loss:  -3.5789\n",
            "Total loss:  -3.5411 | PDE Loss:  -5.6193 | Function Loss:  -3.579\n",
            "Total loss:  -3.5412 | PDE Loss:  -5.6189 | Function Loss:  -3.5792\n",
            "Total loss:  -3.5413 | PDE Loss:  -5.6183 | Function Loss:  -3.5793\n",
            "Total loss:  -3.5414 | PDE Loss:  -5.6175 | Function Loss:  -3.5795\n",
            "Total loss:  -3.5415 | PDE Loss:  -5.617 | Function Loss:  -3.5796\n",
            "Total loss:  -3.5415 | PDE Loss:  -5.6127 | Function Loss:  -3.58\n",
            "Total loss:  -3.5415 | PDE Loss:  -5.6153 | Function Loss:  -3.5798\n",
            "Total loss:  -3.5416 | PDE Loss:  -5.6154 | Function Loss:  -3.5799\n",
            "Total loss:  -3.5417 | PDE Loss:  -5.6154 | Function Loss:  -3.58\n",
            "Total loss:  -3.5419 | PDE Loss:  -5.6161 | Function Loss:  -3.5801\n",
            "Total loss:  -3.542 | PDE Loss:  -5.6167 | Function Loss:  -3.5802\n",
            "Total loss:  -3.5422 | PDE Loss:  -5.6182 | Function Loss:  -3.5803\n",
            "Total loss:  -3.5424 | PDE Loss:  -5.6195 | Function Loss:  -3.5803\n",
            "Total loss:  -3.5425 | PDE Loss:  -5.6214 | Function Loss:  -3.5803\n",
            "Total loss:  -3.5426 | PDE Loss:  -5.6222 | Function Loss:  -3.5804\n",
            "Total loss:  -3.5428 | PDE Loss:  -5.6234 | Function Loss:  -3.5804\n",
            "Total loss:  -3.5429 | PDE Loss:  -5.6251 | Function Loss:  -3.5805\n",
            "Total loss:  -3.543 | PDE Loss:  -5.6249 | Function Loss:  -3.5805\n",
            "Total loss:  -3.5431 | PDE Loss:  -5.6249 | Function Loss:  -3.5806\n",
            "Total loss:  -3.5431 | PDE Loss:  -5.624 | Function Loss:  -3.5808\n",
            "Total loss:  -3.5432 | PDE Loss:  -5.6234 | Function Loss:  -3.5809\n",
            "Total loss:  -3.5432 | PDE Loss:  -5.6231 | Function Loss:  -3.581\n",
            "Total loss:  -3.5433 | PDE Loss:  -5.6225 | Function Loss:  -3.5811\n",
            "Total loss:  -3.5434 | PDE Loss:  -5.6222 | Function Loss:  -3.5812\n",
            "Total loss:  -3.5435 | PDE Loss:  -5.622 | Function Loss:  -3.5814\n",
            "Total loss:  -3.5436 | PDE Loss:  -5.6217 | Function Loss:  -3.5815\n",
            "Total loss:  -3.5437 | PDE Loss:  -5.621 | Function Loss:  -3.5816\n",
            "Total loss:  -3.5438 | PDE Loss:  -5.6208 | Function Loss:  -3.5818\n",
            "Total loss:  -3.5439 | PDE Loss:  -5.6208 | Function Loss:  -3.5819\n",
            "Total loss:  -3.544 | PDE Loss:  -5.6207 | Function Loss:  -3.582\n",
            "Total loss:  -3.5441 | PDE Loss:  -5.6208 | Function Loss:  -3.5821\n",
            "Total loss:  -3.5442 | PDE Loss:  -5.6211 | Function Loss:  -3.5821\n",
            "Total loss:  -3.5442 | PDE Loss:  -5.6218 | Function Loss:  -3.5821\n",
            "Total loss:  -3.5443 | PDE Loss:  -5.6222 | Function Loss:  -3.5822\n",
            "Total loss:  -3.5443 | PDE Loss:  -5.6232 | Function Loss:  -3.5822\n",
            "Total loss:  -3.5444 | PDE Loss:  -5.6243 | Function Loss:  -3.5821\n",
            "Total loss:  -3.5445 | PDE Loss:  -5.6256 | Function Loss:  -3.5821\n",
            "Total loss:  -3.5446 | PDE Loss:  -5.6265 | Function Loss:  -3.5821\n",
            "Total loss:  -3.5446 | PDE Loss:  -5.6272 | Function Loss:  -3.5821\n",
            "Total loss:  -3.5447 | PDE Loss:  -5.6274 | Function Loss:  -3.5821\n",
            "Total loss:  -3.5447 | PDE Loss:  -5.6275 | Function Loss:  -3.5822\n",
            "Total loss:  -3.5448 | PDE Loss:  -5.6272 | Function Loss:  -3.5823\n",
            "Total loss:  -3.5449 | PDE Loss:  -5.6268 | Function Loss:  -3.5824\n",
            "Total loss:  -3.545 | PDE Loss:  -5.6258 | Function Loss:  -3.5826\n",
            "Total loss:  -3.5451 | PDE Loss:  -5.6256 | Function Loss:  -3.5828\n",
            "Total loss:  -3.5453 | PDE Loss:  -5.6227 | Function Loss:  -3.5833\n",
            "Total loss:  -3.5454 | PDE Loss:  -5.6226 | Function Loss:  -3.5834\n",
            "Total loss:  -3.5457 | PDE Loss:  -5.6223 | Function Loss:  -3.5837\n",
            "Total loss:  -3.5459 | PDE Loss:  -5.6228 | Function Loss:  -3.5839\n",
            "Total loss:  -3.5461 | PDE Loss:  -5.6206 | Function Loss:  -3.5843\n",
            "Total loss:  -3.5462 | PDE Loss:  -5.6212 | Function Loss:  -3.5844\n",
            "Total loss:  -3.5463 | PDE Loss:  -5.6209 | Function Loss:  -3.5846\n",
            "Total loss:  -3.5465 | PDE Loss:  -5.6201 | Function Loss:  -3.5848\n",
            "Total loss:  -3.5466 | PDE Loss:  -5.6197 | Function Loss:  -3.5849\n",
            "Total loss:  -3.5467 | PDE Loss:  -5.619 | Function Loss:  -3.5851\n",
            "Total loss:  -3.5468 | PDE Loss:  -5.6182 | Function Loss:  -3.5853\n",
            "Total loss:  -3.5469 | PDE Loss:  -5.6169 | Function Loss:  -3.5855\n",
            "Total loss:  -3.5469 | PDE Loss:  -5.6163 | Function Loss:  -3.5856\n",
            "Total loss:  -3.547 | PDE Loss:  -5.6155 | Function Loss:  -3.5858\n",
            "Total loss:  -3.5472 | PDE Loss:  -5.6147 | Function Loss:  -3.5861\n",
            "Total loss:  -3.5474 | PDE Loss:  -5.6143 | Function Loss:  -3.5863\n",
            "Total loss:  -3.5475 | PDE Loss:  -5.614 | Function Loss:  -3.5865\n",
            "Total loss:  -3.5477 | PDE Loss:  -5.614 | Function Loss:  -3.5866\n",
            "Total loss:  -3.5478 | PDE Loss:  -5.6143 | Function Loss:  -3.5867\n",
            "Total loss:  -3.5479 | PDE Loss:  -5.6146 | Function Loss:  -3.5868\n",
            "Total loss:  -3.548 | PDE Loss:  -5.6148 | Function Loss:  -3.5869\n",
            "Total loss:  -3.5481 | PDE Loss:  -5.6148 | Function Loss:  -3.587\n",
            "Total loss:  -3.5482 | PDE Loss:  -5.614 | Function Loss:  -3.5873\n",
            "Total loss:  -3.5484 | PDE Loss:  -5.6133 | Function Loss:  -3.5875\n",
            "Total loss:  -3.5486 | PDE Loss:  -5.6124 | Function Loss:  -3.5879\n",
            "Total loss:  -3.5488 | PDE Loss:  -5.608 | Function Loss:  -3.5884\n",
            "Total loss:  -3.5489 | PDE Loss:  -5.6086 | Function Loss:  -3.5885\n",
            "Total loss:  -3.5491 | PDE Loss:  -5.6091 | Function Loss:  -3.5886\n",
            "Total loss:  -3.5492 | PDE Loss:  -5.609 | Function Loss:  -3.5888\n",
            "Total loss:  -3.5493 | PDE Loss:  -5.6088 | Function Loss:  -3.589\n",
            "Total loss:  -3.5495 | PDE Loss:  -5.6082 | Function Loss:  -3.5892\n",
            "Total loss:  -3.5496 | PDE Loss:  -5.6083 | Function Loss:  -3.5893\n",
            "Total loss:  -3.5498 | PDE Loss:  -5.6076 | Function Loss:  -3.5896\n",
            "Total loss:  -3.5499 | PDE Loss:  -5.6075 | Function Loss:  -3.5897\n",
            "Total loss:  -3.55 | PDE Loss:  -5.6005 | Function Loss:  -3.5905\n",
            "Total loss:  -3.5503 | PDE Loss:  -5.6033 | Function Loss:  -3.5905\n",
            "Total loss:  -3.5505 | PDE Loss:  -5.6049 | Function Loss:  -3.5906\n",
            "Total loss:  -3.5506 | PDE Loss:  -5.6066 | Function Loss:  -3.5906\n",
            "Total loss:  -3.5508 | PDE Loss:  -5.607 | Function Loss:  -3.5907\n",
            "Total loss:  -3.5509 | PDE Loss:  -5.607 | Function Loss:  -3.5908\n",
            "Total loss:  -3.551 | PDE Loss:  -5.6067 | Function Loss:  -3.591\n",
            "Total loss:  -3.5512 | PDE Loss:  -5.6052 | Function Loss:  -3.5913\n",
            "Total loss:  -3.5513 | PDE Loss:  -5.6042 | Function Loss:  -3.5916\n",
            "Total loss:  -3.5514 | PDE Loss:  -5.6029 | Function Loss:  -3.5918\n",
            "Total loss:  -3.5515 | PDE Loss:  -5.6018 | Function Loss:  -3.592\n",
            "Total loss:  -3.5516 | PDE Loss:  -5.5996 | Function Loss:  -3.5924\n",
            "Total loss:  -3.5517 | PDE Loss:  -5.5997 | Function Loss:  -3.5925\n",
            "Total loss:  -3.5518 | PDE Loss:  -5.5992 | Function Loss:  -3.5926\n",
            "Total loss:  -3.5519 | PDE Loss:  -5.5991 | Function Loss:  -3.5928\n",
            "Total loss:  -3.5522 | PDE Loss:  -5.5984 | Function Loss:  -3.5931\n",
            "Total loss:  -3.5523 | PDE Loss:  -5.5986 | Function Loss:  -3.5932\n",
            "Total loss:  -3.5525 | PDE Loss:  -5.5991 | Function Loss:  -3.5934\n",
            "Total loss:  -3.5527 | PDE Loss:  -5.6003 | Function Loss:  -3.5935\n",
            "Total loss:  -3.5529 | PDE Loss:  -5.6008 | Function Loss:  -3.5936\n",
            "Total loss:  -3.5529 | PDE Loss:  -5.6017 | Function Loss:  -3.5936\n",
            "Total loss:  -3.553 | PDE Loss:  -5.6018 | Function Loss:  -3.5937\n",
            "Total loss:  -3.5531 | PDE Loss:  -5.6027 | Function Loss:  -3.5937\n",
            "Total loss:  -3.5532 | PDE Loss:  -5.6028 | Function Loss:  -3.5937\n",
            "Total loss:  -3.5533 | PDE Loss:  -5.6033 | Function Loss:  -3.5938\n",
            "Total loss:  -3.5534 | PDE Loss:  -5.603 | Function Loss:  -3.594\n",
            "Total loss:  -3.5535 | PDE Loss:  -5.6035 | Function Loss:  -3.594\n",
            "Total loss:  -3.5536 | PDE Loss:  -5.6027 | Function Loss:  -3.5943\n",
            "Total loss:  -3.5537 | PDE Loss:  -5.6025 | Function Loss:  -3.5944\n",
            "Total loss:  -3.5538 | PDE Loss:  -5.602 | Function Loss:  -3.5945\n",
            "Total loss:  -3.554 | PDE Loss:  -5.6014 | Function Loss:  -3.5947\n",
            "Total loss:  -3.5541 | PDE Loss:  -5.6008 | Function Loss:  -3.5949\n",
            "Total loss:  -3.5542 | PDE Loss:  -5.6006 | Function Loss:  -3.5951\n",
            "Total loss:  -3.5545 | PDE Loss:  -5.5987 | Function Loss:  -3.5956\n",
            "Total loss:  -3.5547 | PDE Loss:  -5.5979 | Function Loss:  -3.596\n",
            "Total loss:  -3.555 | PDE Loss:  -5.5973 | Function Loss:  -3.5963\n",
            "Total loss:  -3.5552 | PDE Loss:  -5.5949 | Function Loss:  -3.5968\n",
            "Total loss:  -3.5555 | PDE Loss:  -5.5963 | Function Loss:  -3.5969\n",
            "Total loss:  -3.5557 | PDE Loss:  -5.5967 | Function Loss:  -3.5971\n",
            "Total loss:  -3.5559 | PDE Loss:  -5.5973 | Function Loss:  -3.5972\n",
            "Total loss:  -3.556 | PDE Loss:  -5.5972 | Function Loss:  -3.5974\n",
            "Total loss:  -3.5561 | PDE Loss:  -5.5972 | Function Loss:  -3.5975\n",
            "Total loss:  -3.5564 | PDE Loss:  -5.5977 | Function Loss:  -3.5978\n",
            "Total loss:  -3.5566 | PDE Loss:  -5.5972 | Function Loss:  -3.5981\n",
            "Total loss:  -3.557 | PDE Loss:  -5.5979 | Function Loss:  -3.5984\n",
            "Total loss:  -3.5574 | PDE Loss:  -5.5992 | Function Loss:  -3.5987\n",
            "Total loss:  -3.5577 | PDE Loss:  -5.5991 | Function Loss:  -3.5991\n",
            "Total loss:  -3.5581 | PDE Loss:  -5.6028 | Function Loss:  -3.5991\n",
            "Total loss:  -3.5583 | PDE Loss:  -5.6031 | Function Loss:  -3.5994\n",
            "Total loss:  -3.5586 | PDE Loss:  -5.605 | Function Loss:  -3.5995\n",
            "Total loss:  -3.5589 | PDE Loss:  -5.6071 | Function Loss:  -3.5996\n",
            "Total loss:  -3.5593 | PDE Loss:  -5.6068 | Function Loss:  -3.6\n",
            "Total loss:  -3.5595 | PDE Loss:  -5.6092 | Function Loss:  -3.6001\n",
            "Total loss:  -3.56 | PDE Loss:  -5.6108 | Function Loss:  -3.6004\n",
            "Total loss:  -3.5605 | PDE Loss:  -5.6124 | Function Loss:  -3.6008\n",
            "Total loss:  -3.561 | PDE Loss:  -5.615 | Function Loss:  -3.6011\n",
            "Total loss:  -3.5614 | PDE Loss:  -5.6139 | Function Loss:  -3.6017\n",
            "Total loss:  -3.5617 | PDE Loss:  -5.6141 | Function Loss:  -3.602\n",
            "Total loss:  -3.562 | PDE Loss:  -5.6131 | Function Loss:  -3.6025\n",
            "Total loss:  -3.5623 | PDE Loss:  -5.6118 | Function Loss:  -3.6029\n",
            "Total loss:  -3.5625 | PDE Loss:  -5.6095 | Function Loss:  -3.6034\n",
            "Total loss:  -3.5627 | PDE Loss:  -5.609 | Function Loss:  -3.6036\n",
            "Total loss:  -3.5629 | PDE Loss:  -5.6072 | Function Loss:  -3.604\n",
            "Total loss:  -3.5631 | PDE Loss:  -5.6069 | Function Loss:  -3.6042\n",
            "Total loss:  -3.5634 | PDE Loss:  -5.6057 | Function Loss:  -3.6047\n",
            "Total loss:  -3.5638 | PDE Loss:  -5.6064 | Function Loss:  -3.605\n",
            "Total loss:  -3.564 | PDE Loss:  -5.6065 | Function Loss:  -3.6053\n",
            "Total loss:  -3.5644 | PDE Loss:  -5.6067 | Function Loss:  -3.6057\n",
            "Total loss:  -3.5649 | PDE Loss:  -5.6095 | Function Loss:  -3.6059\n",
            "Total loss:  -3.5652 | PDE Loss:  -5.6101 | Function Loss:  -3.6062\n",
            "Total loss:  -3.5655 | PDE Loss:  -5.611 | Function Loss:  -3.6065\n",
            "Total loss:  -3.5659 | PDE Loss:  -5.6108 | Function Loss:  -3.6069\n",
            "Total loss:  -3.5662 | PDE Loss:  -5.6082 | Function Loss:  -3.6075\n",
            "Total loss:  -3.5665 | PDE Loss:  -5.6077 | Function Loss:  -3.6079\n",
            "Total loss:  -3.5667 | PDE Loss:  -5.6063 | Function Loss:  -3.6082\n",
            "Total loss:  -3.5669 | PDE Loss:  -5.6051 | Function Loss:  -3.6086\n",
            "Total loss:  -3.5671 | PDE Loss:  -5.6033 | Function Loss:  -3.609\n",
            "Total loss:  -3.5672 | PDE Loss:  -5.6018 | Function Loss:  -3.6093\n",
            "Total loss:  -3.5674 | PDE Loss:  -5.6002 | Function Loss:  -3.6096\n",
            "Total loss:  -3.5675 | PDE Loss:  -5.5988 | Function Loss:  -3.6099\n",
            "Total loss:  -3.5677 | PDE Loss:  -5.5973 | Function Loss:  -3.6103\n",
            "Total loss:  -3.5677 | PDE Loss:  -5.5936 | Function Loss:  -3.6107\n",
            "Total loss:  -3.5679 | PDE Loss:  -5.5938 | Function Loss:  -3.6109\n",
            "Total loss:  -3.5681 | PDE Loss:  -5.5942 | Function Loss:  -3.611\n",
            "Total loss:  -3.5682 | PDE Loss:  -5.5942 | Function Loss:  -3.6112\n",
            "Total loss:  -3.5683 | PDE Loss:  -5.5944 | Function Loss:  -3.6112\n",
            "Total loss:  -3.5684 | PDE Loss:  -5.5946 | Function Loss:  -3.6113\n",
            "Total loss:  -3.5685 | PDE Loss:  -5.5942 | Function Loss:  -3.6114\n",
            "Total loss:  -3.5686 | PDE Loss:  -5.5939 | Function Loss:  -3.6116\n",
            "Total loss:  -3.5686 | PDE Loss:  -5.5929 | Function Loss:  -3.6117\n",
            "Total loss:  -3.5687 | PDE Loss:  -5.5919 | Function Loss:  -3.612\n",
            "Total loss:  -3.5688 | PDE Loss:  -5.5902 | Function Loss:  -3.6122\n",
            "Total loss:  -3.5689 | PDE Loss:  -5.5883 | Function Loss:  -3.6126\n",
            "Total loss:  -3.569 | PDE Loss:  -5.5859 | Function Loss:  -3.6129\n",
            "Total loss:  -3.5691 | PDE Loss:  -5.5838 | Function Loss:  -3.6133\n",
            "Total loss:  -3.5692 | PDE Loss:  -5.581 | Function Loss:  -3.6137\n",
            "Total loss:  -3.5694 | PDE Loss:  -5.5786 | Function Loss:  -3.6141\n",
            "Total loss:  -3.5696 | PDE Loss:  -5.5755 | Function Loss:  -3.6146\n",
            "Total loss:  -3.5697 | PDE Loss:  -5.5733 | Function Loss:  -3.6151\n",
            "Total loss:  -3.5699 | PDE Loss:  -5.5705 | Function Loss:  -3.6155\n",
            "Total loss:  -3.57 | PDE Loss:  -5.5688 | Function Loss:  -3.6159\n",
            "Total loss:  -3.5702 | PDE Loss:  -5.5665 | Function Loss:  -3.6163\n",
            "Total loss:  -3.5703 | PDE Loss:  -5.5655 | Function Loss:  -3.6166\n",
            "Total loss:  -3.5704 | PDE Loss:  -5.565 | Function Loss:  -3.6168\n",
            "Total loss:  -3.5705 | PDE Loss:  -5.5649 | Function Loss:  -3.6169\n",
            "Total loss:  -3.5707 | PDE Loss:  -5.5649 | Function Loss:  -3.6171\n",
            "Total loss:  -3.5708 | PDE Loss:  -5.5657 | Function Loss:  -3.6171\n",
            "Total loss:  -3.571 | PDE Loss:  -5.5662 | Function Loss:  -3.6173\n",
            "Total loss:  -3.5712 | PDE Loss:  -5.5675 | Function Loss:  -3.6173\n",
            "Total loss:  -3.5714 | PDE Loss:  -5.5688 | Function Loss:  -3.6175\n",
            "Total loss:  -3.5716 | PDE Loss:  -5.5689 | Function Loss:  -3.6177\n",
            "Total loss:  -3.5719 | PDE Loss:  -5.5727 | Function Loss:  -3.6176\n",
            "Total loss:  -3.5721 | PDE Loss:  -5.5736 | Function Loss:  -3.6177\n",
            "Total loss:  -3.5723 | PDE Loss:  -5.5739 | Function Loss:  -3.6179\n",
            "Total loss:  -3.5724 | PDE Loss:  -5.5738 | Function Loss:  -3.618\n",
            "Total loss:  -3.5725 | PDE Loss:  -5.5738 | Function Loss:  -3.6181\n",
            "Total loss:  -3.5726 | PDE Loss:  -5.5736 | Function Loss:  -3.6182\n",
            "Total loss:  -3.5727 | PDE Loss:  -5.5739 | Function Loss:  -3.6183\n",
            "Total loss:  -3.5729 | PDE Loss:  -5.5747 | Function Loss:  -3.6184\n",
            "Total loss:  -3.5731 | PDE Loss:  -5.5754 | Function Loss:  -3.6186\n",
            "Total loss:  -3.5734 | PDE Loss:  -5.5764 | Function Loss:  -3.6188\n",
            "Total loss:  -3.5737 | PDE Loss:  -5.5776 | Function Loss:  -3.619\n",
            "Total loss:  -3.5739 | PDE Loss:  -5.579 | Function Loss:  -3.6191\n",
            "Total loss:  -3.5741 | PDE Loss:  -5.5806 | Function Loss:  -3.6191\n",
            "Total loss:  -3.5743 | PDE Loss:  -5.5817 | Function Loss:  -3.6192\n",
            "Total loss:  -3.5744 | PDE Loss:  -5.5828 | Function Loss:  -3.6192\n",
            "Total loss:  -3.5743 | PDE Loss:  -5.5819 | Function Loss:  -3.6193\n",
            "Total loss:  -3.5744 | PDE Loss:  -5.5826 | Function Loss:  -3.6193\n",
            "Total loss:  -3.5745 | PDE Loss:  -5.5827 | Function Loss:  -3.6193\n",
            "Total loss:  -3.5746 | PDE Loss:  -5.5825 | Function Loss:  -3.6195\n",
            "Total loss:  -3.5748 | PDE Loss:  -5.5815 | Function Loss:  -3.6198\n",
            "Total loss:  -3.5749 | PDE Loss:  -5.5804 | Function Loss:  -3.62\n",
            "Total loss:  -3.5749 | PDE Loss:  -5.5786 | Function Loss:  -3.6203\n",
            "Total loss:  -3.575 | PDE Loss:  -5.5774 | Function Loss:  -3.6205\n",
            "Total loss:  -3.5751 | PDE Loss:  -5.5762 | Function Loss:  -3.6207\n",
            "Total loss:  -3.5751 | PDE Loss:  -5.5756 | Function Loss:  -3.6208\n",
            "Total loss:  -3.5752 | PDE Loss:  -5.5748 | Function Loss:  -3.621\n",
            "Total loss:  -3.5753 | PDE Loss:  -5.5744 | Function Loss:  -3.6211\n",
            "Total loss:  -3.5754 | PDE Loss:  -5.5747 | Function Loss:  -3.6212\n",
            "Total loss:  -3.5755 | PDE Loss:  -5.5749 | Function Loss:  -3.6214\n",
            "Total loss:  -3.5757 | PDE Loss:  -5.5778 | Function Loss:  -3.6212\n",
            "Total loss:  -3.5758 | PDE Loss:  -5.5781 | Function Loss:  -3.6213\n",
            "Total loss:  -3.576 | PDE Loss:  -5.5786 | Function Loss:  -3.6215\n",
            "Total loss:  -3.5762 | PDE Loss:  -5.5794 | Function Loss:  -3.6216\n",
            "Total loss:  -3.5763 | PDE Loss:  -5.581 | Function Loss:  -3.6216\n",
            "Total loss:  -3.5766 | PDE Loss:  -5.582 | Function Loss:  -3.6217\n",
            "Total loss:  -3.5767 | PDE Loss:  -5.5829 | Function Loss:  -3.6218\n",
            "Total loss:  -3.5769 | PDE Loss:  -5.5842 | Function Loss:  -3.6218\n",
            "Total loss:  -3.577 | PDE Loss:  -5.5852 | Function Loss:  -3.6218\n",
            "Total loss:  -3.5771 | PDE Loss:  -5.5857 | Function Loss:  -3.6219\n",
            "Total loss:  -3.5772 | PDE Loss:  -5.5879 | Function Loss:  -3.6218\n",
            "Total loss:  -3.5774 | PDE Loss:  -5.5882 | Function Loss:  -3.6219\n",
            "Total loss:  -3.5776 | PDE Loss:  -5.5885 | Function Loss:  -3.6221\n",
            "Total loss:  -3.5778 | PDE Loss:  -5.5884 | Function Loss:  -3.6224\n",
            "Total loss:  -3.5781 | PDE Loss:  -5.5887 | Function Loss:  -3.6227\n",
            "Total loss:  -3.5783 | PDE Loss:  -5.59 | Function Loss:  -3.6228\n",
            "Total loss:  -3.5785 | PDE Loss:  -5.5909 | Function Loss:  -3.6229\n",
            "Total loss:  -3.5787 | PDE Loss:  -5.5922 | Function Loss:  -3.6229\n",
            "Total loss:  -3.5787 | PDE Loss:  -5.5932 | Function Loss:  -3.6229\n",
            "Total loss:  -3.5788 | PDE Loss:  -5.5945 | Function Loss:  -3.6229\n",
            "Total loss:  -3.579 | PDE Loss:  -5.5957 | Function Loss:  -3.6229\n",
            "Total loss:  -3.5791 | PDE Loss:  -5.597 | Function Loss:  -3.6229\n",
            "Total loss:  -3.5792 | PDE Loss:  -5.5976 | Function Loss:  -3.623\n",
            "Total loss:  -3.5793 | PDE Loss:  -5.5985 | Function Loss:  -3.623\n",
            "Total loss:  -3.5795 | PDE Loss:  -5.5992 | Function Loss:  -3.6231\n",
            "Total loss:  -3.5796 | PDE Loss:  -5.5994 | Function Loss:  -3.6232\n",
            "Total loss:  -3.5797 | PDE Loss:  -5.5999 | Function Loss:  -3.6233\n",
            "Total loss:  -3.5798 | PDE Loss:  -5.5994 | Function Loss:  -3.6234\n",
            "Total loss:  -3.5799 | PDE Loss:  -5.5994 | Function Loss:  -3.6235\n",
            "Total loss:  -3.58 | PDE Loss:  -5.5994 | Function Loss:  -3.6236\n",
            "Total loss:  -3.5801 | PDE Loss:  -5.5998 | Function Loss:  -3.6237\n",
            "Total loss:  -3.5801 | PDE Loss:  -5.5993 | Function Loss:  -3.6238\n",
            "Total loss:  -3.5803 | PDE Loss:  -5.6 | Function Loss:  -3.6239\n",
            "Total loss:  -3.5804 | PDE Loss:  -5.6 | Function Loss:  -3.624\n",
            "Total loss:  -3.5805 | PDE Loss:  -5.5996 | Function Loss:  -3.6242\n",
            "Total loss:  -3.5807 | PDE Loss:  -5.5998 | Function Loss:  -3.6244\n",
            "Total loss:  -3.5808 | PDE Loss:  -5.6 | Function Loss:  -3.6245\n",
            "Total loss:  -3.5809 | PDE Loss:  -5.6 | Function Loss:  -3.6246\n",
            "Total loss:  -3.581 | PDE Loss:  -5.6004 | Function Loss:  -3.6247\n",
            "Total loss:  -3.5811 | PDE Loss:  -5.6008 | Function Loss:  -3.6247\n",
            "Total loss:  -3.5812 | PDE Loss:  -5.6015 | Function Loss:  -3.6248\n",
            "Total loss:  -3.5814 | PDE Loss:  -5.6022 | Function Loss:  -3.6249\n",
            "Total loss:  -3.5815 | PDE Loss:  -5.6037 | Function Loss:  -3.6248\n",
            "Total loss:  -3.5815 | PDE Loss:  -5.6043 | Function Loss:  -3.6248\n",
            "Total loss:  -3.5816 | PDE Loss:  -5.6056 | Function Loss:  -3.6248\n",
            "Total loss:  -3.5816 | PDE Loss:  -5.6062 | Function Loss:  -3.6248\n",
            "Total loss:  -3.5817 | PDE Loss:  -5.6068 | Function Loss:  -3.6247\n",
            "Total loss:  -3.5818 | PDE Loss:  -5.6072 | Function Loss:  -3.6248\n",
            "Total loss:  -3.5818 | PDE Loss:  -5.6083 | Function Loss:  -3.6248\n",
            "Total loss:  -3.582 | PDE Loss:  -5.6096 | Function Loss:  -3.6248\n",
            "Total loss:  -3.5821 | PDE Loss:  -5.6116 | Function Loss:  -3.6247\n",
            "Total loss:  -3.5823 | PDE Loss:  -5.6159 | Function Loss:  -3.6245\n",
            "Total loss:  -3.5824 | PDE Loss:  -5.6173 | Function Loss:  -3.6245\n",
            "Total loss:  -3.5827 | PDE Loss:  -5.619 | Function Loss:  -3.6246\n",
            "Total loss:  -3.5828 | PDE Loss:  -5.6199 | Function Loss:  -3.6246\n",
            "Total loss:  -3.5829 | PDE Loss:  -5.6209 | Function Loss:  -3.6247\n",
            "Total loss:  -3.583 | PDE Loss:  -5.6225 | Function Loss:  -3.6246\n",
            "Total loss:  -3.5832 | PDE Loss:  -5.6241 | Function Loss:  -3.6246\n",
            "Total loss:  -3.5834 | PDE Loss:  -5.6275 | Function Loss:  -3.6245\n",
            "Total loss:  -3.5836 | PDE Loss:  -5.6296 | Function Loss:  -3.6246\n",
            "Total loss:  -3.5838 | PDE Loss:  -5.6328 | Function Loss:  -3.6245\n",
            "Total loss:  -3.5841 | PDE Loss:  -5.635 | Function Loss:  -3.6245\n",
            "Total loss:  -3.5843 | PDE Loss:  -5.6364 | Function Loss:  -3.6246\n",
            "Total loss:  -3.5845 | PDE Loss:  -5.6366 | Function Loss:  -3.6249\n",
            "Total loss:  -3.5847 | PDE Loss:  -5.6373 | Function Loss:  -3.625\n",
            "Total loss:  -3.5849 | PDE Loss:  -5.637 | Function Loss:  -3.6252\n",
            "Total loss:  -3.5851 | PDE Loss:  -5.636 | Function Loss:  -3.6255\n",
            "Total loss:  -3.5853 | PDE Loss:  -5.6345 | Function Loss:  -3.6259\n",
            "Total loss:  -3.5855 | PDE Loss:  -5.6336 | Function Loss:  -3.6262\n",
            "Total loss:  -3.5856 | PDE Loss:  -5.6276 | Function Loss:  -3.6269\n",
            "Total loss:  -3.5858 | PDE Loss:  -5.6275 | Function Loss:  -3.6272\n",
            "Total loss:  -3.586 | PDE Loss:  -5.6287 | Function Loss:  -3.6273\n",
            "Total loss:  -3.5862 | PDE Loss:  -5.6293 | Function Loss:  -3.6274\n",
            "Total loss:  -3.5864 | PDE Loss:  -5.6309 | Function Loss:  -3.6275\n",
            "Total loss:  -3.5866 | PDE Loss:  -5.6321 | Function Loss:  -3.6276\n",
            "Total loss:  -3.5868 | PDE Loss:  -5.6339 | Function Loss:  -3.6276\n",
            "Total loss:  -3.5869 | PDE Loss:  -5.6356 | Function Loss:  -3.6276\n",
            "Total loss:  -3.5871 | PDE Loss:  -5.636 | Function Loss:  -3.6277\n",
            "Total loss:  -3.5872 | PDE Loss:  -5.6373 | Function Loss:  -3.6278\n",
            "Total loss:  -3.5875 | PDE Loss:  -5.6379 | Function Loss:  -3.628\n",
            "Total loss:  -3.5876 | PDE Loss:  -5.6378 | Function Loss:  -3.6281\n",
            "Total loss:  -3.5878 | PDE Loss:  -5.638 | Function Loss:  -3.6283\n",
            "Total loss:  -3.588 | PDE Loss:  -5.638 | Function Loss:  -3.6286\n",
            "Total loss:  -3.5883 | PDE Loss:  -5.637 | Function Loss:  -3.629\n",
            "Total loss:  -3.5887 | PDE Loss:  -5.6382 | Function Loss:  -3.6293\n",
            "Total loss:  -3.589 | PDE Loss:  -5.637 | Function Loss:  -3.6297\n",
            "Total loss:  -3.5895 | PDE Loss:  -5.6362 | Function Loss:  -3.6303\n",
            "Total loss:  -3.5899 | PDE Loss:  -5.6366 | Function Loss:  -3.6308\n",
            "Total loss:  -3.5903 | PDE Loss:  -5.6372 | Function Loss:  -3.6312\n",
            "Total loss:  -3.5907 | PDE Loss:  -5.638 | Function Loss:  -3.6315\n",
            "Total loss:  -3.591 | PDE Loss:  -5.639 | Function Loss:  -3.6318\n",
            "Total loss:  -3.5914 | PDE Loss:  -5.6369 | Function Loss:  -3.6323\n",
            "Total loss:  -3.5916 | PDE Loss:  -5.6361 | Function Loss:  -3.6327\n",
            "Total loss:  -3.5921 | PDE Loss:  -5.635 | Function Loss:  -3.6333\n",
            "Total loss:  -3.5926 | PDE Loss:  -5.6326 | Function Loss:  -3.6342\n",
            "Total loss:  -3.593 | PDE Loss:  -5.6319 | Function Loss:  -3.6347\n",
            "Total loss:  -3.5933 | PDE Loss:  -5.6304 | Function Loss:  -3.6351\n",
            "Total loss:  -3.5935 | PDE Loss:  -5.6302 | Function Loss:  -3.6354\n",
            "Total loss:  -3.5937 | PDE Loss:  -5.6295 | Function Loss:  -3.6357\n",
            "Total loss:  -3.594 | PDE Loss:  -5.6302 | Function Loss:  -3.6359\n",
            "Total loss:  -3.5942 | PDE Loss:  -5.6288 | Function Loss:  -3.6362\n",
            "Total loss:  -3.5945 | PDE Loss:  -5.6277 | Function Loss:  -3.6367\n",
            "Total loss:  -3.5947 | PDE Loss:  -5.6266 | Function Loss:  -3.637\n",
            "Total loss:  -3.595 | PDE Loss:  -5.6238 | Function Loss:  -3.6376\n",
            "Total loss:  -3.5953 | PDE Loss:  -5.6201 | Function Loss:  -3.6384\n",
            "Total loss:  -3.5956 | PDE Loss:  -5.6152 | Function Loss:  -3.6392\n",
            "Total loss:  -3.5959 | PDE Loss:  -5.6119 | Function Loss:  -3.6399\n",
            "Total loss:  -3.5962 | PDE Loss:  -5.6096 | Function Loss:  -3.6405\n",
            "Total loss:  -3.5965 | PDE Loss:  -5.6073 | Function Loss:  -3.6411\n",
            "Total loss:  -3.5968 | PDE Loss:  -5.6064 | Function Loss:  -3.6415\n",
            "Total loss:  -3.5971 | PDE Loss:  -5.6059 | Function Loss:  -3.6419\n",
            "Total loss:  -3.5975 | PDE Loss:  -5.6068 | Function Loss:  -3.6422\n",
            "Total loss:  -3.598 | PDE Loss:  -5.6079 | Function Loss:  -3.6426\n",
            "Total loss:  -3.5985 | PDE Loss:  -5.6114 | Function Loss:  -3.6429\n",
            "Total loss:  -3.5989 | PDE Loss:  -5.6133 | Function Loss:  -3.6431\n",
            "Total loss:  -3.5993 | PDE Loss:  -5.6149 | Function Loss:  -3.6434\n",
            "Total loss:  -3.5997 | PDE Loss:  -5.6155 | Function Loss:  -3.6437\n",
            "Total loss:  -3.5999 | PDE Loss:  -5.6163 | Function Loss:  -3.6439\n",
            "Total loss:  -3.6 | PDE Loss:  -5.6152 | Function Loss:  -3.6441\n",
            "Total loss:  -3.6001 | PDE Loss:  -5.6148 | Function Loss:  -3.6443\n",
            "Total loss:  -3.6003 | PDE Loss:  -5.6135 | Function Loss:  -3.6446\n",
            "Total loss:  -3.6005 | PDE Loss:  -5.6116 | Function Loss:  -3.645\n",
            "Total loss:  -3.6007 | PDE Loss:  -5.6098 | Function Loss:  -3.6455\n",
            "Total loss:  -3.6009 | PDE Loss:  -5.6077 | Function Loss:  -3.646\n",
            "Total loss:  -3.6013 | PDE Loss:  -5.6068 | Function Loss:  -3.6465\n",
            "Total loss:  -3.6017 | PDE Loss:  -5.6056 | Function Loss:  -3.647\n",
            "Total loss:  -3.6022 | PDE Loss:  -5.6037 | Function Loss:  -3.6477\n",
            "Total loss:  -3.6025 | PDE Loss:  -5.6034 | Function Loss:  -3.6482\n",
            "Total loss:  -3.6028 | PDE Loss:  -5.6031 | Function Loss:  -3.6485\n",
            "Total loss:  -3.603 | PDE Loss:  -5.6042 | Function Loss:  -3.6487\n",
            "Total loss:  -3.6032 | PDE Loss:  -5.6048 | Function Loss:  -3.6488\n",
            "Total loss:  -3.6033 | PDE Loss:  -5.6052 | Function Loss:  -3.6489\n",
            "Total loss:  -3.6036 | PDE Loss:  -5.6053 | Function Loss:  -3.6491\n",
            "Total loss:  -3.604 | PDE Loss:  -5.6056 | Function Loss:  -3.6496\n",
            "Total loss:  -3.6043 | PDE Loss:  -5.605 | Function Loss:  -3.65\n",
            "Total loss:  -3.6048 | PDE Loss:  -5.604 | Function Loss:  -3.6506\n",
            "Total loss:  -3.6052 | PDE Loss:  -5.6038 | Function Loss:  -3.6511\n",
            "Total loss:  -3.6056 | PDE Loss:  -5.6045 | Function Loss:  -3.6515\n",
            "Total loss:  -3.6059 | PDE Loss:  -5.604 | Function Loss:  -3.6519\n",
            "Total loss:  -3.6062 | PDE Loss:  -5.6051 | Function Loss:  -3.652\n",
            "Total loss:  -3.6064 | PDE Loss:  -5.6057 | Function Loss:  -3.6523\n",
            "Total loss:  -3.6067 | PDE Loss:  -5.6064 | Function Loss:  -3.6525\n",
            "Total loss:  -3.6071 | PDE Loss:  -5.6065 | Function Loss:  -3.6529\n",
            "Total loss:  -3.6075 | PDE Loss:  -5.6046 | Function Loss:  -3.6536\n",
            "Total loss:  -3.6079 | PDE Loss:  -5.6035 | Function Loss:  -3.6542\n",
            "Total loss:  -3.6082 | PDE Loss:  -5.6006 | Function Loss:  -3.6548\n",
            "Total loss:  -3.6087 | PDE Loss:  -5.5992 | Function Loss:  -3.6555\n",
            "Total loss:  -3.6091 | PDE Loss:  -5.5958 | Function Loss:  -3.6563\n",
            "Total loss:  -3.6094 | PDE Loss:  -5.5936 | Function Loss:  -3.657\n",
            "Total loss:  -3.6098 | PDE Loss:  -5.5915 | Function Loss:  -3.6576\n",
            "Total loss:  -3.61 | PDE Loss:  -5.5889 | Function Loss:  -3.6582\n",
            "Total loss:  -3.6104 | PDE Loss:  -5.5882 | Function Loss:  -3.6587\n",
            "Total loss:  -3.6106 | PDE Loss:  -5.5866 | Function Loss:  -3.6591\n",
            "Total loss:  -3.6109 | PDE Loss:  -5.5867 | Function Loss:  -3.6595\n",
            "Total loss:  -3.6113 | PDE Loss:  -5.5867 | Function Loss:  -3.6599\n",
            "Total loss:  -3.6116 | PDE Loss:  -5.5877 | Function Loss:  -3.6601\n",
            "Total loss:  -3.6119 | PDE Loss:  -5.5888 | Function Loss:  -3.6603\n",
            "Total loss:  -3.6122 | PDE Loss:  -5.5897 | Function Loss:  -3.6605\n",
            "Total loss:  -3.6124 | PDE Loss:  -5.5912 | Function Loss:  -3.6606\n",
            "Total loss:  -3.6125 | PDE Loss:  -5.5914 | Function Loss:  -3.6607\n",
            "Total loss:  -3.6128 | PDE Loss:  -5.5923 | Function Loss:  -3.6609\n",
            "Total loss:  -3.6131 | PDE Loss:  -5.5924 | Function Loss:  -3.6612\n",
            "Total loss:  -3.6134 | PDE Loss:  -5.5925 | Function Loss:  -3.6615\n",
            "Total loss:  -3.6137 | PDE Loss:  -5.5918 | Function Loss:  -3.6619\n",
            "Total loss:  -3.614 | PDE Loss:  -5.5924 | Function Loss:  -3.6622\n",
            "Total loss:  -3.6142 | PDE Loss:  -5.5925 | Function Loss:  -3.6624\n",
            "Total loss:  -3.6144 | PDE Loss:  -5.5931 | Function Loss:  -3.6626\n",
            "Total loss:  -3.6146 | PDE Loss:  -5.5932 | Function Loss:  -3.6628\n",
            "Total loss:  -3.6148 | PDE Loss:  -5.5935 | Function Loss:  -3.663\n",
            "Total loss:  -3.6149 | PDE Loss:  -5.5935 | Function Loss:  -3.6631\n",
            "Total loss:  -3.615 | PDE Loss:  -5.5934 | Function Loss:  -3.6633\n",
            "Total loss:  -3.6153 | PDE Loss:  -5.5937 | Function Loss:  -3.6635\n",
            "Total loss:  -3.6155 | PDE Loss:  -5.5935 | Function Loss:  -3.6637\n",
            "Total loss:  -3.6157 | PDE Loss:  -5.5936 | Function Loss:  -3.664\n",
            "Total loss:  -3.616 | PDE Loss:  -5.5946 | Function Loss:  -3.6642\n",
            "Total loss:  -3.6161 | PDE Loss:  -5.5945 | Function Loss:  -3.6644\n",
            "Total loss:  -3.6163 | PDE Loss:  -5.5959 | Function Loss:  -3.6644\n",
            "Total loss:  -3.6165 | PDE Loss:  -5.5968 | Function Loss:  -3.6645\n",
            "Total loss:  -3.6166 | PDE Loss:  -5.5982 | Function Loss:  -3.6645\n",
            "Total loss:  -3.6168 | PDE Loss:  -5.5995 | Function Loss:  -3.6645\n",
            "Total loss:  -3.6171 | PDE Loss:  -5.6018 | Function Loss:  -3.6646\n",
            "Total loss:  -3.6172 | PDE Loss:  -5.6032 | Function Loss:  -3.6646\n",
            "Total loss:  -3.6175 | PDE Loss:  -5.6043 | Function Loss:  -3.6648\n",
            "Total loss:  -3.6178 | PDE Loss:  -5.605 | Function Loss:  -3.665\n",
            "Total loss:  -3.6181 | PDE Loss:  -5.605 | Function Loss:  -3.6654\n",
            "Total loss:  -3.6185 | PDE Loss:  -5.6048 | Function Loss:  -3.6658\n",
            "Total loss:  -3.6188 | PDE Loss:  -5.6036 | Function Loss:  -3.6663\n",
            "Total loss:  -3.6191 | PDE Loss:  -5.6033 | Function Loss:  -3.6666\n",
            "Total loss:  -3.6193 | PDE Loss:  -5.6018 | Function Loss:  -3.667\n",
            "Total loss:  -3.6195 | PDE Loss:  -5.6014 | Function Loss:  -3.6673\n",
            "Total loss:  -3.6196 | PDE Loss:  -5.6009 | Function Loss:  -3.6675\n",
            "Total loss:  -3.6198 | PDE Loss:  -5.6007 | Function Loss:  -3.6677\n",
            "Total loss:  -3.6199 | PDE Loss:  -5.6013 | Function Loss:  -3.6678\n",
            "Total loss:  -3.62 | PDE Loss:  -5.6022 | Function Loss:  -3.6678\n",
            "Total loss:  -3.6201 | PDE Loss:  -5.6032 | Function Loss:  -3.6678\n",
            "Total loss:  -3.6202 | PDE Loss:  -5.6048 | Function Loss:  -3.6678\n",
            "Total loss:  -3.6202 | PDE Loss:  -5.6018 | Function Loss:  -3.668\n",
            "Total loss:  -3.6202 | PDE Loss:  -5.604 | Function Loss:  -3.6678\n",
            "Total loss:  -3.6202 | PDE Loss:  -5.6045 | Function Loss:  -3.6678\n",
            "Total loss:  -3.6202 | PDE Loss:  -5.6047 | Function Loss:  -3.6677\n",
            "Total loss:  -3.6202 | PDE Loss:  -5.6048 | Function Loss:  -3.6677\n",
            "Total loss:  -3.6202 | PDE Loss:  -5.6048 | Function Loss:  -3.6678\n",
            "Final Loss:  -3.620249032974243\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x16a844550>"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 900x600 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "hist = pinn.fit(num_epochs=n_epochs,\n",
        "                optimizer=optimizer_LBFGS,\n",
        "                verbose=True)\n",
        "\n",
        "plt.figure(dpi=150)\n",
        "plt.grid(True, which=\"both\", ls=\":\")\n",
        "plt.plot(np.arange(1, len(hist) + 1), hist, label=\"Train Loss\")\n",
        "plt.xscale(\"log\")\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3HG9DQM5fQxN",
        "outputId": "7aafd9d3-4ccb-40eb-fd91-185602c79aba",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "pinn.plotting()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Get the predictions on the test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "U_q8DqdznK6z"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "could not convert string 't' to float64 at row 0, column 1.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 't'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m test_data \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mloadtxt(\u001b[39m\"\u001b[39;49m\u001b[39mTask1/TestingData.txt\u001b[39;49m\u001b[39m\"\u001b[39;49m, delimiter\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m,\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
            "File \u001b[0;32m~/miniconda/envs/pytorch/lib/python3.8/site-packages/numpy/lib/npyio.py:1338\u001b[0m, in \u001b[0;36mloadtxt\u001b[0;34m(fname, dtype, comments, delimiter, converters, skiprows, usecols, unpack, ndmin, encoding, max_rows, quotechar, like)\u001b[0m\n\u001b[1;32m   1335\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(delimiter, \u001b[39mbytes\u001b[39m):\n\u001b[1;32m   1336\u001b[0m     delimiter \u001b[39m=\u001b[39m delimiter\u001b[39m.\u001b[39mdecode(\u001b[39m'\u001b[39m\u001b[39mlatin1\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m-> 1338\u001b[0m arr \u001b[39m=\u001b[39m _read(fname, dtype\u001b[39m=\u001b[39;49mdtype, comment\u001b[39m=\u001b[39;49mcomment, delimiter\u001b[39m=\u001b[39;49mdelimiter,\n\u001b[1;32m   1339\u001b[0m             converters\u001b[39m=\u001b[39;49mconverters, skiplines\u001b[39m=\u001b[39;49mskiprows, usecols\u001b[39m=\u001b[39;49musecols,\n\u001b[1;32m   1340\u001b[0m             unpack\u001b[39m=\u001b[39;49munpack, ndmin\u001b[39m=\u001b[39;49mndmin, encoding\u001b[39m=\u001b[39;49mencoding,\n\u001b[1;32m   1341\u001b[0m             max_rows\u001b[39m=\u001b[39;49mmax_rows, quote\u001b[39m=\u001b[39;49mquotechar)\n\u001b[1;32m   1343\u001b[0m \u001b[39mreturn\u001b[39;00m arr\n",
            "File \u001b[0;32m~/miniconda/envs/pytorch/lib/python3.8/site-packages/numpy/lib/npyio.py:999\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(fname, delimiter, comment, quote, imaginary_unit, usecols, skiplines, max_rows, converters, ndmin, unpack, dtype, encoding)\u001b[0m\n\u001b[1;32m    996\u001b[0m     data \u001b[39m=\u001b[39m _preprocess_comments(data, comments, encoding)\n\u001b[1;32m    998\u001b[0m \u001b[39mif\u001b[39;00m read_dtype_via_object_chunks \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 999\u001b[0m     arr \u001b[39m=\u001b[39m _load_from_filelike(\n\u001b[1;32m   1000\u001b[0m         data, delimiter\u001b[39m=\u001b[39;49mdelimiter, comment\u001b[39m=\u001b[39;49mcomment, quote\u001b[39m=\u001b[39;49mquote,\n\u001b[1;32m   1001\u001b[0m         imaginary_unit\u001b[39m=\u001b[39;49mimaginary_unit,\n\u001b[1;32m   1002\u001b[0m         usecols\u001b[39m=\u001b[39;49musecols, skiplines\u001b[39m=\u001b[39;49mskiplines, max_rows\u001b[39m=\u001b[39;49mmax_rows,\n\u001b[1;32m   1003\u001b[0m         converters\u001b[39m=\u001b[39;49mconverters, dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[1;32m   1004\u001b[0m         encoding\u001b[39m=\u001b[39;49mencoding, filelike\u001b[39m=\u001b[39;49mfilelike,\n\u001b[1;32m   1005\u001b[0m         byte_converters\u001b[39m=\u001b[39;49mbyte_converters)\n\u001b[1;32m   1007\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1008\u001b[0m     \u001b[39m# This branch reads the file into chunks of object arrays and then\u001b[39;00m\n\u001b[1;32m   1009\u001b[0m     \u001b[39m# casts them to the desired actual dtype.  This ensures correct\u001b[39;00m\n\u001b[1;32m   1010\u001b[0m     \u001b[39m# string-length and datetime-unit discovery (like `arr.astype()`).\u001b[39;00m\n\u001b[1;32m   1011\u001b[0m     \u001b[39m# Due to chunking, certain error reports are less clear, currently.\u001b[39;00m\n\u001b[1;32m   1012\u001b[0m     \u001b[39mif\u001b[39;00m filelike:\n",
            "\u001b[0;31mValueError\u001b[0m: could not convert string 't' to float64 at row 0, column 1."
          ]
        }
      ],
      "source": [
        "test_data = np.loadtxt(\"Task1/TestingData.txt\", delimiter=',')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
