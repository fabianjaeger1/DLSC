{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x124d991f0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from Common import NeuralNet\n",
    "import time\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "torch.manual_seed(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pinns:\n",
    "    def __init__(self, n_int_, n_sb_, n_tb_, alpha_f_=0.005, h_f_=5, T_hot_=4, T_0_=1, U_f_=None):\n",
    "        self.n_int = n_int_     # n_int_:= number of intertior points\n",
    "        self.n_sb = n_sb_       # n_sb_ := number of spatial boundary points\n",
    "        self.n_tb = n_tb_       # n_tb_ := number of time boundary points\n",
    "\n",
    "        # Set the paremeters of the equation\n",
    "        self.alpha_f = alpha_f_\n",
    "        self.h_f = h_f_\n",
    "        self.T_hot = T_hot_\n",
    "        self.T_0 = T_0_\n",
    "        self.U_f = U_f_\n",
    "\n",
    "        # Extrema of the solution domain (t,x) in [0, t]x[0, L]\n",
    "        self.domain_extrema = torch.tensor([[0, 8],  # Time dimension\n",
    "                                            [0, 1]])  # Space dimension\n",
    "\n",
    "        # Number of space dimensions\n",
    "        self.space_dimensions = 1\n",
    "\n",
    "        # Parameter to balance role of data and PDE\n",
    "        self.lambda_u = 10\n",
    "\n",
    "        # FF Dense NN to approximate the solution of the underlying reaction-convection-diffusion equations of the fluid\n",
    "        self.approximate_solution = NeuralNet(input_dimension=self.domain_extrema.shape[0], output_dimension=1, # is a NN with input_dim=2 (time & space), output_dim=1 (fluid_temp)\n",
    "                                              n_hidden_layers=4,\n",
    "                                              neurons=20,\n",
    "                                              regularization_param=0.,\n",
    "                                              regularization_exp=2.,\n",
    "                                              retrain_seed=42)\n",
    "        \n",
    "        # FF Dense NN to approximate the solid temperature we wish to infer\n",
    "        self.approximate_coefficient = NeuralNet(input_dimension=self.domain_extrema.shape[0], output_dimension=1,  # is a NN with input_dim=2 (time & space), output_dim=1 (solid_temp)\n",
    "                                                 n_hidden_layers=4,\n",
    "                                                 neurons=20,\n",
    "                                                 regularization_param=0.,\n",
    "                                                 regularization_exp=2.,\n",
    "                                                 retrain_seed=42)\n",
    "\n",
    "        # Generator of Sobol sequences\n",
    "        self.soboleng = torch.quasirandom.SobolEngine(dimension=self.domain_extrema.shape[0])   # it will create a 2 cloumns tensor, the rows nunmber is specified after every time it is used\n",
    "\n",
    "        # Training sets S_sb, S_tb, S_int as torch dataloader\n",
    "        self.training_set_sb, self.training_set_tb, self.training_set_int, self.training_set_meas = self.assemble_datasets()\n",
    "\n",
    "    ################################################################################################\n",
    "    # Function to linearly transform a tensor whose value are between 0 and 1\n",
    "    # to a tensor whose values are between the domain extrema\n",
    "    def convert(self, tens):\n",
    "        assert (tens.shape[1] == self.domain_extrema.shape[0])\n",
    "        return tens * (self.domain_extrema[:, 1] - self.domain_extrema[:, 0]) + self.domain_extrema[:, 0]\n",
    "    \n",
    "    # Function Uf(t) -> given the time it gives back the velocity of the fluid in the relative phase\n",
    "    def fluid_velocity(self, inputs):\n",
    "        Uf = torch.full(inputs.shape, 999)  # give all 999 for semplicity when testing\n",
    "        \n",
    "        # \"\"\"QUI\"\"\"\n",
    "        # print(\"\\nfluid_velocity\")\n",
    "        # print('inputs: ', inputs.shape)\n",
    "\n",
    "        for i, t in enumerate(inputs):\n",
    "            # Charging Phase\n",
    "            if (t <= 1) or (t > 4 and t <=5 ): Uf[i] = 1\n",
    "            # Discharging Phase\n",
    "            elif (t > 2 and t <= 3) or (t > 6 and t <= 7): Uf[i] = -1\n",
    "            # Idle Phase\n",
    "            elif (t > 1 and t <= 2) or (t > 3 and t <= 4) or (t > 5 and t <= 6) or (t > 7 and t <= 8): Uf[i] = 0\n",
    "\n",
    "        return Uf\n",
    "    \n",
    "    ################################################################################################\n",
    "    # Function returning the input-output tensor required to assemble the training set S_tb corresponding to the temporal boundary\n",
    "    def add_temporal_boundary_points(self):\n",
    "        t0 = self.domain_extrema[0, 0]\n",
    "        input_tb = self.soboleng.draw(self.n_tb)    # input_sb has two columns (t, x) both with random numbers in the two respective domains\n",
    "        input_tb[:, 0] = torch.full(input_tb[:, 0].shape, t0)   # overwrite the entier column of time with t0\n",
    "        output_tb = torch.full(input_tb[:, 0].shape, self.T_0).reshape(-1, 1)   # the output has 1 column\n",
    "\n",
    "        # \"\"\"QUI\"\"\"\n",
    "        # print('ADD TEMPORAL BOUDARY POINTS:')\n",
    "        # print('input_tb: ', input_tb.shape)\n",
    "        # print('output_tb: ', output_tb.shape)\n",
    "\n",
    "        return input_tb, output_tb  # input_tb is the sequence of x_n; output_tb is the sequence u0(x_n)\n",
    "\n",
    "    # Function returning the input-output tensor required to assemble the training set S_sb corresponding to the spatial boundary\n",
    "    def add_spatial_boundary_points(self):\n",
    "        x0 = self.domain_extrema[1, 0]\n",
    "        xL = self.domain_extrema[1, 1]\n",
    "\n",
    "        # Dataset with random [t, x] both in [0, 1]\n",
    "        input_sb = self.soboleng.draw(self.n_sb)\n",
    "\n",
    "        # Assigne the spacial boundary x=x0\n",
    "        input_sb_0 = torch.clone(input_sb)\n",
    "        input_sb_0[:, 1] = torch.full(input_sb_0[:, 1].shape, x0)\n",
    "\n",
    "        # Assigne the spacial boundary x=xL\n",
    "        input_sb_L = torch.clone(input_sb)\n",
    "        input_sb_L[:, 1] = torch.full(input_sb_L[:, 1].shape, xL)\n",
    "\n",
    "        # Def a tensor to add the delta in time to each of the input dataset and have the different phases\n",
    "        # This tenosr is full of [0, 0] and it will be filled on the t-column with the relative delta_t\n",
    "        delta_time = torch.zeros_like(input_sb)\n",
    "\n",
    "        # We are going now to define the input dataset for all the different phases over the 2 cycles.\n",
    "        # However, not that even if the input for the same phase over the 2 cycles is different (the t)\n",
    "        #Â the output is always the same (the spatial boundary conditions are the same), so we just need one output per phase.\n",
    "        \"\"\"Charging Phase\"\"\"\n",
    "        # First charging phase -> t in [0, 1] => delta_t=0\n",
    "        input_sb_0_charging_1 = torch.clone(input_sb_0)\n",
    "        input_sb_L_charging_1 = torch.clone(input_sb_L)\n",
    "        \n",
    "        # Second charging phase -> t in [4, 5] => delta_t=4\n",
    "        delta_time[:, 0] = torch.full(delta_time[:, 0].shape, 4)\n",
    "\n",
    "        input_sb_0_charging_2 = torch.clone(input_sb_0)\n",
    "        input_sb_0_charging_2 = input_sb_0_charging_2 + delta_time\n",
    "\n",
    "        input_sb_L_charging_2 = torch.clone(input_sb_L)\n",
    "        input_sb_L_charging_2 = input_sb_L_charging_2 + delta_time\n",
    "\n",
    "        # Output charging phase\n",
    "        output_sb_0_charging = torch.full(input_sb_0[:, 0].shape, self.T_hot).reshape(-1, 1)\n",
    "        output_sb_L_charging = torch.full(input_sb_L[:, 0].shape, 0).reshape(-1, 1)\n",
    "\n",
    "        \"\"\"Discharging Phase\"\"\"\n",
    "        # First discharging phase -> t in [2, 3] => delta_t=2\n",
    "        delta_time[:, 0] = torch.full(delta_time[:, 0].shape, 2)\n",
    "\n",
    "        input_sb_0_discharging_1 = torch.clone(input_sb_0)\n",
    "        input_sb_0_discharging_1 = input_sb_0_discharging_1 + delta_time\n",
    "\n",
    "        input_sb_L_discharging_1 = torch.clone(input_sb_L)\n",
    "        input_sb_L_discharging_1 = input_sb_L_discharging_1 + delta_time\n",
    "\n",
    "        # Second discharging phase -> t in [6, 7] => delta_t=6\n",
    "        delta_time[:, 0] = torch.full(delta_time[:, 0].shape, 6)\n",
    "\n",
    "        input_sb_0_discharging_2 = torch.clone(input_sb_0)\n",
    "        input_sb_0_discharging_2 = input_sb_0_discharging_2 + delta_time\n",
    "\n",
    "        input_sb_L_discharging_2 = torch.clone(input_sb_L)\n",
    "        input_sb_L_discharging_2 = input_sb_L_discharging_2 + delta_time\n",
    "\n",
    "        # Output discharging phase\n",
    "        output_sb_0_discharging = torch.full(input_sb_0[:, 0].shape, 0).reshape(-1, 1)\n",
    "        output_sb_L_discharging = torch.full(input_sb_L[:, 0].shape, self.T_0).reshape(-1, 1)\n",
    "\n",
    "\n",
    "        \"\"\"Idle Phase\"\"\"\n",
    "        # First idle phase -> t in [1, 2] => delta_t=1\n",
    "        delta_time[:, 0] = torch.full(delta_time[:, 0].shape, 1)\n",
    "\n",
    "        input_sb_0_idle_1 = torch.clone(input_sb_0)\n",
    "        input_sb_0_idle_1 = input_sb_0_idle_1 + delta_time\n",
    "\n",
    "        input_sb_L_idle_1 = torch.clone(input_sb_L)\n",
    "        input_sb_L_idle_1 = input_sb_L_idle_1 + delta_time\n",
    "\n",
    "        # Second idle phase -> t in [3, 4] => delta_t=3\n",
    "        delta_time[:, 0] = torch.full(delta_time[:, 0].shape, 3)\n",
    "\n",
    "        input_sb_0_idle_2 = torch.clone(input_sb_0)\n",
    "        input_sb_0_idle_2 = input_sb_0_idle_2 + delta_time\n",
    "\n",
    "        input_sb_L_idle_2 = torch.clone(input_sb_L)\n",
    "        input_sb_L_idle_2 = input_sb_L_idle_2 + delta_time\n",
    "\n",
    "        # Third idle phase -> t in [5, 6] => delta_t=5\n",
    "        delta_time[:, 0] = torch.full(delta_time[:, 0].shape, 5)\n",
    "\n",
    "        input_sb_0_idle_3 = torch.clone(input_sb_0)\n",
    "        input_sb_0_idle_3 = input_sb_0_idle_3 + delta_time\n",
    "\n",
    "        input_sb_L_idle_3 = torch.clone(input_sb_L)\n",
    "        input_sb_L_idle_3 = input_sb_L_idle_3 + delta_time\n",
    "\n",
    "        # Fourth idle phase -> t in [7, 8] => delta_t=7\n",
    "        delta_time[:, 0] = torch.full(delta_time[:, 0].shape, 7)\n",
    "\n",
    "        input_sb_0_idle_4 = torch.clone(input_sb_0)\n",
    "        input_sb_0_idle_4 = input_sb_0_idle_4 + delta_time\n",
    "\n",
    "        input_sb_L_idle_4 = torch.clone(input_sb_L)\n",
    "        input_sb_L_idle_4 = input_sb_L_idle_4 + delta_time\n",
    "\n",
    "        # Output idle phase\n",
    "        output_sb_0_idle = torch.full(input_sb_0[:, 0].shape, 0).reshape(-1, 1)\n",
    "        output_sb_L_idle = torch.full(input_sb_L[:, 0].shape, 0).reshape(-1, 1)\n",
    "\n",
    "        # \"\"\"QUI\"\"\"\n",
    "        # print('CYCLE 1:')\n",
    "        # print('input_sb_0_charging_1: ', input_sb_0_charging_1.shape)\n",
    "        # print('input_sb_L_charging_1: ', input_sb_L_charging_1.shape)\n",
    "        # print('input_sb_0_idle_1: ', input_sb_0_idle_1.shape)\n",
    "        # print('input_sb_L_idle_1: ', input_sb_L_idle_1.shape)\n",
    "        # print('input_sb_0_discharging_1: ', input_sb_0_discharging_1.shape)\n",
    "        # print('input_sb_L_discharging_1: ', input_sb_L_discharging_1.shape)\n",
    "        # print('input_sb_0_idle_2: ', input_sb_0_idle_2.shape)\n",
    "        # print('input_sb_L_idle_2: ', input_sb_L_idle_2.shape)\n",
    "        # print('\\nCYCLE 2:')\n",
    "        # print('input_sb_0_charging_2: ', input_sb_0_charging_2.shape)\n",
    "        # print('input_sb_L_charging_2: ', input_sb_L_charging_2.shape)\n",
    "        # print('input_sb_0_idle_3: ', input_sb_0_idle_3.shape)\n",
    "        # print('input_sb_L_idle_3: ', input_sb_L_idle_3.shape)\n",
    "        # print('input_sb_0_discharging_2: ', input_sb_0_discharging_2.shape)\n",
    "        # print('input_sb_L_discharging_2: ', input_sb_L_discharging_2.shape)\n",
    "        # print('input_sb_0_idle_4: ', input_sb_0_idle_4.shape)\n",
    "        # print('input_sb_L_idle_4: ', input_sb_L_idle_4.shape)\n",
    "        # print('\\nOUTPUT:')\n",
    "        # print('output_sb_0_charging: ', output_sb_0_charging.shape)\n",
    "        # print('output_sb_L_charging: ', output_sb_L_charging.shape)\n",
    "        # print('output_sb_0_idle: ', output_sb_0_idle.shape)\n",
    "        # print('output_sb_L_idle: ', output_sb_L_idle.shape)\n",
    "        # print('output_sb_0_discharging: ', output_sb_0_discharging.shape)\n",
    "        # print('output_sb_L_discharging: ', output_sb_L_discharging.shape)\n",
    "        \n",
    "        # requires the grad for input tensors as we will have to compute the derivatives\n",
    "        return torch.cat([  # cycle 1\n",
    "                        input_sb_0_charging_1.requires_grad_(True), input_sb_L_charging_1.requires_grad_(True),\n",
    "                        input_sb_0_idle_1.requires_grad_(True), input_sb_L_idle_1.requires_grad_(True),\n",
    "                        input_sb_0_discharging_1.requires_grad_(True), input_sb_L_discharging_1.requires_grad_(True),\n",
    "                        input_sb_0_idle_2.requires_grad_(True), input_sb_L_idle_2.requires_grad_(True),\n",
    "                            # cycle 2\n",
    "                        input_sb_0_charging_2.requires_grad_(True), input_sb_L_charging_2.requires_grad_(True),\n",
    "                        input_sb_0_idle_3.requires_grad_(True), input_sb_L_idle_3.requires_grad_(True),\n",
    "                        input_sb_0_discharging_2.requires_grad_(True), input_sb_L_discharging_2.requires_grad_(True),\n",
    "                        input_sb_0_idle_4.requires_grad_(True), input_sb_L_idle_4.requires_grad_(True)\n",
    "                        ], 0), torch.cat([  # cycle 1\n",
    "                                        output_sb_0_charging, output_sb_L_charging,\n",
    "                                        output_sb_0_idle, output_sb_L_idle,\n",
    "                                        output_sb_0_discharging, output_sb_L_discharging,\n",
    "                                        output_sb_0_idle, output_sb_L_idle,\n",
    "                                            # cycle 2\n",
    "                                        output_sb_0_charging, output_sb_L_charging,\n",
    "                                        output_sb_0_idle, output_sb_L_idle,\n",
    "                                        output_sb_0_discharging, output_sb_L_discharging,\n",
    "                                        output_sb_0_idle, output_sb_L_idle\n",
    "                                        ], 0)\n",
    "\n",
    "    # Function returning the input-output tensor required to assemble the training set S_int corresponding to the interior domain where the PDE is enforced\n",
    "    def add_interior_points(self):\n",
    "        # Now we use the convert fct since we want the t is in [0, 8]\n",
    "        input_int = self.convert(self.soboleng.draw(self.n_int))\n",
    "        output_int = torch.zeros((input_int.shape[0], 1))\n",
    "        \n",
    "        return input_int, output_int\n",
    "    \n",
    "    # Function returning the input-output tensor required to assemble the training set S_meas corresponding to the measured points in the domain.\n",
    "    # These points are read from the file \"DataSolution.txt\"\n",
    "    def get_measurement_data(self):\n",
    "        # Read the CSV file into a DataFrame\n",
    "        df_meas = pd.read_csv('Task2/DataSolution.txt')\n",
    "\n",
    "        # Convert the DataFrame to a torch.tensor\n",
    "        tensor_meas = torch.tensor(df_meas.values , dtype=torch.float)\n",
    "        \n",
    "        # The first 2 columns are the inputs: [t, x]\n",
    "        input_meas = tensor_meas[:, :2]\n",
    "        \n",
    "        # The last column are the outputs: [Tf]\n",
    "        output_meas = tensor_meas[:, 2:]\n",
    "        return input_meas, output_meas\n",
    "\n",
    "    # Function returning the training sets S_sb, S_tb, S_int as dataloader\n",
    "    def assemble_datasets(self):\n",
    "        input_sb, output_sb = self.add_spatial_boundary_points()    # S_sb\n",
    "        input_tb, output_tb = self.add_temporal_boundary_points()   # S_tb\n",
    "        input_int, output_int = self.add_interior_points()          # S_int\n",
    "        input_meas, output_meas = self.get_measurement_data()       # S_meas\n",
    "\n",
    "        # \"\"\"QUI\"\"\"\n",
    "        # print('\\nSONO IN assemble_datasets:')\n",
    "        # print('input_sb', input_sb.shape)\n",
    "        # print('output_sb', output_sb.shape)\n",
    "        # print()\n",
    "        # print('input_tb', input_tb.shape)\n",
    "        # print('output_tb', output_tb.shape)\n",
    "        # print()\n",
    "        # print('input_int', input_int.shape)\n",
    "        # print('output_int', output_int.shape)\n",
    "        # print()\n",
    "        # print('input_meas', input_meas.shape)\n",
    "        # print('output_meas', output_meas.shape)\n",
    "        # print()\n",
    "\n",
    "        training_set_sb = DataLoader(torch.utils.data.TensorDataset(input_sb, output_sb), batch_size=16*self.space_dimensions*self.n_sb, shuffle=False)  #batch_size has *8 since there are 8 different phases and for each one we have 2 conditions (x0 & xL)\n",
    "        training_set_tb = DataLoader(torch.utils.data.TensorDataset(input_tb, output_tb), batch_size=self.n_tb, shuffle=False)\n",
    "        training_set_int = DataLoader(torch.utils.data.TensorDataset(input_int, output_int), batch_size=self.n_int, shuffle=False)\n",
    "        training_set_meas = DataLoader(torch.utils.data.TensorDataset(input_meas, output_meas), batch_size=output_meas.shape[0], shuffle=False)\n",
    "\n",
    "        return training_set_sb, training_set_tb, training_set_int, training_set_meas\n",
    "\n",
    "    ################################################################################################\n",
    "    # Function to compute the terms required in the definition of the TEMPORAL boundary residual\n",
    "    def apply_initial_condition(self, input_tb):\n",
    "        u_pred_tb = self.approximate_solution(input_tb)\n",
    "        \n",
    "        return u_pred_tb\n",
    "\n",
    "    # Function to compute the terms required in the definition of the SPATIAL boundary residual\n",
    "    def apply_boundary_conditions(self, input_sb):\n",
    "        # input_tb is a tensor of size [16*self.n_sb, 2]\n",
    "        # as defined in \"add_spatial_boundary_points\" we have 2 boundary conditions for each phase\n",
    "        # we then have to devide the input_sb in 16\n",
    "        assert (input_sb.requires_grad==True)   # make sure the grad is requested so we can compute the derivatives\n",
    "\n",
    "        # Devide all the input datasets\n",
    "            # cycle 1\n",
    "        input_sb_0_charging_1 = input_sb[:int(input_sb.shape[0]/16), :]\n",
    "        input_sb_L_charging_1 = input_sb[int(input_sb.shape[0]/16):2*int(input_sb.shape[0]/16), :]\n",
    "        input_sb_0_idle_1 = input_sb[2*int(input_sb.shape[0]/16):3*int(input_sb.shape[0]/16), :]\n",
    "        input_sb_L_idle_1 = input_sb[3*int(input_sb.shape[0]/16):4*int(input_sb.shape[0]/16), :]\n",
    "        input_sb_0_discharging_1 = input_sb[4*int(input_sb.shape[0]/16):5*int(input_sb.shape[0]/16), :]\n",
    "        input_sb_L_discharging_1 = input_sb[5*int(input_sb.shape[0]/16):6*int(input_sb.shape[0]/16), :]\n",
    "        input_sb_0_idle_2 = input_sb[6*int(input_sb.shape[0]/16):7*int(input_sb.shape[0]/16), :]\n",
    "        input_sb_L_idle_2 = input_sb[7*int(input_sb.shape[0]/16):8*int(input_sb.shape[0]/16), :]\n",
    "            # cycle 2\n",
    "        input_sb_0_charging_2 = input_sb[8*int(input_sb.shape[0]/16):9*int(input_sb.shape[0]/16), :]\n",
    "        input_sb_L_charging_2 = input_sb[9*int(input_sb.shape[0]/16):10*int(input_sb.shape[0]/16), :]\n",
    "        input_sb_0_idle_3 = input_sb[10*int(input_sb.shape[0]/16):11*int(input_sb.shape[0]/16), :]\n",
    "        input_sb_L_idle_3 = input_sb[11*int(input_sb.shape[0]/16):12*int(input_sb.shape[0]/16), :]\n",
    "        input_sb_0_discharging_2 = input_sb[12*int(input_sb.shape[0]/16):13*int(input_sb.shape[0]/16), :]\n",
    "        input_sb_L_discharging_2 = input_sb[13*int(input_sb.shape[0]/16):14*int(input_sb.shape[0]/16), :]\n",
    "        input_sb_0_idle_4 = input_sb[14*int(input_sb.shape[0]/16):15*int(input_sb.shape[0]/16), :]\n",
    "        input_sb_L_idle_4 = input_sb[15*int(input_sb.shape[0]/16):, :]\n",
    "\n",
    "        \"\"\"Charging Phase\"\"\"\n",
    "        # First charging phase\n",
    "            # x0 -> compute Tf\n",
    "        u_pred_sb_0_charging_1 = self.approximate_solution(input_sb_0_charging_1).reshape(-1, 1)\n",
    "\n",
    "            # xL -> compute dTf/dx\n",
    "        u_pred_Tf = self.approximate_solution(input_sb_L_charging_1)\n",
    "        u_pred_sb_L_charging_1 = torch.autograd.grad(u_pred_Tf.sum(), input_sb_L_charging_1, create_graph=True)[0][:, 1].reshape(-1, 1) # take only d/dx\n",
    "\n",
    "        # Second charging phase\n",
    "            # x0 -> compute Tf\n",
    "        u_pred_sb_0_charging_2 = self.approximate_solution(input_sb_0_charging_2).reshape(-1, 1)\n",
    "\n",
    "            # xL -> compute dTf/dx\n",
    "        u_pred_Tf = self.approximate_solution(input_sb_L_charging_2)\n",
    "        u_pred_sb_L_charging_2 = torch.autograd.grad(u_pred_Tf.sum(), input_sb_L_charging_2, create_graph=True)[0][:, 1].reshape(-1, 1) # take only d/dx\n",
    "\n",
    "        \"\"\"Discharging Phase\"\"\"\n",
    "        # First discharging phase\n",
    "            # x0 -> compute dTf/dx\n",
    "        u_pred_Tf = self.approximate_solution(input_sb_0_discharging_1)\n",
    "        u_pred_sb_0_discharging_1 = torch.autograd.grad(u_pred_Tf.sum(), input_sb_0_discharging_1, create_graph=True)[0][:, 1].reshape(-1, 1) # take only d/dx\n",
    "        \n",
    "            # xL -> compute Tf\n",
    "        u_pred_sb_L_discharging_1 = self.approximate_solution(input_sb_L_discharging_1).reshape(-1, 1)\n",
    "\n",
    "        # Second discharging phase\n",
    "            # x0 -> compute dTf/dx\n",
    "        u_pred_Tf = self.approximate_solution(input_sb_0_discharging_2)\n",
    "        u_pred_sb_0_discharging_2 = torch.autograd.grad(u_pred_Tf.sum(), input_sb_0_discharging_2, create_graph=True)[0][:, 1].reshape(-1, 1) # take only d/dx\n",
    "        \n",
    "            # xL -> compute Tf\n",
    "        u_pred_sb_L_discharging_2 = self.approximate_solution(input_sb_L_discharging_2).reshape(-1, 1)\n",
    "\n",
    "        \"\"\"Idle Phase\"\"\"\n",
    "        # First idle phase\n",
    "            # x0 -> compute dTf/dx\n",
    "        u_pred_Tf = self.approximate_solution(input_sb_0_idle_1)\n",
    "        u_pred_sb_0_idle_1 = torch.autograd.grad(u_pred_Tf.sum(), input_sb_0_idle_1, create_graph=True)[0][:, 1].reshape(-1, 1) # take only d/dx\n",
    "\n",
    "            # xL -> compute dTf/dx\n",
    "        u_pred_Tf = self.approximate_solution(input_sb_L_idle_1)\n",
    "        u_pred_sb_L_idle_1 = torch.autograd.grad(u_pred_Tf.sum(), input_sb_L_idle_1, create_graph=True)[0][:, 1].reshape(-1, 1) # take only d/dx\n",
    "\n",
    "        # Second idle phase\n",
    "            # x0 -> compute dTf/dx\n",
    "        u_pred_Tf = self.approximate_solution(input_sb_0_idle_2)\n",
    "        u_pred_sb_0_idle_2 = torch.autograd.grad(u_pred_Tf.sum(), input_sb_0_idle_2, create_graph=True)[0][:, 1].reshape(-1, 1) # take only d/dx\n",
    "\n",
    "            # xL -> compute dTf/dx\n",
    "        u_pred_Tf = self.approximate_solution(input_sb_L_idle_2)\n",
    "        u_pred_sb_L_idle_2 = torch.autograd.grad(u_pred_Tf.sum(), input_sb_L_idle_2, create_graph=True)[0][:, 1].reshape(-1, 1) # take only d/dx\n",
    "\n",
    "        # Third idle phase\n",
    "            # x0 -> compute dTf/dx\n",
    "        u_pred_Tf = self.approximate_solution(input_sb_0_idle_3)\n",
    "        u_pred_sb_0_idle_3 = torch.autograd.grad(u_pred_Tf.sum(), input_sb_0_idle_3, create_graph=True)[0][:, 1].reshape(-1, 1) # take only d/dx\n",
    "\n",
    "            # xL -> compute dTf/dx\n",
    "        u_pred_Tf = self.approximate_solution(input_sb_L_idle_3)\n",
    "        u_pred_sb_L_idle_3 = torch.autograd.grad(u_pred_Tf.sum(), input_sb_L_idle_3, create_graph=True)[0][:, 1].reshape(-1, 1) # take only d/dx\n",
    "\n",
    "        # Fourth idle phase\n",
    "            # x0 -> compute dTf/dx\n",
    "        u_pred_Tf = self.approximate_solution(input_sb_0_idle_4)\n",
    "        u_pred_sb_0_idle_4 = torch.autograd.grad(u_pred_Tf.sum(), input_sb_0_idle_4, create_graph=True)[0][:, 1].reshape(-1, 1) # take only d/dx\n",
    "\n",
    "            # xL -> compute dTf/dx\n",
    "        u_pred_Tf = self.approximate_solution(input_sb_L_idle_4)\n",
    "        u_pred_sb_L_idle_4 = torch.autograd.grad(u_pred_Tf.sum(), input_sb_L_idle_4, create_graph=True)[0][:, 1].reshape(-1, 1) # take only d/dx\n",
    "        \n",
    "        # \"\"\"QUI\"\"\"\n",
    "        # print('CYCLE 1:')\n",
    "        # print('u_pred_sb_0_charging_1: ', u_pred_sb_0_charging_1.shape)\n",
    "        # print('u_pred_sb_L_charging_1: ', u_pred_sb_L_charging_1.shape)\n",
    "        # print('u_pred_sb_0_idle_1: ', u_pred_sb_0_idle_1.shape)\n",
    "        # print('u_pred_sb_L_idle_1: ', u_pred_sb_L_idle_1.shape)\n",
    "        # print('u_pred_sb_0_discharging_1: ', u_pred_sb_0_discharging_1.shape)\n",
    "        # print('u_pred_sb_L_discharging_1: ', u_pred_sb_L_discharging_1.shape)\n",
    "        # print('u_pred_sb_0_idle_2: ', u_pred_sb_0_idle_2.shape)\n",
    "        # print('u_pred_sb_L_idle_2: ', u_pred_sb_L_idle_2.shape)\n",
    "        # print('\\nCYCLE 2:')\n",
    "        # print('u_pred_sb_0_charging_2: ', u_pred_sb_0_charging_2.shape)\n",
    "        # print('u_pred_sb_L_charging_2: ', u_pred_sb_L_charging_2.shape)\n",
    "        # print('u_pred_sb_0_idle_3: ', u_pred_sb_0_idle_3.shape)\n",
    "        # print('u_pred_sb_L_idle_3: ', u_pred_sb_L_idle_3.shape)\n",
    "        # print('u_pred_sb_0_discharging_2: ', u_pred_sb_0_discharging_2.shape)\n",
    "        # print('u_pred_sb_L_discharging_2: ', u_pred_sb_L_discharging_2.shape)\n",
    "        # print('u_pred_sb_0_idle_4: ', u_pred_sb_0_idle_4.shape)\n",
    "        # print('u_pred_sb_L_idle_4: ', u_pred_sb_L_idle_4.shape)\n",
    "\n",
    "        return torch.cat([  # cycle 1\n",
    "                        u_pred_sb_0_charging_1, u_pred_sb_L_charging_1,\n",
    "                        u_pred_sb_0_idle_1, u_pred_sb_L_idle_1,\n",
    "                        u_pred_sb_0_discharging_1, u_pred_sb_L_discharging_1,\n",
    "                        u_pred_sb_0_idle_2, u_pred_sb_L_idle_2,\n",
    "                            # cycle 2\n",
    "                        u_pred_sb_0_charging_2, u_pred_sb_L_charging_2,\n",
    "                        u_pred_sb_0_idle_3, u_pred_sb_L_idle_3,\n",
    "                        u_pred_sb_0_discharging_2, u_pred_sb_L_discharging_2,\n",
    "                        u_pred_sb_0_idle_4, u_pred_sb_L_idle_4\n",
    "                        ], 0)\n",
    "\n",
    "    # Function to compute the PDE residuals\n",
    "    def compute_pde_residual(self, input_int):\n",
    "        input_int.requires_grad = True\n",
    "        u = self.approximate_solution(input_int).reshape(-1,)       # u is the solution (Tf) of the PDE\n",
    "        g = self.approximate_coefficient(input_int).reshape(-1,)    # g is the function (Ts) that is requested\n",
    "\n",
    "        # grad compute the gradient of a \"SCALAR\" function L with respect to some input nxm TENSOR Z=[[x1, y1],[x2,y2],[x3,y3],...,[xn,yn]], m=2\n",
    "        # it returns grad_L = [[dL/dx1, dL/dy1],[dL/dx2, dL/dy2],[dL/dx3, dL/dy3],...,[dL/dxn, dL/dyn]]\n",
    "        # Note: pytorch considers a tensor [u1, u2,u3, ... ,un] a vectorial function\n",
    "        # whereas sum_u = u1 + u2 + u3 + u4 + ... + un as a \"scalar\" one\n",
    "\n",
    "        # In our case ui = u(xi), therefore the line below returns:\n",
    "        # grad_u = [[dsum_u/dx1, dsum_u/dy1],[dsum_u/dx2, dsum_u/dy2],[dsum_u/dx3, dL/dy3],...,[dsum_u/dxm, dsum_u/dyn]]\n",
    "        # and dsum_u/dxi = d(u1 + u2 + u3 + u4 + ... + un)/dxi = d(u(x1) + u(x2) u3(x3) + u4(x4) + ... + u(xn))/dxi = dui/dxi\n",
    "\n",
    "        # Since u for us is u = (uf, us), we have to devide the two cases\n",
    "\n",
    "        grad_u = torch.autograd.grad(u.sum(), input_int, create_graph=True)[0]\n",
    "        grad_u_t = grad_u[:, 0]\n",
    "        grad_u_x = grad_u[:, 1]\n",
    "        grad_u_xx = torch.autograd.grad(grad_u_x.sum(), input_int, create_graph=True)[0][:, 1]\n",
    "\n",
    "        # Compute the velocity of the fluid Uf(t)\n",
    "        Uf = self.fluid_velocity(input_int[:, 0])\n",
    "\n",
    "        residual = (grad_u_t + Uf*grad_u_x) - (self.alpha_f*grad_u_xx - self.h_f*(u-g))\n",
    "\n",
    "        return residual.reshape(-1, )\n",
    "\n",
    "    # Function to compute the total loss (weighted sum of spatial boundary loss, temporal boundary loss and interior loss)\n",
    "    def compute_loss(self, inp_train_sb, u_train_sb, inp_train_tb, u_train_tb, inp_train_int, inp_train_meas, u_train_meas, verbose=True):\n",
    "        u_pred_sb = self.apply_boundary_conditions(inp_train_sb)\n",
    "        u_pred_tb = self.apply_initial_condition(inp_train_tb)\n",
    "        u_pred_meas = self.approximate_solution(inp_train_meas)\n",
    "        print(u_pred_tb.shape)\n",
    "\n",
    "        print(u_pred_tb)\n",
    "        print(u_train_tb)\n",
    "        assert (u_pred_sb.shape[1] == u_train_sb.shape[1])\n",
    "        assert (u_pred_tb.shape[1] == u_train_tb.shape[1])\n",
    "        assert (u_pred_meas.shape[1] == u_train_meas.shape[1])\n",
    "\n",
    "        r_int = self.compute_pde_residual(inp_train_int)\n",
    "        r_sb = u_train_sb - u_pred_sb\n",
    "        r_tb = u_train_tb - u_pred_tb\n",
    "        r_meas = u_train_meas - u_pred_meas\n",
    "\n",
    "        loss_sb = torch.mean(abs(r_sb) ** 2)\n",
    "        loss_tb = torch.mean(abs(r_tb) ** 2)\n",
    "        loss_int = torch.mean(abs(r_int) ** 2)\n",
    "        loss_meas = torch.mean(abs(r_meas) ** 2)\n",
    "\n",
    "        loss_u = loss_sb + loss_tb + loss_meas\n",
    "\n",
    "        loss = torch.log10(self.lambda_u * loss_u + loss_int)\n",
    "        # loss = torch.log10(self.lambda_u * loss_u + loss_int)\n",
    "\n",
    "        if verbose: print(\"Total loss: \", round(loss.item(), 4), \"| PDE Loss: \", round(torch.log10(loss_int).item(), 4), \"| Function Loss: \", round(torch.log10(loss_u).item(), 4))\n",
    "\n",
    "        return loss\n",
    "\n",
    "    ################################################################################################\n",
    "    def fit(self, num_epochs, optimizer, verbose=True):\n",
    "        history = list()\n",
    "\n",
    "        # Loop over epochs\n",
    "        for epoch in range(num_epochs):\n",
    "            if verbose: print(\"################################ \", epoch, \" ################################\")\n",
    "\n",
    "            for j, ((inp_train_sb, u_train_sb), (inp_train_tb, u_train_tb), (inp_train_int, u_train_int), (inp_train_meas, u_train_meas)) in enumerate(zip(self.training_set_sb, self.training_set_tb, self.training_set_int, self.training_set_meas)):\n",
    "                def closure():\n",
    "                    optimizer.zero_grad()\n",
    "                    loss = self.compute_loss(inp_train_sb, u_train_sb, inp_train_tb, u_train_tb, inp_train_int, inp_train_meas, u_train_meas, verbose=verbose)\n",
    "                    loss.backward()\n",
    "\n",
    "                    history.append(loss.item())\n",
    "                    return loss\n",
    "                \n",
    "                optimizer.step(closure=closure)\n",
    "\n",
    "        print('Final Loss: ', history[-1])\n",
    "\n",
    "        return history\n",
    "\n",
    "    ################################################################################################\n",
    "    def plotting(self):\n",
    "        inputs = self.soboleng.draw(100000)\n",
    "\n",
    "        output = self.approximate_solution(inputs).reshape(-1, )\n",
    "        exact_output = self.exact_solution(inputs).reshape(-1, )\n",
    "\n",
    "        fig, axs = plt.subplots(1, 2, figsize=(16, 8), dpi=150)\n",
    "        im1 = axs[0].scatter(inputs[:, 1].detach(), inputs[:, 0].detach(), c=exact_output.detach(), cmap=\"jet\")\n",
    "        axs[0].set_xlabel(\"x\")\n",
    "        axs[0].set_ylabel(\"t\")\n",
    "        plt.colorbar(im1, ax=axs[0])\n",
    "        axs[0].grid(True, which=\"both\", ls=\":\")\n",
    "        im2 = axs[1].scatter(inputs[:, 1].detach(), inputs[:, 0].detach(), c=output.detach(), cmap=\"jet\")\n",
    "        axs[1].set_xlabel(\"x\")\n",
    "        axs[1].set_ylabel(\"t\")\n",
    "        plt.colorbar(im2, ax=axs[1])\n",
    "        axs[1].grid(True, which=\"both\", ls=\":\")\n",
    "        axs[0].set_title(\"Exact Solution\")\n",
    "        axs[1].set_title(\"Approximate Solution\")\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "        err = (torch.mean((output - exact_output) ** 2) / torch.mean(exact_output ** 2)) ** 0.5 * 100\n",
    "        print(\"L2 Relative Error Norm: \", err.item(), \"%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Task2/DataSolution.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m n_sb \u001b[39m=\u001b[39m \u001b[39m64\u001b[39m\n\u001b[1;32m      3\u001b[0m n_tb \u001b[39m=\u001b[39m \u001b[39m64\u001b[39m\n\u001b[0;32m----> 5\u001b[0m pinn \u001b[39m=\u001b[39m Pinns(n_int, n_sb, n_tb)\n",
      "Cell \u001b[0;32mIn[9], line 44\u001b[0m, in \u001b[0;36mPinns.__init__\u001b[0;34m(self, n_int_, n_sb_, n_tb_, alpha_f_, h_f_, T_hot_, T_0_, U_f_)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msoboleng \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mquasirandom\u001b[39m.\u001b[39mSobolEngine(dimension\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdomain_extrema\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m])   \u001b[39m# it will create a 2 cloumns tensor, the rows nunmber is specified after every time it is used\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[39m# Training sets S_sb, S_tb, S_int as torch dataloader\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_set_sb, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_set_tb, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_set_int, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_set_meas \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49massemble_datasets()\n",
      "Cell \u001b[0;32mIn[9], line 271\u001b[0m, in \u001b[0;36mPinns.assemble_datasets\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    269\u001b[0m input_tb, output_tb \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd_temporal_boundary_points()   \u001b[39m# S_tb\u001b[39;00m\n\u001b[1;32m    270\u001b[0m input_int, output_int \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd_interior_points()          \u001b[39m# S_int\u001b[39;00m\n\u001b[0;32m--> 271\u001b[0m input_meas, output_meas \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_measurement_data()       \u001b[39m# S_meas\u001b[39;00m\n\u001b[1;32m    273\u001b[0m \u001b[39m# \"\"\"QUI\"\"\"\u001b[39;00m\n\u001b[1;32m    274\u001b[0m \u001b[39m# print('\\nSONO IN assemble_datasets:')\u001b[39;00m\n\u001b[1;32m    275\u001b[0m \u001b[39m# print('input_sb', input_sb.shape)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[39m# print('output_meas', output_meas.shape)\u001b[39;00m\n\u001b[1;32m    286\u001b[0m \u001b[39m# print()\u001b[39;00m\n\u001b[1;32m    288\u001b[0m training_set_sb \u001b[39m=\u001b[39m DataLoader(torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mTensorDataset(input_sb, output_sb), batch_size\u001b[39m=\u001b[39m\u001b[39m16\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mspace_dimensions\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_sb, shuffle\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)  \u001b[39m#batch_size has *8 since there are 8 different phases and for each one we have 2 conditions (x0 & xL)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[9], line 254\u001b[0m, in \u001b[0;36mPinns.get_measurement_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_measurement_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    253\u001b[0m     \u001b[39m# Read the CSV file into a DataFrame\u001b[39;00m\n\u001b[0;32m--> 254\u001b[0m     df_meas \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(\u001b[39m'\u001b[39;49m\u001b[39mTask2/DataSolution.txt\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m    256\u001b[0m     \u001b[39m# Convert the DataFrame to a torch.tensor\u001b[39;00m\n\u001b[1;32m    257\u001b[0m     tensor_meas \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(df_meas\u001b[39m.\u001b[39mvalues , dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat)\n",
      "File \u001b[0;32m~/miniconda/envs/pytorch/lib/python3.8/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[39m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda/envs/pytorch/lib/python3.8/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda/envs/pytorch/lib/python3.8/site-packages/pandas/io/parsers/readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    946\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[1;32m    947\u001b[0m )\n\u001b[1;32m    948\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 950\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/miniconda/envs/pytorch/lib/python3.8/site-packages/pandas/io/parsers/readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    602\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[1;32m    604\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 605\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    607\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[1;32m    608\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/miniconda/envs/pytorch/lib/python3.8/site-packages/pandas/io/parsers/readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1439\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m   1441\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1442\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[0;32m~/miniconda/envs/pytorch/lib/python3.8/site-packages/pandas/io/parsers/readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode:\n\u001b[1;32m   1734\u001b[0m         mode \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m-> 1735\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(\n\u001b[1;32m   1736\u001b[0m     f,\n\u001b[1;32m   1737\u001b[0m     mode,\n\u001b[1;32m   1738\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1739\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1740\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[1;32m   1741\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[1;32m   1742\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m   1743\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1744\u001b[0m )\n\u001b[1;32m   1745\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/miniconda/envs/pytorch/lib/python3.8/site-packages/pandas/io/common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    851\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    852\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    853\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    854\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[1;32m    855\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[0;32m--> 856\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[1;32m    857\u001b[0m             handle,\n\u001b[1;32m    858\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[1;32m    859\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[1;32m    860\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[1;32m    861\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    862\u001b[0m         )\n\u001b[1;32m    863\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    864\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[1;32m    865\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Task2/DataSolution.txt'"
     ]
    }
   ],
   "source": [
    "n_int = 256\n",
    "n_sb = 64\n",
    "n_tb = 64\n",
    "\n",
    "pinn = Pinns(n_int, n_sb, n_tb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 1\n",
    "optimizer_LBFGS = optim.LBFGS(list(pinn.approximate_solution.parameters()) + list(pinn.approximate_coefficient.parameters()),\n",
    "                              lr=float(0.5),\n",
    "                              max_iter=50000,\n",
    "                              max_eval=50000,\n",
    "                              history_size=150,\n",
    "                              line_search_fn=\"strong_wolfe\",\n",
    "                              tolerance_change=1.0 * np.finfo(float).eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################  0  ################################\n",
      "Total loss:  1.6793 | PDE Loss:  -0.3163 | Function Loss:  0.6749\n",
      "Total loss:  1.6182 | PDE Loss:  -0.0215 | Function Loss:  0.6081\n",
      "Total loss:  1.4394 | PDE Loss:  0.7502 | Function Loss:  0.34\n",
      "Total loss:  1.3976 | PDE Loss:  0.6363 | Function Loss:  0.315\n",
      "Total loss:  1.1993 | PDE Loss:  -0.1245 | Function Loss:  0.1782\n",
      "Total loss:  1.1151 | PDE Loss:  -0.3023 | Function Loss:  0.0982\n",
      "Total loss:  1.1094 | PDE Loss:  -0.3855 | Function Loss:  0.0953\n",
      "Total loss:  1.1066 | PDE Loss:  -0.4141 | Function Loss:  0.0933\n",
      "Total loss:  1.1004 | PDE Loss:  -0.4488 | Function Loss:  0.088\n",
      "Total loss:  1.0932 | PDE Loss:  -0.5054 | Function Loss:  0.0821\n",
      "Total loss:  1.0854 | PDE Loss:  -0.5107 | Function Loss:  0.0742\n",
      "Total loss:  1.0751 | PDE Loss:  -0.5345 | Function Loss:  0.0643\n",
      "Total loss:  1.0678 | PDE Loss:  -0.5401 | Function Loss:  0.057\n",
      "Total loss:  1.0625 | PDE Loss:  -0.5671 | Function Loss:  0.0522\n",
      "Total loss:  1.057 | PDE Loss:  -0.6175 | Function Loss:  0.0478\n",
      "Total loss:  1.0511 | PDE Loss:  -0.6884 | Function Loss:  0.0431\n",
      "Total loss:  1.0442 | PDE Loss:  -0.7781 | Function Loss:  0.0376\n",
      "Total loss:  1.037 | PDE Loss:  -0.7888 | Function Loss:  0.0304\n",
      "Total loss:  1.0312 | PDE Loss:  -0.7671 | Function Loss:  0.0243\n",
      "Total loss:  1.0279 | PDE Loss:  -0.725 | Function Loss:  0.0202\n",
      "Total loss:  1.0252 | PDE Loss:  -0.7129 | Function Loss:  0.0172\n",
      "Total loss:  1.0215 | PDE Loss:  -0.6897 | Function Loss:  0.0129\n",
      "Total loss:  1.0163 | PDE Loss:  -0.6864 | Function Loss:  0.0076\n",
      "Total loss:  1.0091 | PDE Loss:  -0.6611 | Function Loss:  -0.0003\n",
      "Total loss:  0.9992 | PDE Loss:  -0.6833 | Function Loss:  -0.01\n",
      "Total loss:  0.9871 | PDE Loss:  -0.701 | Function Loss:  -0.0219\n",
      "Total loss:  0.9763 | PDE Loss:  -0.7279 | Function Loss:  -0.0324\n",
      "Total loss:  0.973 | PDE Loss:  -0.661 | Function Loss:  -0.0372\n",
      "Total loss:  0.9706 | PDE Loss:  -0.6256 | Function Loss:  -0.0405\n",
      "Total loss:  0.9696 | PDE Loss:  -0.6031 | Function Loss:  -0.0422\n",
      "Total loss:  0.9673 | PDE Loss:  -0.5696 | Function Loss:  -0.0455\n",
      "Total loss:  0.9632 | PDE Loss:  -0.5265 | Function Loss:  -0.0511\n",
      "Total loss:  0.9574 | PDE Loss:  -0.4886 | Function Loss:  -0.0585\n",
      "Total loss:  0.9511 | PDE Loss:  -0.4429 | Function Loss:  -0.0668\n",
      "Total loss:  0.946 | PDE Loss:  -0.4515 | Function Loss:  -0.0717\n",
      "Total loss:  0.9433 | PDE Loss:  -0.4614 | Function Loss:  -0.0741\n",
      "Total loss:  0.9423 | PDE Loss:  -0.4748 | Function Loss:  -0.0747\n",
      "Total loss:  0.9413 | PDE Loss:  -0.49 | Function Loss:  -0.0751\n",
      "Total loss:  0.9399 | PDE Loss:  -0.5121 | Function Loss:  -0.0757\n",
      "Total loss:  0.9377 | PDE Loss:  -0.5457 | Function Loss:  -0.0768\n",
      "Total loss:  0.9343 | PDE Loss:  -0.587 | Function Loss:  -0.079\n",
      "Total loss:  0.9299 | PDE Loss:  -0.627 | Function Loss:  -0.0823\n",
      "Total loss:  0.9249 | PDE Loss:  -0.6302 | Function Loss:  -0.0874\n",
      "Total loss:  0.9223 | PDE Loss:  -0.6442 | Function Loss:  -0.0897\n",
      "Total loss:  0.9212 | PDE Loss:  -0.6421 | Function Loss:  -0.0908\n",
      "Total loss:  0.92 | PDE Loss:  -0.6388 | Function Loss:  -0.0922\n",
      "Total loss:  0.9192 | PDE Loss:  -0.6297 | Function Loss:  -0.0933\n",
      "Total loss:  0.9181 | PDE Loss:  -0.6213 | Function Loss:  -0.0947\n",
      "Total loss:  0.9166 | PDE Loss:  -0.6004 | Function Loss:  -0.0968\n",
      "Total loss:  0.9154 | PDE Loss:  -0.5867 | Function Loss:  -0.0985\n",
      "Total loss:  0.9145 | PDE Loss:  -0.5794 | Function Loss:  -0.0997\n",
      "Total loss:  0.9135 | PDE Loss:  -0.5798 | Function Loss:  -0.1006\n",
      "Total loss:  0.9124 | PDE Loss:  -0.5861 | Function Loss:  -0.1016\n",
      "Total loss:  0.9107 | PDE Loss:  -0.6018 | Function Loss:  -0.1028\n",
      "Total loss:  0.9089 | PDE Loss:  -0.6252 | Function Loss:  -0.104\n",
      "Total loss:  0.9068 | PDE Loss:  -0.6554 | Function Loss:  -0.1053\n",
      "Total loss:  0.9044 | PDE Loss:  -0.7077 | Function Loss:  -0.1064\n",
      "Total loss:  0.9021 | PDE Loss:  -0.7435 | Function Loss:  -0.1079\n",
      "Total loss:  0.9006 | PDE Loss:  -0.7781 | Function Loss:  -0.1086\n",
      "Total loss:  0.8996 | PDE Loss:  -0.7898 | Function Loss:  -0.1093\n",
      "Total loss:  0.8988 | PDE Loss:  -0.7924 | Function Loss:  -0.1101\n",
      "Total loss:  0.8982 | PDE Loss:  -0.7968 | Function Loss:  -0.1106\n",
      "Total loss:  0.8977 | PDE Loss:  -0.7987 | Function Loss:  -0.1111\n",
      "Total loss:  0.8962 | PDE Loss:  -0.8097 | Function Loss:  -0.1124\n",
      "Total loss:  0.8931 | PDE Loss:  -0.8257 | Function Loss:  -0.1153\n",
      "Total loss:  0.8904 | PDE Loss:  -0.8051 | Function Loss:  -0.1184\n",
      "Total loss:  0.8881 | PDE Loss:  -0.7471 | Function Loss:  -0.1221\n",
      "Total loss:  0.8862 | PDE Loss:  -0.6884 | Function Loss:  -0.1255\n",
      "Total loss:  0.8845 | PDE Loss:  -0.6255 | Function Loss:  -0.1292\n",
      "Total loss:  0.8823 | PDE Loss:  -0.5853 | Function Loss:  -0.1327\n",
      "Total loss:  0.8799 | PDE Loss:  -0.5324 | Function Loss:  -0.1372\n",
      "Total loss:  0.8761 | PDE Loss:  -0.4418 | Function Loss:  -0.1454\n",
      "Total loss:  0.8734 | PDE Loss:  -0.3929 | Function Loss:  -0.1508\n",
      "Total loss:  0.8698 | PDE Loss:  -0.3969 | Function Loss:  -0.1544\n",
      "Total loss:  0.8664 | PDE Loss:  -0.3796 | Function Loss:  -0.159\n",
      "Total loss:  0.8634 | PDE Loss:  -0.4106 | Function Loss:  -0.1603\n",
      "Total loss:  0.862 | PDE Loss:  -0.4051 | Function Loss:  -0.1622\n",
      "Total loss:  0.8598 | PDE Loss:  -0.4152 | Function Loss:  -0.1639\n",
      "Total loss:  0.8573 | PDE Loss:  -0.3934 | Function Loss:  -0.1678\n",
      "Total loss:  0.8542 | PDE Loss:  -0.4199 | Function Loss:  -0.1696\n",
      "Total loss:  0.852 | PDE Loss:  -0.4237 | Function Loss:  -0.1717\n",
      "Total loss:  0.8499 | PDE Loss:  -0.4471 | Function Loss:  -0.1726\n",
      "Total loss:  0.8492 | PDE Loss:  -0.4566 | Function Loss:  -0.1728\n",
      "Total loss:  0.8488 | PDE Loss:  -0.4687 | Function Loss:  -0.1726\n",
      "Total loss:  0.8482 | PDE Loss:  -0.4775 | Function Loss:  -0.1728\n",
      "Total loss:  0.8455 | PDE Loss:  -0.5011 | Function Loss:  -0.1745\n",
      "Total loss:  0.8416 | PDE Loss:  -0.5147 | Function Loss:  -0.1779\n",
      "Total loss:  0.8378 | PDE Loss:  -0.5053 | Function Loss:  -0.1823\n",
      "Total loss:  0.8338 | PDE Loss:  -0.4681 | Function Loss:  -0.1885\n",
      "Total loss:  0.8303 | PDE Loss:  -0.4297 | Function Loss:  -0.1942\n",
      "Total loss:  0.8283 | PDE Loss:  -0.4003 | Function Loss:  -0.1981\n",
      "Total loss:  0.8265 | PDE Loss:  -0.3881 | Function Loss:  -0.2008\n",
      "Total loss:  0.8253 | PDE Loss:  -0.3777 | Function Loss:  -0.2028\n",
      "Total loss:  0.8243 | PDE Loss:  -0.3695 | Function Loss:  -0.2045\n",
      "Total loss:  0.8233 | PDE Loss:  -0.3653 | Function Loss:  -0.2058\n",
      "Total loss:  0.8219 | PDE Loss:  -0.3564 | Function Loss:  -0.208\n",
      "Total loss:  0.8189 | PDE Loss:  -0.3471 | Function Loss:  -0.2118\n",
      "Total loss:  0.8161 | PDE Loss:  -0.3128 | Function Loss:  -0.2175\n",
      "Total loss:  0.8136 | PDE Loss:  -0.2957 | Function Loss:  -0.2216\n",
      "Total loss:  0.8105 | PDE Loss:  -0.27 | Function Loss:  -0.2271\n",
      "Total loss:  0.8061 | PDE Loss:  -0.224 | Function Loss:  -0.2365\n",
      "Total loss:  0.8034 | PDE Loss:  -0.2004 | Function Loss:  -0.2419\n",
      "Total loss:  0.8017 | PDE Loss:  -0.2019 | Function Loss:  -0.2437\n",
      "Total loss:  0.8001 | PDE Loss:  -0.2059 | Function Loss:  -0.245\n",
      "Total loss:  0.7985 | PDE Loss:  -0.1989 | Function Loss:  -0.2476\n",
      "Total loss:  0.7969 | PDE Loss:  -0.1883 | Function Loss:  -0.2505\n",
      "Total loss:  0.7952 | PDE Loss:  -0.176 | Function Loss:  -0.2539\n",
      "Total loss:  0.7934 | PDE Loss:  -0.1685 | Function Loss:  -0.2568\n",
      "Total loss:  0.7916 | PDE Loss:  -0.1545 | Function Loss:  -0.2606\n",
      "Total loss:  0.7892 | PDE Loss:  -0.1433 | Function Loss:  -0.2648\n",
      "Total loss:  0.7868 | PDE Loss:  -0.1383 | Function Loss:  -0.2682\n",
      "Total loss:  0.7847 | PDE Loss:  -0.131 | Function Loss:  -0.2715\n",
      "Total loss:  0.7825 | PDE Loss:  -0.1368 | Function Loss:  -0.2733\n",
      "Total loss:  0.7792 | PDE Loss:  -0.1409 | Function Loss:  -0.2764\n",
      "Total loss:  0.7759 | PDE Loss:  -0.1422 | Function Loss:  -0.2801\n",
      "Total loss:  0.7727 | PDE Loss:  -0.1311 | Function Loss:  -0.2852\n",
      "Total loss:  0.7705 | PDE Loss:  -0.1153 | Function Loss:  -0.29\n",
      "Total loss:  0.7692 | PDE Loss:  -0.0919 | Function Loss:  -0.2952\n",
      "Total loss:  0.7683 | PDE Loss:  -0.0823 | Function Loss:  -0.2977\n",
      "Total loss:  0.7672 | PDE Loss:  -0.0702 | Function Loss:  -0.3011\n",
      "Total loss:  0.7653 | PDE Loss:  -0.0583 | Function Loss:  -0.3054\n",
      "Total loss:  0.7619 | PDE Loss:  -0.0437 | Function Loss:  -0.312\n",
      "Total loss:  0.7582 | PDE Loss:  -0.03 | Function Loss:  -0.319\n",
      "Total loss:  0.7556 | PDE Loss:  -0.042 | Function Loss:  -0.3199\n",
      "Total loss:  0.7532 | PDE Loss:  -0.0371 | Function Loss:  -0.3235\n",
      "Total loss:  0.7507 | PDE Loss:  -0.0329 | Function Loss:  -0.3273\n",
      "Total loss:  0.7487 | PDE Loss:  -0.0246 | Function Loss:  -0.3315\n",
      "Total loss:  0.7477 | PDE Loss:  -0.0264 | Function Loss:  -0.3323\n",
      "Total loss:  0.7468 | PDE Loss:  -0.0274 | Function Loss:  -0.3332\n",
      "Total loss:  0.746 | PDE Loss:  -0.0315 | Function Loss:  -0.3333\n",
      "Total loss:  0.7454 | PDE Loss:  -0.0353 | Function Loss:  -0.3333\n",
      "Total loss:  0.7436 | PDE Loss:  -0.0393 | Function Loss:  -0.3347\n",
      "Total loss:  0.7413 | PDE Loss:  -0.0434 | Function Loss:  -0.3366\n",
      "Total loss:  0.7391 | PDE Loss:  -0.041 | Function Loss:  -0.3397\n",
      "Total loss:  0.7375 | PDE Loss:  -0.035 | Function Loss:  -0.3429\n",
      "Total loss:  0.7353 | PDE Loss:  -0.024 | Function Loss:  -0.3478\n",
      "Total loss:  0.7339 | PDE Loss:  -0.0086 | Function Loss:  -0.3528\n",
      "Total loss:  0.7328 | PDE Loss:  -0.0028 | Function Loss:  -0.3554\n",
      "Total loss:  0.732 | PDE Loss:  0.0081 | Function Loss:  -0.3589\n",
      "Total loss:  0.7316 | PDE Loss:  0.0116 | Function Loss:  -0.3602\n",
      "Total loss:  0.7311 | PDE Loss:  0.0141 | Function Loss:  -0.3614\n",
      "Total loss:  0.7306 | PDE Loss:  0.0134 | Function Loss:  -0.3618\n",
      "Total loss:  0.7298 | PDE Loss:  0.0158 | Function Loss:  -0.3634\n",
      "Total loss:  0.729 | PDE Loss:  0.0154 | Function Loss:  -0.3643\n",
      "Total loss:  0.7278 | PDE Loss:  0.0087 | Function Loss:  -0.3642\n",
      "Total loss:  0.7267 | PDE Loss:  0.0111 | Function Loss:  -0.3662\n",
      "Total loss:  0.726 | PDE Loss:  0.0062 | Function Loss:  -0.3659\n",
      "Total loss:  0.7251 | PDE Loss:  0.0005 | Function Loss:  -0.3657\n",
      "Total loss:  0.7243 | PDE Loss:  -0.0014 | Function Loss:  -0.3661\n",
      "Total loss:  0.7239 | PDE Loss:  -0.0022 | Function Loss:  -0.3665\n",
      "Total loss:  0.723 | PDE Loss:  -0.0023 | Function Loss:  -0.3675\n",
      "Total loss:  0.7217 | PDE Loss:  -0.0004 | Function Loss:  -0.3696\n",
      "Total loss:  0.7204 | PDE Loss:  0.0006 | Function Loss:  -0.3715\n",
      "Total loss:  0.719 | PDE Loss:  0.0023 | Function Loss:  -0.3736\n",
      "Total loss:  0.7179 | PDE Loss:  -0.0037 | Function Loss:  -0.3736\n",
      "Total loss:  0.717 | PDE Loss:  -0.0044 | Function Loss:  -0.3744\n",
      "Total loss:  0.7165 | PDE Loss:  -0.0074 | Function Loss:  -0.3744\n",
      "Total loss:  0.7157 | PDE Loss:  -0.0132 | Function Loss:  -0.3741\n",
      "Total loss:  0.7145 | PDE Loss:  -0.018 | Function Loss:  -0.3744\n",
      "Total loss:  0.7129 | PDE Loss:  -0.0184 | Function Loss:  -0.3763\n",
      "Total loss:  0.7109 | PDE Loss:  -0.0183 | Function Loss:  -0.3787\n",
      "Total loss:  0.7096 | PDE Loss:  -0.0077 | Function Loss:  -0.3828\n",
      "Total loss:  0.7086 | PDE Loss:  0.0014 | Function Loss:  -0.3862\n",
      "Total loss:  0.7078 | PDE Loss:  0.0109 | Function Loss:  -0.3896\n",
      "Total loss:  0.7069 | PDE Loss:  0.0226 | Function Loss:  -0.3938\n",
      "Total loss:  0.7058 | PDE Loss:  0.0325 | Function Loss:  -0.3977\n",
      "Total loss:  0.7046 | PDE Loss:  0.0436 | Function Loss:  -0.4023\n",
      "Total loss:  0.7031 | PDE Loss:  0.0509 | Function Loss:  -0.4063\n",
      "Total loss:  0.7012 | PDE Loss:  0.0608 | Function Loss:  -0.4116\n",
      "Total loss:  0.6998 | PDE Loss:  0.0614 | Function Loss:  -0.4137\n",
      "Total loss:  0.6986 | PDE Loss:  0.0636 | Function Loss:  -0.4158\n",
      "Total loss:  0.6977 | PDE Loss:  0.0636 | Function Loss:  -0.4171\n",
      "Total loss:  0.6965 | PDE Loss:  0.0587 | Function Loss:  -0.4172\n",
      "Total loss:  0.6953 | PDE Loss:  0.0579 | Function Loss:  -0.4185\n",
      "Total loss:  0.6945 | PDE Loss:  0.0602 | Function Loss:  -0.4203\n",
      "Total loss:  0.694 | PDE Loss:  0.0602 | Function Loss:  -0.4208\n",
      "Total loss:  0.6938 | PDE Loss:  0.0626 | Function Loss:  -0.4219\n",
      "Total loss:  0.6935 | PDE Loss:  0.0662 | Function Loss:  -0.4234\n",
      "Total loss:  0.6929 | PDE Loss:  0.0716 | Function Loss:  -0.4258\n",
      "Total loss:  0.6918 | PDE Loss:  0.0796 | Function Loss:  -0.4298\n",
      "Total loss:  0.6902 | PDE Loss:  0.0904 | Function Loss:  -0.4354\n",
      "Total loss:  0.688 | PDE Loss:  0.1015 | Function Loss:  -0.4422\n",
      "Total loss:  0.6862 | PDE Loss:  0.1112 | Function Loss:  -0.4482\n",
      "Total loss:  0.6841 | PDE Loss:  0.1216 | Function Loss:  -0.4548\n",
      "Total loss:  0.6832 | PDE Loss:  0.1174 | Function Loss:  -0.4546\n",
      "Total loss:  0.6824 | PDE Loss:  0.1149 | Function Loss:  -0.4546\n",
      "Total loss:  0.6812 | PDE Loss:  0.1074 | Function Loss:  -0.4537\n",
      "Total loss:  0.6797 | PDE Loss:  0.1056 | Function Loss:  -0.455\n",
      "Total loss:  0.6781 | PDE Loss:  0.1031 | Function Loss:  -0.4563\n",
      "Total loss:  0.6766 | PDE Loss:  0.1029 | Function Loss:  -0.4583\n",
      "Total loss:  0.6751 | PDE Loss:  0.108 | Function Loss:  -0.4622\n",
      "Total loss:  0.6728 | PDE Loss:  0.1089 | Function Loss:  -0.4657\n",
      "Total loss:  0.6697 | PDE Loss:  0.1083 | Function Loss:  -0.4697\n",
      "Total loss:  0.6683 | PDE Loss:  0.1043 | Function Loss:  -0.4702\n",
      "Total loss:  0.6662 | PDE Loss:  0.0965 | Function Loss:  -0.4701\n",
      "Total loss:  0.6654 | PDE Loss:  0.0899 | Function Loss:  -0.4688\n",
      "Total loss:  0.6646 | PDE Loss:  0.0881 | Function Loss:  -0.4691\n",
      "Total loss:  0.6642 | PDE Loss:  0.0829 | Function Loss:  -0.4679\n",
      "Total loss:  0.6638 | PDE Loss:  0.0791 | Function Loss:  -0.4671\n",
      "Total loss:  0.663 | PDE Loss:  0.074 | Function Loss:  -0.4663\n",
      "Total loss:  0.6621 | PDE Loss:  0.0662 | Function Loss:  -0.4649\n",
      "Total loss:  0.661 | PDE Loss:  0.0601 | Function Loss:  -0.4643\n",
      "Total loss:  0.6602 | PDE Loss:  0.0568 | Function Loss:  -0.4643\n",
      "Total loss:  0.6595 | PDE Loss:  0.056 | Function Loss:  -0.465\n",
      "Total loss:  0.6588 | PDE Loss:  0.0563 | Function Loss:  -0.466\n",
      "Total loss:  0.6581 | PDE Loss:  0.0582 | Function Loss:  -0.4676\n",
      "Total loss:  0.6572 | PDE Loss:  0.0593 | Function Loss:  -0.4691\n",
      "Total loss:  0.6564 | PDE Loss:  0.0611 | Function Loss:  -0.4708\n",
      "Total loss:  0.6556 | PDE Loss:  0.062 | Function Loss:  -0.4721\n",
      "Total loss:  0.6549 | PDE Loss:  0.063 | Function Loss:  -0.4734\n",
      "Total loss:  0.654 | PDE Loss:  0.0631 | Function Loss:  -0.4747\n",
      "Total loss:  0.6526 | PDE Loss:  0.0643 | Function Loss:  -0.477\n",
      "Total loss:  0.6504 | PDE Loss:  0.0653 | Function Loss:  -0.4804\n",
      "Total loss:  0.6478 | PDE Loss:  0.0719 | Function Loss:  -0.4862\n",
      "Total loss:  0.6459 | PDE Loss:  0.0752 | Function Loss:  -0.49\n",
      "Total loss:  0.6445 | PDE Loss:  0.0792 | Function Loss:  -0.4934\n",
      "Total loss:  0.6439 | PDE Loss:  0.0838 | Function Loss:  -0.496\n",
      "Total loss:  0.6434 | PDE Loss:  0.0812 | Function Loss:  -0.4957\n",
      "Total loss:  0.643 | PDE Loss:  0.0802 | Function Loss:  -0.4958\n",
      "Total loss:  0.6426 | PDE Loss:  0.0754 | Function Loss:  -0.4946\n",
      "Total loss:  0.642 | PDE Loss:  0.072 | Function Loss:  -0.4942\n",
      "Total loss:  0.6407 | PDE Loss:  0.0615 | Function Loss:  -0.4921\n",
      "Total loss:  0.6387 | PDE Loss:  0.0491 | Function Loss:  -0.4905\n",
      "Total loss:  0.6368 | PDE Loss:  0.0339 | Function Loss:  -0.4879\n",
      "Total loss:  0.6354 | PDE Loss:  0.0204 | Function Loss:  -0.4853\n",
      "Total loss:  0.6341 | PDE Loss:  0.01 | Function Loss:  -0.4837\n",
      "Total loss:  0.633 | PDE Loss:  0.0035 | Function Loss:  -0.4832\n",
      "Total loss:  0.6312 | PDE Loss:  -0.0076 | Function Loss:  -0.4821\n",
      "Total loss:  0.6299 | PDE Loss:  -0.0088 | Function Loss:  -0.4835\n",
      "Total loss:  0.6291 | PDE Loss:  -0.0104 | Function Loss:  -0.484\n",
      "Total loss:  0.6285 | PDE Loss:  -0.0075 | Function Loss:  -0.4856\n",
      "Total loss:  0.6279 | PDE Loss:  -0.0062 | Function Loss:  -0.4869\n",
      "Total loss:  0.6272 | PDE Loss:  -0.0013 | Function Loss:  -0.4893\n",
      "Total loss:  0.6265 | PDE Loss:  0.0012 | Function Loss:  -0.4909\n",
      "Total loss:  0.6254 | PDE Loss:  0.0019 | Function Loss:  -0.4926\n",
      "Total loss:  0.6243 | PDE Loss:  0.0017 | Function Loss:  -0.4939\n",
      "Total loss:  0.6232 | PDE Loss:  -0.0054 | Function Loss:  -0.4932\n",
      "Total loss:  0.6221 | PDE Loss:  -0.0107 | Function Loss:  -0.4931\n",
      "Total loss:  0.6213 | PDE Loss:  -0.0141 | Function Loss:  -0.4931\n",
      "Total loss:  0.62 | PDE Loss:  -0.0214 | Function Loss:  -0.4926\n",
      "Total loss:  0.6187 | PDE Loss:  -0.019 | Function Loss:  -0.4949\n",
      "Total loss:  0.6175 | PDE Loss:  -0.0161 | Function Loss:  -0.4974\n",
      "Total loss:  0.6166 | PDE Loss:  -0.0101 | Function Loss:  -0.5004\n",
      "Total loss:  0.6156 | PDE Loss:  0.0 | Function Loss:  -0.5049\n",
      "Total loss:  0.6142 | PDE Loss:  0.0117 | Function Loss:  -0.5106\n",
      "Total loss:  0.6127 | PDE Loss:  0.0273 | Function Loss:  -0.5179\n",
      "Total loss:  0.6116 | PDE Loss:  0.0346 | Function Loss:  -0.522\n",
      "Total loss:  0.6103 | PDE Loss:  0.037 | Function Loss:  -0.5246\n",
      "Total loss:  0.6091 | PDE Loss:  0.0397 | Function Loss:  -0.5274\n",
      "Total loss:  0.6078 | PDE Loss:  0.0352 | Function Loss:  -0.5274\n",
      "Total loss:  0.606 | PDE Loss:  0.0216 | Function Loss:  -0.525\n",
      "Total loss:  0.6043 | PDE Loss:  0.0124 | Function Loss:  -0.5241\n",
      "Total loss:  0.6029 | PDE Loss:  0.0025 | Function Loss:  -0.5226\n",
      "Total loss:  0.602 | PDE Loss:  -0.0013 | Function Loss:  -0.5225\n",
      "Total loss:  0.6015 | PDE Loss:  -0.0011 | Function Loss:  -0.5232\n",
      "Total loss:  0.6005 | PDE Loss:  0.0015 | Function Loss:  -0.5255\n",
      "Total loss:  0.5987 | PDE Loss:  0.0068 | Function Loss:  -0.5296\n",
      "Total loss:  0.5969 | PDE Loss:  0.0099 | Function Loss:  -0.5332\n",
      "Total loss:  0.5949 | PDE Loss:  0.0174 | Function Loss:  -0.5385\n",
      "Total loss:  0.593 | PDE Loss:  0.0148 | Function Loss:  -0.5402\n",
      "Total loss:  0.5905 | PDE Loss:  0.0177 | Function Loss:  -0.5446\n",
      "Total loss:  0.5882 | PDE Loss:  0.0143 | Function Loss:  -0.5465\n",
      "Total loss:  0.5861 | PDE Loss:  0.0044 | Function Loss:  -0.5459\n",
      "Total loss:  0.584 | PDE Loss:  0.0082 | Function Loss:  -0.55\n",
      "Total loss:  0.5805 | PDE Loss:  0.0054 | Function Loss:  -0.5537\n",
      "Total loss:  0.5763 | PDE Loss:  0.005 | Function Loss:  -0.5594\n",
      "Total loss:  0.5754 | PDE Loss:  0.0139 | Function Loss:  -0.5639\n",
      "Total loss:  0.5733 | PDE Loss:  0.0034 | Function Loss:  -0.5629\n",
      "Total loss:  0.5712 | PDE Loss:  0.0047 | Function Loss:  -0.5662\n",
      "Total loss:  0.5702 | PDE Loss:  -0.0008 | Function Loss:  -0.5656\n",
      "Total loss:  0.5689 | PDE Loss:  -0.003 | Function Loss:  -0.5666\n",
      "Total loss:  0.5667 | PDE Loss:  -0.0088 | Function Loss:  -0.5674\n",
      "Total loss:  0.565 | PDE Loss:  -0.0181 | Function Loss:  -0.5664\n",
      "Total loss:  0.5632 | PDE Loss:  -0.0229 | Function Loss:  -0.5672\n",
      "Total loss:  0.5621 | PDE Loss:  -0.0251 | Function Loss:  -0.568\n",
      "Total loss:  0.5609 | PDE Loss:  -0.0269 | Function Loss:  -0.5689\n",
      "Total loss:  0.5599 | PDE Loss:  -0.0312 | Function Loss:  -0.5688\n",
      "Total loss:  0.5589 | PDE Loss:  -0.0286 | Function Loss:  -0.571\n",
      "Total loss:  0.5578 | PDE Loss:  -0.0308 | Function Loss:  -0.5717\n",
      "Total loss:  0.5565 | PDE Loss:  -0.0299 | Function Loss:  -0.5738\n",
      "Total loss:  0.5544 | PDE Loss:  -0.0279 | Function Loss:  -0.5774\n",
      "Total loss:  0.5526 | PDE Loss:  -0.0266 | Function Loss:  -0.5803\n",
      "Total loss:  0.5512 | PDE Loss:  -0.0206 | Function Loss:  -0.5844\n",
      "Total loss:  0.5495 | PDE Loss:  -0.0187 | Function Loss:  -0.5874\n",
      "Total loss:  0.547 | PDE Loss:  -0.0197 | Function Loss:  -0.5903\n",
      "Total loss:  0.5445 | PDE Loss:  -0.0182 | Function Loss:  -0.5944\n",
      "Total loss:  0.5422 | PDE Loss:  -0.0151 | Function Loss:  -0.5988\n",
      "Total loss:  0.5396 | PDE Loss:  -0.0121 | Function Loss:  -0.6035\n",
      "Total loss:  0.5377 | PDE Loss:  -0.0128 | Function Loss:  -0.6059\n",
      "Total loss:  0.5362 | PDE Loss:  -0.0133 | Function Loss:  -0.6077\n",
      "Total loss:  0.535 | PDE Loss:  -0.0192 | Function Loss:  -0.6071\n",
      "Total loss:  0.5341 | PDE Loss:  -0.0199 | Function Loss:  -0.6081\n",
      "Total loss:  0.533 | PDE Loss:  -0.0292 | Function Loss:  -0.6061\n",
      "Total loss:  0.532 | PDE Loss:  -0.0323 | Function Loss:  -0.6063\n",
      "Total loss:  0.5295 | PDE Loss:  -0.0374 | Function Loss:  -0.6078\n",
      "Total loss:  0.5255 | PDE Loss:  -0.0359 | Function Loss:  -0.6138\n",
      "Total loss:  0.5222 | PDE Loss:  -0.0389 | Function Loss:  -0.6173\n",
      "Total loss:  0.5205 | PDE Loss:  -0.0319 | Function Loss:  -0.6224\n",
      "Total loss:  0.5193 | PDE Loss:  -0.0336 | Function Loss:  -0.6234\n",
      "Total loss:  0.5186 | PDE Loss:  -0.0319 | Function Loss:  -0.625\n",
      "Total loss:  0.5172 | PDE Loss:  -0.0279 | Function Loss:  -0.6286\n",
      "Total loss:  0.5154 | PDE Loss:  -0.0225 | Function Loss:  -0.6331\n",
      "Total loss:  0.514 | PDE Loss:  -0.0215 | Function Loss:  -0.6356\n",
      "Total loss:  0.5125 | PDE Loss:  -0.0159 | Function Loss:  -0.6401\n",
      "Total loss:  0.5099 | PDE Loss:  -0.0166 | Function Loss:  -0.6435\n",
      "Total loss:  0.507 | PDE Loss:  -0.0054 | Function Loss:  -0.6525\n",
      "Total loss:  0.5044 | PDE Loss:  -0.011 | Function Loss:  -0.6537\n",
      "Total loss:  0.5019 | PDE Loss:  -0.0071 | Function Loss:  -0.6591\n",
      "Total loss:  0.5002 | PDE Loss:  -0.0123 | Function Loss:  -0.6592\n",
      "Total loss:  0.4992 | PDE Loss:  -0.0183 | Function Loss:  -0.658\n",
      "Total loss:  0.4988 | PDE Loss:  -0.0191 | Function Loss:  -0.6582\n",
      "Total loss:  0.4984 | PDE Loss:  -0.0194 | Function Loss:  -0.6587\n",
      "Total loss:  0.498 | PDE Loss:  -0.0208 | Function Loss:  -0.6587\n",
      "Total loss:  0.4973 | PDE Loss:  -0.0223 | Function Loss:  -0.659\n",
      "Total loss:  0.4965 | PDE Loss:  -0.0248 | Function Loss:  -0.659\n",
      "Total loss:  0.4954 | PDE Loss:  -0.027 | Function Loss:  -0.6597\n",
      "Total loss:  0.4943 | PDE Loss:  -0.0332 | Function Loss:  -0.6587\n",
      "Total loss:  0.4931 | PDE Loss:  -0.0374 | Function Loss:  -0.6586\n",
      "Total loss:  0.4919 | PDE Loss:  -0.0431 | Function Loss:  -0.6579\n",
      "Total loss:  0.4908 | PDE Loss:  -0.0498 | Function Loss:  -0.6568\n",
      "Total loss:  0.4899 | PDE Loss:  -0.0539 | Function Loss:  -0.6563\n",
      "Total loss:  0.4892 | PDE Loss:  -0.0583 | Function Loss:  -0.6555\n",
      "Total loss:  0.4887 | PDE Loss:  -0.061 | Function Loss:  -0.6552\n",
      "Total loss:  0.4881 | PDE Loss:  -0.0621 | Function Loss:  -0.6556\n",
      "Total loss:  0.4874 | PDE Loss:  -0.0629 | Function Loss:  -0.6562\n",
      "Total loss:  0.4866 | PDE Loss:  -0.0648 | Function Loss:  -0.6567\n",
      "Total loss:  0.4855 | PDE Loss:  -0.0639 | Function Loss:  -0.6585\n",
      "Total loss:  0.4838 | PDE Loss:  -0.072 | Function Loss:  -0.6577\n",
      "Total loss:  0.482 | PDE Loss:  -0.0652 | Function Loss:  -0.6629\n",
      "Total loss:  0.4801 | PDE Loss:  -0.0764 | Function Loss:  -0.6611\n",
      "Total loss:  0.4788 | PDE Loss:  -0.0805 | Function Loss:  -0.6614\n",
      "Total loss:  0.477 | PDE Loss:  -0.0817 | Function Loss:  -0.6634\n",
      "Total loss:  0.4751 | PDE Loss:  -0.0851 | Function Loss:  -0.6647\n",
      "Total loss:  0.4736 | PDE Loss:  -0.0836 | Function Loss:  -0.6674\n",
      "Total loss:  0.4723 | PDE Loss:  -0.0818 | Function Loss:  -0.6699\n",
      "Total loss:  0.4712 | PDE Loss:  -0.0819 | Function Loss:  -0.6713\n",
      "Total loss:  0.4705 | PDE Loss:  -0.0781 | Function Loss:  -0.6739\n",
      "Total loss:  0.4698 | PDE Loss:  -0.0763 | Function Loss:  -0.6755\n",
      "Total loss:  0.4693 | PDE Loss:  -0.0778 | Function Loss:  -0.6757\n",
      "Total loss:  0.4682 | PDE Loss:  -0.0788 | Function Loss:  -0.6768\n",
      "Total loss:  0.4667 | PDE Loss:  -0.0851 | Function Loss:  -0.6764\n",
      "Total loss:  0.4652 | PDE Loss:  -0.089 | Function Loss:  -0.6769\n",
      "Total loss:  0.4637 | PDE Loss:  -0.0971 | Function Loss:  -0.6759\n",
      "Total loss:  0.462 | PDE Loss:  -0.1019 | Function Loss:  -0.6764\n",
      "Total loss:  0.4604 | PDE Loss:  -0.1128 | Function Loss:  -0.6746\n",
      "Total loss:  0.4587 | PDE Loss:  -0.1185 | Function Loss:  -0.6749\n",
      "Total loss:  0.4568 | PDE Loss:  -0.1293 | Function Loss:  -0.6736\n",
      "Total loss:  0.4552 | PDE Loss:  -0.133 | Function Loss:  -0.6745\n",
      "Total loss:  0.454 | PDE Loss:  -0.1399 | Function Loss:  -0.6737\n",
      "Total loss:  0.4534 | PDE Loss:  -0.1432 | Function Loss:  -0.6733\n",
      "Total loss:  0.4528 | PDE Loss:  -0.1456 | Function Loss:  -0.6733\n",
      "Total loss:  0.4524 | PDE Loss:  -0.1477 | Function Loss:  -0.6731\n",
      "Total loss:  0.4521 | PDE Loss:  -0.1486 | Function Loss:  -0.6733\n",
      "Total loss:  0.4517 | PDE Loss:  -0.1498 | Function Loss:  -0.6734\n",
      "Total loss:  0.4513 | PDE Loss:  -0.1505 | Function Loss:  -0.6737\n",
      "Total loss:  0.4507 | PDE Loss:  -0.1522 | Function Loss:  -0.6739\n",
      "Total loss:  0.4499 | PDE Loss:  -0.1534 | Function Loss:  -0.6746\n",
      "Total loss:  0.449 | PDE Loss:  -0.1572 | Function Loss:  -0.6746\n",
      "Total loss:  0.4476 | PDE Loss:  -0.1604 | Function Loss:  -0.6754\n",
      "Total loss:  0.4457 | PDE Loss:  -0.1653 | Function Loss:  -0.6763\n",
      "Total loss:  0.4437 | PDE Loss:  -0.1715 | Function Loss:  -0.677\n",
      "Total loss:  0.4417 | PDE Loss:  -0.1726 | Function Loss:  -0.6792\n",
      "Total loss:  0.4402 | PDE Loss:  -0.1727 | Function Loss:  -0.6811\n",
      "Total loss:  0.439 | PDE Loss:  -0.1701 | Function Loss:  -0.6837\n",
      "Total loss:  0.4379 | PDE Loss:  -0.1669 | Function Loss:  -0.6861\n",
      "Total loss:  0.4373 | PDE Loss:  -0.1666 | Function Loss:  -0.687\n",
      "Total loss:  0.4368 | PDE Loss:  -0.1637 | Function Loss:  -0.6886\n",
      "Total loss:  0.4362 | PDE Loss:  -0.1635 | Function Loss:  -0.6895\n",
      "Total loss:  0.4347 | PDE Loss:  -0.1646 | Function Loss:  -0.6911\n",
      "Total loss:  0.4329 | PDE Loss:  -0.1631 | Function Loss:  -0.694\n",
      "Total loss:  0.4313 | PDE Loss:  -0.1662 | Function Loss:  -0.6952\n",
      "Total loss:  0.4293 | PDE Loss:  -0.1679 | Function Loss:  -0.6973\n",
      "Total loss:  0.427 | PDE Loss:  -0.1807 | Function Loss:  -0.696\n",
      "Total loss:  0.4251 | PDE Loss:  -0.1862 | Function Loss:  -0.6968\n",
      "Total loss:  0.4237 | PDE Loss:  -0.1939 | Function Loss:  -0.6962\n",
      "Total loss:  0.4229 | PDE Loss:  -0.1967 | Function Loss:  -0.6963\n",
      "Total loss:  0.4225 | PDE Loss:  -0.1978 | Function Loss:  -0.6965\n",
      "Total loss:  0.4222 | PDE Loss:  -0.1995 | Function Loss:  -0.6964\n",
      "Total loss:  0.422 | PDE Loss:  -0.1986 | Function Loss:  -0.697\n",
      "Total loss:  0.4217 | PDE Loss:  -0.1981 | Function Loss:  -0.6975\n",
      "Total loss:  0.4215 | PDE Loss:  -0.196 | Function Loss:  -0.6985\n",
      "Total loss:  0.4211 | PDE Loss:  -0.1934 | Function Loss:  -0.6998\n",
      "Total loss:  0.4205 | PDE Loss:  -0.1901 | Function Loss:  -0.7016\n",
      "Total loss:  0.4195 | PDE Loss:  -0.1848 | Function Loss:  -0.7047\n",
      "Total loss:  0.418 | PDE Loss:  -0.1799 | Function Loss:  -0.7084\n",
      "Total loss:  0.4159 | PDE Loss:  -0.1736 | Function Loss:  -0.7133\n",
      "Total loss:  0.4133 | PDE Loss:  -0.1699 | Function Loss:  -0.7182\n",
      "Total loss:  0.4099 | PDE Loss:  -0.1687 | Function Loss:  -0.7231\n",
      "Total loss:  0.4071 | PDE Loss:  -0.1698 | Function Loss:  -0.7265\n",
      "Total loss:  0.405 | PDE Loss:  -0.1774 | Function Loss:  -0.7267\n",
      "Total loss:  0.4034 | PDE Loss:  -0.1775 | Function Loss:  -0.7288\n",
      "Total loss:  0.4024 | PDE Loss:  -0.1821 | Function Loss:  -0.7286\n",
      "Total loss:  0.4011 | PDE Loss:  -0.1892 | Function Loss:  -0.7279\n",
      "Total loss:  0.4 | PDE Loss:  -0.1973 | Function Loss:  -0.7265\n",
      "Total loss:  0.3992 | PDE Loss:  -0.1984 | Function Loss:  -0.7272\n",
      "Total loss:  0.3983 | PDE Loss:  -0.2025 | Function Loss:  -0.7271\n",
      "Total loss:  0.3975 | PDE Loss:  -0.2035 | Function Loss:  -0.7278\n",
      "Total loss:  0.3964 | PDE Loss:  -0.2044 | Function Loss:  -0.729\n",
      "Total loss:  0.3953 | PDE Loss:  -0.2052 | Function Loss:  -0.7302\n",
      "Total loss:  0.394 | PDE Loss:  -0.2046 | Function Loss:  -0.7322\n",
      "Total loss:  0.3927 | PDE Loss:  -0.2033 | Function Loss:  -0.7343\n",
      "Total loss:  0.3916 | PDE Loss:  -0.1994 | Function Loss:  -0.7371\n",
      "Total loss:  0.391 | PDE Loss:  -0.1972 | Function Loss:  -0.7387\n",
      "Total loss:  0.3904 | PDE Loss:  -0.1948 | Function Loss:  -0.7404\n",
      "Total loss:  0.3898 | PDE Loss:  -0.1915 | Function Loss:  -0.7423\n",
      "Total loss:  0.3892 | PDE Loss:  -0.1903 | Function Loss:  -0.7435\n",
      "Total loss:  0.3886 | PDE Loss:  -0.1893 | Function Loss:  -0.7448\n",
      "Total loss:  0.3876 | PDE Loss:  -0.1866 | Function Loss:  -0.747\n",
      "Total loss:  0.3868 | PDE Loss:  -0.1908 | Function Loss:  -0.7467\n",
      "Total loss:  0.3856 | PDE Loss:  -0.1934 | Function Loss:  -0.7474\n",
      "Total loss:  0.384 | PDE Loss:  -0.198 | Function Loss:  -0.7478\n",
      "Total loss:  0.3827 | PDE Loss:  -0.2032 | Function Loss:  -0.7478\n",
      "Total loss:  0.3812 | PDE Loss:  -0.2051 | Function Loss:  -0.7491\n",
      "Total loss:  0.3797 | PDE Loss:  -0.212 | Function Loss:  -0.7488\n",
      "Total loss:  0.3785 | PDE Loss:  -0.2095 | Function Loss:  -0.7512\n",
      "Total loss:  0.3774 | PDE Loss:  -0.2096 | Function Loss:  -0.7527\n",
      "Total loss:  0.3768 | PDE Loss:  -0.2196 | Function Loss:  -0.7501\n",
      "Total loss:  0.3762 | PDE Loss:  -0.2153 | Function Loss:  -0.7523\n",
      "Total loss:  0.3757 | PDE Loss:  -0.2133 | Function Loss:  -0.7537\n",
      "Total loss:  0.3754 | PDE Loss:  -0.2108 | Function Loss:  -0.7549\n",
      "Total loss:  0.3751 | PDE Loss:  -0.2082 | Function Loss:  -0.7563\n",
      "Total loss:  0.3746 | PDE Loss:  -0.2065 | Function Loss:  -0.7575\n",
      "Total loss:  0.3741 | PDE Loss:  -0.2027 | Function Loss:  -0.7596\n",
      "Total loss:  0.3734 | PDE Loss:  -0.2018 | Function Loss:  -0.7608\n",
      "Total loss:  0.3727 | PDE Loss:  -0.2009 | Function Loss:  -0.7621\n",
      "Total loss:  0.3719 | PDE Loss:  -0.2019 | Function Loss:  -0.7629\n",
      "Total loss:  0.3711 | PDE Loss:  -0.2053 | Function Loss:  -0.7627\n",
      "Total loss:  0.3704 | PDE Loss:  -0.2075 | Function Loss:  -0.7628\n",
      "Total loss:  0.3699 | PDE Loss:  -0.2113 | Function Loss:  -0.7622\n",
      "Total loss:  0.3695 | PDE Loss:  -0.2141 | Function Loss:  -0.7617\n",
      "Total loss:  0.3688 | PDE Loss:  -0.2178 | Function Loss:  -0.7614\n",
      "Total loss:  0.3678 | PDE Loss:  -0.2227 | Function Loss:  -0.7611\n",
      "Total loss:  0.3666 | PDE Loss:  -0.2254 | Function Loss:  -0.7617\n",
      "Total loss:  0.3656 | PDE Loss:  -0.2293 | Function Loss:  -0.7618\n",
      "Total loss:  0.3645 | PDE Loss:  -0.2283 | Function Loss:  -0.7635\n",
      "Total loss:  0.3637 | PDE Loss:  -0.2257 | Function Loss:  -0.7655\n",
      "Total loss:  0.3631 | PDE Loss:  -0.2232 | Function Loss:  -0.7673\n",
      "Total loss:  0.3624 | PDE Loss:  -0.2208 | Function Loss:  -0.769\n",
      "Total loss:  0.3614 | PDE Loss:  -0.2194 | Function Loss:  -0.7708\n",
      "Total loss:  0.3601 | PDE Loss:  -0.2189 | Function Loss:  -0.7728\n",
      "Total loss:  0.3586 | PDE Loss:  -0.2188 | Function Loss:  -0.7749\n",
      "Total loss:  0.3572 | PDE Loss:  -0.2222 | Function Loss:  -0.7756\n",
      "Total loss:  0.3559 | PDE Loss:  -0.2236 | Function Loss:  -0.7768\n",
      "Total loss:  0.355 | PDE Loss:  -0.2237 | Function Loss:  -0.778\n",
      "Total loss:  0.3542 | PDE Loss:  -0.2276 | Function Loss:  -0.7777\n",
      "Total loss:  0.3533 | PDE Loss:  -0.2257 | Function Loss:  -0.7797\n",
      "Total loss:  0.3525 | PDE Loss:  -0.2269 | Function Loss:  -0.7803\n",
      "Total loss:  0.3517 | PDE Loss:  -0.226 | Function Loss:  -0.7817\n",
      "Total loss:  0.3508 | PDE Loss:  -0.2262 | Function Loss:  -0.7828\n",
      "Total loss:  0.3499 | PDE Loss:  -0.2271 | Function Loss:  -0.7838\n",
      "Total loss:  0.3487 | PDE Loss:  -0.2305 | Function Loss:  -0.7841\n",
      "Total loss:  0.3471 | PDE Loss:  -0.2347 | Function Loss:  -0.7848\n",
      "Total loss:  0.3456 | PDE Loss:  -0.2408 | Function Loss:  -0.7847\n",
      "Total loss:  0.3443 | PDE Loss:  -0.2444 | Function Loss:  -0.7852\n",
      "Total loss:  0.3428 | PDE Loss:  -0.2483 | Function Loss:  -0.7859\n",
      "Total loss:  0.3412 | PDE Loss:  -0.2545 | Function Loss:  -0.7859\n",
      "Total loss:  0.34 | PDE Loss:  -0.2576 | Function Loss:  -0.7865\n",
      "Total loss:  0.339 | PDE Loss:  -0.2607 | Function Loss:  -0.7868\n",
      "Total loss:  0.3379 | PDE Loss:  -0.2654 | Function Loss:  -0.7866\n",
      "Total loss:  0.3369 | PDE Loss:  -0.2704 | Function Loss:  -0.7863\n",
      "Total loss:  0.336 | PDE Loss:  -0.2735 | Function Loss:  -0.7865\n",
      "Total loss:  0.3347 | PDE Loss:  -0.2803 | Function Loss:  -0.786\n",
      "Total loss:  0.3334 | PDE Loss:  -0.2858 | Function Loss:  -0.786\n",
      "Total loss:  0.3321 | PDE Loss:  -0.287 | Function Loss:  -0.7873\n",
      "Total loss:  0.3309 | PDE Loss:  -0.2887 | Function Loss:  -0.7883\n",
      "Total loss:  0.3296 | PDE Loss:  -0.2874 | Function Loss:  -0.7905\n",
      "Total loss:  0.3281 | PDE Loss:  -0.2837 | Function Loss:  -0.7936\n",
      "Total loss:  0.3266 | PDE Loss:  -0.2838 | Function Loss:  -0.7956\n",
      "Total loss:  0.3256 | PDE Loss:  -0.2789 | Function Loss:  -0.7986\n",
      "Total loss:  0.3248 | PDE Loss:  -0.2782 | Function Loss:  -0.7998\n",
      "Total loss:  0.3242 | PDE Loss:  -0.2754 | Function Loss:  -0.8016\n",
      "Total loss:  0.3237 | PDE Loss:  -0.2753 | Function Loss:  -0.8023\n",
      "Total loss:  0.3233 | PDE Loss:  -0.2747 | Function Loss:  -0.8029\n",
      "Total loss:  0.323 | PDE Loss:  -0.2729 | Function Loss:  -0.8039\n",
      "Total loss:  0.3227 | PDE Loss:  -0.274 | Function Loss:  -0.8041\n",
      "Total loss:  0.322 | PDE Loss:  -0.2737 | Function Loss:  -0.805\n",
      "Total loss:  0.3214 | PDE Loss:  -0.2752 | Function Loss:  -0.8054\n",
      "Total loss:  0.3206 | PDE Loss:  -0.2776 | Function Loss:  -0.8056\n",
      "Total loss:  0.3198 | PDE Loss:  -0.2813 | Function Loss:  -0.8055\n",
      "Total loss:  0.319 | PDE Loss:  -0.2849 | Function Loss:  -0.8054\n",
      "Total loss:  0.3182 | PDE Loss:  -0.2897 | Function Loss:  -0.8048\n",
      "Total loss:  0.3174 | PDE Loss:  -0.2939 | Function Loss:  -0.8045\n",
      "Total loss:  0.3167 | PDE Loss:  -0.298 | Function Loss:  -0.8042\n",
      "Total loss:  0.3157 | PDE Loss:  -0.3006 | Function Loss:  -0.8046\n",
      "Total loss:  0.3149 | PDE Loss:  -0.3044 | Function Loss:  -0.8044\n",
      "Total loss:  0.3142 | PDE Loss:  -0.3071 | Function Loss:  -0.8045\n",
      "Total loss:  0.3136 | PDE Loss:  -0.3087 | Function Loss:  -0.8048\n",
      "Total loss:  0.3128 | PDE Loss:  -0.3101 | Function Loss:  -0.8055\n",
      "Total loss:  0.3116 | PDE Loss:  -0.311 | Function Loss:  -0.8067\n",
      "Total loss:  0.3103 | PDE Loss:  -0.3137 | Function Loss:  -0.8075\n",
      "Total loss:  0.3091 | PDE Loss:  -0.3148 | Function Loss:  -0.8088\n",
      "Total loss:  0.3082 | PDE Loss:  -0.3168 | Function Loss:  -0.8094\n",
      "Total loss:  0.3073 | PDE Loss:  -0.3184 | Function Loss:  -0.81\n",
      "Total loss:  0.3064 | PDE Loss:  -0.3213 | Function Loss:  -0.8104\n",
      "Total loss:  0.3051 | PDE Loss:  -0.3226 | Function Loss:  -0.8116\n",
      "Total loss:  0.3032 | PDE Loss:  -0.3273 | Function Loss:  -0.8127\n",
      "Total loss:  0.3008 | PDE Loss:  -0.329 | Function Loss:  -0.8153\n",
      "Total loss:  0.2983 | PDE Loss:  -0.3379 | Function Loss:  -0.8158\n",
      "Total loss:  0.2959 | PDE Loss:  -0.347 | Function Loss:  -0.8163\n",
      "Total loss:  0.2944 | PDE Loss:  -0.3493 | Function Loss:  -0.8175\n",
      "Total loss:  0.2929 | PDE Loss:  -0.3568 | Function Loss:  -0.8173\n",
      "Total loss:  0.2914 | PDE Loss:  -0.3618 | Function Loss:  -0.8178\n",
      "Total loss:  0.2902 | PDE Loss:  -0.3665 | Function Loss:  -0.8179\n",
      "Total loss:  0.2898 | PDE Loss:  -0.3696 | Function Loss:  -0.8176\n",
      "Total loss:  0.2894 | PDE Loss:  -0.3701 | Function Loss:  -0.818\n",
      "Total loss:  0.289 | PDE Loss:  -0.3719 | Function Loss:  -0.818\n",
      "Total loss:  0.2886 | PDE Loss:  -0.3738 | Function Loss:  -0.8179\n",
      "Total loss:  0.2882 | PDE Loss:  -0.3736 | Function Loss:  -0.8185\n",
      "Total loss:  0.2878 | PDE Loss:  -0.3754 | Function Loss:  -0.8186\n",
      "Total loss:  0.2874 | PDE Loss:  -0.3773 | Function Loss:  -0.8185\n",
      "Total loss:  0.2872 | PDE Loss:  -0.3764 | Function Loss:  -0.8191\n",
      "Total loss:  0.287 | PDE Loss:  -0.3766 | Function Loss:  -0.8193\n",
      "Total loss:  0.2867 | PDE Loss:  -0.3775 | Function Loss:  -0.8194\n",
      "Total loss:  0.2863 | PDE Loss:  -0.3771 | Function Loss:  -0.82\n",
      "Total loss:  0.2857 | PDE Loss:  -0.3778 | Function Loss:  -0.8205\n",
      "Total loss:  0.2851 | PDE Loss:  -0.3777 | Function Loss:  -0.8213\n",
      "Total loss:  0.2843 | PDE Loss:  -0.3776 | Function Loss:  -0.8223\n",
      "Total loss:  0.286 | PDE Loss:  -0.3666 | Function Loss:  -0.8234\n",
      "Total loss:  0.284 | PDE Loss:  -0.3757 | Function Loss:  -0.8233\n",
      "Total loss:  0.2834 | PDE Loss:  -0.376 | Function Loss:  -0.8239\n",
      "Total loss:  0.2827 | PDE Loss:  -0.3736 | Function Loss:  -0.8256\n",
      "Total loss:  0.282 | PDE Loss:  -0.3721 | Function Loss:  -0.8269\n",
      "Total loss:  0.2814 | PDE Loss:  -0.3693 | Function Loss:  -0.8284\n",
      "Total loss:  0.281 | PDE Loss:  -0.3683 | Function Loss:  -0.8292\n",
      "Total loss:  0.2806 | PDE Loss:  -0.368 | Function Loss:  -0.8299\n",
      "Total loss:  0.2801 | PDE Loss:  -0.3686 | Function Loss:  -0.8304\n",
      "Total loss:  0.2796 | PDE Loss:  -0.3698 | Function Loss:  -0.8307\n",
      "Total loss:  0.279 | PDE Loss:  -0.3711 | Function Loss:  -0.8311\n",
      "Total loss:  0.2784 | PDE Loss:  -0.3726 | Function Loss:  -0.8314\n",
      "Total loss:  0.2778 | PDE Loss:  -0.3744 | Function Loss:  -0.8317\n",
      "Total loss:  0.277 | PDE Loss:  -0.3728 | Function Loss:  -0.8332\n",
      "Total loss:  0.276 | PDE Loss:  -0.3736 | Function Loss:  -0.8342\n",
      "Total loss:  0.2751 | PDE Loss:  -0.3696 | Function Loss:  -0.8365\n",
      "Total loss:  0.2742 | PDE Loss:  -0.366 | Function Loss:  -0.8388\n",
      "Total loss:  0.273 | PDE Loss:  -0.3615 | Function Loss:  -0.8417\n",
      "Total loss:  0.2715 | PDE Loss:  -0.3533 | Function Loss:  -0.8461\n",
      "Total loss:  0.2701 | PDE Loss:  -0.3474 | Function Loss:  -0.8498\n",
      "Total loss:  0.2688 | PDE Loss:  -0.3407 | Function Loss:  -0.8536\n",
      "Total loss:  0.2676 | PDE Loss:  -0.3338 | Function Loss:  -0.8576\n",
      "Total loss:  0.2663 | PDE Loss:  -0.3279 | Function Loss:  -0.8613\n",
      "Total loss:  0.2649 | PDE Loss:  -0.3219 | Function Loss:  -0.8652\n",
      "Total loss:  0.2634 | PDE Loss:  -0.3178 | Function Loss:  -0.8688\n",
      "Total loss:  0.262 | PDE Loss:  -0.3152 | Function Loss:  -0.8715\n",
      "Total loss:  0.2611 | PDE Loss:  -0.3087 | Function Loss:  -0.8752\n",
      "Total loss:  0.2605 | PDE Loss:  -0.3108 | Function Loss:  -0.8753\n",
      "Total loss:  0.2599 | PDE Loss:  -0.3107 | Function Loss:  -0.876\n",
      "Total loss:  0.2595 | PDE Loss:  -0.3104 | Function Loss:  -0.8767\n",
      "Total loss:  0.259 | PDE Loss:  -0.3096 | Function Loss:  -0.8777\n",
      "Total loss:  0.2588 | PDE Loss:  -0.3093 | Function Loss:  -0.8781\n",
      "Total loss:  0.2585 | PDE Loss:  -0.3094 | Function Loss:  -0.8784\n",
      "Total loss:  0.2582 | PDE Loss:  -0.3089 | Function Loss:  -0.8791\n",
      "Total loss:  0.2577 | PDE Loss:  -0.3079 | Function Loss:  -0.8801\n",
      "Total loss:  0.2573 | PDE Loss:  -0.3071 | Function Loss:  -0.881\n",
      "Total loss:  0.2567 | PDE Loss:  -0.3043 | Function Loss:  -0.8828\n",
      "Total loss:  0.2558 | PDE Loss:  -0.3037 | Function Loss:  -0.8843\n",
      "Total loss:  0.2549 | PDE Loss:  -0.2984 | Function Loss:  -0.8876\n",
      "Total loss:  0.2539 | PDE Loss:  -0.2931 | Function Loss:  -0.8911\n",
      "Total loss:  0.2525 | PDE Loss:  -0.2895 | Function Loss:  -0.8944\n",
      "Total loss:  0.2513 | PDE Loss:  -0.2854 | Function Loss:  -0.8978\n",
      "Total loss:  0.25 | PDE Loss:  -0.2832 | Function Loss:  -0.9005\n",
      "Total loss:  0.249 | PDE Loss:  -0.2806 | Function Loss:  -0.9031\n",
      "Total loss:  0.2482 | PDE Loss:  -0.2831 | Function Loss:  -0.9032\n",
      "Total loss:  0.2475 | PDE Loss:  -0.2817 | Function Loss:  -0.9047\n",
      "Total loss:  0.2467 | PDE Loss:  -0.2846 | Function Loss:  -0.9046\n",
      "Total loss:  0.2457 | PDE Loss:  -0.2873 | Function Loss:  -0.9049\n",
      "Total loss:  0.2445 | PDE Loss:  -0.2917 | Function Loss:  -0.9048\n",
      "Total loss:  0.2435 | PDE Loss:  -0.2947 | Function Loss:  -0.905\n",
      "Total loss:  0.2428 | PDE Loss:  -0.2964 | Function Loss:  -0.9053\n",
      "Total loss:  0.2421 | PDE Loss:  -0.2979 | Function Loss:  -0.9057\n",
      "Total loss:  0.2415 | PDE Loss:  -0.2987 | Function Loss:  -0.9062\n",
      "Total loss:  0.2408 | PDE Loss:  -0.2953 | Function Loss:  -0.9085\n",
      "Total loss:  0.2404 | PDE Loss:  -0.294 | Function Loss:  -0.9097\n",
      "Total loss:  0.2398 | PDE Loss:  -0.2926 | Function Loss:  -0.9111\n",
      "Total loss:  0.2392 | PDE Loss:  -0.2915 | Function Loss:  -0.9124\n",
      "Total loss:  0.2389 | PDE Loss:  -0.2943 | Function Loss:  -0.9117\n",
      "Total loss:  0.2384 | PDE Loss:  -0.2944 | Function Loss:  -0.9123\n",
      "Total loss:  0.238 | PDE Loss:  -0.2941 | Function Loss:  -0.913\n",
      "Total loss:  0.2377 | PDE Loss:  -0.2954 | Function Loss:  -0.9129\n",
      "Total loss:  0.2373 | PDE Loss:  -0.2961 | Function Loss:  -0.9132\n",
      "Total loss:  0.2369 | PDE Loss:  -0.2975 | Function Loss:  -0.9132\n",
      "Total loss:  0.2363 | PDE Loss:  -0.2961 | Function Loss:  -0.9145\n",
      "Total loss:  0.2357 | PDE Loss:  -0.2961 | Function Loss:  -0.9154\n",
      "Total loss:  0.2351 | PDE Loss:  -0.2943 | Function Loss:  -0.917\n",
      "Total loss:  0.2344 | PDE Loss:  -0.2915 | Function Loss:  -0.9191\n",
      "Total loss:  0.234 | PDE Loss:  -0.2893 | Function Loss:  -0.9207\n",
      "Total loss:  0.2336 | PDE Loss:  -0.2871 | Function Loss:  -0.9223\n",
      "Total loss:  0.233 | PDE Loss:  -0.2849 | Function Loss:  -0.924\n",
      "Total loss:  0.2323 | PDE Loss:  -0.284 | Function Loss:  -0.9255\n",
      "Total loss:  0.2313 | PDE Loss:  -0.2785 | Function Loss:  -0.9294\n",
      "Total loss:  0.2304 | PDE Loss:  -0.2792 | Function Loss:  -0.9303\n",
      "Total loss:  0.2284 | PDE Loss:  -0.2818 | Function Loss:  -0.932\n",
      "Total loss:  0.2271 | PDE Loss:  -0.2828 | Function Loss:  -0.9335\n",
      "Total loss:  0.2263 | PDE Loss:  -0.2854 | Function Loss:  -0.9335\n",
      "Total loss:  0.2258 | PDE Loss:  -0.2854 | Function Loss:  -0.9343\n",
      "Total loss:  0.2253 | PDE Loss:  -0.2893 | Function Loss:  -0.9333\n",
      "Total loss:  0.225 | PDE Loss:  -0.2909 | Function Loss:  -0.9329\n",
      "Total loss:  0.2248 | PDE Loss:  -0.2923 | Function Loss:  -0.9327\n",
      "Total loss:  0.2245 | PDE Loss:  -0.2941 | Function Loss:  -0.9322\n",
      "Total loss:  0.2242 | PDE Loss:  -0.2952 | Function Loss:  -0.9322\n",
      "Total loss:  0.2239 | PDE Loss:  -0.2965 | Function Loss:  -0.932\n",
      "Total loss:  0.2235 | PDE Loss:  -0.2978 | Function Loss:  -0.932\n",
      "Total loss:  0.223 | PDE Loss:  -0.2974 | Function Loss:  -0.933\n",
      "Total loss:  0.2225 | PDE Loss:  -0.2973 | Function Loss:  -0.9337\n",
      "Total loss:  0.2221 | PDE Loss:  -0.2968 | Function Loss:  -0.9345\n",
      "Total loss:  0.2216 | PDE Loss:  -0.2945 | Function Loss:  -0.9362\n",
      "Total loss:  0.2212 | PDE Loss:  -0.292 | Function Loss:  -0.938\n",
      "Total loss:  0.2208 | PDE Loss:  -0.2922 | Function Loss:  -0.9385\n",
      "Total loss:  0.2201 | PDE Loss:  -0.2893 | Function Loss:  -0.9407\n",
      "Total loss:  0.2195 | PDE Loss:  -0.2895 | Function Loss:  -0.9415\n",
      "Total loss:  0.2189 | PDE Loss:  -0.2897 | Function Loss:  -0.9423\n",
      "Total loss:  0.2183 | PDE Loss:  -0.2907 | Function Loss:  -0.9427\n",
      "Total loss:  0.2178 | PDE Loss:  -0.2917 | Function Loss:  -0.943\n",
      "Total loss:  0.2173 | PDE Loss:  -0.2938 | Function Loss:  -0.9427\n",
      "Total loss:  0.2168 | PDE Loss:  -0.2951 | Function Loss:  -0.9428\n",
      "Total loss:  0.2164 | PDE Loss:  -0.2968 | Function Loss:  -0.9427\n",
      "Total loss:  0.216 | PDE Loss:  -0.2981 | Function Loss:  -0.9427\n",
      "Total loss:  0.2156 | PDE Loss:  -0.299 | Function Loss:  -0.9428\n",
      "Total loss:  0.2153 | PDE Loss:  -0.2999 | Function Loss:  -0.9429\n",
      "Total loss:  0.215 | PDE Loss:  -0.3008 | Function Loss:  -0.943\n",
      "Total loss:  0.2147 | PDE Loss:  -0.302 | Function Loss:  -0.9429\n",
      "Total loss:  0.2145 | PDE Loss:  -0.3021 | Function Loss:  -0.9432\n",
      "Total loss:  0.2142 | PDE Loss:  -0.3029 | Function Loss:  -0.9432\n",
      "Total loss:  0.2139 | PDE Loss:  -0.3035 | Function Loss:  -0.9433\n",
      "Total loss:  0.2136 | PDE Loss:  -0.3045 | Function Loss:  -0.9434\n",
      "Total loss:  0.2132 | PDE Loss:  -0.3054 | Function Loss:  -0.9436\n",
      "Total loss:  0.2127 | PDE Loss:  -0.3061 | Function Loss:  -0.9439\n",
      "Total loss:  0.2122 | PDE Loss:  -0.3064 | Function Loss:  -0.9446\n",
      "Total loss:  0.2116 | PDE Loss:  -0.306 | Function Loss:  -0.9455\n",
      "Total loss:  0.2111 | PDE Loss:  -0.3053 | Function Loss:  -0.9467\n",
      "Total loss:  0.2105 | PDE Loss:  -0.3045 | Function Loss:  -0.9478\n",
      "Total loss:  0.21 | PDE Loss:  -0.3032 | Function Loss:  -0.9491\n",
      "Total loss:  0.2093 | PDE Loss:  -0.3031 | Function Loss:  -0.9501\n",
      "Total loss:  0.2086 | PDE Loss:  -0.3024 | Function Loss:  -0.9515\n",
      "Total loss:  0.208 | PDE Loss:  -0.3024 | Function Loss:  -0.9524\n",
      "Total loss:  0.2073 | PDE Loss:  -0.3032 | Function Loss:  -0.953\n",
      "Total loss:  0.2066 | PDE Loss:  -0.303 | Function Loss:  -0.9542\n",
      "Total loss:  0.2057 | PDE Loss:  -0.3054 | Function Loss:  -0.9544\n",
      "Total loss:  0.2047 | PDE Loss:  -0.306 | Function Loss:  -0.9555\n",
      "Total loss:  0.2038 | PDE Loss:  -0.3067 | Function Loss:  -0.9565\n",
      "Total loss:  0.203 | PDE Loss:  -0.3079 | Function Loss:  -0.9571\n",
      "Total loss:  0.2024 | PDE Loss:  -0.3081 | Function Loss:  -0.9579\n",
      "Total loss:  0.202 | PDE Loss:  -0.3077 | Function Loss:  -0.9586\n",
      "Total loss:  0.2016 | PDE Loss:  -0.3089 | Function Loss:  -0.9586\n",
      "Total loss:  0.2012 | PDE Loss:  -0.3089 | Function Loss:  -0.9593\n",
      "Total loss:  0.2008 | PDE Loss:  -0.3091 | Function Loss:  -0.9598\n",
      "Total loss:  0.2002 | PDE Loss:  -0.3097 | Function Loss:  -0.9604\n",
      "Total loss:  0.1996 | PDE Loss:  -0.3098 | Function Loss:  -0.9612\n",
      "Total loss:  0.1991 | PDE Loss:  -0.3122 | Function Loss:  -0.9608\n",
      "Total loss:  0.1988 | PDE Loss:  -0.313 | Function Loss:  -0.961\n",
      "Total loss:  0.1982 | PDE Loss:  -0.3143 | Function Loss:  -0.9612\n",
      "Total loss:  0.1976 | PDE Loss:  -0.3168 | Function Loss:  -0.961\n",
      "Total loss:  0.1982 | PDE Loss:  -0.3217 | Function Loss:  -0.958\n",
      "Total loss:  0.1973 | PDE Loss:  -0.3191 | Function Loss:  -0.9604\n",
      "Total loss:  0.1966 | PDE Loss:  -0.319 | Function Loss:  -0.9615\n",
      "Total loss:  0.196 | PDE Loss:  -0.3193 | Function Loss:  -0.9622\n",
      "Total loss:  0.1951 | PDE Loss:  -0.3186 | Function Loss:  -0.9638\n",
      "Total loss:  0.1944 | PDE Loss:  -0.3171 | Function Loss:  -0.9655\n",
      "Total loss:  0.1939 | PDE Loss:  -0.3156 | Function Loss:  -0.9669\n",
      "Total loss:  0.1935 | PDE Loss:  -0.315 | Function Loss:  -0.9678\n",
      "Total loss:  0.1931 | PDE Loss:  -0.3133 | Function Loss:  -0.9691\n",
      "Total loss:  0.1928 | PDE Loss:  -0.3132 | Function Loss:  -0.9695\n",
      "Total loss:  0.1924 | PDE Loss:  -0.314 | Function Loss:  -0.9697\n",
      "Total loss:  0.192 | PDE Loss:  -0.3156 | Function Loss:  -0.9697\n",
      "Total loss:  0.1916 | PDE Loss:  -0.3174 | Function Loss:  -0.9694\n",
      "Total loss:  0.1911 | PDE Loss:  -0.3205 | Function Loss:  -0.9687\n",
      "Total loss:  0.1907 | PDE Loss:  -0.323 | Function Loss:  -0.9682\n",
      "Total loss:  0.1904 | PDE Loss:  -0.3252 | Function Loss:  -0.9677\n",
      "Total loss:  0.1901 | PDE Loss:  -0.3269 | Function Loss:  -0.9674\n",
      "Total loss:  0.1897 | PDE Loss:  -0.3284 | Function Loss:  -0.9672\n",
      "Total loss:  0.1893 | PDE Loss:  -0.33 | Function Loss:  -0.9672\n",
      "Total loss:  0.1887 | PDE Loss:  -0.332 | Function Loss:  -0.9671\n",
      "Total loss:  0.1881 | PDE Loss:  -0.3348 | Function Loss:  -0.9668\n",
      "Total loss:  0.1874 | PDE Loss:  -0.3386 | Function Loss:  -0.9662\n",
      "Total loss:  0.1866 | PDE Loss:  -0.3433 | Function Loss:  -0.9654\n",
      "Total loss:  0.1857 | PDE Loss:  -0.3489 | Function Loss:  -0.9643\n",
      "Total loss:  0.1847 | PDE Loss:  -0.3556 | Function Loss:  -0.963\n",
      "Total loss:  0.1838 | PDE Loss:  -0.3617 | Function Loss:  -0.9618\n",
      "Total loss:  0.1831 | PDE Loss:  -0.3649 | Function Loss:  -0.9615\n",
      "Total loss:  0.1825 | PDE Loss:  -0.3676 | Function Loss:  -0.9613\n",
      "Total loss:  0.182 | PDE Loss:  -0.3682 | Function Loss:  -0.9617\n",
      "Total loss:  0.1814 | PDE Loss:  -0.369 | Function Loss:  -0.9623\n",
      "Total loss:  0.1808 | PDE Loss:  -0.3682 | Function Loss:  -0.9634\n",
      "Total loss:  0.1803 | PDE Loss:  -0.3664 | Function Loss:  -0.9648\n",
      "Total loss:  0.1798 | PDE Loss:  -0.3632 | Function Loss:  -0.9668\n",
      "Total loss:  0.1794 | PDE Loss:  -0.3614 | Function Loss:  -0.968\n",
      "Total loss:  0.179 | PDE Loss:  -0.3587 | Function Loss:  -0.9697\n",
      "Total loss:  0.1786 | PDE Loss:  -0.3575 | Function Loss:  -0.9707\n",
      "Total loss:  0.1783 | PDE Loss:  -0.3565 | Function Loss:  -0.9716\n",
      "Total loss:  0.1779 | PDE Loss:  -0.3558 | Function Loss:  -0.9725\n",
      "Total loss:  0.1774 | PDE Loss:  -0.3559 | Function Loss:  -0.9732\n",
      "Total loss:  0.1768 | PDE Loss:  -0.355 | Function Loss:  -0.9743\n",
      "Total loss:  0.1763 | PDE Loss:  -0.3551 | Function Loss:  -0.9751\n",
      "Total loss:  0.1755 | PDE Loss:  -0.3548 | Function Loss:  -0.9763\n",
      "Total loss:  0.1748 | PDE Loss:  -0.3545 | Function Loss:  -0.9775\n",
      "Total loss:  0.1741 | PDE Loss:  -0.3557 | Function Loss:  -0.9779\n",
      "Total loss:  0.1734 | PDE Loss:  -0.3554 | Function Loss:  -0.979\n",
      "Total loss:  0.1727 | PDE Loss:  -0.3569 | Function Loss:  -0.9793\n",
      "Total loss:  0.172 | PDE Loss:  -0.3574 | Function Loss:  -0.9802\n",
      "Total loss:  0.1713 | PDE Loss:  -0.3626 | Function Loss:  -0.979\n",
      "Total loss:  0.1707 | PDE Loss:  -0.3621 | Function Loss:  -0.98\n",
      "Total loss:  0.1702 | PDE Loss:  -0.3622 | Function Loss:  -0.9808\n",
      "Total loss:  0.1695 | PDE Loss:  -0.3624 | Function Loss:  -0.9815\n",
      "Total loss:  0.169 | PDE Loss:  -0.3633 | Function Loss:  -0.9819\n",
      "Total loss:  0.1686 | PDE Loss:  -0.3647 | Function Loss:  -0.9819\n",
      "Total loss:  0.1683 | PDE Loss:  -0.3653 | Function Loss:  -0.9821\n",
      "Total loss:  0.168 | PDE Loss:  -0.3666 | Function Loss:  -0.982\n",
      "Total loss:  0.1677 | PDE Loss:  -0.3672 | Function Loss:  -0.9821\n",
      "Total loss:  0.1676 | PDE Loss:  -0.3671 | Function Loss:  -0.9824\n",
      "Total loss:  0.1673 | PDE Loss:  -0.367 | Function Loss:  -0.9828\n",
      "Total loss:  0.167 | PDE Loss:  -0.3666 | Function Loss:  -0.9834\n",
      "Total loss:  0.1666 | PDE Loss:  -0.3655 | Function Loss:  -0.9844\n",
      "Total loss:  0.1662 | PDE Loss:  -0.3652 | Function Loss:  -0.9851\n",
      "Total loss:  0.1657 | PDE Loss:  -0.3648 | Function Loss:  -0.9859\n",
      "Total loss:  0.1651 | PDE Loss:  -0.3645 | Function Loss:  -0.987\n",
      "Total loss:  0.1643 | PDE Loss:  -0.3622 | Function Loss:  -0.989\n",
      "Total loss:  0.1635 | PDE Loss:  -0.3634 | Function Loss:  -0.9897\n",
      "Total loss:  0.1628 | PDE Loss:  -0.3619 | Function Loss:  -0.9913\n",
      "Total loss:  0.1621 | PDE Loss:  -0.3625 | Function Loss:  -0.9921\n",
      "Total loss:  0.1615 | PDE Loss:  -0.3632 | Function Loss:  -0.9926\n",
      "Total loss:  0.1611 | PDE Loss:  -0.3645 | Function Loss:  -0.9927\n",
      "Total loss:  0.1608 | PDE Loss:  -0.3662 | Function Loss:  -0.9924\n",
      "Total loss:  0.1604 | PDE Loss:  -0.3669 | Function Loss:  -0.9926\n",
      "Total loss:  0.1601 | PDE Loss:  -0.3675 | Function Loss:  -0.9928\n",
      "Total loss:  0.1598 | PDE Loss:  -0.3696 | Function Loss:  -0.9924\n",
      "Total loss:  0.1595 | PDE Loss:  -0.3704 | Function Loss:  -0.9924\n",
      "Total loss:  0.1593 | PDE Loss:  -0.3713 | Function Loss:  -0.9924\n",
      "Total loss:  0.159 | PDE Loss:  -0.3724 | Function Loss:  -0.9923\n",
      "Total loss:  0.1588 | PDE Loss:  -0.374 | Function Loss:  -0.992\n",
      "Total loss:  0.1585 | PDE Loss:  -0.376 | Function Loss:  -0.9916\n",
      "Total loss:  0.1581 | PDE Loss:  -0.3783 | Function Loss:  -0.9911\n",
      "Total loss:  0.1577 | PDE Loss:  -0.3816 | Function Loss:  -0.9904\n",
      "Total loss:  0.1572 | PDE Loss:  -0.3865 | Function Loss:  -0.989\n",
      "Total loss:  0.1568 | PDE Loss:  -0.3879 | Function Loss:  -0.989\n",
      "Total loss:  0.1563 | PDE Loss:  -0.3897 | Function Loss:  -0.9891\n",
      "Total loss:  0.1557 | PDE Loss:  -0.3919 | Function Loss:  -0.989\n",
      "Total loss:  0.1553 | PDE Loss:  -0.3924 | Function Loss:  -0.9894\n",
      "Total loss:  0.155 | PDE Loss:  -0.3933 | Function Loss:  -0.9895\n",
      "Total loss:  0.1547 | PDE Loss:  -0.3935 | Function Loss:  -0.9898\n",
      "Total loss:  0.1545 | PDE Loss:  -0.395 | Function Loss:  -0.9895\n",
      "Total loss:  0.1543 | PDE Loss:  -0.3946 | Function Loss:  -0.99\n",
      "Total loss:  0.154 | PDE Loss:  -0.3944 | Function Loss:  -0.9904\n",
      "Total loss:  0.1537 | PDE Loss:  -0.394 | Function Loss:  -0.9909\n",
      "Total loss:  0.1534 | PDE Loss:  -0.3941 | Function Loss:  -0.9914\n",
      "Total loss:  0.153 | PDE Loss:  -0.3926 | Function Loss:  -0.9926\n",
      "Total loss:  0.1525 | PDE Loss:  -0.3963 | Function Loss:  -0.9917\n",
      "Total loss:  0.1521 | PDE Loss:  -0.3953 | Function Loss:  -0.9926\n",
      "Total loss:  0.1516 | PDE Loss:  -0.394 | Function Loss:  -0.9939\n",
      "Total loss:  0.151 | PDE Loss:  -0.3947 | Function Loss:  -0.9945\n",
      "Total loss:  0.1503 | PDE Loss:  -0.3943 | Function Loss:  -0.9956\n",
      "Total loss:  0.1496 | PDE Loss:  -0.3952 | Function Loss:  -0.9963\n",
      "Total loss:  0.1487 | PDE Loss:  -0.3976 | Function Loss:  -0.9965\n",
      "Total loss:  0.1479 | PDE Loss:  -0.4009 | Function Loss:  -0.9963\n",
      "Total loss:  0.1474 | PDE Loss:  -0.4034 | Function Loss:  -0.996\n",
      "Total loss:  0.1469 | PDE Loss:  -0.4061 | Function Loss:  -0.9958\n",
      "Total loss:  0.1463 | PDE Loss:  -0.4106 | Function Loss:  -0.9948\n",
      "Total loss:  0.1458 | PDE Loss:  -0.4129 | Function Loss:  -0.9946\n",
      "Total loss:  0.145 | PDE Loss:  -0.4193 | Function Loss:  -0.9933\n",
      "Total loss:  0.1439 | PDE Loss:  -0.4214 | Function Loss:  -0.994\n",
      "Total loss:  0.1434 | PDE Loss:  -0.4231 | Function Loss:  -0.9941\n",
      "Total loss:  0.1427 | PDE Loss:  -0.4246 | Function Loss:  -0.9945\n",
      "Total loss:  0.1422 | PDE Loss:  -0.4265 | Function Loss:  -0.9944\n",
      "Total loss:  0.142 | PDE Loss:  -0.4265 | Function Loss:  -0.9947\n",
      "Total loss:  0.1419 | PDE Loss:  -0.4258 | Function Loss:  -0.9952\n",
      "Total loss:  0.1418 | PDE Loss:  -0.4255 | Function Loss:  -0.9954\n",
      "Total loss:  0.1417 | PDE Loss:  -0.4251 | Function Loss:  -0.9957\n",
      "Total loss:  0.1415 | PDE Loss:  -0.4249 | Function Loss:  -0.996\n",
      "Total loss:  0.1413 | PDE Loss:  -0.4254 | Function Loss:  -0.9961\n",
      "Total loss:  0.141 | PDE Loss:  -0.426 | Function Loss:  -0.9962\n",
      "Total loss:  0.1407 | PDE Loss:  -0.4273 | Function Loss:  -0.9962\n",
      "Total loss:  0.1403 | PDE Loss:  -0.4288 | Function Loss:  -0.9962\n",
      "Total loss:  0.1398 | PDE Loss:  -0.4323 | Function Loss:  -0.9956\n",
      "Total loss:  0.1394 | PDE Loss:  -0.4335 | Function Loss:  -0.9957\n",
      "Total loss:  0.1389 | PDE Loss:  -0.4351 | Function Loss:  -0.9958\n",
      "Total loss:  0.1382 | PDE Loss:  -0.4374 | Function Loss:  -0.996\n",
      "Total loss:  0.1373 | PDE Loss:  -0.4387 | Function Loss:  -0.9967\n",
      "Total loss:  0.1366 | PDE Loss:  -0.4397 | Function Loss:  -0.9973\n",
      "Total loss:  0.1358 | PDE Loss:  -0.4394 | Function Loss:  -0.9984\n",
      "Total loss:  0.1352 | PDE Loss:  -0.4388 | Function Loss:  -0.9995\n",
      "Total loss:  0.1348 | PDE Loss:  -0.4379 | Function Loss:  -1.0004\n",
      "Total loss:  0.1345 | PDE Loss:  -0.4377 | Function Loss:  -1.0008\n",
      "Total loss:  0.1344 | PDE Loss:  -0.438 | Function Loss:  -1.0009\n",
      "Total loss:  0.1342 | PDE Loss:  -0.4387 | Function Loss:  -1.0009\n",
      "Total loss:  0.1341 | PDE Loss:  -0.4397 | Function Loss:  -1.0007\n",
      "Total loss:  0.1339 | PDE Loss:  -0.4416 | Function Loss:  -1.0003\n",
      "Total loss:  0.1338 | PDE Loss:  -0.443 | Function Loss:  -1.0\n",
      "Total loss:  0.1336 | PDE Loss:  -0.4449 | Function Loss:  -0.9995\n",
      "Total loss:  0.1335 | PDE Loss:  -0.4466 | Function Loss:  -0.9991\n",
      "Total loss:  0.1333 | PDE Loss:  -0.4487 | Function Loss:  -0.9986\n",
      "Total loss:  0.133 | PDE Loss:  -0.451 | Function Loss:  -0.9981\n",
      "Total loss:  0.1326 | PDE Loss:  -0.4543 | Function Loss:  -0.9975\n",
      "Total loss:  0.1322 | PDE Loss:  -0.4562 | Function Loss:  -0.9973\n",
      "Total loss:  0.1318 | PDE Loss:  -0.4586 | Function Loss:  -0.9971\n",
      "Total loss:  0.1321 | PDE Loss:  -0.4569 | Function Loss:  -0.9973\n",
      "Total loss:  0.1315 | PDE Loss:  -0.4586 | Function Loss:  -0.9975\n",
      "Total loss:  0.1309 | PDE Loss:  -0.4594 | Function Loss:  -0.998\n",
      "Total loss:  0.1305 | PDE Loss:  -0.4598 | Function Loss:  -0.9984\n",
      "Total loss:  0.1302 | PDE Loss:  -0.4594 | Function Loss:  -0.9989\n",
      "Total loss:  0.1301 | PDE Loss:  -0.4593 | Function Loss:  -0.9992\n",
      "Total loss:  0.1299 | PDE Loss:  -0.4588 | Function Loss:  -0.9996\n",
      "Total loss:  0.1298 | PDE Loss:  -0.4584 | Function Loss:  -0.9999\n",
      "Total loss:  0.1296 | PDE Loss:  -0.4583 | Function Loss:  -1.0001\n",
      "Total loss:  0.1294 | PDE Loss:  -0.4578 | Function Loss:  -1.0006\n",
      "Total loss:  0.1292 | PDE Loss:  -0.4578 | Function Loss:  -1.0009\n",
      "Total loss:  0.1289 | PDE Loss:  -0.4575 | Function Loss:  -1.0014\n",
      "Total loss:  0.1286 | PDE Loss:  -0.4578 | Function Loss:  -1.0016\n",
      "Total loss:  0.1284 | PDE Loss:  -0.4579 | Function Loss:  -1.002\n",
      "Total loss:  0.1281 | PDE Loss:  -0.4586 | Function Loss:  -1.0021\n",
      "Total loss:  0.1277 | PDE Loss:  -0.4594 | Function Loss:  -1.0023\n",
      "Total loss:  0.1274 | PDE Loss:  -0.4599 | Function Loss:  -1.0026\n",
      "Total loss:  0.1271 | PDE Loss:  -0.4608 | Function Loss:  -1.0027\n",
      "Total loss:  0.1268 | PDE Loss:  -0.4615 | Function Loss:  -1.0028\n",
      "Total loss:  0.1266 | PDE Loss:  -0.4613 | Function Loss:  -1.0032\n",
      "Total loss:  0.1263 | PDE Loss:  -0.4604 | Function Loss:  -1.0039\n",
      "Total loss:  0.1261 | PDE Loss:  -0.4596 | Function Loss:  -1.0045\n",
      "Total loss:  0.1258 | PDE Loss:  -0.4588 | Function Loss:  -1.0051\n",
      "Total loss:  0.1255 | PDE Loss:  -0.4576 | Function Loss:  -1.0059\n",
      "Total loss:  0.1252 | PDE Loss:  -0.4572 | Function Loss:  -1.0065\n",
      "Total loss:  0.125 | PDE Loss:  -0.4565 | Function Loss:  -1.0071\n",
      "Total loss:  0.1247 | PDE Loss:  -0.4567 | Function Loss:  -1.0074\n",
      "Total loss:  0.1243 | PDE Loss:  -0.458 | Function Loss:  -1.0074\n",
      "Total loss:  0.124 | PDE Loss:  -0.4595 | Function Loss:  -1.0074\n",
      "Total loss:  0.1236 | PDE Loss:  -0.4627 | Function Loss:  -1.0067\n",
      "Total loss:  0.1233 | PDE Loss:  -0.4647 | Function Loss:  -1.0065\n",
      "Total loss:  0.1229 | PDE Loss:  -0.4692 | Function Loss:  -1.0054\n",
      "Total loss:  0.1225 | PDE Loss:  -0.4715 | Function Loss:  -1.0052\n",
      "Total loss:  0.1222 | PDE Loss:  -0.4738 | Function Loss:  -1.0048\n",
      "Total loss:  0.1218 | PDE Loss:  -0.4758 | Function Loss:  -1.0047\n",
      "Total loss:  0.1215 | PDE Loss:  -0.4752 | Function Loss:  -1.0053\n",
      "Total loss:  0.1212 | PDE Loss:  -0.4758 | Function Loss:  -1.0054\n",
      "Total loss:  0.1211 | PDE Loss:  -0.4753 | Function Loss:  -1.0058\n",
      "Total loss:  0.1209 | PDE Loss:  -0.4752 | Function Loss:  -1.006\n",
      "Total loss:  0.1208 | PDE Loss:  -0.4742 | Function Loss:  -1.0065\n",
      "Total loss:  0.1207 | PDE Loss:  -0.4741 | Function Loss:  -1.0067\n",
      "Total loss:  0.1206 | PDE Loss:  -0.4734 | Function Loss:  -1.0071\n",
      "Total loss:  0.1203 | PDE Loss:  -0.4734 | Function Loss:  -1.0075\n",
      "Total loss:  0.1199 | PDE Loss:  -0.4735 | Function Loss:  -1.008\n",
      "Total loss:  0.1193 | PDE Loss:  -0.4743 | Function Loss:  -1.0085\n",
      "Total loss:  0.1186 | PDE Loss:  -0.4756 | Function Loss:  -1.0091\n",
      "Total loss:  0.1178 | PDE Loss:  -0.4768 | Function Loss:  -1.0097\n",
      "Total loss:  0.117 | PDE Loss:  -0.4786 | Function Loss:  -1.0101\n",
      "Total loss:  0.1165 | PDE Loss:  -0.4779 | Function Loss:  -1.011\n",
      "Total loss:  0.1159 | PDE Loss:  -0.4802 | Function Loss:  -1.011\n",
      "Total loss:  0.1157 | PDE Loss:  -0.4796 | Function Loss:  -1.0115\n",
      "Total loss:  0.1154 | PDE Loss:  -0.4813 | Function Loss:  -1.0113\n",
      "Total loss:  0.1153 | PDE Loss:  -0.4805 | Function Loss:  -1.0118\n",
      "Total loss:  0.1152 | PDE Loss:  -0.4808 | Function Loss:  -1.0118\n",
      "Total loss:  0.1151 | PDE Loss:  -0.48 | Function Loss:  -1.0122\n",
      "Total loss:  0.115 | PDE Loss:  -0.4795 | Function Loss:  -1.0125\n",
      "Total loss:  0.1149 | PDE Loss:  -0.4785 | Function Loss:  -1.0129\n",
      "Total loss:  0.1148 | PDE Loss:  -0.4773 | Function Loss:  -1.0135\n",
      "Total loss:  0.1147 | PDE Loss:  -0.4761 | Function Loss:  -1.014\n",
      "Total loss:  0.1146 | PDE Loss:  -0.4748 | Function Loss:  -1.0146\n",
      "Total loss:  0.1145 | PDE Loss:  -0.4738 | Function Loss:  -1.0152\n",
      "Total loss:  0.1143 | PDE Loss:  -0.4708 | Function Loss:  -1.0165\n",
      "Total loss:  0.1141 | PDE Loss:  -0.4706 | Function Loss:  -1.0167\n",
      "Total loss:  0.1138 | PDE Loss:  -0.4704 | Function Loss:  -1.0172\n",
      "Total loss:  0.1134 | PDE Loss:  -0.4717 | Function Loss:  -1.0173\n",
      "Total loss:  0.1131 | PDE Loss:  -0.4716 | Function Loss:  -1.0178\n",
      "Total loss:  0.1126 | PDE Loss:  -0.4724 | Function Loss:  -1.0181\n",
      "Total loss:  0.1122 | PDE Loss:  -0.4718 | Function Loss:  -1.019\n",
      "Total loss:  0.1118 | PDE Loss:  -0.4719 | Function Loss:  -1.0194\n",
      "Total loss:  0.1115 | PDE Loss:  -0.4712 | Function Loss:  -1.0202\n",
      "Total loss:  0.111 | PDE Loss:  -0.4706 | Function Loss:  -1.0209\n",
      "Total loss:  0.1107 | PDE Loss:  -0.4704 | Function Loss:  -1.0215\n",
      "Total loss:  0.1104 | PDE Loss:  -0.4694 | Function Loss:  -1.0223\n",
      "Total loss:  0.1101 | PDE Loss:  -0.4707 | Function Loss:  -1.0222\n",
      "Total loss:  0.1098 | PDE Loss:  -0.4694 | Function Loss:  -1.023\n",
      "Total loss:  0.1096 | PDE Loss:  -0.4704 | Function Loss:  -1.0229\n",
      "Total loss:  0.1094 | PDE Loss:  -0.4712 | Function Loss:  -1.0229\n",
      "Total loss:  0.1093 | PDE Loss:  -0.4718 | Function Loss:  -1.0229\n",
      "Total loss:  0.1091 | PDE Loss:  -0.4731 | Function Loss:  -1.0227\n",
      "Total loss:  0.1089 | PDE Loss:  -0.4744 | Function Loss:  -1.0226\n",
      "Total loss:  0.1086 | PDE Loss:  -0.4756 | Function Loss:  -1.0224\n",
      "Total loss:  0.1084 | PDE Loss:  -0.4767 | Function Loss:  -1.0223\n",
      "Total loss:  0.1081 | PDE Loss:  -0.4775 | Function Loss:  -1.0225\n",
      "Total loss:  0.1078 | PDE Loss:  -0.478 | Function Loss:  -1.0227\n",
      "Total loss:  0.1076 | PDE Loss:  -0.4784 | Function Loss:  -1.0229\n",
      "Total loss:  0.1074 | PDE Loss:  -0.4782 | Function Loss:  -1.0232\n",
      "Total loss:  0.1072 | PDE Loss:  -0.4787 | Function Loss:  -1.0233\n",
      "Total loss:  0.107 | PDE Loss:  -0.4787 | Function Loss:  -1.0235\n",
      "Total loss:  0.1068 | PDE Loss:  -0.4802 | Function Loss:  -1.0232\n",
      "Total loss:  0.1067 | PDE Loss:  -0.4802 | Function Loss:  -1.0234\n",
      "Total loss:  0.1066 | PDE Loss:  -0.4823 | Function Loss:  -1.0229\n",
      "Total loss:  0.1064 | PDE Loss:  -0.4837 | Function Loss:  -1.0226\n",
      "Total loss:  0.1062 | PDE Loss:  -0.4859 | Function Loss:  -1.022\n",
      "Total loss:  0.106 | PDE Loss:  -0.4876 | Function Loss:  -1.0219\n",
      "Total loss:  0.1056 | PDE Loss:  -0.4908 | Function Loss:  -1.0213\n",
      "Total loss:  0.1053 | PDE Loss:  -0.4926 | Function Loss:  -1.0211\n",
      "Total loss:  0.1048 | PDE Loss:  -0.4936 | Function Loss:  -1.0213\n",
      "Total loss:  0.1043 | PDE Loss:  -0.494 | Function Loss:  -1.0219\n",
      "Total loss:  0.1038 | PDE Loss:  -0.4937 | Function Loss:  -1.0226\n",
      "Total loss:  0.1033 | PDE Loss:  -0.4924 | Function Loss:  -1.0238\n",
      "Total loss:  0.1028 | PDE Loss:  -0.4889 | Function Loss:  -1.0256\n",
      "Total loss:  0.1022 | PDE Loss:  -0.486 | Function Loss:  -1.0274\n",
      "Total loss:  0.1017 | PDE Loss:  -0.4814 | Function Loss:  -1.0297\n",
      "Total loss:  0.1013 | PDE Loss:  -0.4793 | Function Loss:  -1.031\n",
      "Total loss:  0.1009 | PDE Loss:  -0.4778 | Function Loss:  -1.0322\n",
      "Total loss:  0.1004 | PDE Loss:  -0.4781 | Function Loss:  -1.0327\n",
      "Total loss:  0.1001 | PDE Loss:  -0.4784 | Function Loss:  -1.033\n",
      "Total loss:  0.0998 | PDE Loss:  -0.4798 | Function Loss:  -1.0328\n",
      "Total loss:  0.0996 | PDE Loss:  -0.481 | Function Loss:  -1.0328\n",
      "Total loss:  0.0993 | PDE Loss:  -0.4832 | Function Loss:  -1.0324\n",
      "Total loss:  0.099 | PDE Loss:  -0.4853 | Function Loss:  -1.032\n",
      "Total loss:  0.0988 | PDE Loss:  -0.4865 | Function Loss:  -1.0319\n",
      "Total loss:  0.0985 | PDE Loss:  -0.4877 | Function Loss:  -1.0318\n",
      "Total loss:  0.0982 | PDE Loss:  -0.4883 | Function Loss:  -1.0321\n",
      "Total loss:  0.0979 | PDE Loss:  -0.4888 | Function Loss:  -1.0323\n",
      "Total loss:  0.0977 | PDE Loss:  -0.4885 | Function Loss:  -1.0327\n",
      "Total loss:  0.0974 | PDE Loss:  -0.4879 | Function Loss:  -1.0332\n",
      "Total loss:  0.0972 | PDE Loss:  -0.4874 | Function Loss:  -1.0337\n",
      "Total loss:  0.0969 | PDE Loss:  -0.4865 | Function Loss:  -1.0344\n",
      "Total loss:  0.0966 | PDE Loss:  -0.486 | Function Loss:  -1.035\n",
      "Total loss:  0.0963 | PDE Loss:  -0.4858 | Function Loss:  -1.0355\n",
      "Total loss:  0.096 | PDE Loss:  -0.4831 | Function Loss:  -1.0369\n",
      "Total loss:  0.0957 | PDE Loss:  -0.4836 | Function Loss:  -1.0371\n",
      "Total loss:  0.0952 | PDE Loss:  -0.4846 | Function Loss:  -1.0375\n",
      "Total loss:  0.0948 | PDE Loss:  -0.4832 | Function Loss:  -1.0385\n",
      "Total loss:  0.0944 | PDE Loss:  -0.4841 | Function Loss:  -1.0387\n",
      "Total loss:  0.0941 | PDE Loss:  -0.4842 | Function Loss:  -1.039\n",
      "Total loss:  0.0938 | PDE Loss:  -0.4833 | Function Loss:  -1.0398\n",
      "Total loss:  0.0934 | PDE Loss:  -0.4824 | Function Loss:  -1.0406\n",
      "Total loss:  0.0931 | PDE Loss:  -0.4815 | Function Loss:  -1.0414\n",
      "Total loss:  0.0927 | PDE Loss:  -0.4811 | Function Loss:  -1.042\n",
      "Total loss:  0.0924 | PDE Loss:  -0.4809 | Function Loss:  -1.0426\n",
      "Total loss:  0.092 | PDE Loss:  -0.4813 | Function Loss:  -1.0429\n",
      "Total loss:  0.0917 | PDE Loss:  -0.4813 | Function Loss:  -1.0433\n",
      "Total loss:  0.0915 | PDE Loss:  -0.4827 | Function Loss:  -1.0432\n",
      "Total loss:  0.0913 | PDE Loss:  -0.4829 | Function Loss:  -1.0433\n",
      "Total loss:  0.0912 | PDE Loss:  -0.483 | Function Loss:  -1.0434\n",
      "Total loss:  0.0911 | PDE Loss:  -0.4832 | Function Loss:  -1.0434\n",
      "Total loss:  0.091 | PDE Loss:  -0.4825 | Function Loss:  -1.0439\n",
      "Total loss:  0.0908 | PDE Loss:  -0.482 | Function Loss:  -1.0443\n",
      "Total loss:  0.0907 | PDE Loss:  -0.4813 | Function Loss:  -1.0448\n",
      "Total loss:  0.0904 | PDE Loss:  -0.4801 | Function Loss:  -1.0457\n",
      "Total loss:  0.0899 | PDE Loss:  -0.4786 | Function Loss:  -1.0468\n",
      "Total loss:  0.0893 | PDE Loss:  -0.4766 | Function Loss:  -1.0484\n",
      "Total loss:  0.0888 | PDE Loss:  -0.4758 | Function Loss:  -1.0494\n",
      "Total loss:  0.0882 | PDE Loss:  -0.475 | Function Loss:  -1.0505\n",
      "Total loss:  0.0878 | PDE Loss:  -0.4762 | Function Loss:  -1.0506\n",
      "Total loss:  0.0877 | PDE Loss:  -0.4751 | Function Loss:  -1.0512\n",
      "Total loss:  0.0874 | PDE Loss:  -0.4762 | Function Loss:  -1.0512\n",
      "Total loss:  0.0873 | PDE Loss:  -0.4773 | Function Loss:  -1.0509\n",
      "Total loss:  0.0871 | PDE Loss:  -0.4785 | Function Loss:  -1.0507\n",
      "Total loss:  0.0869 | PDE Loss:  -0.4795 | Function Loss:  -1.0506\n",
      "Total loss:  0.0868 | PDE Loss:  -0.4809 | Function Loss:  -1.0503\n",
      "Total loss:  0.0866 | PDE Loss:  -0.4816 | Function Loss:  -1.0502\n",
      "Total loss:  0.0864 | PDE Loss:  -0.4827 | Function Loss:  -1.0501\n",
      "Total loss:  0.0862 | PDE Loss:  -0.4834 | Function Loss:  -1.0501\n",
      "Total loss:  0.0859 | PDE Loss:  -0.4843 | Function Loss:  -1.0501\n",
      "Total loss:  0.0857 | PDE Loss:  -0.4847 | Function Loss:  -1.0503\n",
      "Total loss:  0.0856 | PDE Loss:  -0.4848 | Function Loss:  -1.0505\n",
      "Total loss:  0.0854 | PDE Loss:  -0.4848 | Function Loss:  -1.0507\n",
      "Total loss:  0.0852 | PDE Loss:  -0.4842 | Function Loss:  -1.0512\n",
      "Total loss:  0.085 | PDE Loss:  -0.4837 | Function Loss:  -1.0516\n",
      "Total loss:  0.0848 | PDE Loss:  -0.4831 | Function Loss:  -1.0521\n",
      "Total loss:  0.0846 | PDE Loss:  -0.4822 | Function Loss:  -1.0527\n",
      "Total loss:  0.0844 | PDE Loss:  -0.4821 | Function Loss:  -1.0531\n",
      "Total loss:  0.0841 | PDE Loss:  -0.4823 | Function Loss:  -1.0534\n",
      "Total loss:  0.0839 | PDE Loss:  -0.4827 | Function Loss:  -1.0536\n",
      "Total loss:  0.0836 | PDE Loss:  -0.4839 | Function Loss:  -1.0535\n",
      "Total loss:  0.0832 | PDE Loss:  -0.4851 | Function Loss:  -1.0536\n",
      "Total loss:  0.0829 | PDE Loss:  -0.4868 | Function Loss:  -1.0534\n",
      "Total loss:  0.0824 | PDE Loss:  -0.4891 | Function Loss:  -1.0532\n",
      "Total loss:  0.082 | PDE Loss:  -0.4916 | Function Loss:  -1.0529\n",
      "Total loss:  0.0814 | PDE Loss:  -0.4941 | Function Loss:  -1.0527\n",
      "Total loss:  0.081 | PDE Loss:  -0.4968 | Function Loss:  -1.0524\n",
      "Total loss:  0.0806 | PDE Loss:  -0.4977 | Function Loss:  -1.0526\n",
      "Total loss:  0.0802 | PDE Loss:  -0.4986 | Function Loss:  -1.0528\n",
      "Total loss:  0.0797 | PDE Loss:  -0.4985 | Function Loss:  -1.0535\n",
      "Total loss:  0.0791 | PDE Loss:  -0.4981 | Function Loss:  -1.0544\n",
      "Total loss:  0.0786 | PDE Loss:  -0.4978 | Function Loss:  -1.0552\n",
      "Total loss:  0.0781 | PDE Loss:  -0.4967 | Function Loss:  -1.0563\n",
      "Total loss:  0.0776 | PDE Loss:  -0.4962 | Function Loss:  -1.0572\n",
      "Total loss:  0.0772 | PDE Loss:  -0.4955 | Function Loss:  -1.0579\n",
      "Total loss:  0.077 | PDE Loss:  -0.4956 | Function Loss:  -1.0583\n",
      "Total loss:  0.0767 | PDE Loss:  -0.4967 | Function Loss:  -1.0582\n",
      "Total loss:  0.0764 | PDE Loss:  -0.4973 | Function Loss:  -1.0584\n",
      "Total loss:  0.0762 | PDE Loss:  -0.4986 | Function Loss:  -1.0583\n",
      "Total loss:  0.0758 | PDE Loss:  -0.5003 | Function Loss:  -1.0581\n",
      "Total loss:  0.0755 | PDE Loss:  -0.5016 | Function Loss:  -1.058\n",
      "Total loss:  0.0753 | PDE Loss:  -0.5026 | Function Loss:  -1.058\n",
      "Total loss:  0.075 | PDE Loss:  -0.5027 | Function Loss:  -1.0583\n",
      "Total loss:  0.0749 | PDE Loss:  -0.5028 | Function Loss:  -1.0585\n",
      "Total loss:  0.0747 | PDE Loss:  -0.5023 | Function Loss:  -1.0589\n",
      "Total loss:  0.0745 | PDE Loss:  -0.5019 | Function Loss:  -1.0593\n",
      "Total loss:  0.0743 | PDE Loss:  -0.5011 | Function Loss:  -1.06\n",
      "Total loss:  0.074 | PDE Loss:  -0.5006 | Function Loss:  -1.0605\n",
      "Total loss:  0.0738 | PDE Loss:  -0.5003 | Function Loss:  -1.0609\n",
      "Total loss:  0.0735 | PDE Loss:  -0.5003 | Function Loss:  -1.0613\n",
      "Total loss:  0.0732 | PDE Loss:  -0.5011 | Function Loss:  -1.0614\n",
      "Total loss:  0.0729 | PDE Loss:  -0.5015 | Function Loss:  -1.0616\n",
      "Total loss:  0.0726 | PDE Loss:  -0.5023 | Function Loss:  -1.0617\n",
      "Total loss:  0.0722 | PDE Loss:  -0.5032 | Function Loss:  -1.062\n",
      "Total loss:  0.0718 | PDE Loss:  -0.5035 | Function Loss:  -1.0624\n",
      "Total loss:  0.0713 | PDE Loss:  -0.5027 | Function Loss:  -1.0634\n",
      "Total loss:  0.0707 | PDE Loss:  -0.5018 | Function Loss:  -1.0646\n",
      "Total loss:  0.0701 | PDE Loss:  -0.5011 | Function Loss:  -1.0656\n",
      "Total loss:  0.0698 | PDE Loss:  -0.5004 | Function Loss:  -1.0664\n",
      "Total loss:  0.0696 | PDE Loss:  -0.5002 | Function Loss:  -1.0667\n",
      "Total loss:  0.0693 | PDE Loss:  -0.501 | Function Loss:  -1.0668\n",
      "Total loss:  0.069 | PDE Loss:  -0.5024 | Function Loss:  -1.0667\n",
      "Total loss:  0.0687 | PDE Loss:  -0.5045 | Function Loss:  -1.0663\n",
      "Total loss:  0.0685 | PDE Loss:  -0.5062 | Function Loss:  -1.066\n",
      "Total loss:  0.0682 | PDE Loss:  -0.5099 | Function Loss:  -1.065\n",
      "Total loss:  0.068 | PDE Loss:  -0.5117 | Function Loss:  -1.0647\n",
      "Total loss:  0.0678 | PDE Loss:  -0.5126 | Function Loss:  -1.0646\n",
      "Total loss:  0.0676 | PDE Loss:  -0.5136 | Function Loss:  -1.0645\n",
      "Total loss:  0.0673 | PDE Loss:  -0.5146 | Function Loss:  -1.0645\n",
      "Total loss:  0.0671 | PDE Loss:  -0.5142 | Function Loss:  -1.065\n",
      "Total loss:  0.0669 | PDE Loss:  -0.5139 | Function Loss:  -1.0654\n",
      "Total loss:  0.0667 | PDE Loss:  -0.5133 | Function Loss:  -1.0659\n",
      "Total loss:  0.0664 | PDE Loss:  -0.5132 | Function Loss:  -1.0663\n",
      "Total loss:  0.0661 | PDE Loss:  -0.5132 | Function Loss:  -1.0667\n",
      "Total loss:  0.0656 | PDE Loss:  -0.5143 | Function Loss:  -1.0669\n",
      "Total loss:  0.0652 | PDE Loss:  -0.515 | Function Loss:  -1.0673\n",
      "Total loss:  0.0649 | PDE Loss:  -0.5167 | Function Loss:  -1.0671\n",
      "Total loss:  0.0645 | PDE Loss:  -0.5185 | Function Loss:  -1.0669\n",
      "Total loss:  0.0642 | PDE Loss:  -0.521 | Function Loss:  -1.0665\n",
      "Total loss:  0.0639 | PDE Loss:  -0.5227 | Function Loss:  -1.0664\n",
      "Total loss:  0.0635 | PDE Loss:  -0.5244 | Function Loss:  -1.0662\n",
      "Total loss:  0.0631 | PDE Loss:  -0.5262 | Function Loss:  -1.0661\n",
      "Total loss:  0.0627 | PDE Loss:  -0.5273 | Function Loss:  -1.0664\n",
      "Total loss:  0.0623 | PDE Loss:  -0.5281 | Function Loss:  -1.0666\n",
      "Total loss:  0.0618 | PDE Loss:  -0.5281 | Function Loss:  -1.0673\n",
      "Total loss:  0.0614 | PDE Loss:  -0.5276 | Function Loss:  -1.068\n",
      "Total loss:  0.061 | PDE Loss:  -0.527 | Function Loss:  -1.0687\n",
      "Total loss:  0.0607 | PDE Loss:  -0.5261 | Function Loss:  -1.0695\n",
      "Total loss:  0.0605 | PDE Loss:  -0.5264 | Function Loss:  -1.0697\n",
      "Total loss:  0.0603 | PDE Loss:  -0.5271 | Function Loss:  -1.0697\n",
      "Total loss:  0.06 | PDE Loss:  -0.5283 | Function Loss:  -1.0697\n",
      "Total loss:  0.0596 | PDE Loss:  -0.5296 | Function Loss:  -1.0697\n",
      "Total loss:  0.0593 | PDE Loss:  -0.5312 | Function Loss:  -1.0696\n",
      "Total loss:  0.0588 | PDE Loss:  -0.5306 | Function Loss:  -1.0704\n",
      "Total loss:  0.0584 | PDE Loss:  -0.5339 | Function Loss:  -1.0699\n",
      "Total loss:  0.058 | PDE Loss:  -0.5333 | Function Loss:  -1.0706\n",
      "Total loss:  0.0576 | PDE Loss:  -0.5352 | Function Loss:  -1.0705\n",
      "Total loss:  0.0573 | PDE Loss:  -0.5346 | Function Loss:  -1.0711\n",
      "Total loss:  0.0571 | PDE Loss:  -0.5342 | Function Loss:  -1.0715\n",
      "Total loss:  0.0569 | PDE Loss:  -0.5339 | Function Loss:  -1.0719\n",
      "Total loss:  0.0567 | PDE Loss:  -0.5342 | Function Loss:  -1.072\n",
      "Total loss:  0.0565 | PDE Loss:  -0.5354 | Function Loss:  -1.0719\n",
      "Total loss:  0.0563 | PDE Loss:  -0.537 | Function Loss:  -1.0717\n",
      "Total loss:  0.056 | PDE Loss:  -0.5391 | Function Loss:  -1.0713\n",
      "Total loss:  0.0556 | PDE Loss:  -0.5417 | Function Loss:  -1.071\n",
      "Total loss:  0.0552 | PDE Loss:  -0.5438 | Function Loss:  -1.0708\n",
      "Total loss:  0.0549 | PDE Loss:  -0.5455 | Function Loss:  -1.0706\n",
      "Total loss:  0.0545 | PDE Loss:  -0.547 | Function Loss:  -1.0706\n",
      "Total loss:  0.0543 | PDE Loss:  -0.5476 | Function Loss:  -1.0708\n",
      "Total loss:  0.0539 | PDE Loss:  -0.5474 | Function Loss:  -1.0713\n",
      "Total loss:  0.0535 | PDE Loss:  -0.5477 | Function Loss:  -1.0717\n",
      "Total loss:  0.0531 | PDE Loss:  -0.5469 | Function Loss:  -1.0725\n",
      "Total loss:  0.0526 | PDE Loss:  -0.5468 | Function Loss:  -1.0733\n",
      "Total loss:  0.0521 | PDE Loss:  -0.546 | Function Loss:  -1.0741\n",
      "Total loss:  0.0517 | PDE Loss:  -0.5462 | Function Loss:  -1.0746\n",
      "Total loss:  0.0513 | PDE Loss:  -0.5465 | Function Loss:  -1.075\n",
      "Total loss:  0.051 | PDE Loss:  -0.5477 | Function Loss:  -1.0751\n",
      "Total loss:  0.0508 | PDE Loss:  -0.5491 | Function Loss:  -1.0749\n",
      "Total loss:  0.0505 | PDE Loss:  -0.5504 | Function Loss:  -1.0748\n",
      "Total loss:  0.0503 | PDE Loss:  -0.552 | Function Loss:  -1.0746\n",
      "Total loss:  0.05 | PDE Loss:  -0.5524 | Function Loss:  -1.0749\n",
      "Total loss:  0.0497 | PDE Loss:  -0.5543 | Function Loss:  -1.0747\n",
      "Total loss:  0.0494 | PDE Loss:  -0.553 | Function Loss:  -1.0754\n",
      "Total loss:  0.0491 | PDE Loss:  -0.5523 | Function Loss:  -1.076\n",
      "Total loss:  0.0489 | PDE Loss:  -0.5508 | Function Loss:  -1.0769\n",
      "Total loss:  0.0486 | PDE Loss:  -0.5491 | Function Loss:  -1.0778\n",
      "Total loss:  0.0482 | PDE Loss:  -0.5467 | Function Loss:  -1.0792\n",
      "Total loss:  0.0477 | PDE Loss:  -0.5441 | Function Loss:  -1.0808\n",
      "Total loss:  0.0471 | PDE Loss:  -0.542 | Function Loss:  -1.0823\n",
      "Total loss:  0.0463 | PDE Loss:  -0.539 | Function Loss:  -1.0843\n",
      "Total loss:  0.0456 | PDE Loss:  -0.5358 | Function Loss:  -1.0864\n",
      "Total loss:  0.0451 | PDE Loss:  -0.536 | Function Loss:  -1.0871\n",
      "Total loss:  0.0443 | PDE Loss:  -0.5373 | Function Loss:  -1.0876\n",
      "Total loss:  0.0437 | PDE Loss:  -0.5395 | Function Loss:  -1.0876\n",
      "Total loss:  0.0434 | PDE Loss:  -0.5408 | Function Loss:  -1.0877\n",
      "Total loss:  0.0431 | PDE Loss:  -0.543 | Function Loss:  -1.0873\n",
      "Total loss:  0.0429 | PDE Loss:  -0.544 | Function Loss:  -1.0872\n",
      "Total loss:  0.0427 | PDE Loss:  -0.5443 | Function Loss:  -1.0874\n",
      "Total loss:  0.0425 | PDE Loss:  -0.5442 | Function Loss:  -1.0877\n",
      "Total loss:  0.0423 | PDE Loss:  -0.5438 | Function Loss:  -1.088\n",
      "Total loss:  0.0422 | PDE Loss:  -0.5434 | Function Loss:  -1.0883\n",
      "Total loss:  0.0421 | PDE Loss:  -0.5433 | Function Loss:  -1.0885\n",
      "Total loss:  0.0419 | PDE Loss:  -0.5436 | Function Loss:  -1.0887\n",
      "Total loss:  0.0416 | PDE Loss:  -0.5437 | Function Loss:  -1.089\n",
      "Total loss:  0.0414 | PDE Loss:  -0.5448 | Function Loss:  -1.089\n",
      "Total loss:  0.0411 | PDE Loss:  -0.5451 | Function Loss:  -1.0893\n",
      "Total loss:  0.0408 | PDE Loss:  -0.5463 | Function Loss:  -1.0893\n",
      "Total loss:  0.0405 | PDE Loss:  -0.5468 | Function Loss:  -1.0895\n",
      "Total loss:  0.0401 | PDE Loss:  -0.5471 | Function Loss:  -1.0899\n",
      "Total loss:  0.0397 | PDE Loss:  -0.5471 | Function Loss:  -1.0905\n",
      "Total loss:  0.0392 | PDE Loss:  -0.5463 | Function Loss:  -1.0914\n",
      "Total loss:  0.0387 | PDE Loss:  -0.5451 | Function Loss:  -1.0925\n",
      "Total loss:  0.038 | PDE Loss:  -0.5438 | Function Loss:  -1.0939\n",
      "Total loss:  0.0374 | PDE Loss:  -0.5421 | Function Loss:  -1.0954\n",
      "Total loss:  0.0367 | PDE Loss:  -0.5413 | Function Loss:  -1.0965\n",
      "Total loss:  0.0361 | PDE Loss:  -0.5395 | Function Loss:  -1.0981\n",
      "Total loss:  0.0357 | PDE Loss:  -0.5385 | Function Loss:  -1.099\n",
      "Total loss:  0.0353 | PDE Loss:  -0.5381 | Function Loss:  -1.0997\n",
      "Total loss:  0.0349 | PDE Loss:  -0.5372 | Function Loss:  -1.1005\n",
      "Total loss:  0.0346 | PDE Loss:  -0.5374 | Function Loss:  -1.1009\n",
      "Total loss:  0.0343 | PDE Loss:  -0.5371 | Function Loss:  -1.1014\n",
      "Total loss:  0.034 | PDE Loss:  -0.5378 | Function Loss:  -1.1015\n",
      "Total loss:  0.0338 | PDE Loss:  -0.5388 | Function Loss:  -1.1015\n",
      "Total loss:  0.0335 | PDE Loss:  -0.54 | Function Loss:  -1.1014\n",
      "Total loss:  0.0332 | PDE Loss:  -0.5413 | Function Loss:  -1.1013\n",
      "Total loss:  0.0329 | PDE Loss:  -0.5421 | Function Loss:  -1.1014\n",
      "Total loss:  0.0326 | PDE Loss:  -0.5432 | Function Loss:  -1.1015\n",
      "Total loss:  0.0322 | PDE Loss:  -0.5434 | Function Loss:  -1.102\n",
      "Total loss:  0.0319 | PDE Loss:  -0.5429 | Function Loss:  -1.1026\n",
      "Total loss:  0.0316 | PDE Loss:  -0.5417 | Function Loss:  -1.1034\n",
      "Total loss:  0.0314 | PDE Loss:  -0.5406 | Function Loss:  -1.104\n",
      "Total loss:  0.0313 | PDE Loss:  -0.5396 | Function Loss:  -1.1046\n",
      "Total loss:  0.0312 | PDE Loss:  -0.5386 | Function Loss:  -1.1051\n",
      "Total loss:  0.0311 | PDE Loss:  -0.5383 | Function Loss:  -1.1053\n",
      "Total loss:  0.0308 | PDE Loss:  -0.5383 | Function Loss:  -1.1057\n",
      "Total loss:  0.0304 | PDE Loss:  -0.5381 | Function Loss:  -1.1063\n",
      "Total loss:  0.0301 | PDE Loss:  -0.5385 | Function Loss:  -1.1066\n",
      "Total loss:  0.0297 | PDE Loss:  -0.5391 | Function Loss:  -1.1069\n",
      "Total loss:  0.0293 | PDE Loss:  -0.54 | Function Loss:  -1.1072\n",
      "Total loss:  0.029 | PDE Loss:  -0.54 | Function Loss:  -1.1076\n",
      "Total loss:  0.0288 | PDE Loss:  -0.5405 | Function Loss:  -1.1077\n",
      "Total loss:  0.0286 | PDE Loss:  -0.5408 | Function Loss:  -1.1078\n",
      "Total loss:  0.0283 | PDE Loss:  -0.5407 | Function Loss:  -1.1082\n",
      "Total loss:  0.0281 | PDE Loss:  -0.54 | Function Loss:  -1.1088\n",
      "Total loss:  0.0277 | PDE Loss:  -0.5394 | Function Loss:  -1.1096\n",
      "Total loss:  0.0274 | PDE Loss:  -0.5381 | Function Loss:  -1.1105\n",
      "Total loss:  0.0271 | PDE Loss:  -0.5381 | Function Loss:  -1.1109\n",
      "Total loss:  0.0268 | PDE Loss:  -0.5375 | Function Loss:  -1.1115\n",
      "Total loss:  0.0265 | PDE Loss:  -0.5383 | Function Loss:  -1.1117\n",
      "Total loss:  0.0262 | PDE Loss:  -0.5389 | Function Loss:  -1.1118\n",
      "Total loss:  0.0259 | PDE Loss:  -0.5399 | Function Loss:  -1.1118\n",
      "Total loss:  0.0256 | PDE Loss:  -0.5412 | Function Loss:  -1.1118\n",
      "Total loss:  0.0252 | PDE Loss:  -0.5427 | Function Loss:  -1.1117\n",
      "Total loss:  0.0249 | PDE Loss:  -0.544 | Function Loss:  -1.1117\n",
      "Total loss:  0.0245 | PDE Loss:  -0.5458 | Function Loss:  -1.1116\n",
      "Total loss:  0.0241 | PDE Loss:  -0.5469 | Function Loss:  -1.1117\n",
      "Total loss:  0.0238 | PDE Loss:  -0.5474 | Function Loss:  -1.1119\n",
      "Total loss:  0.0233 | PDE Loss:  -0.5476 | Function Loss:  -1.1125\n",
      "Total loss:  0.0229 | PDE Loss:  -0.5482 | Function Loss:  -1.1129\n",
      "Total loss:  0.0224 | PDE Loss:  -0.5489 | Function Loss:  -1.1133\n",
      "Total loss:  0.022 | PDE Loss:  -0.5506 | Function Loss:  -1.1133\n",
      "Total loss:  0.0215 | PDE Loss:  -0.5523 | Function Loss:  -1.1132\n",
      "Total loss:  0.0211 | PDE Loss:  -0.5548 | Function Loss:  -1.1129\n",
      "Total loss:  0.0207 | PDE Loss:  -0.5573 | Function Loss:  -1.1126\n",
      "Total loss:  0.0201 | PDE Loss:  -0.5605 | Function Loss:  -1.1122\n",
      "Total loss:  0.0196 | PDE Loss:  -0.5633 | Function Loss:  -1.112\n",
      "Total loss:  0.019 | PDE Loss:  -0.5663 | Function Loss:  -1.1116\n",
      "Total loss:  0.0186 | PDE Loss:  -0.5681 | Function Loss:  -1.1116\n",
      "Total loss:  0.0181 | PDE Loss:  -0.5709 | Function Loss:  -1.1113\n",
      "Total loss:  0.0176 | PDE Loss:  -0.5718 | Function Loss:  -1.1116\n",
      "Total loss:  0.0172 | PDE Loss:  -0.5717 | Function Loss:  -1.1122\n",
      "Total loss:  0.0167 | PDE Loss:  -0.572 | Function Loss:  -1.1127\n",
      "Total loss:  0.0165 | PDE Loss:  -0.571 | Function Loss:  -1.1134\n",
      "Total loss:  0.0163 | PDE Loss:  -0.5706 | Function Loss:  -1.1138\n",
      "Total loss:  0.016 | PDE Loss:  -0.5702 | Function Loss:  -1.1144\n",
      "Total loss:  0.0156 | PDE Loss:  -0.5704 | Function Loss:  -1.1148\n",
      "Total loss:  0.0153 | PDE Loss:  -0.5709 | Function Loss:  -1.1151\n",
      "Total loss:  0.015 | PDE Loss:  -0.5715 | Function Loss:  -1.1153\n",
      "Total loss:  0.0146 | PDE Loss:  -0.5728 | Function Loss:  -1.1153\n",
      "Total loss:  0.0142 | PDE Loss:  -0.5736 | Function Loss:  -1.1155\n",
      "Total loss:  0.0138 | PDE Loss:  -0.5751 | Function Loss:  -1.1156\n",
      "Total loss:  0.0135 | PDE Loss:  -0.5763 | Function Loss:  -1.1157\n",
      "Total loss:  0.0132 | PDE Loss:  -0.5767 | Function Loss:  -1.1159\n",
      "Total loss:  0.0129 | PDE Loss:  -0.5767 | Function Loss:  -1.1162\n",
      "Total loss:  0.0126 | PDE Loss:  -0.5764 | Function Loss:  -1.1168\n",
      "Total loss:  0.0123 | PDE Loss:  -0.5757 | Function Loss:  -1.1174\n",
      "Total loss:  0.0119 | PDE Loss:  -0.5752 | Function Loss:  -1.1181\n",
      "Total loss:  0.0113 | PDE Loss:  -0.5735 | Function Loss:  -1.1195\n",
      "Total loss:  0.0107 | PDE Loss:  -0.5734 | Function Loss:  -1.1204\n",
      "Total loss:  0.0102 | PDE Loss:  -0.5735 | Function Loss:  -1.1211\n",
      "Total loss:  0.0095 | PDE Loss:  -0.5753 | Function Loss:  -1.1214\n",
      "Total loss:  0.0089 | PDE Loss:  -0.5765 | Function Loss:  -1.1217\n",
      "Total loss:  0.0085 | PDE Loss:  -0.5784 | Function Loss:  -1.1216\n",
      "Total loss:  0.0081 | PDE Loss:  -0.58 | Function Loss:  -1.1216\n",
      "Total loss:  0.0077 | PDE Loss:  -0.5818 | Function Loss:  -1.1215\n",
      "Total loss:  0.0074 | PDE Loss:  -0.5824 | Function Loss:  -1.1217\n",
      "Total loss:  0.0072 | PDE Loss:  -0.5829 | Function Loss:  -1.1217\n",
      "Total loss:  0.0071 | PDE Loss:  -0.5831 | Function Loss:  -1.1219\n",
      "Total loss:  0.007 | PDE Loss:  -0.5829 | Function Loss:  -1.1221\n",
      "Total loss:  0.0069 | PDE Loss:  -0.5835 | Function Loss:  -1.122\n",
      "Total loss:  0.0066 | PDE Loss:  -0.5827 | Function Loss:  -1.1227\n",
      "Total loss:  0.0064 | PDE Loss:  -0.5829 | Function Loss:  -1.1229\n",
      "Total loss:  0.006 | PDE Loss:  -0.5839 | Function Loss:  -1.1231\n",
      "Total loss:  0.0055 | PDE Loss:  -0.5851 | Function Loss:  -1.1233\n",
      "Total loss:  0.0051 | PDE Loss:  -0.5864 | Function Loss:  -1.1234\n",
      "Total loss:  0.0047 | PDE Loss:  -0.5877 | Function Loss:  -1.1235\n",
      "Total loss:  0.0043 | PDE Loss:  -0.5892 | Function Loss:  -1.1235\n",
      "Total loss:  0.0038 | PDE Loss:  -0.5904 | Function Loss:  -1.1238\n",
      "Total loss:  0.0034 | PDE Loss:  -0.5924 | Function Loss:  -1.1237\n",
      "Total loss:  0.003 | PDE Loss:  -0.5934 | Function Loss:  -1.1238\n",
      "Total loss:  0.0027 | PDE Loss:  -0.5943 | Function Loss:  -1.1239\n",
      "Total loss:  0.0025 | PDE Loss:  -0.5945 | Function Loss:  -1.1241\n",
      "Total loss:  0.0023 | PDE Loss:  -0.5948 | Function Loss:  -1.1243\n",
      "Total loss:  0.0021 | PDE Loss:  -0.5951 | Function Loss:  -1.1245\n",
      "Total loss:  0.0017 | PDE Loss:  -0.5961 | Function Loss:  -1.1246\n",
      "Total loss:  0.0013 | PDE Loss:  -0.5968 | Function Loss:  -1.125\n",
      "Total loss:  0.0009 | PDE Loss:  -0.5984 | Function Loss:  -1.125\n",
      "Total loss:  0.0006 | PDE Loss:  -0.6 | Function Loss:  -1.1249\n",
      "Total loss:  0.0002 | PDE Loss:  -0.6015 | Function Loss:  -1.1248\n",
      "Total loss:  -0.0001 | PDE Loss:  -0.603 | Function Loss:  -1.1248\n",
      "Total loss:  -0.0005 | PDE Loss:  -0.6042 | Function Loss:  -1.1249\n",
      "Total loss:  -0.0009 | PDE Loss:  -0.6045 | Function Loss:  -1.1253\n",
      "Total loss:  -0.0013 | PDE Loss:  -0.6049 | Function Loss:  -1.1257\n",
      "Total loss:  -0.0018 | PDE Loss:  -0.6041 | Function Loss:  -1.1267\n",
      "Total loss:  -0.0023 | PDE Loss:  -0.603 | Function Loss:  -1.1277\n",
      "Total loss:  -0.0028 | PDE Loss:  -0.602 | Function Loss:  -1.1287\n",
      "Total loss:  -0.0034 | PDE Loss:  -0.5993 | Function Loss:  -1.1305\n",
      "Total loss:  -0.0039 | PDE Loss:  -0.598 | Function Loss:  -1.1315\n",
      "Total loss:  -0.0043 | PDE Loss:  -0.5967 | Function Loss:  -1.1325\n",
      "Total loss:  -0.0048 | PDE Loss:  -0.595 | Function Loss:  -1.1337\n",
      "Total loss:  -0.0052 | PDE Loss:  -0.5946 | Function Loss:  -1.1345\n",
      "Total loss:  -0.0055 | PDE Loss:  -0.5944 | Function Loss:  -1.1349\n",
      "Total loss:  -0.0058 | PDE Loss:  -0.5944 | Function Loss:  -1.1353\n",
      "Total loss:  -0.0061 | PDE Loss:  -0.595 | Function Loss:  -1.1355\n",
      "Total loss:  -0.0063 | PDE Loss:  -0.5951 | Function Loss:  -1.1357\n",
      "Total loss:  -0.0065 | PDE Loss:  -0.5955 | Function Loss:  -1.1359\n",
      "Total loss:  -0.0069 | PDE Loss:  -0.5956 | Function Loss:  -1.1363\n",
      "Total loss:  -0.0071 | PDE Loss:  -0.5955 | Function Loss:  -1.1367\n",
      "Total loss:  -0.0074 | PDE Loss:  -0.5954 | Function Loss:  -1.1372\n",
      "Total loss:  -0.0077 | PDE Loss:  -0.5952 | Function Loss:  -1.1376\n",
      "Total loss:  -0.008 | PDE Loss:  -0.5947 | Function Loss:  -1.1382\n",
      "Total loss:  -0.0083 | PDE Loss:  -0.5951 | Function Loss:  -1.1385\n",
      "Total loss:  -0.0086 | PDE Loss:  -0.5951 | Function Loss:  -1.1389\n",
      "Total loss:  -0.0089 | PDE Loss:  -0.5957 | Function Loss:  -1.139\n",
      "Total loss:  -0.0092 | PDE Loss:  -0.5969 | Function Loss:  -1.139\n",
      "Total loss:  -0.0094 | PDE Loss:  -0.5979 | Function Loss:  -1.139\n",
      "Total loss:  -0.0097 | PDE Loss:  -0.5991 | Function Loss:  -1.1389\n",
      "Total loss:  -0.0099 | PDE Loss:  -0.5997 | Function Loss:  -1.139\n",
      "Total loss:  -0.0102 | PDE Loss:  -0.6005 | Function Loss:  -1.1391\n",
      "Total loss:  -0.0105 | PDE Loss:  -0.6004 | Function Loss:  -1.1395\n",
      "Total loss:  -0.0109 | PDE Loss:  -0.5997 | Function Loss:  -1.1403\n",
      "Total loss:  -0.0114 | PDE Loss:  -0.5982 | Function Loss:  -1.1415\n",
      "Total loss:  -0.0118 | PDE Loss:  -0.5958 | Function Loss:  -1.143\n",
      "Total loss:  -0.0122 | PDE Loss:  -0.5945 | Function Loss:  -1.1439\n",
      "Total loss:  -0.0127 | PDE Loss:  -0.592 | Function Loss:  -1.1454\n",
      "Total loss:  -0.0131 | PDE Loss:  -0.5903 | Function Loss:  -1.1467\n",
      "Total loss:  -0.0135 | PDE Loss:  -0.5888 | Function Loss:  -1.1478\n",
      "Total loss:  -0.0138 | PDE Loss:  -0.588 | Function Loss:  -1.1485\n",
      "Total loss:  -0.0142 | PDE Loss:  -0.5886 | Function Loss:  -1.1488\n",
      "Total loss:  -0.0147 | PDE Loss:  -0.5879 | Function Loss:  -1.1497\n",
      "Total loss:  -0.0151 | PDE Loss:  -0.5899 | Function Loss:  -1.1495\n",
      "Total loss:  -0.0154 | PDE Loss:  -0.5913 | Function Loss:  -1.1494\n",
      "Total loss:  -0.0158 | PDE Loss:  -0.5939 | Function Loss:  -1.149\n",
      "Total loss:  -0.0161 | PDE Loss:  -0.5958 | Function Loss:  -1.1488\n",
      "Total loss:  -0.0165 | PDE Loss:  -0.5984 | Function Loss:  -1.1484\n",
      "Total loss:  -0.0169 | PDE Loss:  -0.5994 | Function Loss:  -1.1485\n",
      "Total loss:  -0.0171 | PDE Loss:  -0.6002 | Function Loss:  -1.1486\n",
      "Total loss:  -0.0174 | PDE Loss:  -0.6 | Function Loss:  -1.149\n",
      "Total loss:  -0.0177 | PDE Loss:  -0.5994 | Function Loss:  -1.1496\n",
      "Total loss:  -0.018 | PDE Loss:  -0.5985 | Function Loss:  -1.1504\n",
      "Total loss:  -0.0183 | PDE Loss:  -0.5975 | Function Loss:  -1.1512\n",
      "Total loss:  -0.0186 | PDE Loss:  -0.5968 | Function Loss:  -1.1518\n",
      "Total loss:  -0.0189 | PDE Loss:  -0.5962 | Function Loss:  -1.1524\n",
      "Total loss:  -0.0192 | PDE Loss:  -0.5967 | Function Loss:  -1.1526\n",
      "Total loss:  -0.0195 | PDE Loss:  -0.5973 | Function Loss:  -1.1528\n",
      "Total loss:  -0.0198 | PDE Loss:  -0.5985 | Function Loss:  -1.1528\n",
      "Total loss:  -0.0201 | PDE Loss:  -0.5997 | Function Loss:  -1.1528\n",
      "Total loss:  -0.0205 | PDE Loss:  -0.6012 | Function Loss:  -1.1527\n",
      "Total loss:  -0.0208 | PDE Loss:  -0.6025 | Function Loss:  -1.1527\n",
      "Total loss:  -0.0211 | PDE Loss:  -0.6031 | Function Loss:  -1.1529\n",
      "Total loss:  -0.0214 | PDE Loss:  -0.6039 | Function Loss:  -1.1531\n",
      "Total loss:  -0.0217 | PDE Loss:  -0.604 | Function Loss:  -1.1534\n",
      "Total loss:  -0.0219 | PDE Loss:  -0.6042 | Function Loss:  -1.1537\n",
      "Total loss:  -0.0221 | PDE Loss:  -0.604 | Function Loss:  -1.154\n",
      "Total loss:  -0.0223 | PDE Loss:  -0.6045 | Function Loss:  -1.1541\n",
      "Total loss:  -0.0225 | PDE Loss:  -0.605 | Function Loss:  -1.1541\n",
      "Total loss:  -0.0226 | PDE Loss:  -0.6053 | Function Loss:  -1.1543\n",
      "Total loss:  -0.0229 | PDE Loss:  -0.6064 | Function Loss:  -1.1542\n",
      "Total loss:  -0.0232 | PDE Loss:  -0.6076 | Function Loss:  -1.1542\n",
      "Total loss:  -0.0236 | PDE Loss:  -0.609 | Function Loss:  -1.1542\n",
      "Total loss:  -0.024 | PDE Loss:  -0.6109 | Function Loss:  -1.1541\n",
      "Total loss:  -0.0245 | PDE Loss:  -0.612 | Function Loss:  -1.1544\n",
      "Total loss:  -0.0249 | PDE Loss:  -0.6141 | Function Loss:  -1.1542\n",
      "Total loss:  -0.0253 | PDE Loss:  -0.6142 | Function Loss:  -1.1547\n",
      "Total loss:  -0.0257 | PDE Loss:  -0.6149 | Function Loss:  -1.155\n",
      "Total loss:  -0.0262 | PDE Loss:  -0.6152 | Function Loss:  -1.1556\n",
      "Total loss:  -0.0268 | PDE Loss:  -0.6156 | Function Loss:  -1.1563\n",
      "Total loss:  -0.0275 | PDE Loss:  -0.6158 | Function Loss:  -1.1571\n",
      "Total loss:  -0.0282 | PDE Loss:  -0.6161 | Function Loss:  -1.1579\n",
      "Total loss:  -0.0288 | PDE Loss:  -0.6162 | Function Loss:  -1.1588\n",
      "Total loss:  -0.0295 | PDE Loss:  -0.6166 | Function Loss:  -1.1595\n",
      "Total loss:  -0.0302 | PDE Loss:  -0.6175 | Function Loss:  -1.1602\n",
      "Total loss:  -0.0308 | PDE Loss:  -0.6183 | Function Loss:  -1.1607\n",
      "Total loss:  -0.0315 | PDE Loss:  -0.6203 | Function Loss:  -1.1609\n",
      "Total loss:  -0.0321 | PDE Loss:  -0.6228 | Function Loss:  -1.1609\n",
      "Total loss:  -0.0326 | PDE Loss:  -0.6244 | Function Loss:  -1.161\n",
      "Total loss:  -0.033 | PDE Loss:  -0.626 | Function Loss:  -1.161\n",
      "Total loss:  -0.0333 | PDE Loss:  -0.627 | Function Loss:  -1.161\n",
      "Total loss:  -0.0335 | PDE Loss:  -0.6272 | Function Loss:  -1.1613\n",
      "Total loss:  -0.0338 | PDE Loss:  -0.6271 | Function Loss:  -1.1616\n",
      "Total loss:  -0.0341 | PDE Loss:  -0.6268 | Function Loss:  -1.1622\n",
      "Total loss:  -0.0344 | PDE Loss:  -0.626 | Function Loss:  -1.1629\n",
      "Total loss:  -0.0347 | PDE Loss:  -0.6253 | Function Loss:  -1.1635\n",
      "Total loss:  -0.0349 | PDE Loss:  -0.6249 | Function Loss:  -1.1639\n",
      "Total loss:  -0.0351 | PDE Loss:  -0.625 | Function Loss:  -1.1642\n",
      "Total loss:  -0.0353 | PDE Loss:  -0.6251 | Function Loss:  -1.1644\n",
      "Total loss:  -0.0356 | PDE Loss:  -0.6257 | Function Loss:  -1.1645\n",
      "Total loss:  -0.0357 | PDE Loss:  -0.6262 | Function Loss:  -1.1646\n",
      "Total loss:  -0.036 | PDE Loss:  -0.6267 | Function Loss:  -1.1648\n",
      "Total loss:  -0.0362 | PDE Loss:  -0.6279 | Function Loss:  -1.1647\n",
      "Total loss:  -0.0365 | PDE Loss:  -0.6277 | Function Loss:  -1.1651\n",
      "Total loss:  -0.0367 | PDE Loss:  -0.6279 | Function Loss:  -1.1654\n",
      "Total loss:  -0.037 | PDE Loss:  -0.6281 | Function Loss:  -1.1656\n",
      "Total loss:  -0.0373 | PDE Loss:  -0.6281 | Function Loss:  -1.166\n",
      "Total loss:  -0.0376 | PDE Loss:  -0.6283 | Function Loss:  -1.1664\n",
      "Total loss:  -0.038 | PDE Loss:  -0.6292 | Function Loss:  -1.1666\n",
      "Total loss:  -0.0384 | PDE Loss:  -0.6303 | Function Loss:  -1.1668\n",
      "Total loss:  -0.0389 | PDE Loss:  -0.6321 | Function Loss:  -1.1668\n",
      "Total loss:  -0.0394 | PDE Loss:  -0.6349 | Function Loss:  -1.1665\n",
      "Total loss:  -0.0397 | PDE Loss:  -0.636 | Function Loss:  -1.1666\n",
      "Total loss:  -0.0402 | PDE Loss:  -0.6382 | Function Loss:  -1.1665\n",
      "Total loss:  -0.0406 | PDE Loss:  -0.6392 | Function Loss:  -1.1667\n",
      "Total loss:  -0.0409 | PDE Loss:  -0.6403 | Function Loss:  -1.1668\n",
      "Total loss:  -0.0411 | PDE Loss:  -0.6407 | Function Loss:  -1.1669\n",
      "Total loss:  -0.0414 | PDE Loss:  -0.6409 | Function Loss:  -1.1672\n",
      "Total loss:  -0.0418 | PDE Loss:  -0.6411 | Function Loss:  -1.1677\n",
      "Total loss:  -0.0423 | PDE Loss:  -0.6399 | Function Loss:  -1.1688\n",
      "Total loss:  -0.0428 | PDE Loss:  -0.6402 | Function Loss:  -1.1693\n",
      "Total loss:  -0.0432 | PDE Loss:  -0.6406 | Function Loss:  -1.1697\n",
      "Total loss:  -0.0438 | PDE Loss:  -0.6407 | Function Loss:  -1.1704\n",
      "Total loss:  -0.0442 | PDE Loss:  -0.6411 | Function Loss:  -1.1709\n",
      "Total loss:  -0.0445 | PDE Loss:  -0.6412 | Function Loss:  -1.1712\n",
      "Total loss:  -0.0447 | PDE Loss:  -0.6411 | Function Loss:  -1.1715\n",
      "Total loss:  -0.0448 | PDE Loss:  -0.6409 | Function Loss:  -1.1718\n",
      "Total loss:  -0.045 | PDE Loss:  -0.6405 | Function Loss:  -1.1722\n",
      "Total loss:  -0.0453 | PDE Loss:  -0.6399 | Function Loss:  -1.1727\n",
      "Total loss:  -0.0455 | PDE Loss:  -0.6391 | Function Loss:  -1.1733\n",
      "Total loss:  -0.0457 | PDE Loss:  -0.6383 | Function Loss:  -1.1738\n",
      "Total loss:  -0.0459 | PDE Loss:  -0.6377 | Function Loss:  -1.1743\n",
      "Total loss:  -0.0463 | PDE Loss:  -0.6366 | Function Loss:  -1.1753\n",
      "Total loss:  -0.0468 | PDE Loss:  -0.6371 | Function Loss:  -1.1757\n",
      "Total loss:  -0.0473 | PDE Loss:  -0.638 | Function Loss:  -1.1761\n",
      "Total loss:  -0.0479 | PDE Loss:  -0.6408 | Function Loss:  -1.176\n",
      "Total loss:  -0.0484 | PDE Loss:  -0.643 | Function Loss:  -1.1758\n",
      "Total loss:  -0.0487 | PDE Loss:  -0.6452 | Function Loss:  -1.1756\n",
      "Total loss:  -0.0491 | PDE Loss:  -0.6478 | Function Loss:  -1.1751\n",
      "Total loss:  -0.0493 | PDE Loss:  -0.6497 | Function Loss:  -1.1748\n",
      "Total loss:  -0.0496 | PDE Loss:  -0.6513 | Function Loss:  -1.1746\n",
      "Total loss:  -0.0498 | PDE Loss:  -0.6529 | Function Loss:  -1.1744\n",
      "Total loss:  -0.0501 | PDE Loss:  -0.6541 | Function Loss:  -1.1744\n",
      "Total loss:  -0.0503 | PDE Loss:  -0.6548 | Function Loss:  -1.1745\n",
      "Total loss:  -0.0506 | PDE Loss:  -0.6554 | Function Loss:  -1.1746\n",
      "Total loss:  -0.0508 | PDE Loss:  -0.6552 | Function Loss:  -1.1749\n",
      "Total loss:  -0.0511 | PDE Loss:  -0.6549 | Function Loss:  -1.1754\n",
      "Total loss:  -0.0515 | PDE Loss:  -0.6544 | Function Loss:  -1.1762\n",
      "Total loss:  -0.052 | PDE Loss:  -0.6533 | Function Loss:  -1.1772\n",
      "Total loss:  -0.0524 | PDE Loss:  -0.6532 | Function Loss:  -1.1778\n",
      "Total loss:  -0.0527 | PDE Loss:  -0.6535 | Function Loss:  -1.1781\n",
      "Total loss:  -0.053 | PDE Loss:  -0.6541 | Function Loss:  -1.1782\n",
      "Total loss:  -0.0532 | PDE Loss:  -0.6549 | Function Loss:  -1.1783\n",
      "Total loss:  -0.0534 | PDE Loss:  -0.6562 | Function Loss:  -1.1781\n",
      "Total loss:  -0.0537 | PDE Loss:  -0.6575 | Function Loss:  -1.178\n",
      "Total loss:  -0.0539 | PDE Loss:  -0.6594 | Function Loss:  -1.1777\n",
      "Total loss:  -0.0541 | PDE Loss:  -0.6608 | Function Loss:  -1.1775\n",
      "Total loss:  -0.0543 | PDE Loss:  -0.6623 | Function Loss:  -1.1773\n",
      "Total loss:  -0.0546 | PDE Loss:  -0.6638 | Function Loss:  -1.1772\n",
      "Total loss:  -0.055 | PDE Loss:  -0.6652 | Function Loss:  -1.1772\n",
      "Total loss:  -0.0553 | PDE Loss:  -0.6662 | Function Loss:  -1.1774\n",
      "Total loss:  -0.0556 | PDE Loss:  -0.6667 | Function Loss:  -1.1776\n",
      "Total loss:  -0.056 | PDE Loss:  -0.6674 | Function Loss:  -1.1779\n",
      "Total loss:  -0.0567 | PDE Loss:  -0.6683 | Function Loss:  -1.1785\n",
      "Total loss:  -0.0574 | PDE Loss:  -0.6687 | Function Loss:  -1.1793\n",
      "Total loss:  -0.058 | PDE Loss:  -0.6695 | Function Loss:  -1.1799\n",
      "Total loss:  -0.0586 | PDE Loss:  -0.6706 | Function Loss:  -1.1802\n",
      "Total loss:  -0.0591 | PDE Loss:  -0.6718 | Function Loss:  -1.1805\n",
      "Total loss:  -0.0595 | PDE Loss:  -0.6728 | Function Loss:  -1.1808\n",
      "Total loss:  -0.0598 | PDE Loss:  -0.6738 | Function Loss:  -1.1808\n",
      "Total loss:  -0.0601 | PDE Loss:  -0.6747 | Function Loss:  -1.1809\n",
      "Total loss:  -0.0605 | PDE Loss:  -0.6761 | Function Loss:  -1.181\n",
      "Total loss:  -0.0609 | PDE Loss:  -0.6764 | Function Loss:  -1.1814\n",
      "Total loss:  -0.0612 | PDE Loss:  -0.677 | Function Loss:  -1.1817\n",
      "Total loss:  -0.0616 | PDE Loss:  -0.6762 | Function Loss:  -1.1825\n",
      "Total loss:  -0.0621 | PDE Loss:  -0.6761 | Function Loss:  -1.1831\n",
      "Total loss:  -0.0625 | PDE Loss:  -0.6748 | Function Loss:  -1.1841\n",
      "Total loss:  -0.063 | PDE Loss:  -0.6742 | Function Loss:  -1.185\n",
      "Total loss:  -0.0635 | PDE Loss:  -0.6739 | Function Loss:  -1.1857\n",
      "Total loss:  -0.0641 | PDE Loss:  -0.6735 | Function Loss:  -1.1866\n",
      "Total loss:  -0.0647 | PDE Loss:  -0.674 | Function Loss:  -1.1873\n",
      "Total loss:  -0.0654 | PDE Loss:  -0.6741 | Function Loss:  -1.1881\n",
      "Total loss:  -0.066 | PDE Loss:  -0.6746 | Function Loss:  -1.1888\n",
      "Total loss:  -0.0665 | PDE Loss:  -0.6748 | Function Loss:  -1.1894\n",
      "Total loss:  -0.067 | PDE Loss:  -0.6754 | Function Loss:  -1.1899\n",
      "Total loss:  -0.0673 | PDE Loss:  -0.6752 | Function Loss:  -1.1904\n",
      "Total loss:  -0.0676 | PDE Loss:  -0.6759 | Function Loss:  -1.1905\n",
      "Total loss:  -0.0679 | PDE Loss:  -0.6759 | Function Loss:  -1.1909\n",
      "Total loss:  -0.0681 | PDE Loss:  -0.6758 | Function Loss:  -1.1911\n",
      "Total loss:  -0.0682 | PDE Loss:  -0.6758 | Function Loss:  -1.1913\n",
      "Total loss:  -0.0684 | PDE Loss:  -0.6752 | Function Loss:  -1.1918\n",
      "Total loss:  -0.0686 | PDE Loss:  -0.6751 | Function Loss:  -1.1921\n",
      "Total loss:  -0.0689 | PDE Loss:  -0.6744 | Function Loss:  -1.1926\n",
      "Total loss:  -0.0691 | PDE Loss:  -0.674 | Function Loss:  -1.1931\n",
      "Total loss:  -0.0694 | PDE Loss:  -0.6737 | Function Loss:  -1.1936\n",
      "Total loss:  -0.0697 | PDE Loss:  -0.674 | Function Loss:  -1.1939\n",
      "Total loss:  -0.07 | PDE Loss:  -0.6743 | Function Loss:  -1.1942\n",
      "Total loss:  -0.0702 | PDE Loss:  -0.675 | Function Loss:  -1.1943\n",
      "Total loss:  -0.0707 | PDE Loss:  -0.676 | Function Loss:  -1.1945\n",
      "Total loss:  -0.0712 | PDE Loss:  -0.677 | Function Loss:  -1.1948\n",
      "Total loss:  -0.0719 | PDE Loss:  -0.6777 | Function Loss:  -1.1956\n",
      "Total loss:  -0.0726 | PDE Loss:  -0.6787 | Function Loss:  -1.1962\n",
      "Total loss:  -0.0739 | PDE Loss:  -0.6787 | Function Loss:  -1.198\n",
      "Total loss:  -0.0753 | PDE Loss:  -0.6805 | Function Loss:  -1.1992\n",
      "Total loss:  -0.0764 | PDE Loss:  -0.6796 | Function Loss:  -1.201\n",
      "Total loss:  -0.0776 | PDE Loss:  -0.6801 | Function Loss:  -1.2024\n",
      "Total loss:  -0.0787 | PDE Loss:  -0.6809 | Function Loss:  -1.2035\n",
      "Total loss:  -0.0799 | PDE Loss:  -0.683 | Function Loss:  -1.2045\n",
      "Total loss:  -0.0808 | PDE Loss:  -0.683 | Function Loss:  -1.2056\n",
      "Total loss:  -0.0814 | PDE Loss:  -0.686 | Function Loss:  -1.2055\n",
      "Total loss:  -0.0821 | PDE Loss:  -0.688 | Function Loss:  -1.2058\n",
      "Total loss:  -0.0828 | PDE Loss:  -0.6914 | Function Loss:  -1.2055\n",
      "Total loss:  -0.0835 | PDE Loss:  -0.6948 | Function Loss:  -1.2054\n",
      "Total loss:  -0.0841 | PDE Loss:  -0.6975 | Function Loss:  -1.2053\n",
      "Total loss:  -0.0847 | PDE Loss:  -0.7001 | Function Loss:  -1.2053\n",
      "Total loss:  -0.0853 | PDE Loss:  -0.7012 | Function Loss:  -1.2057\n",
      "Total loss:  -0.086 | PDE Loss:  -0.7015 | Function Loss:  -1.2065\n",
      "Total loss:  -0.0865 | PDE Loss:  -0.7 | Function Loss:  -1.2078\n",
      "Total loss:  -0.0869 | PDE Loss:  -0.6997 | Function Loss:  -1.2083\n",
      "Total loss:  -0.0872 | PDE Loss:  -0.6985 | Function Loss:  -1.2091\n",
      "Total loss:  -0.0875 | PDE Loss:  -0.6982 | Function Loss:  -1.2096\n",
      "Total loss:  -0.0877 | PDE Loss:  -0.6983 | Function Loss:  -1.2098\n",
      "Total loss:  -0.0879 | PDE Loss:  -0.6981 | Function Loss:  -1.2102\n",
      "Total loss:  -0.0882 | PDE Loss:  -0.6996 | Function Loss:  -1.21\n",
      "Total loss:  -0.0883 | PDE Loss:  -0.6997 | Function Loss:  -1.2102\n",
      "Total loss:  -0.0887 | PDE Loss:  -0.7015 | Function Loss:  -1.2101\n",
      "Total loss:  -0.0891 | PDE Loss:  -0.7038 | Function Loss:  -1.2099\n",
      "Total loss:  -0.0894 | PDE Loss:  -0.7041 | Function Loss:  -1.2102\n",
      "Total loss:  -0.0897 | PDE Loss:  -0.7055 | Function Loss:  -1.2102\n",
      "Total loss:  -0.09 | PDE Loss:  -0.7059 | Function Loss:  -1.2104\n",
      "Total loss:  -0.0903 | PDE Loss:  -0.7065 | Function Loss:  -1.2106\n",
      "Total loss:  -0.0905 | PDE Loss:  -0.7073 | Function Loss:  -1.2106\n",
      "Total loss:  -0.0908 | PDE Loss:  -0.7083 | Function Loss:  -1.2107\n",
      "Total loss:  -0.0912 | PDE Loss:  -0.7102 | Function Loss:  -1.2107\n",
      "Total loss:  -0.0918 | PDE Loss:  -0.7134 | Function Loss:  -1.2104\n",
      "Total loss:  -0.0924 | PDE Loss:  -0.7152 | Function Loss:  -1.2107\n",
      "Total loss:  -0.093 | PDE Loss:  -0.7174 | Function Loss:  -1.2107\n",
      "Total loss:  -0.0936 | PDE Loss:  -0.7204 | Function Loss:  -1.2106\n",
      "Total loss:  -0.0939 | PDE Loss:  -0.7217 | Function Loss:  -1.2106\n",
      "Total loss:  -0.0942 | PDE Loss:  -0.7225 | Function Loss:  -1.2107\n",
      "Total loss:  -0.0945 | PDE Loss:  -0.723 | Function Loss:  -1.211\n",
      "Total loss:  -0.0949 | PDE Loss:  -0.7229 | Function Loss:  -1.2116\n",
      "Total loss:  -0.0953 | PDE Loss:  -0.722 | Function Loss:  -1.2123\n",
      "Total loss:  -0.0956 | PDE Loss:  -0.721 | Function Loss:  -1.213\n",
      "Total loss:  -0.0959 | PDE Loss:  -0.7199 | Function Loss:  -1.2137\n",
      "Total loss:  -0.0961 | PDE Loss:  -0.7191 | Function Loss:  -1.2143\n",
      "Total loss:  -0.0964 | PDE Loss:  -0.718 | Function Loss:  -1.215\n",
      "Total loss:  -0.0966 | PDE Loss:  -0.7177 | Function Loss:  -1.2154\n",
      "Total loss:  -0.097 | PDE Loss:  -0.7171 | Function Loss:  -1.216\n",
      "Total loss:  -0.0974 | PDE Loss:  -0.7171 | Function Loss:  -1.2166\n",
      "Total loss:  -0.0978 | PDE Loss:  -0.7177 | Function Loss:  -1.217\n",
      "Total loss:  -0.0982 | PDE Loss:  -0.7188 | Function Loss:  -1.2172\n",
      "Total loss:  -0.0987 | PDE Loss:  -0.7199 | Function Loss:  -1.2175\n",
      "Total loss:  -0.0993 | PDE Loss:  -0.7213 | Function Loss:  -1.2177\n",
      "Total loss:  -0.0998 | PDE Loss:  -0.7213 | Function Loss:  -1.2184\n",
      "Total loss:  -0.1002 | PDE Loss:  -0.7216 | Function Loss:  -1.219\n",
      "Total loss:  -0.1006 | PDE Loss:  -0.7214 | Function Loss:  -1.2195\n",
      "Total loss:  -0.101 | PDE Loss:  -0.7199 | Function Loss:  -1.2204\n",
      "Total loss:  -0.1012 | PDE Loss:  -0.7189 | Function Loss:  -1.2211\n",
      "Total loss:  -0.1015 | PDE Loss:  -0.7181 | Function Loss:  -1.2217\n",
      "Total loss:  -0.1017 | PDE Loss:  -0.7163 | Function Loss:  -1.2225\n",
      "Total loss:  -0.1019 | PDE Loss:  -0.7159 | Function Loss:  -1.2229\n",
      "Total loss:  -0.1022 | PDE Loss:  -0.716 | Function Loss:  -1.2233\n",
      "Total loss:  -0.1025 | PDE Loss:  -0.7155 | Function Loss:  -1.2238\n",
      "Total loss:  -0.1027 | PDE Loss:  -0.7162 | Function Loss:  -1.2238\n",
      "Total loss:  -0.1029 | PDE Loss:  -0.7167 | Function Loss:  -1.224\n",
      "Total loss:  -0.1032 | PDE Loss:  -0.7187 | Function Loss:  -1.2238\n",
      "Total loss:  -0.1035 | PDE Loss:  -0.719 | Function Loss:  -1.2241\n",
      "Total loss:  -0.1038 | PDE Loss:  -0.7194 | Function Loss:  -1.2243\n",
      "Total loss:  -0.1042 | PDE Loss:  -0.7194 | Function Loss:  -1.2249\n",
      "Total loss:  -0.1045 | PDE Loss:  -0.7186 | Function Loss:  -1.2255\n",
      "Total loss:  -0.1048 | PDE Loss:  -0.7172 | Function Loss:  -1.2263\n",
      "Total loss:  -0.105 | PDE Loss:  -0.7158 | Function Loss:  -1.227\n",
      "Total loss:  -0.1053 | PDE Loss:  -0.7146 | Function Loss:  -1.2279\n",
      "Total loss:  -0.1058 | PDE Loss:  -0.7119 | Function Loss:  -1.2294\n",
      "Total loss:  -0.1064 | PDE Loss:  -0.7115 | Function Loss:  -1.2303\n",
      "Total loss:  -0.107 | PDE Loss:  -0.7116 | Function Loss:  -1.2312\n",
      "Total loss:  -0.1079 | PDE Loss:  -0.7135 | Function Loss:  -1.2317\n",
      "Total loss:  -0.1087 | PDE Loss:  -0.7138 | Function Loss:  -1.2326\n",
      "Total loss:  -0.1093 | PDE Loss:  -0.7155 | Function Loss:  -1.2328\n",
      "Total loss:  -0.1097 | PDE Loss:  -0.7177 | Function Loss:  -1.2326\n",
      "Total loss:  -0.1101 | PDE Loss:  -0.7193 | Function Loss:  -1.2326\n",
      "Total loss:  -0.1105 | PDE Loss:  -0.7219 | Function Loss:  -1.2323\n",
      "Total loss:  -0.1107 | PDE Loss:  -0.7226 | Function Loss:  -1.2325\n",
      "Total loss:  -0.1111 | PDE Loss:  -0.7245 | Function Loss:  -1.2324\n",
      "Total loss:  -0.1115 | PDE Loss:  -0.7249 | Function Loss:  -1.2327\n",
      "Total loss:  -0.1118 | PDE Loss:  -0.7252 | Function Loss:  -1.233\n",
      "Total loss:  -0.1121 | PDE Loss:  -0.725 | Function Loss:  -1.2334\n",
      "Total loss:  -0.1124 | PDE Loss:  -0.7256 | Function Loss:  -1.2337\n",
      "Total loss:  -0.1128 | PDE Loss:  -0.7247 | Function Loss:  -1.2345\n",
      "Total loss:  -0.1131 | PDE Loss:  -0.7252 | Function Loss:  -1.2348\n",
      "Total loss:  -0.1135 | PDE Loss:  -0.7255 | Function Loss:  -1.2351\n",
      "Total loss:  -0.1138 | PDE Loss:  -0.7254 | Function Loss:  -1.2356\n",
      "Total loss:  -0.1141 | PDE Loss:  -0.7263 | Function Loss:  -1.2357\n",
      "Total loss:  -0.1145 | PDE Loss:  -0.7266 | Function Loss:  -1.2361\n",
      "Total loss:  -0.1149 | PDE Loss:  -0.7272 | Function Loss:  -1.2366\n",
      "Total loss:  -0.1155 | PDE Loss:  -0.7274 | Function Loss:  -1.2372\n",
      "Total loss:  -0.116 | PDE Loss:  -0.7277 | Function Loss:  -1.2378\n",
      "Total loss:  -0.1166 | PDE Loss:  -0.7266 | Function Loss:  -1.2389\n",
      "Total loss:  -0.1172 | PDE Loss:  -0.7256 | Function Loss:  -1.2401\n",
      "Total loss:  -0.1178 | PDE Loss:  -0.7247 | Function Loss:  -1.2411\n",
      "Total loss:  -0.1182 | PDE Loss:  -0.7225 | Function Loss:  -1.2424\n",
      "Total loss:  -0.1185 | PDE Loss:  -0.7216 | Function Loss:  -1.2431\n",
      "Total loss:  -0.1187 | PDE Loss:  -0.721 | Function Loss:  -1.2436\n",
      "Total loss:  -0.119 | PDE Loss:  -0.7196 | Function Loss:  -1.2444\n",
      "Total loss:  -0.1193 | PDE Loss:  -0.72 | Function Loss:  -1.2447\n",
      "Total loss:  -0.1196 | PDE Loss:  -0.7199 | Function Loss:  -1.2452\n",
      "Total loss:  -0.1201 | PDE Loss:  -0.7205 | Function Loss:  -1.2456\n",
      "Total loss:  -0.1207 | PDE Loss:  -0.7211 | Function Loss:  -1.2461\n",
      "Total loss:  -0.1214 | PDE Loss:  -0.7223 | Function Loss:  -1.2467\n",
      "Total loss:  -0.1221 | PDE Loss:  -0.723 | Function Loss:  -1.2474\n",
      "Total loss:  -0.1228 | PDE Loss:  -0.724 | Function Loss:  -1.2481\n",
      "Total loss:  -0.1235 | PDE Loss:  -0.7246 | Function Loss:  -1.2487\n",
      "Total loss:  -0.1243 | PDE Loss:  -0.725 | Function Loss:  -1.2497\n",
      "Total loss:  -0.1251 | PDE Loss:  -0.7252 | Function Loss:  -1.2507\n",
      "Total loss:  -0.1257 | PDE Loss:  -0.7251 | Function Loss:  -1.2515\n",
      "Total loss:  -0.1262 | PDE Loss:  -0.7261 | Function Loss:  -1.2518\n",
      "Total loss:  -0.1266 | PDE Loss:  -0.7258 | Function Loss:  -1.2525\n",
      "Total loss:  -0.1269 | PDE Loss:  -0.7257 | Function Loss:  -1.2529\n",
      "Total loss:  -0.1271 | PDE Loss:  -0.7263 | Function Loss:  -1.253\n",
      "Total loss:  -0.1272 | PDE Loss:  -0.7262 | Function Loss:  -1.2532\n",
      "Total loss:  -0.1274 | PDE Loss:  -0.7266 | Function Loss:  -1.2533\n",
      "Total loss:  -0.1276 | PDE Loss:  -0.727 | Function Loss:  -1.2534\n",
      "Total loss:  -0.1278 | PDE Loss:  -0.7275 | Function Loss:  -1.2535\n",
      "Total loss:  -0.128 | PDE Loss:  -0.7279 | Function Loss:  -1.2536\n",
      "Total loss:  -0.1281 | PDE Loss:  -0.7286 | Function Loss:  -1.2536\n",
      "Total loss:  -0.1283 | PDE Loss:  -0.729 | Function Loss:  -1.2537\n",
      "Total loss:  -0.1284 | PDE Loss:  -0.7293 | Function Loss:  -1.2538\n",
      "Total loss:  -0.1286 | PDE Loss:  -0.7298 | Function Loss:  -1.2539\n",
      "Total loss:  -0.1288 | PDE Loss:  -0.73 | Function Loss:  -1.254\n",
      "Total loss:  -0.129 | PDE Loss:  -0.7308 | Function Loss:  -1.2541\n",
      "Total loss:  -0.1292 | PDE Loss:  -0.7312 | Function Loss:  -1.2542\n",
      "Total loss:  -0.1295 | PDE Loss:  -0.7323 | Function Loss:  -1.2542\n",
      "Total loss:  -0.1299 | PDE Loss:  -0.7331 | Function Loss:  -1.2545\n",
      "Total loss:  -0.1303 | PDE Loss:  -0.735 | Function Loss:  -1.2544\n",
      "Total loss:  -0.1308 | PDE Loss:  -0.7356 | Function Loss:  -1.2549\n",
      "Total loss:  -0.1313 | PDE Loss:  -0.7371 | Function Loss:  -1.255\n",
      "Total loss:  -0.1318 | PDE Loss:  -0.7375 | Function Loss:  -1.2555\n",
      "Total loss:  -0.1322 | PDE Loss:  -0.738 | Function Loss:  -1.256\n",
      "Total loss:  -0.1326 | PDE Loss:  -0.7378 | Function Loss:  -1.2565\n",
      "Total loss:  -0.1328 | PDE Loss:  -0.7376 | Function Loss:  -1.2569\n",
      "Total loss:  -0.1331 | PDE Loss:  -0.7368 | Function Loss:  -1.2575\n",
      "Total loss:  -0.1335 | PDE Loss:  -0.7356 | Function Loss:  -1.2585\n",
      "Total loss:  -0.1338 | PDE Loss:  -0.7347 | Function Loss:  -1.2591\n",
      "Total loss:  -0.134 | PDE Loss:  -0.7321 | Function Loss:  -1.2603\n",
      "Total loss:  -0.1342 | PDE Loss:  -0.7319 | Function Loss:  -1.2606\n",
      "Total loss:  -0.1344 | PDE Loss:  -0.7321 | Function Loss:  -1.2608\n",
      "Total loss:  -0.1346 | PDE Loss:  -0.7319 | Function Loss:  -1.2612\n",
      "Total loss:  -0.1348 | PDE Loss:  -0.7318 | Function Loss:  -1.2615\n",
      "Total loss:  -0.135 | PDE Loss:  -0.7314 | Function Loss:  -1.2619\n",
      "Total loss:  -0.1352 | PDE Loss:  -0.731 | Function Loss:  -1.2623\n",
      "Total loss:  -0.1355 | PDE Loss:  -0.7302 | Function Loss:  -1.2629\n",
      "Total loss:  -0.1358 | PDE Loss:  -0.7295 | Function Loss:  -1.2635\n",
      "Total loss:  -0.1361 | PDE Loss:  -0.7284 | Function Loss:  -1.2644\n",
      "Total loss:  -0.1365 | PDE Loss:  -0.7274 | Function Loss:  -1.2652\n",
      "Total loss:  -0.1368 | PDE Loss:  -0.7264 | Function Loss:  -1.266\n",
      "Total loss:  -0.1372 | PDE Loss:  -0.7256 | Function Loss:  -1.2667\n",
      "Total loss:  -0.1375 | PDE Loss:  -0.7256 | Function Loss:  -1.2673\n",
      "Total loss:  -0.138 | PDE Loss:  -0.7256 | Function Loss:  -1.2678\n",
      "Total loss:  -0.1384 | PDE Loss:  -0.7264 | Function Loss:  -1.2682\n",
      "Total loss:  -0.1389 | PDE Loss:  -0.7274 | Function Loss:  -1.2684\n",
      "Total loss:  -0.1394 | PDE Loss:  -0.7289 | Function Loss:  -1.2686\n",
      "Total loss:  -0.14 | PDE Loss:  -0.7306 | Function Loss:  -1.2688\n",
      "Total loss:  -0.1407 | PDE Loss:  -0.7331 | Function Loss:  -1.2689\n",
      "Total loss:  -0.1415 | PDE Loss:  -0.7352 | Function Loss:  -1.2693\n",
      "Total loss:  -0.1422 | PDE Loss:  -0.7372 | Function Loss:  -1.2695\n",
      "Total loss:  -0.1431 | PDE Loss:  -0.7395 | Function Loss:  -1.2699\n",
      "Total loss:  -0.1438 | PDE Loss:  -0.7395 | Function Loss:  -1.2709\n",
      "Total loss:  -0.1445 | PDE Loss:  -0.7409 | Function Loss:  -1.2713\n",
      "Total loss:  -0.145 | PDE Loss:  -0.74 | Function Loss:  -1.2723\n",
      "Total loss:  -0.1452 | PDE Loss:  -0.7403 | Function Loss:  -1.2725\n",
      "Total loss:  -0.1455 | PDE Loss:  -0.7403 | Function Loss:  -1.2729\n",
      "Total loss:  -0.146 | PDE Loss:  -0.74 | Function Loss:  -1.2736\n",
      "Total loss:  -0.1464 | PDE Loss:  -0.7399 | Function Loss:  -1.2742\n",
      "Total loss:  -0.1467 | PDE Loss:  -0.7398 | Function Loss:  -1.2747\n",
      "Total loss:  -0.147 | PDE Loss:  -0.7389 | Function Loss:  -1.2754\n",
      "Total loss:  -0.1473 | PDE Loss:  -0.7399 | Function Loss:  -1.2754\n",
      "Total loss:  -0.1475 | PDE Loss:  -0.7398 | Function Loss:  -1.2758\n",
      "Total loss:  -0.1478 | PDE Loss:  -0.7394 | Function Loss:  -1.2762\n",
      "Total loss:  -0.1481 | PDE Loss:  -0.7402 | Function Loss:  -1.2764\n",
      "Total loss:  -0.1485 | PDE Loss:  -0.7412 | Function Loss:  -1.2766\n",
      "Total loss:  -0.1488 | PDE Loss:  -0.7419 | Function Loss:  -1.2768\n",
      "Total loss:  -0.1491 | PDE Loss:  -0.7428 | Function Loss:  -1.2769\n",
      "Total loss:  -0.1495 | PDE Loss:  -0.7434 | Function Loss:  -1.2772\n",
      "Total loss:  -0.1499 | PDE Loss:  -0.7437 | Function Loss:  -1.2777\n",
      "Total loss:  -0.1504 | PDE Loss:  -0.7441 | Function Loss:  -1.2781\n",
      "Total loss:  -0.1507 | PDE Loss:  -0.7445 | Function Loss:  -1.2785\n",
      "Total loss:  -0.1511 | PDE Loss:  -0.7444 | Function Loss:  -1.279\n",
      "Total loss:  -0.1514 | PDE Loss:  -0.7443 | Function Loss:  -1.2795\n",
      "Total loss:  -0.1518 | PDE Loss:  -0.7445 | Function Loss:  -1.2798\n",
      "Total loss:  -0.1521 | PDE Loss:  -0.7437 | Function Loss:  -1.2806\n",
      "Total loss:  -0.1525 | PDE Loss:  -0.7433 | Function Loss:  -1.2812\n",
      "Total loss:  -0.153 | PDE Loss:  -0.742 | Function Loss:  -1.2824\n",
      "Total loss:  -0.1534 | PDE Loss:  -0.7404 | Function Loss:  -1.2835\n",
      "Total loss:  -0.1538 | PDE Loss:  -0.7398 | Function Loss:  -1.2842\n",
      "Total loss:  -0.1541 | PDE Loss:  -0.7388 | Function Loss:  -1.285\n",
      "Total loss:  -0.1544 | PDE Loss:  -0.7386 | Function Loss:  -1.2854\n",
      "Total loss:  -0.1547 | PDE Loss:  -0.7389 | Function Loss:  -1.2858\n",
      "Total loss:  -0.1552 | PDE Loss:  -0.7388 | Function Loss:  -1.2864\n",
      "Total loss:  -0.1556 | PDE Loss:  -0.74 | Function Loss:  -1.2866\n",
      "Total loss:  -0.1561 | PDE Loss:  -0.7408 | Function Loss:  -1.287\n",
      "Total loss:  -0.1566 | PDE Loss:  -0.7422 | Function Loss:  -1.2872\n",
      "Total loss:  -0.1571 | PDE Loss:  -0.7432 | Function Loss:  -1.2874\n",
      "Total loss:  -0.1574 | PDE Loss:  -0.744 | Function Loss:  -1.2876\n",
      "Total loss:  -0.1576 | PDE Loss:  -0.7443 | Function Loss:  -1.2878\n",
      "Total loss:  -0.1579 | PDE Loss:  -0.7445 | Function Loss:  -1.2881\n",
      "Total loss:  -0.158 | PDE Loss:  -0.7437 | Function Loss:  -1.2886\n",
      "Total loss:  -0.1582 | PDE Loss:  -0.7432 | Function Loss:  -1.289\n",
      "Total loss:  -0.1584 | PDE Loss:  -0.7421 | Function Loss:  -1.2896\n",
      "Total loss:  -0.1585 | PDE Loss:  -0.7414 | Function Loss:  -1.29\n",
      "Total loss:  -0.1586 | PDE Loss:  -0.7411 | Function Loss:  -1.2903\n",
      "Total loss:  -0.1587 | PDE Loss:  -0.7403 | Function Loss:  -1.2907\n",
      "Total loss:  -0.1588 | PDE Loss:  -0.7404 | Function Loss:  -1.2908\n",
      "Total loss:  -0.159 | PDE Loss:  -0.7406 | Function Loss:  -1.2909\n",
      "Total loss:  -0.1591 | PDE Loss:  -0.7412 | Function Loss:  -1.2909\n",
      "Total loss:  -0.1594 | PDE Loss:  -0.7418 | Function Loss:  -1.2911\n",
      "Total loss:  -0.1597 | PDE Loss:  -0.743 | Function Loss:  -1.2911\n",
      "Total loss:  -0.1601 | PDE Loss:  -0.744 | Function Loss:  -1.2913\n",
      "Total loss:  -0.1606 | PDE Loss:  -0.7454 | Function Loss:  -1.2915\n",
      "Total loss:  -0.1612 | PDE Loss:  -0.7466 | Function Loss:  -1.2919\n",
      "Total loss:  -0.1618 | PDE Loss:  -0.7471 | Function Loss:  -1.2925\n",
      "Total loss:  -0.1623 | PDE Loss:  -0.7475 | Function Loss:  -1.293\n",
      "Total loss:  -0.1627 | PDE Loss:  -0.7465 | Function Loss:  -1.2939\n",
      "Total loss:  -0.1629 | PDE Loss:  -0.7461 | Function Loss:  -1.2944\n",
      "Total loss:  -0.1632 | PDE Loss:  -0.7454 | Function Loss:  -1.2949\n",
      "Total loss:  -0.1634 | PDE Loss:  -0.7449 | Function Loss:  -1.2955\n",
      "Total loss:  -0.1638 | PDE Loss:  -0.7452 | Function Loss:  -1.2958\n",
      "Total loss:  -0.1641 | PDE Loss:  -0.7451 | Function Loss:  -1.2963\n",
      "Total loss:  -0.1644 | PDE Loss:  -0.7453 | Function Loss:  -1.2966\n",
      "Total loss:  -0.1647 | PDE Loss:  -0.7453 | Function Loss:  -1.297\n",
      "Total loss:  -0.1648 | PDE Loss:  -0.7453 | Function Loss:  -1.2972\n",
      "Total loss:  -0.165 | PDE Loss:  -0.7452 | Function Loss:  -1.2975\n",
      "Total loss:  -0.1651 | PDE Loss:  -0.7448 | Function Loss:  -1.2978\n",
      "Total loss:  -0.1652 | PDE Loss:  -0.7444 | Function Loss:  -1.2981\n",
      "Total loss:  -0.1654 | PDE Loss:  -0.7436 | Function Loss:  -1.2986\n",
      "Total loss:  -0.1655 | PDE Loss:  -0.7428 | Function Loss:  -1.299\n",
      "Total loss:  -0.1657 | PDE Loss:  -0.7416 | Function Loss:  -1.2997\n",
      "Total loss:  -0.1659 | PDE Loss:  -0.7403 | Function Loss:  -1.3004\n",
      "Total loss:  -0.1661 | PDE Loss:  -0.7387 | Function Loss:  -1.3013\n",
      "Total loss:  -0.1664 | PDE Loss:  -0.7373 | Function Loss:  -1.3023\n",
      "Total loss:  -0.1668 | PDE Loss:  -0.7357 | Function Loss:  -1.3034\n",
      "Total loss:  -0.1672 | PDE Loss:  -0.7351 | Function Loss:  -1.3042\n",
      "Total loss:  -0.1677 | PDE Loss:  -0.7347 | Function Loss:  -1.305\n",
      "Total loss:  -0.1682 | PDE Loss:  -0.7355 | Function Loss:  -1.3053\n",
      "Total loss:  -0.1686 | PDE Loss:  -0.7358 | Function Loss:  -1.3058\n",
      "Total loss:  -0.1689 | PDE Loss:  -0.7367 | Function Loss:  -1.3059\n",
      "Total loss:  -0.1693 | PDE Loss:  -0.738 | Function Loss:  -1.3059\n",
      "Total loss:  -0.1696 | PDE Loss:  -0.7394 | Function Loss:  -1.3058\n",
      "Total loss:  -0.1699 | PDE Loss:  -0.7406 | Function Loss:  -1.3058\n",
      "Total loss:  -0.1702 | PDE Loss:  -0.7416 | Function Loss:  -1.3058\n",
      "Total loss:  -0.1705 | PDE Loss:  -0.7426 | Function Loss:  -1.3058\n",
      "Total loss:  -0.1707 | PDE Loss:  -0.7431 | Function Loss:  -1.306\n",
      "Total loss:  -0.1711 | PDE Loss:  -0.7434 | Function Loss:  -1.3064\n",
      "Total loss:  -0.1715 | PDE Loss:  -0.7434 | Function Loss:  -1.3069\n",
      "Total loss:  -0.1719 | PDE Loss:  -0.7436 | Function Loss:  -1.3074\n",
      "Total loss:  -0.1723 | PDE Loss:  -0.7429 | Function Loss:  -1.3083\n",
      "Total loss:  -0.1728 | PDE Loss:  -0.7421 | Function Loss:  -1.3092\n",
      "Total loss:  -0.1731 | PDE Loss:  -0.7413 | Function Loss:  -1.31\n",
      "Total loss:  -0.1733 | PDE Loss:  -0.7399 | Function Loss:  -1.3108\n",
      "Total loss:  -0.1735 | PDE Loss:  -0.7395 | Function Loss:  -1.3111\n",
      "Total loss:  -0.1736 | PDE Loss:  -0.7389 | Function Loss:  -1.3115\n",
      "Total loss:  -0.1737 | PDE Loss:  -0.7382 | Function Loss:  -1.312\n",
      "Total loss:  -0.1739 | PDE Loss:  -0.7378 | Function Loss:  -1.3124\n",
      "Total loss:  -0.1741 | PDE Loss:  -0.7371 | Function Loss:  -1.3129\n",
      "Total loss:  -0.1743 | PDE Loss:  -0.7369 | Function Loss:  -1.3132\n",
      "Total loss:  -0.1745 | PDE Loss:  -0.7368 | Function Loss:  -1.3135\n",
      "Total loss:  -0.1748 | PDE Loss:  -0.737 | Function Loss:  -1.3139\n",
      "Total loss:  -0.1752 | PDE Loss:  -0.7381 | Function Loss:  -1.314\n",
      "Total loss:  -0.1755 | PDE Loss:  -0.7391 | Function Loss:  -1.3141\n",
      "Total loss:  -0.1758 | PDE Loss:  -0.7399 | Function Loss:  -1.3141\n",
      "Total loss:  -0.176 | PDE Loss:  -0.7415 | Function Loss:  -1.3139\n",
      "Total loss:  -0.1762 | PDE Loss:  -0.7421 | Function Loss:  -1.3138\n",
      "Total loss:  -0.1763 | PDE Loss:  -0.7425 | Function Loss:  -1.3139\n",
      "Total loss:  -0.1765 | PDE Loss:  -0.7433 | Function Loss:  -1.3139\n",
      "Total loss:  -0.1767 | PDE Loss:  -0.743 | Function Loss:  -1.3142\n",
      "Total loss:  -0.1767 | PDE Loss:  -0.743 | Function Loss:  -1.3143\n",
      "Total loss:  -0.1768 | PDE Loss:  -0.7426 | Function Loss:  -1.3146\n",
      "Total loss:  -0.1769 | PDE Loss:  -0.742 | Function Loss:  -1.3149\n",
      "Total loss:  -0.177 | PDE Loss:  -0.7414 | Function Loss:  -1.3152\n",
      "Total loss:  -0.1771 | PDE Loss:  -0.7409 | Function Loss:  -1.3155\n",
      "Total loss:  -0.1771 | PDE Loss:  -0.7402 | Function Loss:  -1.3159\n",
      "Total loss:  -0.1772 | PDE Loss:  -0.7399 | Function Loss:  -1.3161\n",
      "Total loss:  -0.1773 | PDE Loss:  -0.7396 | Function Loss:  -1.3163\n",
      "Total loss:  -0.1774 | PDE Loss:  -0.7397 | Function Loss:  -1.3165\n",
      "Total loss:  -0.1776 | PDE Loss:  -0.7396 | Function Loss:  -1.3168\n",
      "Total loss:  -0.1777 | PDE Loss:  -0.74 | Function Loss:  -1.3168\n",
      "Total loss:  -0.1779 | PDE Loss:  -0.7404 | Function Loss:  -1.3168\n",
      "Total loss:  -0.1781 | PDE Loss:  -0.7409 | Function Loss:  -1.3169\n",
      "Total loss:  -0.1783 | PDE Loss:  -0.7417 | Function Loss:  -1.317\n",
      "Total loss:  -0.1786 | PDE Loss:  -0.7423 | Function Loss:  -1.3171\n",
      "Total loss:  -0.1788 | PDE Loss:  -0.7429 | Function Loss:  -1.3172\n",
      "Total loss:  -0.1791 | PDE Loss:  -0.744 | Function Loss:  -1.3172\n",
      "Total loss:  -0.1795 | PDE Loss:  -0.7437 | Function Loss:  -1.3178\n",
      "Total loss:  -0.1797 | PDE Loss:  -0.7444 | Function Loss:  -1.3178\n",
      "Total loss:  -0.18 | PDE Loss:  -0.7444 | Function Loss:  -1.3183\n",
      "Total loss:  -0.1803 | PDE Loss:  -0.7448 | Function Loss:  -1.3186\n",
      "Total loss:  -0.1805 | PDE Loss:  -0.7443 | Function Loss:  -1.319\n",
      "Total loss:  -0.1807 | PDE Loss:  -0.744 | Function Loss:  -1.3194\n",
      "Total loss:  -0.1811 | PDE Loss:  -0.7433 | Function Loss:  -1.3202\n",
      "Total loss:  -0.1815 | PDE Loss:  -0.7431 | Function Loss:  -1.3208\n",
      "Total loss:  -0.1818 | PDE Loss:  -0.743 | Function Loss:  -1.3213\n",
      "Total loss:  -0.1822 | PDE Loss:  -0.7432 | Function Loss:  -1.3217\n",
      "Total loss:  -0.1825 | PDE Loss:  -0.7435 | Function Loss:  -1.3221\n",
      "Total loss:  -0.1828 | PDE Loss:  -0.7426 | Function Loss:  -1.3228\n",
      "Total loss:  -0.1832 | PDE Loss:  -0.7423 | Function Loss:  -1.3235\n",
      "Total loss:  -0.1836 | PDE Loss:  -0.742 | Function Loss:  -1.3241\n",
      "Total loss:  -0.184 | PDE Loss:  -0.7409 | Function Loss:  -1.3251\n",
      "Total loss:  -0.1845 | PDE Loss:  -0.7403 | Function Loss:  -1.326\n",
      "Total loss:  -0.185 | PDE Loss:  -0.7389 | Function Loss:  -1.3273\n",
      "Total loss:  -0.1856 | PDE Loss:  -0.7381 | Function Loss:  -1.3284\n",
      "Total loss:  -0.1863 | PDE Loss:  -0.7368 | Function Loss:  -1.3299\n",
      "Total loss:  -0.1871 | PDE Loss:  -0.7367 | Function Loss:  -1.3311\n",
      "Total loss:  -0.1878 | PDE Loss:  -0.7367 | Function Loss:  -1.332\n",
      "Total loss:  -0.1883 | PDE Loss:  -0.7376 | Function Loss:  -1.3324\n",
      "Total loss:  -0.1888 | PDE Loss:  -0.738 | Function Loss:  -1.3329\n",
      "Total loss:  -0.1892 | PDE Loss:  -0.7396 | Function Loss:  -1.3328\n",
      "Total loss:  -0.1894 | PDE Loss:  -0.74 | Function Loss:  -1.3329\n",
      "Total loss:  -0.1897 | PDE Loss:  -0.7407 | Function Loss:  -1.3331\n",
      "Total loss:  -0.19 | PDE Loss:  -0.7405 | Function Loss:  -1.3335\n",
      "Total loss:  -0.1902 | PDE Loss:  -0.741 | Function Loss:  -1.3337\n",
      "Total loss:  -0.1903 | PDE Loss:  -0.7407 | Function Loss:  -1.3339\n",
      "Total loss:  -0.1905 | PDE Loss:  -0.7403 | Function Loss:  -1.3343\n",
      "Total loss:  -0.1906 | PDE Loss:  -0.7395 | Function Loss:  -1.3348\n",
      "Total loss:  -0.1908 | PDE Loss:  -0.7388 | Function Loss:  -1.3353\n",
      "Total loss:  -0.1909 | PDE Loss:  -0.7382 | Function Loss:  -1.3358\n",
      "Total loss:  -0.1911 | PDE Loss:  -0.7377 | Function Loss:  -1.3363\n",
      "Total loss:  -0.1914 | PDE Loss:  -0.7376 | Function Loss:  -1.3366\n",
      "Total loss:  -0.1915 | PDE Loss:  -0.7372 | Function Loss:  -1.337\n",
      "Total loss:  -0.1918 | PDE Loss:  -0.7369 | Function Loss:  -1.3375\n",
      "Total loss:  -0.192 | PDE Loss:  -0.7377 | Function Loss:  -1.3374\n",
      "Total loss:  -0.1922 | PDE Loss:  -0.7387 | Function Loss:  -1.3373\n",
      "Total loss:  -0.1924 | PDE Loss:  -0.74 | Function Loss:  -1.3372\n",
      "Total loss:  -0.1927 | PDE Loss:  -0.7414 | Function Loss:  -1.337\n",
      "Total loss:  -0.1929 | PDE Loss:  -0.7429 | Function Loss:  -1.3367\n",
      "Total loss:  -0.1931 | PDE Loss:  -0.744 | Function Loss:  -1.3366\n",
      "Total loss:  -0.1933 | PDE Loss:  -0.745 | Function Loss:  -1.3364\n",
      "Total loss:  -0.1935 | PDE Loss:  -0.7461 | Function Loss:  -1.3362\n",
      "Total loss:  -0.1936 | PDE Loss:  -0.7468 | Function Loss:  -1.3362\n",
      "Total loss:  -0.1938 | PDE Loss:  -0.7477 | Function Loss:  -1.336\n",
      "Total loss:  -0.194 | PDE Loss:  -0.7483 | Function Loss:  -1.3361\n",
      "Total loss:  -0.194 | PDE Loss:  -0.7493 | Function Loss:  -1.3357\n",
      "Total loss:  -0.1943 | PDE Loss:  -0.7499 | Function Loss:  -1.3359\n",
      "Total loss:  -0.1945 | PDE Loss:  -0.7498 | Function Loss:  -1.3362\n",
      "Total loss:  -0.1947 | PDE Loss:  -0.7497 | Function Loss:  -1.3366\n",
      "Total loss:  -0.1949 | PDE Loss:  -0.7494 | Function Loss:  -1.337\n",
      "Total loss:  -0.1952 | PDE Loss:  -0.7492 | Function Loss:  -1.3374\n",
      "Total loss:  -0.1954 | PDE Loss:  -0.7487 | Function Loss:  -1.3379\n",
      "Total loss:  -0.1956 | PDE Loss:  -0.7485 | Function Loss:  -1.3382\n",
      "Total loss:  -0.1957 | PDE Loss:  -0.7481 | Function Loss:  -1.3386\n",
      "Total loss:  -0.1959 | PDE Loss:  -0.7481 | Function Loss:  -1.3388\n",
      "Total loss:  -0.196 | PDE Loss:  -0.7481 | Function Loss:  -1.339\n",
      "Total loss:  -0.1963 | PDE Loss:  -0.7476 | Function Loss:  -1.3396\n",
      "Total loss:  -0.1966 | PDE Loss:  -0.748 | Function Loss:  -1.3398\n",
      "Total loss:  -0.197 | PDE Loss:  -0.7481 | Function Loss:  -1.3404\n",
      "Total loss:  -0.1976 | PDE Loss:  -0.7486 | Function Loss:  -1.341\n",
      "Total loss:  -0.1981 | PDE Loss:  -0.7489 | Function Loss:  -1.3416\n",
      "Total loss:  -0.1986 | PDE Loss:  -0.749 | Function Loss:  -1.3422\n",
      "Total loss:  -0.1989 | PDE Loss:  -0.7496 | Function Loss:  -1.3424\n",
      "Total loss:  -0.1992 | PDE Loss:  -0.7492 | Function Loss:  -1.343\n",
      "Total loss:  -0.1994 | PDE Loss:  -0.7495 | Function Loss:  -1.3432\n",
      "Total loss:  -0.1996 | PDE Loss:  -0.7494 | Function Loss:  -1.3435\n",
      "Total loss:  -0.1998 | PDE Loss:  -0.7496 | Function Loss:  -1.3437\n",
      "Total loss:  -0.2002 | PDE Loss:  -0.7494 | Function Loss:  -1.3443\n",
      "Total loss:  -0.2006 | PDE Loss:  -0.75 | Function Loss:  -1.3447\n",
      "Total loss:  -0.201 | PDE Loss:  -0.7502 | Function Loss:  -1.3451\n",
      "Total loss:  -0.2014 | PDE Loss:  -0.7505 | Function Loss:  -1.3456\n",
      "Total loss:  -0.2019 | PDE Loss:  -0.751 | Function Loss:  -1.346\n",
      "Total loss:  -0.2022 | PDE Loss:  -0.7513 | Function Loss:  -1.3462\n",
      "Total loss:  -0.2023 | PDE Loss:  -0.7514 | Function Loss:  -1.3465\n",
      "Total loss:  -0.2025 | PDE Loss:  -0.7513 | Function Loss:  -1.3467\n",
      "Total loss:  -0.2026 | PDE Loss:  -0.7511 | Function Loss:  -1.347\n",
      "Total loss:  -0.2028 | PDE Loss:  -0.7507 | Function Loss:  -1.3473\n",
      "Total loss:  -0.2029 | PDE Loss:  -0.7501 | Function Loss:  -1.3478\n",
      "Total loss:  -0.2031 | PDE Loss:  -0.7498 | Function Loss:  -1.3482\n",
      "Total loss:  -0.2033 | PDE Loss:  -0.7491 | Function Loss:  -1.3487\n",
      "Total loss:  -0.2035 | PDE Loss:  -0.7485 | Function Loss:  -1.3493\n",
      "Total loss:  -0.2037 | PDE Loss:  -0.7477 | Function Loss:  -1.3499\n",
      "Total loss:  -0.2039 | PDE Loss:  -0.7476 | Function Loss:  -1.3502\n",
      "Total loss:  -0.2043 | PDE Loss:  -0.7475 | Function Loss:  -1.3508\n",
      "Total loss:  -0.2046 | PDE Loss:  -0.7482 | Function Loss:  -1.3509\n",
      "Total loss:  -0.2049 | PDE Loss:  -0.7483 | Function Loss:  -1.3513\n",
      "Total loss:  -0.2052 | PDE Loss:  -0.7483 | Function Loss:  -1.3517\n",
      "Total loss:  -0.2056 | PDE Loss:  -0.749 | Function Loss:  -1.352\n",
      "Total loss:  -0.206 | PDE Loss:  -0.7492 | Function Loss:  -1.3526\n",
      "Total loss:  -0.2066 | PDE Loss:  -0.7505 | Function Loss:  -1.3528\n",
      "Total loss:  -0.2071 | PDE Loss:  -0.7503 | Function Loss:  -1.3536\n",
      "Total loss:  -0.2075 | PDE Loss:  -0.751 | Function Loss:  -1.3538\n",
      "Total loss:  -0.2078 | PDE Loss:  -0.751 | Function Loss:  -1.3543\n",
      "Total loss:  -0.208 | PDE Loss:  -0.751 | Function Loss:  -1.3546\n",
      "Total loss:  -0.2082 | PDE Loss:  -0.7512 | Function Loss:  -1.3548\n",
      "Total loss:  -0.2084 | PDE Loss:  -0.7518 | Function Loss:  -1.3548\n",
      "Total loss:  -0.2086 | PDE Loss:  -0.7522 | Function Loss:  -1.3548\n",
      "Total loss:  -0.2087 | PDE Loss:  -0.7533 | Function Loss:  -1.3546\n",
      "Total loss:  -0.2089 | PDE Loss:  -0.7542 | Function Loss:  -1.3546\n",
      "Total loss:  -0.209 | PDE Loss:  -0.757 | Function Loss:  -1.3535\n",
      "Total loss:  -0.2092 | PDE Loss:  -0.7574 | Function Loss:  -1.3537\n",
      "Total loss:  -0.2094 | PDE Loss:  -0.7576 | Function Loss:  -1.3539\n",
      "Total loss:  -0.2096 | PDE Loss:  -0.7579 | Function Loss:  -1.354\n",
      "Total loss:  -0.2097 | PDE Loss:  -0.7584 | Function Loss:  -1.354\n",
      "Total loss:  -0.2099 | PDE Loss:  -0.7585 | Function Loss:  -1.3542\n",
      "Total loss:  -0.21 | PDE Loss:  -0.759 | Function Loss:  -1.3542\n",
      "Total loss:  -0.2102 | PDE Loss:  -0.7588 | Function Loss:  -1.3545\n",
      "Total loss:  -0.2103 | PDE Loss:  -0.7585 | Function Loss:  -1.3548\n",
      "Total loss:  -0.2105 | PDE Loss:  -0.7579 | Function Loss:  -1.3553\n",
      "Total loss:  -0.2106 | PDE Loss:  -0.757 | Function Loss:  -1.3559\n",
      "Total loss:  -0.2108 | PDE Loss:  -0.7559 | Function Loss:  -1.3565\n",
      "Total loss:  -0.2109 | PDE Loss:  -0.7548 | Function Loss:  -1.3571\n",
      "Total loss:  -0.2111 | PDE Loss:  -0.7534 | Function Loss:  -1.3579\n",
      "Total loss:  -0.2113 | PDE Loss:  -0.752 | Function Loss:  -1.3588\n",
      "Total loss:  -0.2116 | PDE Loss:  -0.7506 | Function Loss:  -1.3597\n",
      "Total loss:  -0.2117 | PDE Loss:  -0.7472 | Function Loss:  -1.3613\n",
      "Total loss:  -0.212 | PDE Loss:  -0.7477 | Function Loss:  -1.3615\n",
      "Total loss:  -0.2122 | PDE Loss:  -0.7487 | Function Loss:  -1.3614\n",
      "Total loss:  -0.2125 | PDE Loss:  -0.7499 | Function Loss:  -1.3613\n",
      "Total loss:  -0.2127 | PDE Loss:  -0.751 | Function Loss:  -1.3611\n",
      "Total loss:  -0.2128 | PDE Loss:  -0.7519 | Function Loss:  -1.361\n",
      "Total loss:  -0.213 | PDE Loss:  -0.7525 | Function Loss:  -1.3609\n",
      "Total loss:  -0.2131 | PDE Loss:  -0.7528 | Function Loss:  -1.361\n",
      "Total loss:  -0.2133 | PDE Loss:  -0.7532 | Function Loss:  -1.3611\n",
      "Total loss:  -0.2136 | PDE Loss:  -0.7536 | Function Loss:  -1.3613\n",
      "Total loss:  -0.2138 | PDE Loss:  -0.7532 | Function Loss:  -1.3618\n",
      "Total loss:  -0.214 | PDE Loss:  -0.7537 | Function Loss:  -1.3619\n",
      "Total loss:  -0.2143 | PDE Loss:  -0.7531 | Function Loss:  -1.3625\n",
      "Total loss:  -0.2145 | PDE Loss:  -0.7532 | Function Loss:  -1.3627\n",
      "Total loss:  -0.2147 | PDE Loss:  -0.7535 | Function Loss:  -1.3629\n",
      "Total loss:  -0.2149 | PDE Loss:  -0.7536 | Function Loss:  -1.3632\n",
      "Total loss:  -0.2151 | PDE Loss:  -0.7551 | Function Loss:  -1.3629\n",
      "Total loss:  -0.2152 | PDE Loss:  -0.7557 | Function Loss:  -1.3628\n",
      "Total loss:  -0.2154 | PDE Loss:  -0.7567 | Function Loss:  -1.3626\n",
      "Total loss:  -0.2156 | PDE Loss:  -0.758 | Function Loss:  -1.3624\n",
      "Total loss:  -0.2158 | PDE Loss:  -0.7597 | Function Loss:  -1.362\n",
      "Total loss:  -0.216 | PDE Loss:  -0.7607 | Function Loss:  -1.3619\n",
      "Total loss:  -0.2161 | PDE Loss:  -0.7612 | Function Loss:  -1.3619\n",
      "Total loss:  -0.2164 | PDE Loss:  -0.7615 | Function Loss:  -1.3621\n",
      "Total loss:  -0.2166 | PDE Loss:  -0.7611 | Function Loss:  -1.3626\n",
      "Total loss:  -0.2169 | PDE Loss:  -0.7607 | Function Loss:  -1.3631\n",
      "Total loss:  -0.2171 | PDE Loss:  -0.7597 | Function Loss:  -1.3638\n",
      "Total loss:  -0.2172 | PDE Loss:  -0.7592 | Function Loss:  -1.3642\n",
      "Total loss:  -0.2174 | PDE Loss:  -0.7588 | Function Loss:  -1.3646\n",
      "Total loss:  -0.2176 | PDE Loss:  -0.7585 | Function Loss:  -1.365\n",
      "Total loss:  -0.2178 | PDE Loss:  -0.7583 | Function Loss:  -1.3654\n",
      "Total loss:  -0.2181 | PDE Loss:  -0.7586 | Function Loss:  -1.3657\n",
      "Total loss:  -0.2184 | PDE Loss:  -0.7589 | Function Loss:  -1.366\n",
      "Total loss:  -0.2188 | PDE Loss:  -0.7598 | Function Loss:  -1.3661\n",
      "Total loss:  -0.2191 | PDE Loss:  -0.7607 | Function Loss:  -1.3662\n",
      "Total loss:  -0.2193 | PDE Loss:  -0.7614 | Function Loss:  -1.3662\n",
      "Total loss:  -0.2194 | PDE Loss:  -0.7621 | Function Loss:  -1.3661\n",
      "Total loss:  -0.2196 | PDE Loss:  -0.7626 | Function Loss:  -1.3662\n",
      "Total loss:  -0.2198 | PDE Loss:  -0.7629 | Function Loss:  -1.3663\n",
      "Total loss:  -0.22 | PDE Loss:  -0.7639 | Function Loss:  -1.3662\n",
      "Total loss:  -0.2202 | PDE Loss:  -0.7634 | Function Loss:  -1.3667\n",
      "Total loss:  -0.2205 | PDE Loss:  -0.763 | Function Loss:  -1.3672\n",
      "Total loss:  -0.2208 | PDE Loss:  -0.762 | Function Loss:  -1.3681\n",
      "Total loss:  -0.2211 | PDE Loss:  -0.7617 | Function Loss:  -1.3685\n",
      "Total loss:  -0.2212 | PDE Loss:  -0.7611 | Function Loss:  -1.3691\n",
      "Total loss:  -0.2214 | PDE Loss:  -0.761 | Function Loss:  -1.3694\n",
      "Total loss:  -0.2216 | PDE Loss:  -0.7606 | Function Loss:  -1.3698\n",
      "Total loss:  -0.2218 | PDE Loss:  -0.7603 | Function Loss:  -1.3701\n",
      "Total loss:  -0.222 | PDE Loss:  -0.7604 | Function Loss:  -1.3704\n",
      "Total loss:  -0.2223 | PDE Loss:  -0.7603 | Function Loss:  -1.3709\n",
      "Total loss:  -0.2226 | PDE Loss:  -0.7603 | Function Loss:  -1.3713\n",
      "Total loss:  -0.2229 | PDE Loss:  -0.7605 | Function Loss:  -1.3716\n",
      "Total loss:  -0.2231 | PDE Loss:  -0.7606 | Function Loss:  -1.3719\n",
      "Total loss:  -0.2233 | PDE Loss:  -0.7609 | Function Loss:  -1.372\n",
      "Total loss:  -0.2235 | PDE Loss:  -0.761 | Function Loss:  -1.3722\n",
      "Total loss:  -0.2237 | PDE Loss:  -0.7614 | Function Loss:  -1.3723\n",
      "Total loss:  -0.2238 | PDE Loss:  -0.7616 | Function Loss:  -1.3725\n",
      "Total loss:  -0.224 | PDE Loss:  -0.7615 | Function Loss:  -1.3728\n",
      "Total loss:  -0.2241 | PDE Loss:  -0.7617 | Function Loss:  -1.3729\n",
      "Total loss:  -0.2242 | PDE Loss:  -0.7618 | Function Loss:  -1.373\n",
      "Total loss:  -0.2244 | PDE Loss:  -0.7623 | Function Loss:  -1.373\n",
      "Total loss:  -0.2245 | PDE Loss:  -0.7625 | Function Loss:  -1.3731\n",
      "Total loss:  -0.2247 | PDE Loss:  -0.7629 | Function Loss:  -1.3731\n",
      "Total loss:  -0.2249 | PDE Loss:  -0.7639 | Function Loss:  -1.373\n",
      "Total loss:  -0.2251 | PDE Loss:  -0.7642 | Function Loss:  -1.3732\n",
      "Total loss:  -0.2253 | PDE Loss:  -0.765 | Function Loss:  -1.3731\n",
      "Total loss:  -0.2255 | PDE Loss:  -0.7655 | Function Loss:  -1.3733\n",
      "Total loss:  -0.2257 | PDE Loss:  -0.7666 | Function Loss:  -1.3732\n",
      "Total loss:  -0.2259 | PDE Loss:  -0.7668 | Function Loss:  -1.3733\n",
      "Total loss:  -0.2261 | PDE Loss:  -0.7669 | Function Loss:  -1.3735\n",
      "Total loss:  -0.2262 | PDE Loss:  -0.7669 | Function Loss:  -1.3737\n",
      "Total loss:  -0.2264 | PDE Loss:  -0.7669 | Function Loss:  -1.374\n",
      "Total loss:  -0.2266 | PDE Loss:  -0.7671 | Function Loss:  -1.3742\n",
      "Total loss:  -0.2269 | PDE Loss:  -0.7676 | Function Loss:  -1.3744\n",
      "Total loss:  -0.2272 | PDE Loss:  -0.7682 | Function Loss:  -1.3746\n",
      "Total loss:  -0.2276 | PDE Loss:  -0.7699 | Function Loss:  -1.3745\n",
      "Total loss:  -0.228 | PDE Loss:  -0.7711 | Function Loss:  -1.3744\n",
      "Total loss:  -0.2283 | PDE Loss:  -0.7726 | Function Loss:  -1.3743\n",
      "Total loss:  -0.2287 | PDE Loss:  -0.7744 | Function Loss:  -1.3742\n",
      "Total loss:  -0.2291 | PDE Loss:  -0.7763 | Function Loss:  -1.3739\n",
      "Total loss:  -0.2293 | PDE Loss:  -0.7776 | Function Loss:  -1.3738\n",
      "Total loss:  -0.2296 | PDE Loss:  -0.7785 | Function Loss:  -1.3737\n",
      "Total loss:  -0.2297 | PDE Loss:  -0.7789 | Function Loss:  -1.3738\n",
      "Total loss:  -0.2299 | PDE Loss:  -0.7793 | Function Loss:  -1.3739\n",
      "Total loss:  -0.2302 | PDE Loss:  -0.7799 | Function Loss:  -1.3741\n",
      "Total loss:  -0.2304 | PDE Loss:  -0.7796 | Function Loss:  -1.3746\n",
      "Total loss:  -0.2305 | PDE Loss:  -0.7794 | Function Loss:  -1.3748\n",
      "Total loss:  -0.2306 | PDE Loss:  -0.7791 | Function Loss:  -1.375\n",
      "Total loss:  -0.2307 | PDE Loss:  -0.7788 | Function Loss:  -1.3753\n",
      "Total loss:  -0.2308 | PDE Loss:  -0.7783 | Function Loss:  -1.3756\n",
      "Total loss:  -0.2309 | PDE Loss:  -0.7781 | Function Loss:  -1.3758\n",
      "Total loss:  -0.231 | PDE Loss:  -0.7778 | Function Loss:  -1.3761\n",
      "Total loss:  -0.2313 | PDE Loss:  -0.7778 | Function Loss:  -1.3764\n",
      "Total loss:  -0.2315 | PDE Loss:  -0.7763 | Function Loss:  -1.3773\n",
      "Total loss:  -0.2318 | PDE Loss:  -0.7768 | Function Loss:  -1.3775\n",
      "Total loss:  -0.2321 | PDE Loss:  -0.7777 | Function Loss:  -1.3776\n",
      "Total loss:  -0.2324 | PDE Loss:  -0.7785 | Function Loss:  -1.3777\n",
      "Total loss:  -0.2327 | PDE Loss:  -0.7794 | Function Loss:  -1.3778\n",
      "Total loss:  -0.233 | PDE Loss:  -0.7801 | Function Loss:  -1.378\n",
      "Total loss:  -0.2334 | PDE Loss:  -0.7807 | Function Loss:  -1.3782\n",
      "Total loss:  -0.2336 | PDE Loss:  -0.7811 | Function Loss:  -1.3784\n",
      "Total loss:  -0.2339 | PDE Loss:  -0.7816 | Function Loss:  -1.3786\n",
      "Total loss:  -0.2341 | PDE Loss:  -0.7817 | Function Loss:  -1.3789\n",
      "Total loss:  -0.2343 | PDE Loss:  -0.782 | Function Loss:  -1.379\n",
      "Total loss:  -0.2345 | PDE Loss:  -0.7823 | Function Loss:  -1.3791\n",
      "Total loss:  -0.2347 | PDE Loss:  -0.7828 | Function Loss:  -1.3792\n",
      "Total loss:  -0.2348 | PDE Loss:  -0.7833 | Function Loss:  -1.3792\n",
      "Total loss:  -0.235 | PDE Loss:  -0.784 | Function Loss:  -1.3791\n",
      "Total loss:  -0.2351 | PDE Loss:  -0.7848 | Function Loss:  -1.379\n",
      "Total loss:  -0.2353 | PDE Loss:  -0.786 | Function Loss:  -1.3788\n",
      "Total loss:  -0.2355 | PDE Loss:  -0.7872 | Function Loss:  -1.3786\n",
      "Total loss:  -0.2356 | PDE Loss:  -0.7882 | Function Loss:  -1.3784\n",
      "Total loss:  -0.2357 | PDE Loss:  -0.7889 | Function Loss:  -1.3783\n",
      "Total loss:  -0.2358 | PDE Loss:  -0.7894 | Function Loss:  -1.3782\n",
      "Total loss:  -0.2359 | PDE Loss:  -0.7896 | Function Loss:  -1.3783\n",
      "Total loss:  -0.2361 | PDE Loss:  -0.7898 | Function Loss:  -1.3785\n",
      "Total loss:  -0.2364 | PDE Loss:  -0.7898 | Function Loss:  -1.3789\n",
      "Total loss:  -0.2367 | PDE Loss:  -0.7895 | Function Loss:  -1.3794\n",
      "Total loss:  -0.237 | PDE Loss:  -0.7896 | Function Loss:  -1.3797\n",
      "Total loss:  -0.2372 | PDE Loss:  -0.789 | Function Loss:  -1.3803\n",
      "Total loss:  -0.2374 | PDE Loss:  -0.789 | Function Loss:  -1.3806\n",
      "Total loss:  -0.2377 | PDE Loss:  -0.7894 | Function Loss:  -1.3809\n",
      "Total loss:  -0.238 | PDE Loss:  -0.7897 | Function Loss:  -1.3811\n",
      "Total loss:  -0.2381 | PDE Loss:  -0.7903 | Function Loss:  -1.3811\n",
      "Total loss:  -0.2383 | PDE Loss:  -0.791 | Function Loss:  -1.381\n",
      "Total loss:  -0.2386 | PDE Loss:  -0.792 | Function Loss:  -1.381\n",
      "Total loss:  -0.2388 | PDE Loss:  -0.7929 | Function Loss:  -1.381\n",
      "Total loss:  -0.2391 | PDE Loss:  -0.7937 | Function Loss:  -1.381\n",
      "Total loss:  -0.2393 | PDE Loss:  -0.7943 | Function Loss:  -1.3812\n",
      "Total loss:  -0.2396 | PDE Loss:  -0.7948 | Function Loss:  -1.3814\n",
      "Total loss:  -0.2398 | PDE Loss:  -0.7952 | Function Loss:  -1.3815\n",
      "Total loss:  -0.24 | PDE Loss:  -0.7954 | Function Loss:  -1.3817\n",
      "Total loss:  -0.2401 | PDE Loss:  -0.7956 | Function Loss:  -1.3818\n",
      "Total loss:  -0.2403 | PDE Loss:  -0.796 | Function Loss:  -1.3818\n",
      "Total loss:  -0.2405 | PDE Loss:  -0.7962 | Function Loss:  -1.382\n",
      "Total loss:  -0.2407 | PDE Loss:  -0.797 | Function Loss:  -1.382\n",
      "Total loss:  -0.2409 | PDE Loss:  -0.7976 | Function Loss:  -1.382\n",
      "Total loss:  -0.2411 | PDE Loss:  -0.7982 | Function Loss:  -1.3821\n",
      "Total loss:  -0.2414 | PDE Loss:  -0.7988 | Function Loss:  -1.3823\n",
      "Total loss:  -0.2416 | PDE Loss:  -0.7992 | Function Loss:  -1.3824\n",
      "Total loss:  -0.2418 | PDE Loss:  -0.7996 | Function Loss:  -1.3826\n",
      "Total loss:  -0.242 | PDE Loss:  -0.7997 | Function Loss:  -1.3828\n",
      "Total loss:  -0.2422 | PDE Loss:  -0.7997 | Function Loss:  -1.3831\n",
      "Total loss:  -0.2424 | PDE Loss:  -0.7994 | Function Loss:  -1.3835\n",
      "Total loss:  -0.2426 | PDE Loss:  -0.7992 | Function Loss:  -1.3838\n",
      "Total loss:  -0.2427 | PDE Loss:  -0.799 | Function Loss:  -1.3841\n",
      "Total loss:  -0.243 | PDE Loss:  -0.7993 | Function Loss:  -1.3844\n",
      "Total loss:  -0.2435 | PDE Loss:  -0.7991 | Function Loss:  -1.3851\n",
      "Total loss:  -0.2439 | PDE Loss:  -0.7996 | Function Loss:  -1.3854\n",
      "Total loss:  -0.2442 | PDE Loss:  -0.8004 | Function Loss:  -1.3856\n",
      "Total loss:  -0.2447 | PDE Loss:  -0.8016 | Function Loss:  -1.3858\n",
      "Total loss:  -0.2451 | PDE Loss:  -0.8029 | Function Loss:  -1.3858\n",
      "Total loss:  -0.2453 | PDE Loss:  -0.8038 | Function Loss:  -1.3858\n",
      "Total loss:  -0.2454 | PDE Loss:  -0.8042 | Function Loss:  -1.3859\n",
      "Total loss:  -0.2456 | PDE Loss:  -0.8045 | Function Loss:  -1.386\n",
      "Total loss:  -0.2457 | PDE Loss:  -0.8045 | Function Loss:  -1.3861\n",
      "Total loss:  -0.2458 | PDE Loss:  -0.8045 | Function Loss:  -1.3862\n",
      "Total loss:  -0.2459 | PDE Loss:  -0.8045 | Function Loss:  -1.3863\n",
      "Total loss:  -0.2461 | PDE Loss:  -0.8049 | Function Loss:  -1.3864\n",
      "Total loss:  -0.2462 | PDE Loss:  -0.805 | Function Loss:  -1.3866\n",
      "Total loss:  -0.2464 | PDE Loss:  -0.8057 | Function Loss:  -1.3866\n",
      "Total loss:  -0.2465 | PDE Loss:  -0.806 | Function Loss:  -1.3867\n",
      "Total loss:  -0.2467 | PDE Loss:  -0.8063 | Function Loss:  -1.3867\n",
      "Total loss:  -0.2468 | PDE Loss:  -0.8065 | Function Loss:  -1.3868\n",
      "Total loss:  -0.247 | PDE Loss:  -0.8065 | Function Loss:  -1.3871\n",
      "Total loss:  -0.2471 | PDE Loss:  -0.8064 | Function Loss:  -1.3873\n",
      "Total loss:  -0.2474 | PDE Loss:  -0.806 | Function Loss:  -1.3878\n",
      "Total loss:  -0.2476 | PDE Loss:  -0.8055 | Function Loss:  -1.3883\n",
      "Total loss:  -0.2479 | PDE Loss:  -0.8048 | Function Loss:  -1.389\n",
      "Total loss:  -0.2482 | PDE Loss:  -0.8038 | Function Loss:  -1.3897\n",
      "Total loss:  -0.2484 | PDE Loss:  -0.8031 | Function Loss:  -1.3904\n",
      "Total loss:  -0.2486 | PDE Loss:  -0.8026 | Function Loss:  -1.3908\n",
      "Total loss:  -0.2488 | PDE Loss:  -0.8025 | Function Loss:  -1.3911\n",
      "Total loss:  -0.2489 | PDE Loss:  -0.8027 | Function Loss:  -1.3913\n",
      "Total loss:  -0.2491 | PDE Loss:  -0.803 | Function Loss:  -1.3913\n",
      "Total loss:  -0.2492 | PDE Loss:  -0.8035 | Function Loss:  -1.3913\n",
      "Total loss:  -0.2493 | PDE Loss:  -0.804 | Function Loss:  -1.3913\n",
      "Total loss:  -0.2495 | PDE Loss:  -0.8041 | Function Loss:  -1.3915\n",
      "Total loss:  -0.2497 | PDE Loss:  -0.8055 | Function Loss:  -1.3913\n",
      "Total loss:  -0.2499 | PDE Loss:  -0.8053 | Function Loss:  -1.3916\n",
      "Total loss:  -0.2501 | PDE Loss:  -0.8049 | Function Loss:  -1.392\n",
      "Total loss:  -0.2503 | PDE Loss:  -0.8042 | Function Loss:  -1.3926\n",
      "Total loss:  -0.2505 | PDE Loss:  -0.8035 | Function Loss:  -1.3931\n",
      "Total loss:  -0.2507 | PDE Loss:  -0.8026 | Function Loss:  -1.3937\n",
      "Total loss:  -0.2509 | PDE Loss:  -0.8015 | Function Loss:  -1.3944\n",
      "Total loss:  -0.2511 | PDE Loss:  -0.8005 | Function Loss:  -1.3951\n",
      "Total loss:  -0.2514 | PDE Loss:  -0.7995 | Function Loss:  -1.3959\n",
      "Total loss:  -0.2516 | PDE Loss:  -0.7984 | Function Loss:  -1.3967\n",
      "Total loss:  -0.2519 | PDE Loss:  -0.7975 | Function Loss:  -1.3974\n",
      "Total loss:  -0.2521 | PDE Loss:  -0.7975 | Function Loss:  -1.3977\n",
      "Total loss:  -0.2523 | PDE Loss:  -0.7978 | Function Loss:  -1.3979\n",
      "Total loss:  -0.2525 | PDE Loss:  -0.7986 | Function Loss:  -1.3979\n",
      "Total loss:  -0.2527 | PDE Loss:  -0.799 | Function Loss:  -1.3979\n",
      "Total loss:  -0.2528 | PDE Loss:  -0.7996 | Function Loss:  -1.3979\n",
      "Total loss:  -0.253 | PDE Loss:  -0.8 | Function Loss:  -1.398\n",
      "Total loss:  -0.2533 | PDE Loss:  -0.8002 | Function Loss:  -1.3982\n",
      "Total loss:  -0.2535 | PDE Loss:  -0.8002 | Function Loss:  -1.3986\n",
      "Total loss:  -0.2537 | PDE Loss:  -0.7996 | Function Loss:  -1.399\n",
      "Total loss:  -0.2538 | PDE Loss:  -0.7991 | Function Loss:  -1.3994\n",
      "Total loss:  -0.2539 | PDE Loss:  -0.7984 | Function Loss:  -1.3999\n",
      "Total loss:  -0.254 | PDE Loss:  -0.7977 | Function Loss:  -1.4003\n",
      "Total loss:  -0.2543 | PDE Loss:  -0.7969 | Function Loss:  -1.401\n",
      "Total loss:  -0.2541 | PDE Loss:  -0.7947 | Function Loss:  -1.4016\n",
      "Total loss:  -0.2544 | PDE Loss:  -0.7963 | Function Loss:  -1.4014\n",
      "Total loss:  -0.2547 | PDE Loss:  -0.7956 | Function Loss:  -1.4021\n",
      "Total loss:  -0.255 | PDE Loss:  -0.7953 | Function Loss:  -1.4026\n",
      "Total loss:  -0.2552 | PDE Loss:  -0.7952 | Function Loss:  -1.403\n",
      "Total loss:  -0.2555 | PDE Loss:  -0.7957 | Function Loss:  -1.4032\n",
      "Total loss:  -0.2558 | PDE Loss:  -0.7959 | Function Loss:  -1.4036\n",
      "Total loss:  -0.2561 | PDE Loss:  -0.7965 | Function Loss:  -1.4037\n",
      "Total loss:  -0.2563 | PDE Loss:  -0.7966 | Function Loss:  -1.4039\n",
      "Total loss:  -0.2565 | PDE Loss:  -0.7968 | Function Loss:  -1.4041\n",
      "Total loss:  -0.2568 | PDE Loss:  -0.7973 | Function Loss:  -1.4043\n",
      "Total loss:  -0.257 | PDE Loss:  -0.7953 | Function Loss:  -1.4055\n",
      "Total loss:  -0.2573 | PDE Loss:  -0.7961 | Function Loss:  -1.4055\n",
      "Total loss:  -0.2575 | PDE Loss:  -0.7967 | Function Loss:  -1.4056\n",
      "Total loss:  -0.2577 | PDE Loss:  -0.7967 | Function Loss:  -1.4059\n",
      "Total loss:  -0.258 | PDE Loss:  -0.797 | Function Loss:  -1.4061\n",
      "Total loss:  -0.2581 | PDE Loss:  -0.797 | Function Loss:  -1.4064\n",
      "Total loss:  -0.2583 | PDE Loss:  -0.7972 | Function Loss:  -1.4065\n",
      "Total loss:  -0.2584 | PDE Loss:  -0.7974 | Function Loss:  -1.4066\n",
      "Total loss:  -0.2586 | PDE Loss:  -0.7976 | Function Loss:  -1.4067\n",
      "Total loss:  -0.2587 | PDE Loss:  -0.798 | Function Loss:  -1.4067\n",
      "Total loss:  -0.2588 | PDE Loss:  -0.7986 | Function Loss:  -1.4067\n",
      "Total loss:  -0.259 | PDE Loss:  -0.7992 | Function Loss:  -1.4067\n",
      "Total loss:  -0.2592 | PDE Loss:  -0.7999 | Function Loss:  -1.4067\n",
      "Total loss:  -0.2594 | PDE Loss:  -0.8005 | Function Loss:  -1.4068\n",
      "Total loss:  -0.2597 | PDE Loss:  -0.8012 | Function Loss:  -1.4069\n",
      "Total loss:  -0.2601 | PDE Loss:  -0.8017 | Function Loss:  -1.4072\n",
      "Total loss:  -0.2604 | PDE Loss:  -0.8029 | Function Loss:  -1.4072\n",
      "Total loss:  -0.2608 | PDE Loss:  -0.8039 | Function Loss:  -1.4073\n",
      "Total loss:  -0.2611 | PDE Loss:  -0.8044 | Function Loss:  -1.4076\n",
      "Total loss:  -0.2614 | PDE Loss:  -0.8053 | Function Loss:  -1.4076\n",
      "Total loss:  -0.2617 | PDE Loss:  -0.806 | Function Loss:  -1.4077\n",
      "Total loss:  -0.2619 | PDE Loss:  -0.807 | Function Loss:  -1.4076\n",
      "Total loss:  -0.262 | PDE Loss:  -0.808 | Function Loss:  -1.4074\n",
      "Total loss:  -0.2622 | PDE Loss:  -0.8088 | Function Loss:  -1.4073\n",
      "Total loss:  -0.2623 | PDE Loss:  -0.8097 | Function Loss:  -1.4071\n",
      "Total loss:  -0.2625 | PDE Loss:  -0.8109 | Function Loss:  -1.4068\n",
      "Total loss:  -0.2626 | PDE Loss:  -0.8117 | Function Loss:  -1.4067\n",
      "Total loss:  -0.2626 | PDE Loss:  -0.8125 | Function Loss:  -1.4065\n",
      "Total loss:  -0.2627 | PDE Loss:  -0.813 | Function Loss:  -1.4064\n",
      "Total loss:  -0.2628 | PDE Loss:  -0.8136 | Function Loss:  -1.4063\n",
      "Total loss:  -0.2631 | PDE Loss:  -0.8143 | Function Loss:  -1.4063\n",
      "Total loss:  -0.2633 | PDE Loss:  -0.8152 | Function Loss:  -1.4064\n",
      "Total loss:  -0.2636 | PDE Loss:  -0.8158 | Function Loss:  -1.4065\n",
      "Total loss:  -0.2639 | PDE Loss:  -0.8165 | Function Loss:  -1.4067\n",
      "Total loss:  -0.2641 | PDE Loss:  -0.817 | Function Loss:  -1.4068\n",
      "Total loss:  -0.2644 | PDE Loss:  -0.8176 | Function Loss:  -1.4069\n",
      "Total loss:  -0.2647 | PDE Loss:  -0.8179 | Function Loss:  -1.4073\n",
      "Total loss:  -0.2649 | PDE Loss:  -0.8178 | Function Loss:  -1.4076\n",
      "Total loss:  -0.2652 | PDE Loss:  -0.8175 | Function Loss:  -1.408\n",
      "Total loss:  -0.2654 | PDE Loss:  -0.8171 | Function Loss:  -1.4085\n",
      "Total loss:  -0.2656 | PDE Loss:  -0.8167 | Function Loss:  -1.4089\n",
      "Total loss:  -0.2657 | PDE Loss:  -0.8161 | Function Loss:  -1.4094\n",
      "Total loss:  -0.2659 | PDE Loss:  -0.816 | Function Loss:  -1.4097\n",
      "Total loss:  -0.2661 | PDE Loss:  -0.8154 | Function Loss:  -1.4102\n",
      "Total loss:  -0.2663 | PDE Loss:  -0.8152 | Function Loss:  -1.4105\n",
      "Total loss:  -0.2665 | PDE Loss:  -0.815 | Function Loss:  -1.4108\n",
      "Total loss:  -0.2667 | PDE Loss:  -0.8147 | Function Loss:  -1.4113\n",
      "Total loss:  -0.267 | PDE Loss:  -0.8142 | Function Loss:  -1.4119\n",
      "Total loss:  -0.2673 | PDE Loss:  -0.8134 | Function Loss:  -1.4126\n",
      "Total loss:  -0.2676 | PDE Loss:  -0.8123 | Function Loss:  -1.4135\n",
      "Total loss:  -0.2678 | PDE Loss:  -0.8116 | Function Loss:  -1.4141\n",
      "Total loss:  -0.2681 | PDE Loss:  -0.8106 | Function Loss:  -1.4148\n",
      "Total loss:  -0.2683 | PDE Loss:  -0.8103 | Function Loss:  -1.4152\n",
      "Total loss:  -0.2686 | PDE Loss:  -0.81 | Function Loss:  -1.4157\n",
      "Total loss:  -0.2688 | PDE Loss:  -0.8098 | Function Loss:  -1.4162\n",
      "Total loss:  -0.2691 | PDE Loss:  -0.8101 | Function Loss:  -1.4165\n",
      "Total loss:  -0.2694 | PDE Loss:  -0.8104 | Function Loss:  -1.4168\n",
      "Total loss:  -0.2697 | PDE Loss:  -0.8106 | Function Loss:  -1.4171\n",
      "Total loss:  -0.27 | PDE Loss:  -0.8107 | Function Loss:  -1.4175\n",
      "Total loss:  -0.2703 | PDE Loss:  -0.8105 | Function Loss:  -1.4179\n",
      "Total loss:  -0.2705 | PDE Loss:  -0.8102 | Function Loss:  -1.4184\n",
      "Total loss:  -0.2708 | PDE Loss:  -0.8098 | Function Loss:  -1.419\n",
      "Total loss:  -0.2712 | PDE Loss:  -0.8094 | Function Loss:  -1.4196\n",
      "Total loss:  -0.2715 | PDE Loss:  -0.8087 | Function Loss:  -1.4204\n",
      "Total loss:  -0.2718 | PDE Loss:  -0.8083 | Function Loss:  -1.421\n",
      "Total loss:  -0.2721 | PDE Loss:  -0.8077 | Function Loss:  -1.4216\n",
      "Total loss:  -0.2724 | PDE Loss:  -0.8077 | Function Loss:  -1.4221\n",
      "Total loss:  -0.2726 | PDE Loss:  -0.8076 | Function Loss:  -1.4225\n",
      "Total loss:  -0.2729 | PDE Loss:  -0.8076 | Function Loss:  -1.4228\n",
      "Total loss:  -0.2731 | PDE Loss:  -0.8078 | Function Loss:  -1.4231\n",
      "Total loss:  -0.2734 | PDE Loss:  -0.808 | Function Loss:  -1.4233\n",
      "Total loss:  -0.2736 | PDE Loss:  -0.8082 | Function Loss:  -1.4236\n",
      "Total loss:  -0.2736 | PDE Loss:  -0.8071 | Function Loss:  -1.424\n",
      "Total loss:  -0.2739 | PDE Loss:  -0.808 | Function Loss:  -1.424\n",
      "Total loss:  -0.274 | PDE Loss:  -0.8082 | Function Loss:  -1.4241\n",
      "Total loss:  -0.2742 | PDE Loss:  -0.8085 | Function Loss:  -1.4243\n",
      "Total loss:  -0.2744 | PDE Loss:  -0.8089 | Function Loss:  -1.4244\n",
      "Total loss:  -0.2746 | PDE Loss:  -0.8091 | Function Loss:  -1.4246\n",
      "Total loss:  -0.2748 | PDE Loss:  -0.8092 | Function Loss:  -1.4249\n",
      "Total loss:  -0.275 | PDE Loss:  -0.8093 | Function Loss:  -1.4252\n",
      "Total loss:  -0.2753 | PDE Loss:  -0.8083 | Function Loss:  -1.426\n",
      "Total loss:  -0.2756 | PDE Loss:  -0.8082 | Function Loss:  -1.4263\n",
      "Total loss:  -0.2758 | PDE Loss:  -0.8079 | Function Loss:  -1.4268\n",
      "Total loss:  -0.276 | PDE Loss:  -0.8076 | Function Loss:  -1.4272\n",
      "Total loss:  -0.2762 | PDE Loss:  -0.8071 | Function Loss:  -1.4277\n",
      "Total loss:  -0.2763 | PDE Loss:  -0.8068 | Function Loss:  -1.4281\n",
      "Total loss:  -0.2765 | PDE Loss:  -0.8063 | Function Loss:  -1.4285\n",
      "Total loss:  -0.2768 | PDE Loss:  -0.8061 | Function Loss:  -1.429\n",
      "Total loss:  -0.2771 | PDE Loss:  -0.8056 | Function Loss:  -1.4296\n",
      "Total loss:  -0.2773 | PDE Loss:  -0.8056 | Function Loss:  -1.4299\n",
      "Total loss:  -0.2775 | PDE Loss:  -0.8058 | Function Loss:  -1.4301\n",
      "Total loss:  -0.2777 | PDE Loss:  -0.8063 | Function Loss:  -1.4302\n",
      "Total loss:  -0.2779 | PDE Loss:  -0.8068 | Function Loss:  -1.4303\n",
      "Total loss:  -0.2782 | PDE Loss:  -0.8077 | Function Loss:  -1.4303\n",
      "Total loss:  -0.2785 | PDE Loss:  -0.8081 | Function Loss:  -1.4306\n",
      "Total loss:  -0.2788 | PDE Loss:  -0.8089 | Function Loss:  -1.4306\n",
      "Total loss:  -0.2791 | PDE Loss:  -0.809 | Function Loss:  -1.431\n",
      "Total loss:  -0.2793 | PDE Loss:  -0.8091 | Function Loss:  -1.4313\n",
      "Total loss:  -0.2796 | PDE Loss:  -0.809 | Function Loss:  -1.4317\n",
      "Total loss:  -0.2799 | PDE Loss:  -0.8086 | Function Loss:  -1.4323\n",
      "Total loss:  -0.2801 | PDE Loss:  -0.8085 | Function Loss:  -1.4327\n",
      "Total loss:  -0.2805 | PDE Loss:  -0.809 | Function Loss:  -1.4331\n",
      "Total loss:  -0.2809 | PDE Loss:  -0.809 | Function Loss:  -1.4336\n",
      "Total loss:  -0.2812 | PDE Loss:  -0.8096 | Function Loss:  -1.4337\n",
      "Total loss:  -0.2814 | PDE Loss:  -0.8101 | Function Loss:  -1.4338\n",
      "Total loss:  -0.2816 | PDE Loss:  -0.8104 | Function Loss:  -1.4339\n",
      "Total loss:  -0.2818 | PDE Loss:  -0.8114 | Function Loss:  -1.4339\n",
      "Total loss:  -0.2821 | PDE Loss:  -0.8121 | Function Loss:  -1.434\n",
      "Total loss:  -0.2824 | PDE Loss:  -0.8131 | Function Loss:  -1.434\n",
      "Total loss:  -0.2828 | PDE Loss:  -0.8137 | Function Loss:  -1.4343\n",
      "Total loss:  -0.2831 | PDE Loss:  -0.8147 | Function Loss:  -1.4344\n",
      "Total loss:  -0.2834 | PDE Loss:  -0.8149 | Function Loss:  -1.4347\n",
      "Total loss:  -0.2837 | PDE Loss:  -0.8155 | Function Loss:  -1.4348\n",
      "Total loss:  -0.2839 | PDE Loss:  -0.8158 | Function Loss:  -1.4351\n",
      "Total loss:  -0.2837 | PDE Loss:  -0.8153 | Function Loss:  -1.435\n",
      "Total loss:  -0.2841 | PDE Loss:  -0.8159 | Function Loss:  -1.4352\n",
      "Total loss:  -0.2843 | PDE Loss:  -0.8165 | Function Loss:  -1.4353\n",
      "Total loss:  -0.2845 | PDE Loss:  -0.8169 | Function Loss:  -1.4354\n",
      "Total loss:  -0.2846 | PDE Loss:  -0.8176 | Function Loss:  -1.4353\n",
      "Total loss:  -0.2848 | PDE Loss:  -0.8179 | Function Loss:  -1.4353\n",
      "Total loss:  -0.2849 | PDE Loss:  -0.8184 | Function Loss:  -1.4353\n",
      "Total loss:  -0.285 | PDE Loss:  -0.8189 | Function Loss:  -1.4353\n",
      "Total loss:  -0.2852 | PDE Loss:  -0.8194 | Function Loss:  -1.4353\n",
      "Total loss:  -0.2854 | PDE Loss:  -0.8201 | Function Loss:  -1.4353\n",
      "Total loss:  -0.2857 | PDE Loss:  -0.821 | Function Loss:  -1.4353\n",
      "Total loss:  -0.286 | PDE Loss:  -0.8218 | Function Loss:  -1.4355\n",
      "Total loss:  -0.2863 | PDE Loss:  -0.822 | Function Loss:  -1.4359\n",
      "Total loss:  -0.2867 | PDE Loss:  -0.8229 | Function Loss:  -1.436\n",
      "Total loss:  -0.2866 | PDE Loss:  -0.8209 | Function Loss:  -1.4368\n",
      "Total loss:  -0.2868 | PDE Loss:  -0.8223 | Function Loss:  -1.4365\n",
      "Total loss:  -0.2871 | PDE Loss:  -0.8228 | Function Loss:  -1.4367\n",
      "Total loss:  -0.2874 | PDE Loss:  -0.8238 | Function Loss:  -1.4366\n",
      "Total loss:  -0.2876 | PDE Loss:  -0.8239 | Function Loss:  -1.4369\n",
      "Total loss:  -0.2878 | PDE Loss:  -0.8238 | Function Loss:  -1.4372\n",
      "Total loss:  -0.288 | PDE Loss:  -0.8238 | Function Loss:  -1.4375\n",
      "Total loss:  -0.2882 | PDE Loss:  -0.8238 | Function Loss:  -1.4378\n",
      "Total loss:  -0.2884 | PDE Loss:  -0.8238 | Function Loss:  -1.438\n",
      "Total loss:  -0.2886 | PDE Loss:  -0.8242 | Function Loss:  -1.4382\n",
      "Total loss:  -0.2888 | PDE Loss:  -0.8237 | Function Loss:  -1.4387\n",
      "Total loss:  -0.2891 | PDE Loss:  -0.8255 | Function Loss:  -1.4383\n",
      "Total loss:  -0.2893 | PDE Loss:  -0.8249 | Function Loss:  -1.4389\n",
      "Total loss:  -0.2896 | PDE Loss:  -0.8244 | Function Loss:  -1.4395\n",
      "Total loss:  -0.2899 | PDE Loss:  -0.824 | Function Loss:  -1.4401\n",
      "Total loss:  -0.2901 | PDE Loss:  -0.8236 | Function Loss:  -1.4406\n",
      "Total loss:  -0.2904 | PDE Loss:  -0.8232 | Function Loss:  -1.4411\n",
      "Total loss:  -0.2907 | PDE Loss:  -0.8227 | Function Loss:  -1.4417\n",
      "Total loss:  -0.2909 | PDE Loss:  -0.8226 | Function Loss:  -1.4421\n",
      "Total loss:  -0.2913 | PDE Loss:  -0.8227 | Function Loss:  -1.4426\n",
      "Total loss:  -0.2916 | PDE Loss:  -0.8225 | Function Loss:  -1.4431\n",
      "Total loss:  -0.2918 | PDE Loss:  -0.823 | Function Loss:  -1.4432\n",
      "Total loss:  -0.2921 | PDE Loss:  -0.8235 | Function Loss:  -1.4434\n",
      "Total loss:  -0.2923 | PDE Loss:  -0.8247 | Function Loss:  -1.4431\n",
      "Total loss:  -0.2924 | PDE Loss:  -0.825 | Function Loss:  -1.4432\n",
      "Total loss:  -0.2925 | PDE Loss:  -0.8254 | Function Loss:  -1.4432\n",
      "Total loss:  -0.2926 | PDE Loss:  -0.8259 | Function Loss:  -1.4431\n",
      "Total loss:  -0.2927 | PDE Loss:  -0.8263 | Function Loss:  -1.443\n",
      "Total loss:  -0.2928 | PDE Loss:  -0.8267 | Function Loss:  -1.443\n",
      "Total loss:  -0.2929 | PDE Loss:  -0.8271 | Function Loss:  -1.443\n",
      "Total loss:  -0.293 | PDE Loss:  -0.8274 | Function Loss:  -1.4431\n",
      "Total loss:  -0.2932 | PDE Loss:  -0.8277 | Function Loss:  -1.4432\n",
      "Total loss:  -0.2934 | PDE Loss:  -0.8279 | Function Loss:  -1.4434\n",
      "Total loss:  -0.2936 | PDE Loss:  -0.8278 | Function Loss:  -1.4437\n",
      "Total loss:  -0.2937 | PDE Loss:  -0.8278 | Function Loss:  -1.4439\n",
      "Total loss:  -0.2939 | PDE Loss:  -0.8276 | Function Loss:  -1.4442\n",
      "Total loss:  -0.294 | PDE Loss:  -0.8276 | Function Loss:  -1.4444\n",
      "Total loss:  -0.2942 | PDE Loss:  -0.8275 | Function Loss:  -1.4447\n",
      "Total loss:  -0.2944 | PDE Loss:  -0.8276 | Function Loss:  -1.445\n",
      "Total loss:  -0.2948 | PDE Loss:  -0.8275 | Function Loss:  -1.4455\n",
      "Total loss:  -0.2951 | PDE Loss:  -0.8273 | Function Loss:  -1.4461\n",
      "Total loss:  -0.2954 | PDE Loss:  -0.8272 | Function Loss:  -1.4465\n",
      "Total loss:  -0.2957 | PDE Loss:  -0.827 | Function Loss:  -1.447\n",
      "Total loss:  -0.296 | PDE Loss:  -0.8264 | Function Loss:  -1.4477\n",
      "Total loss:  -0.2962 | PDE Loss:  -0.826 | Function Loss:  -1.4481\n",
      "Total loss:  -0.2964 | PDE Loss:  -0.8252 | Function Loss:  -1.4488\n",
      "Total loss:  -0.2966 | PDE Loss:  -0.8249 | Function Loss:  -1.4493\n",
      "Total loss:  -0.2968 | PDE Loss:  -0.8245 | Function Loss:  -1.4497\n",
      "Total loss:  -0.2971 | PDE Loss:  -0.8246 | Function Loss:  -1.45\n",
      "Total loss:  -0.2973 | PDE Loss:  -0.8245 | Function Loss:  -1.4503\n",
      "Total loss:  -0.2975 | PDE Loss:  -0.825 | Function Loss:  -1.4504\n",
      "Total loss:  -0.2976 | PDE Loss:  -0.8253 | Function Loss:  -1.4505\n",
      "Total loss:  -0.2978 | PDE Loss:  -0.8257 | Function Loss:  -1.4505\n",
      "Total loss:  -0.2979 | PDE Loss:  -0.8262 | Function Loss:  -1.4506\n",
      "Total loss:  -0.2981 | PDE Loss:  -0.8269 | Function Loss:  -1.4505\n",
      "Total loss:  -0.2983 | PDE Loss:  -0.8273 | Function Loss:  -1.4506\n",
      "Total loss:  -0.2985 | PDE Loss:  -0.8276 | Function Loss:  -1.4508\n",
      "Total loss:  -0.2987 | PDE Loss:  -0.8276 | Function Loss:  -1.451\n",
      "Total loss:  -0.2989 | PDE Loss:  -0.8274 | Function Loss:  -1.4514\n",
      "Total loss:  -0.299 | PDE Loss:  -0.8272 | Function Loss:  -1.4517\n",
      "Total loss:  -0.2991 | PDE Loss:  -0.8269 | Function Loss:  -1.452\n",
      "Total loss:  -0.2993 | PDE Loss:  -0.8268 | Function Loss:  -1.4522\n",
      "Total loss:  -0.2994 | PDE Loss:  -0.8267 | Function Loss:  -1.4524\n",
      "Total loss:  -0.2995 | PDE Loss:  -0.8267 | Function Loss:  -1.4525\n",
      "Total loss:  -0.2996 | PDE Loss:  -0.827 | Function Loss:  -1.4525\n",
      "Total loss:  -0.2997 | PDE Loss:  -0.8272 | Function Loss:  -1.4526\n",
      "Total loss:  -0.2999 | PDE Loss:  -0.8275 | Function Loss:  -1.4528\n",
      "Total loss:  -0.3002 | PDE Loss:  -0.8282 | Function Loss:  -1.4529\n",
      "Total loss:  -0.3004 | PDE Loss:  -0.8287 | Function Loss:  -1.4531\n",
      "Total loss:  -0.3006 | PDE Loss:  -0.8284 | Function Loss:  -1.4534\n",
      "Total loss:  -0.301 | PDE Loss:  -0.8294 | Function Loss:  -1.4535\n",
      "Total loss:  -0.3013 | PDE Loss:  -0.8299 | Function Loss:  -1.4538\n",
      "Total loss:  -0.3017 | PDE Loss:  -0.8301 | Function Loss:  -1.4543\n",
      "Total loss:  -0.3021 | PDE Loss:  -0.8312 | Function Loss:  -1.4544\n",
      "Total loss:  -0.3024 | PDE Loss:  -0.8315 | Function Loss:  -1.4546\n",
      "Total loss:  -0.3026 | PDE Loss:  -0.8321 | Function Loss:  -1.4547\n",
      "Total loss:  -0.3029 | PDE Loss:  -0.833 | Function Loss:  -1.4547\n",
      "Total loss:  -0.3031 | PDE Loss:  -0.8335 | Function Loss:  -1.4548\n",
      "Total loss:  -0.3033 | PDE Loss:  -0.8341 | Function Loss:  -1.4548\n",
      "Total loss:  -0.3035 | PDE Loss:  -0.8345 | Function Loss:  -1.4549\n",
      "Total loss:  -0.3037 | PDE Loss:  -0.8348 | Function Loss:  -1.4551\n",
      "Total loss:  -0.3039 | PDE Loss:  -0.8352 | Function Loss:  -1.4552\n",
      "Total loss:  -0.3041 | PDE Loss:  -0.8351 | Function Loss:  -1.4556\n",
      "Total loss:  -0.3043 | PDE Loss:  -0.8353 | Function Loss:  -1.4558\n",
      "Total loss:  -0.3046 | PDE Loss:  -0.8351 | Function Loss:  -1.4562\n",
      "Total loss:  -0.3047 | PDE Loss:  -0.8351 | Function Loss:  -1.4565\n",
      "Total loss:  -0.3048 | PDE Loss:  -0.835 | Function Loss:  -1.4567\n",
      "Total loss:  -0.305 | PDE Loss:  -0.8349 | Function Loss:  -1.4569\n",
      "Total loss:  -0.3051 | PDE Loss:  -0.8349 | Function Loss:  -1.4571\n",
      "Total loss:  -0.3053 | PDE Loss:  -0.8348 | Function Loss:  -1.4574\n",
      "Total loss:  -0.3054 | PDE Loss:  -0.835 | Function Loss:  -1.4575\n",
      "Total loss:  -0.3056 | PDE Loss:  -0.835 | Function Loss:  -1.4577\n",
      "Total loss:  -0.3057 | PDE Loss:  -0.8355 | Function Loss:  -1.4577\n",
      "Total loss:  -0.3059 | PDE Loss:  -0.8353 | Function Loss:  -1.458\n",
      "Total loss:  -0.306 | PDE Loss:  -0.8356 | Function Loss:  -1.458\n",
      "Total loss:  -0.3061 | PDE Loss:  -0.8359 | Function Loss:  -1.4581\n",
      "Total loss:  -0.3062 | PDE Loss:  -0.8362 | Function Loss:  -1.4581\n",
      "Total loss:  -0.3063 | PDE Loss:  -0.8365 | Function Loss:  -1.4581\n",
      "Total loss:  -0.3065 | PDE Loss:  -0.837 | Function Loss:  -1.4581\n",
      "Total loss:  -0.3067 | PDE Loss:  -0.8382 | Function Loss:  -1.4579\n",
      "Total loss:  -0.3068 | PDE Loss:  -0.8389 | Function Loss:  -1.4579\n",
      "Total loss:  -0.307 | PDE Loss:  -0.8397 | Function Loss:  -1.4578\n",
      "Total loss:  -0.3072 | PDE Loss:  -0.8404 | Function Loss:  -1.4577\n",
      "Total loss:  -0.3074 | PDE Loss:  -0.8414 | Function Loss:  -1.4576\n",
      "Total loss:  -0.3076 | PDE Loss:  -0.8425 | Function Loss:  -1.4574\n",
      "Total loss:  -0.3078 | PDE Loss:  -0.8433 | Function Loss:  -1.4574\n",
      "Total loss:  -0.308 | PDE Loss:  -0.8447 | Function Loss:  -1.4572\n",
      "Total loss:  -0.3083 | PDE Loss:  -0.8453 | Function Loss:  -1.4573\n",
      "Total loss:  -0.3086 | PDE Loss:  -0.8469 | Function Loss:  -1.457\n",
      "Total loss:  -0.3088 | PDE Loss:  -0.8476 | Function Loss:  -1.4571\n",
      "Total loss:  -0.309 | PDE Loss:  -0.8482 | Function Loss:  -1.4572\n",
      "Total loss:  -0.3093 | PDE Loss:  -0.8486 | Function Loss:  -1.4574\n",
      "Total loss:  -0.3096 | PDE Loss:  -0.8487 | Function Loss:  -1.4577\n",
      "Total loss:  -0.3099 | PDE Loss:  -0.8486 | Function Loss:  -1.4582\n",
      "Total loss:  -0.3101 | PDE Loss:  -0.8485 | Function Loss:  -1.4586\n",
      "Total loss:  -0.3104 | PDE Loss:  -0.848 | Function Loss:  -1.4592\n",
      "Total loss:  -0.3107 | PDE Loss:  -0.8472 | Function Loss:  -1.4599\n",
      "Total loss:  -0.311 | PDE Loss:  -0.8463 | Function Loss:  -1.4607\n",
      "Total loss:  -0.3112 | PDE Loss:  -0.8453 | Function Loss:  -1.4614\n",
      "Total loss:  -0.3114 | PDE Loss:  -0.8441 | Function Loss:  -1.4622\n",
      "Total loss:  -0.3117 | PDE Loss:  -0.8432 | Function Loss:  -1.463\n",
      "Total loss:  -0.312 | PDE Loss:  -0.8418 | Function Loss:  -1.464\n",
      "Total loss:  -0.3123 | PDE Loss:  -0.8412 | Function Loss:  -1.4646\n",
      "Total loss:  -0.3126 | PDE Loss:  -0.8408 | Function Loss:  -1.4653\n",
      "Total loss:  -0.313 | PDE Loss:  -0.8408 | Function Loss:  -1.4658\n",
      "Total loss:  -0.3132 | PDE Loss:  -0.8406 | Function Loss:  -1.4662\n",
      "Total loss:  -0.3135 | PDE Loss:  -0.8405 | Function Loss:  -1.4666\n",
      "Total loss:  -0.3136 | PDE Loss:  -0.8402 | Function Loss:  -1.467\n",
      "Total loss:  -0.3138 | PDE Loss:  -0.8399 | Function Loss:  -1.4674\n",
      "Total loss:  -0.314 | PDE Loss:  -0.8393 | Function Loss:  -1.4678\n",
      "Total loss:  -0.3142 | PDE Loss:  -0.8389 | Function Loss:  -1.4683\n",
      "Total loss:  -0.3144 | PDE Loss:  -0.8382 | Function Loss:  -1.4689\n",
      "Total loss:  -0.3147 | PDE Loss:  -0.8379 | Function Loss:  -1.4694\n",
      "Total loss:  -0.315 | PDE Loss:  -0.8376 | Function Loss:  -1.4701\n",
      "Total loss:  -0.3154 | PDE Loss:  -0.837 | Function Loss:  -1.4708\n",
      "Total loss:  -0.3157 | PDE Loss:  -0.8369 | Function Loss:  -1.4713\n",
      "Total loss:  -0.3159 | PDE Loss:  -0.837 | Function Loss:  -1.4715\n",
      "Total loss:  -0.3161 | PDE Loss:  -0.8372 | Function Loss:  -1.4718\n",
      "Total loss:  -0.3164 | PDE Loss:  -0.8375 | Function Loss:  -1.472\n",
      "Total loss:  -0.3166 | PDE Loss:  -0.8375 | Function Loss:  -1.4723\n",
      "Total loss:  -0.3167 | PDE Loss:  -0.8375 | Function Loss:  -1.4725\n",
      "Total loss:  -0.3169 | PDE Loss:  -0.8373 | Function Loss:  -1.4729\n",
      "Total loss:  -0.3172 | PDE Loss:  -0.8374 | Function Loss:  -1.4732\n",
      "Total loss:  -0.3174 | PDE Loss:  -0.8377 | Function Loss:  -1.4734\n",
      "Total loss:  -0.3176 | PDE Loss:  -0.8379 | Function Loss:  -1.4736\n",
      "Total loss:  -0.3178 | PDE Loss:  -0.8384 | Function Loss:  -1.4737\n",
      "Total loss:  -0.318 | PDE Loss:  -0.8389 | Function Loss:  -1.4738\n",
      "Total loss:  -0.3182 | PDE Loss:  -0.8391 | Function Loss:  -1.474\n",
      "Total loss:  -0.3185 | PDE Loss:  -0.8397 | Function Loss:  -1.474\n",
      "Total loss:  -0.3187 | PDE Loss:  -0.8397 | Function Loss:  -1.4744\n",
      "Total loss:  -0.3188 | PDE Loss:  -0.8398 | Function Loss:  -1.4745\n",
      "Total loss:  -0.319 | PDE Loss:  -0.8399 | Function Loss:  -1.4747\n",
      "Total loss:  -0.3193 | PDE Loss:  -0.8394 | Function Loss:  -1.4754\n",
      "Total loss:  -0.3196 | PDE Loss:  -0.8384 | Function Loss:  -1.4763\n",
      "Total loss:  -0.32 | PDE Loss:  -0.8374 | Function Loss:  -1.4772\n",
      "Total loss:  -0.3204 | PDE Loss:  -0.8356 | Function Loss:  -1.4786\n",
      "Total loss:  -0.3208 | PDE Loss:  -0.8345 | Function Loss:  -1.4796\n",
      "Total loss:  -0.3211 | PDE Loss:  -0.8327 | Function Loss:  -1.481\n",
      "Total loss:  -0.3214 | PDE Loss:  -0.832 | Function Loss:  -1.4816\n",
      "Total loss:  -0.3216 | PDE Loss:  -0.8314 | Function Loss:  -1.4822\n",
      "Total loss:  -0.3218 | PDE Loss:  -0.8314 | Function Loss:  -1.4826\n",
      "Total loss:  -0.3221 | PDE Loss:  -0.8318 | Function Loss:  -1.4828\n",
      "Total loss:  -0.3223 | PDE Loss:  -0.8319 | Function Loss:  -1.4831\n",
      "Total loss:  -0.3228 | PDE Loss:  -0.8318 | Function Loss:  -1.4839\n",
      "Total loss:  -0.3231 | PDE Loss:  -0.8328 | Function Loss:  -1.4838\n",
      "Total loss:  -0.3234 | PDE Loss:  -0.8332 | Function Loss:  -1.484\n",
      "Total loss:  -0.3238 | PDE Loss:  -0.8343 | Function Loss:  -1.4841\n",
      "Total loss:  -0.324 | PDE Loss:  -0.8348 | Function Loss:  -1.4842\n",
      "Total loss:  -0.3243 | PDE Loss:  -0.8357 | Function Loss:  -1.4842\n",
      "Total loss:  -0.3246 | PDE Loss:  -0.8368 | Function Loss:  -1.4841\n",
      "Total loss:  -0.325 | PDE Loss:  -0.8383 | Function Loss:  -1.484\n",
      "Total loss:  -0.3255 | PDE Loss:  -0.8403 | Function Loss:  -1.4839\n",
      "Total loss:  -0.326 | PDE Loss:  -0.8414 | Function Loss:  -1.4841\n",
      "Total loss:  -0.3264 | PDE Loss:  -0.8422 | Function Loss:  -1.4844\n",
      "Total loss:  -0.3268 | PDE Loss:  -0.8429 | Function Loss:  -1.4846\n",
      "Total loss:  -0.3271 | PDE Loss:  -0.843 | Function Loss:  -1.485\n",
      "Total loss:  -0.3273 | PDE Loss:  -0.8429 | Function Loss:  -1.4853\n",
      "Total loss:  -0.3274 | PDE Loss:  -0.8426 | Function Loss:  -1.4857\n",
      "Total loss:  -0.3275 | PDE Loss:  -0.8424 | Function Loss:  -1.4859\n",
      "Total loss:  -0.3276 | PDE Loss:  -0.8421 | Function Loss:  -1.4862\n",
      "Total loss:  -0.3278 | PDE Loss:  -0.8422 | Function Loss:  -1.4864\n",
      "Total loss:  -0.328 | PDE Loss:  -0.8419 | Function Loss:  -1.4867\n",
      "Total loss:  -0.3281 | PDE Loss:  -0.8427 | Function Loss:  -1.4866\n",
      "Total loss:  -0.3282 | PDE Loss:  -0.8431 | Function Loss:  -1.4866\n",
      "Total loss:  -0.3284 | PDE Loss:  -0.844 | Function Loss:  -1.4864\n",
      "Total loss:  -0.3286 | PDE Loss:  -0.8452 | Function Loss:  -1.4862\n",
      "Total loss:  -0.3288 | PDE Loss:  -0.8456 | Function Loss:  -1.4863\n",
      "Total loss:  -0.329 | PDE Loss:  -0.8481 | Function Loss:  -1.4856\n",
      "Total loss:  -0.3292 | PDE Loss:  -0.8489 | Function Loss:  -1.4855\n",
      "Total loss:  -0.3295 | PDE Loss:  -0.8498 | Function Loss:  -1.4855\n",
      "Total loss:  -0.3297 | PDE Loss:  -0.8505 | Function Loss:  -1.4856\n",
      "Total loss:  -0.3299 | PDE Loss:  -0.8506 | Function Loss:  -1.4857\n",
      "Total loss:  -0.33 | PDE Loss:  -0.8509 | Function Loss:  -1.4858\n",
      "Total loss:  -0.3301 | PDE Loss:  -0.8509 | Function Loss:  -1.4859\n",
      "Total loss:  -0.3303 | PDE Loss:  -0.851 | Function Loss:  -1.4861\n",
      "Total loss:  -0.3305 | PDE Loss:  -0.8508 | Function Loss:  -1.4864\n",
      "Total loss:  -0.3306 | PDE Loss:  -0.8509 | Function Loss:  -1.4866\n",
      "Total loss:  -0.3307 | PDE Loss:  -0.8509 | Function Loss:  -1.4868\n",
      "Total loss:  -0.3308 | PDE Loss:  -0.851 | Function Loss:  -1.4869\n",
      "Total loss:  -0.331 | PDE Loss:  -0.8505 | Function Loss:  -1.4873\n",
      "Total loss:  -0.331 | PDE Loss:  -0.8505 | Function Loss:  -1.4874\n",
      "Total loss:  -0.3312 | PDE Loss:  -0.8506 | Function Loss:  -1.4875\n",
      "Total loss:  -0.3313 | PDE Loss:  -0.8506 | Function Loss:  -1.4877\n",
      "Total loss:  -0.3314 | PDE Loss:  -0.8506 | Function Loss:  -1.4879\n",
      "Total loss:  -0.3315 | PDE Loss:  -0.8506 | Function Loss:  -1.4881\n",
      "Total loss:  -0.3317 | PDE Loss:  -0.8505 | Function Loss:  -1.4884\n",
      "Total loss:  -0.3319 | PDE Loss:  -0.8509 | Function Loss:  -1.4885\n",
      "Total loss:  -0.3321 | PDE Loss:  -0.8499 | Function Loss:  -1.4892\n",
      "Total loss:  -0.3323 | PDE Loss:  -0.8502 | Function Loss:  -1.4894\n",
      "Total loss:  -0.3326 | PDE Loss:  -0.8505 | Function Loss:  -1.4897\n",
      "Total loss:  -0.333 | PDE Loss:  -0.8504 | Function Loss:  -1.4903\n",
      "Total loss:  -0.3334 | PDE Loss:  -0.8501 | Function Loss:  -1.491\n",
      "Total loss:  -0.3339 | PDE Loss:  -0.8492 | Function Loss:  -1.492\n",
      "Total loss:  -0.3343 | PDE Loss:  -0.8487 | Function Loss:  -1.4929\n",
      "Total loss:  -0.3349 | PDE Loss:  -0.8481 | Function Loss:  -1.494\n",
      "Total loss:  -0.3354 | PDE Loss:  -0.8474 | Function Loss:  -1.4951\n",
      "Total loss:  -0.3358 | PDE Loss:  -0.8482 | Function Loss:  -1.4953\n",
      "Total loss:  -0.336 | PDE Loss:  -0.8477 | Function Loss:  -1.4958\n",
      "Total loss:  -0.3362 | PDE Loss:  -0.8486 | Function Loss:  -1.4956\n",
      "Total loss:  -0.3363 | PDE Loss:  -0.8493 | Function Loss:  -1.4955\n",
      "Total loss:  -0.3364 | PDE Loss:  -0.8505 | Function Loss:  -1.4952\n",
      "Total loss:  -0.3366 | PDE Loss:  -0.8512 | Function Loss:  -1.495\n",
      "Total loss:  -0.3367 | PDE Loss:  -0.8521 | Function Loss:  -1.4949\n",
      "Total loss:  -0.3369 | PDE Loss:  -0.8524 | Function Loss:  -1.495\n",
      "Total loss:  -0.337 | PDE Loss:  -0.8525 | Function Loss:  -1.4951\n",
      "Total loss:  -0.3371 | PDE Loss:  -0.8511 | Function Loss:  -1.4959\n",
      "Total loss:  -0.3374 | PDE Loss:  -0.8512 | Function Loss:  -1.4962\n",
      "Total loss:  -0.3376 | PDE Loss:  -0.851 | Function Loss:  -1.4966\n",
      "Total loss:  -0.3378 | PDE Loss:  -0.8504 | Function Loss:  -1.4972\n",
      "Total loss:  -0.338 | PDE Loss:  -0.85 | Function Loss:  -1.4976\n",
      "Total loss:  -0.3382 | PDE Loss:  -0.8495 | Function Loss:  -1.4981\n",
      "Total loss:  -0.3384 | PDE Loss:  -0.8494 | Function Loss:  -1.4984\n",
      "Total loss:  -0.3385 | PDE Loss:  -0.8496 | Function Loss:  -1.4986\n",
      "Total loss:  -0.3387 | PDE Loss:  -0.8499 | Function Loss:  -1.4987\n",
      "Total loss:  -0.3389 | PDE Loss:  -0.8507 | Function Loss:  -1.4986\n",
      "Total loss:  -0.339 | PDE Loss:  -0.8516 | Function Loss:  -1.4984\n",
      "Total loss:  -0.3391 | PDE Loss:  -0.8523 | Function Loss:  -1.4982\n",
      "Total loss:  -0.3392 | PDE Loss:  -0.853 | Function Loss:  -1.4981\n",
      "Total loss:  -0.3393 | PDE Loss:  -0.8534 | Function Loss:  -1.498\n",
      "Total loss:  -0.3394 | PDE Loss:  -0.8537 | Function Loss:  -1.498\n",
      "Total loss:  -0.3394 | PDE Loss:  -0.8537 | Function Loss:  -1.4981\n",
      "Total loss:  -0.3395 | PDE Loss:  -0.8534 | Function Loss:  -1.4984\n",
      "Total loss:  -0.3396 | PDE Loss:  -0.8528 | Function Loss:  -1.4988\n",
      "Total loss:  -0.3397 | PDE Loss:  -0.8526 | Function Loss:  -1.499\n",
      "Total loss:  -0.3398 | PDE Loss:  -0.8517 | Function Loss:  -1.4995\n",
      "Total loss:  -0.3399 | PDE Loss:  -0.8509 | Function Loss:  -1.5\n",
      "Total loss:  -0.34 | PDE Loss:  -0.85 | Function Loss:  -1.5005\n",
      "Total loss:  -0.3401 | PDE Loss:  -0.8495 | Function Loss:  -1.5008\n",
      "Total loss:  -0.3401 | PDE Loss:  -0.8492 | Function Loss:  -1.5011\n",
      "Total loss:  -0.3402 | PDE Loss:  -0.8492 | Function Loss:  -1.5012\n",
      "Total loss:  -0.3402 | PDE Loss:  -0.8494 | Function Loss:  -1.5012\n",
      "Total loss:  -0.3404 | PDE Loss:  -0.8499 | Function Loss:  -1.5011\n",
      "Total loss:  -0.3405 | PDE Loss:  -0.8501 | Function Loss:  -1.5013\n",
      "Total loss:  -0.3407 | PDE Loss:  -0.8511 | Function Loss:  -1.5011\n",
      "Total loss:  -0.3409 | PDE Loss:  -0.8519 | Function Loss:  -1.5011\n",
      "Total loss:  -0.3412 | PDE Loss:  -0.8528 | Function Loss:  -1.501\n",
      "Total loss:  -0.3414 | PDE Loss:  -0.8533 | Function Loss:  -1.5011\n",
      "Total loss:  -0.3417 | PDE Loss:  -0.8537 | Function Loss:  -1.5013\n",
      "Total loss:  -0.3419 | PDE Loss:  -0.8535 | Function Loss:  -1.5017\n",
      "Total loss:  -0.3422 | PDE Loss:  -0.8532 | Function Loss:  -1.5024\n",
      "Total loss:  -0.3426 | PDE Loss:  -0.8523 | Function Loss:  -1.5033\n",
      "Total loss:  -0.343 | PDE Loss:  -0.851 | Function Loss:  -1.5045\n",
      "Total loss:  -0.3434 | PDE Loss:  -0.8498 | Function Loss:  -1.5056\n",
      "Total loss:  -0.3437 | PDE Loss:  -0.8488 | Function Loss:  -1.5065\n",
      "Total loss:  -0.3439 | PDE Loss:  -0.8479 | Function Loss:  -1.5072\n",
      "Total loss:  -0.3441 | PDE Loss:  -0.8481 | Function Loss:  -1.5074\n",
      "Total loss:  -0.3442 | PDE Loss:  -0.8479 | Function Loss:  -1.5076\n",
      "Total loss:  -0.3443 | PDE Loss:  -0.8484 | Function Loss:  -1.5076\n",
      "Total loss:  -0.3444 | PDE Loss:  -0.8491 | Function Loss:  -1.5074\n",
      "Total loss:  -0.3446 | PDE Loss:  -0.8499 | Function Loss:  -1.5072\n",
      "Total loss:  -0.3448 | PDE Loss:  -0.8515 | Function Loss:  -1.5068\n",
      "Total loss:  -0.345 | PDE Loss:  -0.8525 | Function Loss:  -1.5066\n",
      "Total loss:  -0.3452 | PDE Loss:  -0.8535 | Function Loss:  -1.5065\n",
      "Total loss:  -0.3454 | PDE Loss:  -0.854 | Function Loss:  -1.5066\n",
      "Total loss:  -0.3456 | PDE Loss:  -0.8544 | Function Loss:  -1.5067\n",
      "Total loss:  -0.3458 | PDE Loss:  -0.8541 | Function Loss:  -1.5071\n",
      "Total loss:  -0.346 | PDE Loss:  -0.854 | Function Loss:  -1.5074\n",
      "Total loss:  -0.3462 | PDE Loss:  -0.8537 | Function Loss:  -1.5078\n",
      "Total loss:  -0.3464 | PDE Loss:  -0.8537 | Function Loss:  -1.5081\n",
      "Total loss:  -0.3466 | PDE Loss:  -0.8539 | Function Loss:  -1.5083\n",
      "Total loss:  -0.3468 | PDE Loss:  -0.8543 | Function Loss:  -1.5085\n",
      "Total loss:  -0.347 | PDE Loss:  -0.8553 | Function Loss:  -1.5084\n",
      "Total loss:  -0.3473 | PDE Loss:  -0.8564 | Function Loss:  -1.5082\n",
      "Total loss:  -0.3475 | PDE Loss:  -0.8581 | Function Loss:  -1.5078\n",
      "Total loss:  -0.3478 | PDE Loss:  -0.8598 | Function Loss:  -1.5074\n",
      "Total loss:  -0.348 | PDE Loss:  -0.8614 | Function Loss:  -1.507\n",
      "Total loss:  -0.3482 | PDE Loss:  -0.8629 | Function Loss:  -1.5067\n",
      "Total loss:  -0.3485 | PDE Loss:  -0.8644 | Function Loss:  -1.5064\n",
      "Total loss:  -0.3488 | PDE Loss:  -0.8656 | Function Loss:  -1.5063\n",
      "Total loss:  -0.3491 | PDE Loss:  -0.8669 | Function Loss:  -1.5061\n",
      "Total loss:  -0.3493 | PDE Loss:  -0.8674 | Function Loss:  -1.5063\n",
      "Total loss:  -0.3496 | PDE Loss:  -0.8683 | Function Loss:  -1.5063\n",
      "Total loss:  -0.3499 | PDE Loss:  -0.8686 | Function Loss:  -1.5066\n",
      "Total loss:  -0.3502 | PDE Loss:  -0.8688 | Function Loss:  -1.5069\n",
      "Total loss:  -0.3505 | PDE Loss:  -0.8688 | Function Loss:  -1.5074\n",
      "Total loss:  -0.3507 | PDE Loss:  -0.8687 | Function Loss:  -1.5078\n",
      "Total loss:  -0.3509 | PDE Loss:  -0.8685 | Function Loss:  -1.5081\n",
      "Total loss:  -0.3511 | PDE Loss:  -0.8684 | Function Loss:  -1.5084\n",
      "Total loss:  -0.3512 | PDE Loss:  -0.8682 | Function Loss:  -1.5087\n",
      "Total loss:  -0.3514 | PDE Loss:  -0.868 | Function Loss:  -1.509\n",
      "Total loss:  -0.3515 | PDE Loss:  -0.8681 | Function Loss:  -1.5092\n",
      "Total loss:  -0.3517 | PDE Loss:  -0.8675 | Function Loss:  -1.5097\n",
      "Total loss:  -0.3519 | PDE Loss:  -0.8675 | Function Loss:  -1.5099\n",
      "Total loss:  -0.3521 | PDE Loss:  -0.8678 | Function Loss:  -1.5101\n",
      "Total loss:  -0.3523 | PDE Loss:  -0.8678 | Function Loss:  -1.5104\n",
      "Total loss:  -0.3525 | PDE Loss:  -0.8679 | Function Loss:  -1.5107\n",
      "Total loss:  -0.3527 | PDE Loss:  -0.8676 | Function Loss:  -1.5111\n",
      "Total loss:  -0.3529 | PDE Loss:  -0.8672 | Function Loss:  -1.5116\n",
      "Total loss:  -0.3531 | PDE Loss:  -0.867 | Function Loss:  -1.5119\n",
      "Total loss:  -0.3533 | PDE Loss:  -0.8662 | Function Loss:  -1.5125\n",
      "Total loss:  -0.3535 | PDE Loss:  -0.8661 | Function Loss:  -1.5128\n",
      "Total loss:  -0.3536 | PDE Loss:  -0.8657 | Function Loss:  -1.5132\n",
      "Total loss:  -0.3538 | PDE Loss:  -0.8658 | Function Loss:  -1.5134\n",
      "Total loss:  -0.354 | PDE Loss:  -0.8654 | Function Loss:  -1.5139\n",
      "Total loss:  -0.3542 | PDE Loss:  -0.8655 | Function Loss:  -1.5141\n",
      "Total loss:  -0.3544 | PDE Loss:  -0.8655 | Function Loss:  -1.5144\n",
      "Total loss:  -0.3546 | PDE Loss:  -0.8657 | Function Loss:  -1.5147\n",
      "Total loss:  -0.3548 | PDE Loss:  -0.866 | Function Loss:  -1.5148\n",
      "Total loss:  -0.3551 | PDE Loss:  -0.8663 | Function Loss:  -1.5151\n",
      "Total loss:  -0.3553 | PDE Loss:  -0.8667 | Function Loss:  -1.5153\n",
      "Total loss:  -0.3556 | PDE Loss:  -0.8663 | Function Loss:  -1.5158\n",
      "Total loss:  -0.3557 | PDE Loss:  -0.8664 | Function Loss:  -1.516\n",
      "Total loss:  -0.356 | PDE Loss:  -0.8668 | Function Loss:  -1.5162\n",
      "Total loss:  -0.3564 | PDE Loss:  -0.8665 | Function Loss:  -1.5169\n",
      "Total loss:  -0.3568 | PDE Loss:  -0.8672 | Function Loss:  -1.5171\n",
      "Total loss:  -0.3572 | PDE Loss:  -0.8675 | Function Loss:  -1.5176\n",
      "Total loss:  -0.3577 | PDE Loss:  -0.868 | Function Loss:  -1.518\n",
      "Total loss:  -0.3581 | PDE Loss:  -0.868 | Function Loss:  -1.5186\n",
      "Total loss:  -0.3584 | PDE Loss:  -0.8682 | Function Loss:  -1.519\n",
      "Total loss:  -0.3587 | PDE Loss:  -0.8682 | Function Loss:  -1.5195\n",
      "Total loss:  -0.359 | PDE Loss:  -0.8681 | Function Loss:  -1.5199\n",
      "Total loss:  -0.3592 | PDE Loss:  -0.8682 | Function Loss:  -1.5202\n",
      "Total loss:  -0.3594 | PDE Loss:  -0.8681 | Function Loss:  -1.5205\n",
      "Total loss:  -0.3595 | PDE Loss:  -0.8681 | Function Loss:  -1.5207\n",
      "Total loss:  -0.3596 | PDE Loss:  -0.8682 | Function Loss:  -1.5208\n",
      "Total loss:  -0.3597 | PDE Loss:  -0.8683 | Function Loss:  -1.5209\n",
      "Total loss:  -0.3599 | PDE Loss:  -0.8684 | Function Loss:  -1.521\n",
      "Total loss:  -0.36 | PDE Loss:  -0.8687 | Function Loss:  -1.5211\n",
      "Total loss:  -0.3601 | PDE Loss:  -0.8691 | Function Loss:  -1.521\n",
      "Total loss:  -0.3602 | PDE Loss:  -0.8693 | Function Loss:  -1.5211\n",
      "Total loss:  -0.3603 | PDE Loss:  -0.8698 | Function Loss:  -1.521\n",
      "Total loss:  -0.3604 | PDE Loss:  -0.8702 | Function Loss:  -1.5211\n",
      "Total loss:  -0.3606 | PDE Loss:  -0.8707 | Function Loss:  -1.521\n",
      "Total loss:  -0.3607 | PDE Loss:  -0.8709 | Function Loss:  -1.5212\n",
      "Total loss:  -0.3609 | PDE Loss:  -0.8714 | Function Loss:  -1.5212\n",
      "Total loss:  -0.3611 | PDE Loss:  -0.872 | Function Loss:  -1.5212\n",
      "Total loss:  -0.3614 | PDE Loss:  -0.8726 | Function Loss:  -1.5213\n",
      "Total loss:  -0.3616 | PDE Loss:  -0.8733 | Function Loss:  -1.5214\n",
      "Total loss:  -0.3617 | PDE Loss:  -0.874 | Function Loss:  -1.5213\n",
      "Total loss:  -0.3619 | PDE Loss:  -0.8747 | Function Loss:  -1.5212\n",
      "Total loss:  -0.362 | PDE Loss:  -0.8756 | Function Loss:  -1.521\n",
      "Total loss:  -0.3622 | PDE Loss:  -0.8765 | Function Loss:  -1.5208\n",
      "Total loss:  -0.3623 | PDE Loss:  -0.8776 | Function Loss:  -1.5205\n",
      "Total loss:  -0.3624 | PDE Loss:  -0.8783 | Function Loss:  -1.5204\n",
      "Total loss:  -0.3626 | PDE Loss:  -0.8789 | Function Loss:  -1.5203\n",
      "Total loss:  -0.3627 | PDE Loss:  -0.8793 | Function Loss:  -1.5203\n",
      "Total loss:  -0.3629 | PDE Loss:  -0.8794 | Function Loss:  -1.5205\n",
      "Total loss:  -0.363 | PDE Loss:  -0.8792 | Function Loss:  -1.5208\n",
      "Total loss:  -0.3632 | PDE Loss:  -0.8789 | Function Loss:  -1.5212\n",
      "Total loss:  -0.3633 | PDE Loss:  -0.8782 | Function Loss:  -1.5217\n",
      "Total loss:  -0.3635 | PDE Loss:  -0.8774 | Function Loss:  -1.5224\n",
      "Total loss:  -0.3637 | PDE Loss:  -0.8766 | Function Loss:  -1.523\n",
      "Total loss:  -0.3639 | PDE Loss:  -0.8755 | Function Loss:  -1.5238\n",
      "Total loss:  -0.3642 | PDE Loss:  -0.8744 | Function Loss:  -1.5246\n",
      "Total loss:  -0.3644 | PDE Loss:  -0.8738 | Function Loss:  -1.5252\n",
      "Total loss:  -0.3646 | PDE Loss:  -0.8735 | Function Loss:  -1.5257\n",
      "Total loss:  -0.3648 | PDE Loss:  -0.8736 | Function Loss:  -1.5259\n",
      "Total loss:  -0.365 | PDE Loss:  -0.874 | Function Loss:  -1.526\n",
      "Total loss:  -0.3653 | PDE Loss:  -0.874 | Function Loss:  -1.5264\n",
      "Total loss:  -0.3655 | PDE Loss:  -0.8746 | Function Loss:  -1.5265\n",
      "Total loss:  -0.3657 | PDE Loss:  -0.875 | Function Loss:  -1.5266\n",
      "Total loss:  -0.366 | PDE Loss:  -0.8756 | Function Loss:  -1.5267\n",
      "Total loss:  -0.3662 | PDE Loss:  -0.8757 | Function Loss:  -1.527\n",
      "Total loss:  -0.3664 | PDE Loss:  -0.8757 | Function Loss:  -1.5273\n",
      "Total loss:  -0.3667 | PDE Loss:  -0.8754 | Function Loss:  -1.5278\n",
      "Total loss:  -0.367 | PDE Loss:  -0.8751 | Function Loss:  -1.5283\n",
      "Total loss:  -0.3674 | PDE Loss:  -0.8743 | Function Loss:  -1.5293\n",
      "Total loss:  -0.3676 | PDE Loss:  -0.8736 | Function Loss:  -1.53\n",
      "Total loss:  -0.3679 | PDE Loss:  -0.8718 | Function Loss:  -1.5312\n",
      "Total loss:  -0.3682 | PDE Loss:  -0.8719 | Function Loss:  -1.5315\n",
      "Total loss:  -0.3684 | PDE Loss:  -0.8726 | Function Loss:  -1.5316\n",
      "Total loss:  -0.3688 | PDE Loss:  -0.873 | Function Loss:  -1.5319\n",
      "Total loss:  -0.369 | PDE Loss:  -0.8734 | Function Loss:  -1.5321\n",
      "Total loss:  -0.3693 | PDE Loss:  -0.8733 | Function Loss:  -1.5325\n",
      "Total loss:  -0.3695 | PDE Loss:  -0.8731 | Function Loss:  -1.533\n",
      "Total loss:  -0.3698 | PDE Loss:  -0.8726 | Function Loss:  -1.5336\n",
      "Total loss:  -0.3699 | PDE Loss:  -0.8722 | Function Loss:  -1.534\n",
      "Total loss:  -0.3701 | PDE Loss:  -0.8716 | Function Loss:  -1.5344\n",
      "Total loss:  -0.3702 | PDE Loss:  -0.8713 | Function Loss:  -1.5347\n",
      "Total loss:  -0.3703 | PDE Loss:  -0.8712 | Function Loss:  -1.5349\n",
      "Total loss:  -0.3702 | PDE Loss:  -0.8707 | Function Loss:  -1.535\n",
      "Total loss:  -0.3703 | PDE Loss:  -0.8711 | Function Loss:  -1.535\n",
      "Total loss:  -0.3704 | PDE Loss:  -0.8714 | Function Loss:  -1.5351\n",
      "Total loss:  -0.3705 | PDE Loss:  -0.8718 | Function Loss:  -1.535\n",
      "Total loss:  -0.3707 | PDE Loss:  -0.8727 | Function Loss:  -1.5348\n",
      "Total loss:  -0.3708 | PDE Loss:  -0.8739 | Function Loss:  -1.5345\n",
      "Total loss:  -0.3709 | PDE Loss:  -0.8749 | Function Loss:  -1.5342\n",
      "Total loss:  -0.371 | PDE Loss:  -0.8763 | Function Loss:  -1.5337\n",
      "Total loss:  -0.3711 | PDE Loss:  -0.8773 | Function Loss:  -1.5334\n",
      "Total loss:  -0.3712 | PDE Loss:  -0.8774 | Function Loss:  -1.5335\n",
      "Total loss:  -0.3714 | PDE Loss:  -0.8775 | Function Loss:  -1.5336\n",
      "Total loss:  -0.3715 | PDE Loss:  -0.8774 | Function Loss:  -1.5339\n",
      "Total loss:  -0.3717 | PDE Loss:  -0.8776 | Function Loss:  -1.5341\n",
      "Total loss:  -0.3719 | PDE Loss:  -0.8773 | Function Loss:  -1.5344\n",
      "Total loss:  -0.3721 | PDE Loss:  -0.8777 | Function Loss:  -1.5345\n",
      "Total loss:  -0.3722 | PDE Loss:  -0.8784 | Function Loss:  -1.5344\n",
      "Total loss:  -0.3724 | PDE Loss:  -0.8789 | Function Loss:  -1.5345\n",
      "Total loss:  -0.3725 | PDE Loss:  -0.8797 | Function Loss:  -1.5343\n",
      "Total loss:  -0.3726 | PDE Loss:  -0.8803 | Function Loss:  -1.5342\n",
      "Total loss:  -0.3727 | PDE Loss:  -0.8809 | Function Loss:  -1.5341\n",
      "Total loss:  -0.3728 | PDE Loss:  -0.8814 | Function Loss:  -1.534\n",
      "Total loss:  -0.3729 | PDE Loss:  -0.8821 | Function Loss:  -1.5338\n",
      "Total loss:  -0.373 | PDE Loss:  -0.8831 | Function Loss:  -1.5336\n",
      "Total loss:  -0.3732 | PDE Loss:  -0.8841 | Function Loss:  -1.5333\n",
      "Total loss:  -0.3734 | PDE Loss:  -0.8855 | Function Loss:  -1.5329\n",
      "Total loss:  -0.3736 | PDE Loss:  -0.8871 | Function Loss:  -1.5326\n",
      "Total loss:  -0.3738 | PDE Loss:  -0.8888 | Function Loss:  -1.5322\n",
      "Total loss:  -0.3741 | PDE Loss:  -0.8907 | Function Loss:  -1.5317\n",
      "Total loss:  -0.3744 | PDE Loss:  -0.8918 | Function Loss:  -1.5316\n",
      "Total loss:  -0.3747 | PDE Loss:  -0.8933 | Function Loss:  -1.5314\n",
      "Total loss:  -0.3749 | PDE Loss:  -0.8941 | Function Loss:  -1.5314\n",
      "Total loss:  -0.3751 | PDE Loss:  -0.8943 | Function Loss:  -1.5316\n",
      "Total loss:  -0.3752 | PDE Loss:  -0.8943 | Function Loss:  -1.5318\n",
      "Total loss:  -0.3754 | PDE Loss:  -0.894 | Function Loss:  -1.5322\n",
      "Total loss:  -0.3756 | PDE Loss:  -0.8938 | Function Loss:  -1.5326\n",
      "Total loss:  -0.3758 | PDE Loss:  -0.8935 | Function Loss:  -1.533\n",
      "Total loss:  -0.3759 | PDE Loss:  -0.8933 | Function Loss:  -1.5332\n",
      "Total loss:  -0.3761 | PDE Loss:  -0.8934 | Function Loss:  -1.5334\n",
      "Total loss:  -0.3763 | PDE Loss:  -0.8939 | Function Loss:  -1.5335\n",
      "Total loss:  -0.3765 | PDE Loss:  -0.8944 | Function Loss:  -1.5335\n",
      "Total loss:  -0.3767 | PDE Loss:  -0.8951 | Function Loss:  -1.5335\n",
      "Total loss:  -0.3769 | PDE Loss:  -0.8958 | Function Loss:  -1.5335\n",
      "Total loss:  -0.3771 | PDE Loss:  -0.8958 | Function Loss:  -1.5338\n",
      "Total loss:  -0.3772 | PDE Loss:  -0.8962 | Function Loss:  -1.5338\n",
      "Total loss:  -0.3774 | PDE Loss:  -0.8961 | Function Loss:  -1.5341\n",
      "Total loss:  -0.3775 | PDE Loss:  -0.8957 | Function Loss:  -1.5344\n",
      "Total loss:  -0.3777 | PDE Loss:  -0.8951 | Function Loss:  -1.5349\n",
      "Total loss:  -0.3778 | PDE Loss:  -0.8942 | Function Loss:  -1.5355\n",
      "Total loss:  -0.378 | PDE Loss:  -0.8931 | Function Loss:  -1.5363\n",
      "Total loss:  -0.3783 | PDE Loss:  -0.8915 | Function Loss:  -1.5374\n",
      "Total loss:  -0.3785 | PDE Loss:  -0.8902 | Function Loss:  -1.5383\n",
      "Total loss:  -0.3787 | PDE Loss:  -0.8892 | Function Loss:  -1.539\n",
      "Total loss:  -0.3789 | PDE Loss:  -0.8888 | Function Loss:  -1.5395\n",
      "Total loss:  -0.379 | PDE Loss:  -0.8888 | Function Loss:  -1.5397\n",
      "Total loss:  -0.3792 | PDE Loss:  -0.8891 | Function Loss:  -1.5398\n",
      "Total loss:  -0.3793 | PDE Loss:  -0.8897 | Function Loss:  -1.5397\n",
      "Total loss:  -0.3794 | PDE Loss:  -0.8904 | Function Loss:  -1.5395\n",
      "Total loss:  -0.3795 | PDE Loss:  -0.8913 | Function Loss:  -1.5393\n",
      "Total loss:  -0.3796 | PDE Loss:  -0.892 | Function Loss:  -1.5391\n",
      "Total loss:  -0.3797 | PDE Loss:  -0.8927 | Function Loss:  -1.5389\n",
      "Total loss:  -0.3798 | PDE Loss:  -0.8931 | Function Loss:  -1.5388\n",
      "Total loss:  -0.3798 | PDE Loss:  -0.8939 | Function Loss:  -1.5386\n",
      "Total loss:  -0.38 | PDE Loss:  -0.8941 | Function Loss:  -1.5387\n",
      "Total loss:  -0.3801 | PDE Loss:  -0.8945 | Function Loss:  -1.5387\n",
      "Total loss:  -0.3802 | PDE Loss:  -0.8946 | Function Loss:  -1.5388\n",
      "Total loss:  -0.3805 | PDE Loss:  -0.8946 | Function Loss:  -1.5392\n",
      "Total loss:  -0.3807 | PDE Loss:  -0.8943 | Function Loss:  -1.5396\n",
      "Total loss:  -0.3809 | PDE Loss:  -0.8945 | Function Loss:  -1.5398\n",
      "Total loss:  -0.3811 | PDE Loss:  -0.8947 | Function Loss:  -1.5401\n",
      "Total loss:  -0.3814 | PDE Loss:  -0.8959 | Function Loss:  -1.54\n",
      "Total loss:  -0.3817 | PDE Loss:  -0.8967 | Function Loss:  -1.54\n",
      "Total loss:  -0.382 | PDE Loss:  -0.8979 | Function Loss:  -1.5399\n",
      "Total loss:  -0.3822 | PDE Loss:  -0.8993 | Function Loss:  -1.5396\n",
      "Total loss:  -0.3824 | PDE Loss:  -0.9001 | Function Loss:  -1.5395\n",
      "Total loss:  -0.3826 | PDE Loss:  -0.9009 | Function Loss:  -1.5394\n",
      "Total loss:  -0.3827 | PDE Loss:  -0.9012 | Function Loss:  -1.5395\n",
      "Total loss:  -0.3829 | PDE Loss:  -0.9013 | Function Loss:  -1.5397\n",
      "Total loss:  -0.383 | PDE Loss:  -0.9012 | Function Loss:  -1.54\n",
      "Total loss:  -0.3833 | PDE Loss:  -0.9009 | Function Loss:  -1.5404\n",
      "Total loss:  -0.3834 | PDE Loss:  -0.9005 | Function Loss:  -1.5409\n",
      "Total loss:  -0.3837 | PDE Loss:  -0.9001 | Function Loss:  -1.5414\n",
      "Total loss:  -0.3839 | PDE Loss:  -0.8997 | Function Loss:  -1.5419\n",
      "Total loss:  -0.3842 | PDE Loss:  -0.8997 | Function Loss:  -1.5423\n",
      "Total loss:  -0.3844 | PDE Loss:  -0.8997 | Function Loss:  -1.5426\n",
      "Total loss:  -0.3847 | PDE Loss:  -0.9001 | Function Loss:  -1.5428\n",
      "Total loss:  -0.3849 | PDE Loss:  -0.901 | Function Loss:  -1.5427\n",
      "Total loss:  -0.3851 | PDE Loss:  -0.9016 | Function Loss:  -1.5427\n",
      "Total loss:  -0.3853 | PDE Loss:  -0.9023 | Function Loss:  -1.5428\n",
      "Total loss:  -0.3855 | PDE Loss:  -0.9027 | Function Loss:  -1.5429\n",
      "Total loss:  -0.3857 | PDE Loss:  -0.9029 | Function Loss:  -1.5431\n",
      "Total loss:  -0.3859 | PDE Loss:  -0.9025 | Function Loss:  -1.5436\n",
      "Total loss:  -0.3862 | PDE Loss:  -0.9021 | Function Loss:  -1.5441\n",
      "Total loss:  -0.3864 | PDE Loss:  -0.901 | Function Loss:  -1.5448\n",
      "Total loss:  -0.3865 | PDE Loss:  -0.9002 | Function Loss:  -1.5454\n",
      "Total loss:  -0.3867 | PDE Loss:  -0.8992 | Function Loss:  -1.5462\n",
      "Total loss:  -0.387 | PDE Loss:  -0.8982 | Function Loss:  -1.5471\n",
      "Total loss:  -0.3874 | PDE Loss:  -0.8978 | Function Loss:  -1.5477\n",
      "Total loss:  -0.3876 | PDE Loss:  -0.8975 | Function Loss:  -1.5481\n",
      "Total loss:  -0.3877 | PDE Loss:  -0.8977 | Function Loss:  -1.5483\n",
      "Total loss:  -0.3879 | PDE Loss:  -0.8979 | Function Loss:  -1.5484\n",
      "Total loss:  -0.388 | PDE Loss:  -0.8986 | Function Loss:  -1.5482\n",
      "Total loss:  -0.3881 | PDE Loss:  -0.8989 | Function Loss:  -1.5482\n",
      "Total loss:  -0.3881 | PDE Loss:  -0.8992 | Function Loss:  -1.5482\n",
      "Total loss:  -0.3882 | PDE Loss:  -0.8996 | Function Loss:  -1.5481\n",
      "Total loss:  -0.3883 | PDE Loss:  -0.9 | Function Loss:  -1.5481\n",
      "Total loss:  -0.3885 | PDE Loss:  -0.9005 | Function Loss:  -1.5481\n",
      "Total loss:  -0.3887 | PDE Loss:  -0.9008 | Function Loss:  -1.5482\n",
      "Total loss:  -0.3888 | PDE Loss:  -0.9012 | Function Loss:  -1.5483\n",
      "Total loss:  -0.389 | PDE Loss:  -0.9016 | Function Loss:  -1.5484\n",
      "Total loss:  -0.3892 | PDE Loss:  -0.9018 | Function Loss:  -1.5486\n",
      "Total loss:  -0.3894 | PDE Loss:  -0.9027 | Function Loss:  -1.5485\n",
      "Total loss:  -0.3896 | PDE Loss:  -0.903 | Function Loss:  -1.5487\n",
      "Total loss:  -0.3898 | PDE Loss:  -0.9034 | Function Loss:  -1.5488\n",
      "Total loss:  -0.39 | PDE Loss:  -0.904 | Function Loss:  -1.5487\n",
      "Total loss:  -0.3901 | PDE Loss:  -0.9044 | Function Loss:  -1.5487\n",
      "Total loss:  -0.3903 | PDE Loss:  -0.9054 | Function Loss:  -1.5485\n",
      "Total loss:  -0.3905 | PDE Loss:  -0.9059 | Function Loss:  -1.5486\n",
      "Total loss:  -0.3906 | PDE Loss:  -0.9068 | Function Loss:  -1.5484\n",
      "Total loss:  -0.3907 | PDE Loss:  -0.9073 | Function Loss:  -1.5483\n",
      "Total loss:  -0.3908 | PDE Loss:  -0.9076 | Function Loss:  -1.5484\n",
      "Total loss:  -0.3909 | PDE Loss:  -0.9077 | Function Loss:  -1.5484\n",
      "Total loss:  -0.391 | PDE Loss:  -0.9077 | Function Loss:  -1.5486\n",
      "Total loss:  -0.3911 | PDE Loss:  -0.9078 | Function Loss:  -1.5487\n",
      "Total loss:  -0.3912 | PDE Loss:  -0.9075 | Function Loss:  -1.549\n",
      "Total loss:  -0.3913 | PDE Loss:  -0.9077 | Function Loss:  -1.5491\n",
      "Total loss:  -0.3915 | PDE Loss:  -0.9079 | Function Loss:  -1.5492\n",
      "Total loss:  -0.3911 | PDE Loss:  -0.9098 | Function Loss:  -1.5478\n",
      "Total loss:  -0.3915 | PDE Loss:  -0.9087 | Function Loss:  -1.5489\n",
      "Total loss:  -0.3917 | PDE Loss:  -0.909 | Function Loss:  -1.549\n",
      "Total loss:  -0.3918 | PDE Loss:  -0.9095 | Function Loss:  -1.549\n",
      "Total loss:  -0.392 | PDE Loss:  -0.91 | Function Loss:  -1.549\n",
      "Total loss:  -0.3921 | PDE Loss:  -0.9106 | Function Loss:  -1.5489\n",
      "Total loss:  -0.3922 | PDE Loss:  -0.911 | Function Loss:  -1.5489\n",
      "Total loss:  -0.3923 | PDE Loss:  -0.9114 | Function Loss:  -1.5489\n",
      "Total loss:  -0.3925 | PDE Loss:  -0.9118 | Function Loss:  -1.5489\n",
      "Total loss:  -0.3926 | PDE Loss:  -0.912 | Function Loss:  -1.549\n",
      "Total loss:  -0.3927 | PDE Loss:  -0.912 | Function Loss:  -1.5492\n",
      "Total loss:  -0.3929 | PDE Loss:  -0.9121 | Function Loss:  -1.5493\n",
      "Total loss:  -0.393 | PDE Loss:  -0.9121 | Function Loss:  -1.5496\n",
      "Total loss:  -0.3932 | PDE Loss:  -0.9122 | Function Loss:  -1.5497\n",
      "Total loss:  -0.3934 | PDE Loss:  -0.9121 | Function Loss:  -1.5501\n",
      "Total loss:  -0.3936 | PDE Loss:  -0.9124 | Function Loss:  -1.5502\n",
      "Total loss:  -0.3937 | PDE Loss:  -0.9126 | Function Loss:  -1.5503\n",
      "Total loss:  -0.3939 | PDE Loss:  -0.9129 | Function Loss:  -1.5504\n",
      "Total loss:  -0.394 | PDE Loss:  -0.9133 | Function Loss:  -1.5505\n",
      "Total loss:  -0.3942 | PDE Loss:  -0.9133 | Function Loss:  -1.5507\n",
      "Total loss:  -0.3943 | PDE Loss:  -0.9135 | Function Loss:  -1.5508\n",
      "Total loss:  -0.3945 | PDE Loss:  -0.9136 | Function Loss:  -1.551\n",
      "Total loss:  -0.3947 | PDE Loss:  -0.9137 | Function Loss:  -1.5513\n",
      "Total loss:  -0.3951 | PDE Loss:  -0.9135 | Function Loss:  -1.5519\n",
      "Total loss:  -0.3953 | PDE Loss:  -0.9134 | Function Loss:  -1.5523\n",
      "Total loss:  -0.3956 | PDE Loss:  -0.9129 | Function Loss:  -1.553\n",
      "Total loss:  -0.3958 | PDE Loss:  -0.9122 | Function Loss:  -1.5535\n",
      "Total loss:  -0.396 | PDE Loss:  -0.9116 | Function Loss:  -1.5541\n",
      "Total loss:  -0.3962 | PDE Loss:  -0.9111 | Function Loss:  -1.5546\n",
      "Total loss:  -0.3964 | PDE Loss:  -0.9108 | Function Loss:  -1.555\n",
      "Total loss:  -0.3966 | PDE Loss:  -0.9104 | Function Loss:  -1.5555\n",
      "Total loss:  -0.3969 | PDE Loss:  -0.9104 | Function Loss:  -1.5559\n",
      "Total loss:  -0.3972 | PDE Loss:  -0.9106 | Function Loss:  -1.5562\n",
      "Total loss:  -0.3976 | PDE Loss:  -0.911 | Function Loss:  -1.5565\n",
      "Total loss:  -0.3979 | PDE Loss:  -0.9116 | Function Loss:  -1.5568\n",
      "Total loss:  -0.3983 | PDE Loss:  -0.9122 | Function Loss:  -1.5571\n",
      "Total loss:  -0.3985 | PDE Loss:  -0.9132 | Function Loss:  -1.557\n",
      "Total loss:  -0.3987 | PDE Loss:  -0.9135 | Function Loss:  -1.5571\n",
      "Total loss:  -0.3988 | PDE Loss:  -0.9135 | Function Loss:  -1.5573\n",
      "Total loss:  -0.3989 | PDE Loss:  -0.9136 | Function Loss:  -1.5574\n",
      "Total loss:  -0.399 | PDE Loss:  -0.9133 | Function Loss:  -1.5577\n",
      "Total loss:  -0.3991 | PDE Loss:  -0.9132 | Function Loss:  -1.5578\n",
      "Total loss:  -0.3992 | PDE Loss:  -0.913 | Function Loss:  -1.558\n",
      "Total loss:  -0.3992 | PDE Loss:  -0.913 | Function Loss:  -1.5581\n",
      "Total loss:  -0.3993 | PDE Loss:  -0.9131 | Function Loss:  -1.5582\n",
      "Total loss:  -0.3994 | PDE Loss:  -0.9134 | Function Loss:  -1.5582\n",
      "Total loss:  -0.3995 | PDE Loss:  -0.9138 | Function Loss:  -1.5581\n",
      "Total loss:  -0.3996 | PDE Loss:  -0.9143 | Function Loss:  -1.558\n",
      "Total loss:  -0.3997 | PDE Loss:  -0.9149 | Function Loss:  -1.5579\n",
      "Total loss:  -0.3998 | PDE Loss:  -0.9155 | Function Loss:  -1.5578\n",
      "Total loss:  -0.4 | PDE Loss:  -0.9162 | Function Loss:  -1.5578\n",
      "Total loss:  -0.4001 | PDE Loss:  -0.9164 | Function Loss:  -1.5579\n",
      "Total loss:  -0.4003 | PDE Loss:  -0.9164 | Function Loss:  -1.5581\n",
      "Total loss:  -0.4004 | PDE Loss:  -0.916 | Function Loss:  -1.5584\n",
      "Total loss:  -0.4005 | PDE Loss:  -0.9153 | Function Loss:  -1.5589\n",
      "Total loss:  -0.4006 | PDE Loss:  -0.9144 | Function Loss:  -1.5595\n",
      "Total loss:  -0.4001 | PDE Loss:  -0.9079 | Function Loss:  -1.5616\n",
      "Total loss:  -0.4007 | PDE Loss:  -0.9133 | Function Loss:  -1.56\n",
      "Total loss:  -0.4008 | PDE Loss:  -0.9127 | Function Loss:  -1.5605\n",
      "Total loss:  -0.4009 | PDE Loss:  -0.9119 | Function Loss:  -1.561\n",
      "Total loss:  -0.401 | PDE Loss:  -0.9114 | Function Loss:  -1.5614\n",
      "Total loss:  -0.4012 | PDE Loss:  -0.9108 | Function Loss:  -1.5619\n",
      "Total loss:  -0.4013 | PDE Loss:  -0.9107 | Function Loss:  -1.5622\n",
      "Total loss:  -0.4015 | PDE Loss:  -0.9106 | Function Loss:  -1.5624\n",
      "Total loss:  -0.4017 | PDE Loss:  -0.9109 | Function Loss:  -1.5626\n",
      "Total loss:  -0.4019 | PDE Loss:  -0.9115 | Function Loss:  -1.5627\n",
      "Total loss:  -0.4022 | PDE Loss:  -0.9122 | Function Loss:  -1.5627\n",
      "Total loss:  -0.4024 | PDE Loss:  -0.913 | Function Loss:  -1.5627\n",
      "Total loss:  -0.4026 | PDE Loss:  -0.9136 | Function Loss:  -1.5627\n",
      "Total loss:  -0.4028 | PDE Loss:  -0.914 | Function Loss:  -1.5628\n",
      "Total loss:  -0.403 | PDE Loss:  -0.9143 | Function Loss:  -1.5629\n",
      "Total loss:  -0.4031 | PDE Loss:  -0.9142 | Function Loss:  -1.5631\n",
      "Total loss:  -0.4033 | PDE Loss:  -0.9142 | Function Loss:  -1.5634\n",
      "Total loss:  -0.4034 | PDE Loss:  -0.9138 | Function Loss:  -1.5638\n",
      "Total loss:  -0.4036 | PDE Loss:  -0.9136 | Function Loss:  -1.5641\n",
      "Total loss:  -0.4037 | PDE Loss:  -0.9132 | Function Loss:  -1.5645\n",
      "Total loss:  -0.4039 | PDE Loss:  -0.9132 | Function Loss:  -1.5647\n",
      "Total loss:  -0.404 | PDE Loss:  -0.913 | Function Loss:  -1.565\n",
      "Total loss:  -0.4042 | PDE Loss:  -0.9137 | Function Loss:  -1.5649\n",
      "Total loss:  -0.4043 | PDE Loss:  -0.9142 | Function Loss:  -1.5649\n",
      "Total loss:  -0.4045 | PDE Loss:  -0.9146 | Function Loss:  -1.5649\n",
      "Total loss:  -0.4047 | PDE Loss:  -0.9159 | Function Loss:  -1.5647\n",
      "Total loss:  -0.4049 | PDE Loss:  -0.9172 | Function Loss:  -1.5643\n",
      "Total loss:  -0.405 | PDE Loss:  -0.9187 | Function Loss:  -1.5639\n",
      "Total loss:  -0.4051 | PDE Loss:  -0.9192 | Function Loss:  -1.5638\n",
      "Total loss:  -0.4051 | PDE Loss:  -0.9194 | Function Loss:  -1.5638\n",
      "Total loss:  -0.4052 | PDE Loss:  -0.9195 | Function Loss:  -1.5639\n",
      "Total loss:  -0.4053 | PDE Loss:  -0.9194 | Function Loss:  -1.5641\n",
      "Total loss:  -0.4054 | PDE Loss:  -0.9192 | Function Loss:  -1.5643\n",
      "Total loss:  -0.4055 | PDE Loss:  -0.919 | Function Loss:  -1.5645\n",
      "Total loss:  -0.4056 | PDE Loss:  -0.9188 | Function Loss:  -1.5648\n",
      "Total loss:  -0.4058 | PDE Loss:  -0.919 | Function Loss:  -1.5649\n",
      "Total loss:  -0.4059 | PDE Loss:  -0.9186 | Function Loss:  -1.5652\n",
      "Total loss:  -0.406 | PDE Loss:  -0.919 | Function Loss:  -1.5652\n",
      "Total loss:  -0.4061 | PDE Loss:  -0.9193 | Function Loss:  -1.5652\n",
      "Total loss:  -0.4062 | PDE Loss:  -0.9197 | Function Loss:  -1.5651\n",
      "Total loss:  -0.4062 | PDE Loss:  -0.9199 | Function Loss:  -1.5651\n",
      "Total loss:  -0.4063 | PDE Loss:  -0.9201 | Function Loss:  -1.5652\n",
      "Total loss:  -0.4064 | PDE Loss:  -0.92 | Function Loss:  -1.5654\n",
      "Total loss:  -0.4066 | PDE Loss:  -0.9202 | Function Loss:  -1.5655\n",
      "Total loss:  -0.4068 | PDE Loss:  -0.9197 | Function Loss:  -1.566\n",
      "Total loss:  -0.4069 | PDE Loss:  -0.9193 | Function Loss:  -1.5664\n",
      "Total loss:  -0.4071 | PDE Loss:  -0.9189 | Function Loss:  -1.5669\n",
      "Total loss:  -0.4073 | PDE Loss:  -0.9184 | Function Loss:  -1.5674\n",
      "Total loss:  -0.4075 | PDE Loss:  -0.9183 | Function Loss:  -1.5677\n",
      "Total loss:  -0.4077 | PDE Loss:  -0.9181 | Function Loss:  -1.5681\n",
      "Total loss:  -0.4079 | PDE Loss:  -0.9181 | Function Loss:  -1.5683\n",
      "Total loss:  -0.408 | PDE Loss:  -0.9183 | Function Loss:  -1.5685\n",
      "Total loss:  -0.4082 | PDE Loss:  -0.9183 | Function Loss:  -1.5687\n",
      "Total loss:  -0.4083 | PDE Loss:  -0.9183 | Function Loss:  -1.5689\n",
      "Total loss:  -0.4086 | PDE Loss:  -0.9185 | Function Loss:  -1.5691\n",
      "Total loss:  -0.4088 | PDE Loss:  -0.9188 | Function Loss:  -1.5694\n",
      "Total loss:  -0.4091 | PDE Loss:  -0.9188 | Function Loss:  -1.5697\n",
      "Total loss:  -0.4093 | PDE Loss:  -0.9193 | Function Loss:  -1.5698\n",
      "Total loss:  -0.4094 | PDE Loss:  -0.9192 | Function Loss:  -1.5701\n",
      "Total loss:  -0.4095 | PDE Loss:  -0.9189 | Function Loss:  -1.5704\n",
      "Total loss:  -0.4097 | PDE Loss:  -0.9194 | Function Loss:  -1.5704\n",
      "Total loss:  -0.4098 | PDE Loss:  -0.9193 | Function Loss:  -1.5706\n",
      "Total loss:  -0.41 | PDE Loss:  -0.9189 | Function Loss:  -1.571\n",
      "Total loss:  -0.4101 | PDE Loss:  -0.9189 | Function Loss:  -1.5712\n",
      "Total loss:  -0.4103 | PDE Loss:  -0.9192 | Function Loss:  -1.5713\n",
      "Total loss:  -0.4105 | PDE Loss:  -0.9196 | Function Loss:  -1.5714\n",
      "Total loss:  -0.4106 | PDE Loss:  -0.9201 | Function Loss:  -1.5714\n",
      "Total loss:  -0.4108 | PDE Loss:  -0.9207 | Function Loss:  -1.5714\n",
      "Total loss:  -0.4109 | PDE Loss:  -0.9212 | Function Loss:  -1.5713\n",
      "Total loss:  -0.4111 | PDE Loss:  -0.9217 | Function Loss:  -1.5713\n",
      "Total loss:  -0.4113 | PDE Loss:  -0.9221 | Function Loss:  -1.5714\n",
      "Total loss:  -0.4114 | PDE Loss:  -0.9216 | Function Loss:  -1.5718\n",
      "Total loss:  -0.4115 | PDE Loss:  -0.9219 | Function Loss:  -1.5718\n",
      "Total loss:  -0.4116 | PDE Loss:  -0.9218 | Function Loss:  -1.572\n",
      "Total loss:  -0.4117 | PDE Loss:  -0.9216 | Function Loss:  -1.5722\n",
      "Total loss:  -0.4117 | PDE Loss:  -0.9213 | Function Loss:  -1.5724\n",
      "Total loss:  -0.4118 | PDE Loss:  -0.9211 | Function Loss:  -1.5727\n",
      "Total loss:  -0.4119 | PDE Loss:  -0.9209 | Function Loss:  -1.573\n",
      "Total loss:  -0.4121 | PDE Loss:  -0.9207 | Function Loss:  -1.5732\n",
      "Total loss:  -0.4122 | PDE Loss:  -0.9207 | Function Loss:  -1.5734\n",
      "Total loss:  -0.4123 | PDE Loss:  -0.9209 | Function Loss:  -1.5735\n",
      "Total loss:  -0.4125 | PDE Loss:  -0.9212 | Function Loss:  -1.5736\n",
      "Total loss:  -0.4126 | PDE Loss:  -0.9219 | Function Loss:  -1.5734\n",
      "Total loss:  -0.4127 | PDE Loss:  -0.9224 | Function Loss:  -1.5734\n",
      "Total loss:  -0.4128 | PDE Loss:  -0.9231 | Function Loss:  -1.5732\n",
      "Total loss:  -0.4129 | PDE Loss:  -0.9236 | Function Loss:  -1.5731\n",
      "Total loss:  -0.413 | PDE Loss:  -0.924 | Function Loss:  -1.5731\n",
      "Total loss:  -0.4131 | PDE Loss:  -0.9242 | Function Loss:  -1.5731\n",
      "Total loss:  -0.4132 | PDE Loss:  -0.9245 | Function Loss:  -1.5731\n",
      "Total loss:  -0.4132 | PDE Loss:  -0.9236 | Function Loss:  -1.5736\n",
      "Total loss:  -0.4133 | PDE Loss:  -0.924 | Function Loss:  -1.5736\n",
      "Total loss:  -0.4134 | PDE Loss:  -0.9241 | Function Loss:  -1.5736\n",
      "Total loss:  -0.4134 | PDE Loss:  -0.9241 | Function Loss:  -1.5737\n",
      "Total loss:  -0.4135 | PDE Loss:  -0.924 | Function Loss:  -1.5738\n",
      "Total loss:  -0.4135 | PDE Loss:  -0.9239 | Function Loss:  -1.5739\n",
      "Total loss:  -0.4136 | PDE Loss:  -0.9237 | Function Loss:  -1.5741\n",
      "Total loss:  -0.4136 | PDE Loss:  -0.9234 | Function Loss:  -1.5743\n",
      "Total loss:  -0.4137 | PDE Loss:  -0.9232 | Function Loss:  -1.5745\n",
      "Total loss:  -0.4138 | PDE Loss:  -0.9224 | Function Loss:  -1.575\n",
      "Total loss:  -0.4139 | PDE Loss:  -0.9224 | Function Loss:  -1.5751\n",
      "Total loss:  -0.4139 | PDE Loss:  -0.9223 | Function Loss:  -1.5752\n",
      "Total loss:  -0.414 | PDE Loss:  -0.9223 | Function Loss:  -1.5753\n",
      "Total loss:  -0.4141 | PDE Loss:  -0.9223 | Function Loss:  -1.5754\n",
      "Total loss:  -0.4142 | PDE Loss:  -0.9223 | Function Loss:  -1.5756\n",
      "Total loss:  -0.4144 | PDE Loss:  -0.9221 | Function Loss:  -1.576\n",
      "Total loss:  -0.4145 | PDE Loss:  -0.9219 | Function Loss:  -1.5763\n",
      "Total loss:  -0.4147 | PDE Loss:  -0.9217 | Function Loss:  -1.5766\n",
      "Total loss:  -0.4149 | PDE Loss:  -0.9214 | Function Loss:  -1.577\n",
      "Total loss:  -0.415 | PDE Loss:  -0.9213 | Function Loss:  -1.5772\n",
      "Total loss:  -0.4152 | PDE Loss:  -0.9215 | Function Loss:  -1.5774\n",
      "Total loss:  -0.4153 | PDE Loss:  -0.9218 | Function Loss:  -1.5775\n",
      "Total loss:  -0.4155 | PDE Loss:  -0.9223 | Function Loss:  -1.5774\n",
      "Total loss:  -0.4156 | PDE Loss:  -0.9229 | Function Loss:  -1.5773\n",
      "Total loss:  -0.4157 | PDE Loss:  -0.9236 | Function Loss:  -1.5772\n",
      "Total loss:  -0.4158 | PDE Loss:  -0.9243 | Function Loss:  -1.5771\n",
      "Total loss:  -0.4159 | PDE Loss:  -0.9248 | Function Loss:  -1.577\n",
      "Total loss:  -0.4161 | PDE Loss:  -0.9253 | Function Loss:  -1.5769\n",
      "Total loss:  -0.4162 | PDE Loss:  -0.9257 | Function Loss:  -1.577\n",
      "Total loss:  -0.4164 | PDE Loss:  -0.9261 | Function Loss:  -1.5771\n",
      "Total loss:  -0.4166 | PDE Loss:  -0.9265 | Function Loss:  -1.5771\n",
      "Total loss:  -0.4167 | PDE Loss:  -0.9269 | Function Loss:  -1.5772\n",
      "Total loss:  -0.4169 | PDE Loss:  -0.9276 | Function Loss:  -1.5771\n",
      "Total loss:  -0.4171 | PDE Loss:  -0.9282 | Function Loss:  -1.5772\n",
      "Total loss:  -0.4173 | PDE Loss:  -0.9287 | Function Loss:  -1.5772\n",
      "Total loss:  -0.4175 | PDE Loss:  -0.9294 | Function Loss:  -1.5772\n",
      "Total loss:  -0.4176 | PDE Loss:  -0.9299 | Function Loss:  -1.5771\n",
      "Total loss:  -0.4177 | PDE Loss:  -0.9304 | Function Loss:  -1.5771\n",
      "Total loss:  -0.4178 | PDE Loss:  -0.9309 | Function Loss:  -1.577\n",
      "Total loss:  -0.4179 | PDE Loss:  -0.9312 | Function Loss:  -1.577\n",
      "Total loss:  -0.418 | PDE Loss:  -0.9315 | Function Loss:  -1.577\n",
      "Total loss:  -0.4182 | PDE Loss:  -0.9317 | Function Loss:  -1.5772\n",
      "Total loss:  -0.4184 | PDE Loss:  -0.9317 | Function Loss:  -1.5775\n",
      "Total loss:  -0.4185 | PDE Loss:  -0.9315 | Function Loss:  -1.5777\n",
      "Total loss:  -0.4186 | PDE Loss:  -0.9311 | Function Loss:  -1.5781\n",
      "Total loss:  -0.4187 | PDE Loss:  -0.9307 | Function Loss:  -1.5783\n",
      "Total loss:  -0.4188 | PDE Loss:  -0.9303 | Function Loss:  -1.5786\n",
      "Total loss:  -0.4188 | PDE Loss:  -0.93 | Function Loss:  -1.5789\n",
      "Total loss:  -0.4189 | PDE Loss:  -0.9294 | Function Loss:  -1.5793\n",
      "Total loss:  -0.419 | PDE Loss:  -0.9294 | Function Loss:  -1.5794\n",
      "Total loss:  -0.4191 | PDE Loss:  -0.9297 | Function Loss:  -1.5794\n",
      "Total loss:  -0.4193 | PDE Loss:  -0.9303 | Function Loss:  -1.5793\n",
      "Total loss:  -0.4193 | PDE Loss:  -0.9309 | Function Loss:  -1.5792\n",
      "Total loss:  -0.4194 | PDE Loss:  -0.9317 | Function Loss:  -1.579\n",
      "Total loss:  -0.4195 | PDE Loss:  -0.9322 | Function Loss:  -1.5789\n",
      "Total loss:  -0.4196 | PDE Loss:  -0.9333 | Function Loss:  -1.5785\n",
      "Total loss:  -0.4197 | PDE Loss:  -0.9333 | Function Loss:  -1.5787\n",
      "Total loss:  -0.4198 | PDE Loss:  -0.9334 | Function Loss:  -1.5788\n",
      "Total loss:  -0.42 | PDE Loss:  -0.9331 | Function Loss:  -1.5791\n",
      "Total loss:  -0.4201 | PDE Loss:  -0.9326 | Function Loss:  -1.5795\n",
      "Total loss:  -0.4202 | PDE Loss:  -0.9322 | Function Loss:  -1.5798\n",
      "Total loss:  -0.4202 | PDE Loss:  -0.9318 | Function Loss:  -1.5801\n",
      "Total loss:  -0.4203 | PDE Loss:  -0.9316 | Function Loss:  -1.5803\n",
      "Total loss:  -0.4203 | PDE Loss:  -0.9314 | Function Loss:  -1.5804\n",
      "Total loss:  -0.4204 | PDE Loss:  -0.9311 | Function Loss:  -1.5806\n",
      "Total loss:  -0.4205 | PDE Loss:  -0.9311 | Function Loss:  -1.5807\n",
      "Total loss:  -0.4205 | PDE Loss:  -0.9311 | Function Loss:  -1.5808\n",
      "Total loss:  -0.4206 | PDE Loss:  -0.9309 | Function Loss:  -1.581\n",
      "Total loss:  -0.4208 | PDE Loss:  -0.9306 | Function Loss:  -1.5814\n",
      "Total loss:  -0.4209 | PDE Loss:  -0.9299 | Function Loss:  -1.5819\n",
      "Total loss:  -0.421 | PDE Loss:  -0.9291 | Function Loss:  -1.5824\n",
      "Total loss:  -0.4211 | PDE Loss:  -0.9284 | Function Loss:  -1.5829\n",
      "Total loss:  -0.4212 | PDE Loss:  -0.9275 | Function Loss:  -1.5834\n",
      "Total loss:  -0.4213 | PDE Loss:  -0.9264 | Function Loss:  -1.5841\n",
      "Total loss:  -0.4215 | PDE Loss:  -0.9254 | Function Loss:  -1.5848\n",
      "Total loss:  -0.4216 | PDE Loss:  -0.9246 | Function Loss:  -1.5853\n",
      "Total loss:  -0.4217 | PDE Loss:  -0.924 | Function Loss:  -1.5858\n",
      "Total loss:  -0.4219 | PDE Loss:  -0.9239 | Function Loss:  -1.586\n",
      "Total loss:  -0.4219 | PDE Loss:  -0.9231 | Function Loss:  -1.5865\n",
      "Total loss:  -0.422 | PDE Loss:  -0.9236 | Function Loss:  -1.5863\n",
      "Total loss:  -0.4221 | PDE Loss:  -0.9243 | Function Loss:  -1.5861\n",
      "Total loss:  -0.4221 | PDE Loss:  -0.9251 | Function Loss:  -1.5858\n",
      "Total loss:  -0.4222 | PDE Loss:  -0.9258 | Function Loss:  -1.5857\n",
      "Total loss:  -0.4223 | PDE Loss:  -0.9264 | Function Loss:  -1.5855\n",
      "Total loss:  -0.4224 | PDE Loss:  -0.9268 | Function Loss:  -1.5854\n",
      "Total loss:  -0.4225 | PDE Loss:  -0.9271 | Function Loss:  -1.5855\n",
      "Total loss:  -0.4226 | PDE Loss:  -0.9272 | Function Loss:  -1.5856\n",
      "Total loss:  -0.4228 | PDE Loss:  -0.9273 | Function Loss:  -1.5858\n",
      "Total loss:  -0.4229 | PDE Loss:  -0.927 | Function Loss:  -1.5861\n",
      "Total loss:  -0.423 | PDE Loss:  -0.9268 | Function Loss:  -1.5863\n",
      "Total loss:  -0.4231 | PDE Loss:  -0.9264 | Function Loss:  -1.5866\n",
      "Total loss:  -0.4232 | PDE Loss:  -0.9263 | Function Loss:  -1.5868\n",
      "Total loss:  -0.4233 | PDE Loss:  -0.9262 | Function Loss:  -1.587\n",
      "Total loss:  -0.4234 | PDE Loss:  -0.9261 | Function Loss:  -1.5872\n",
      "Total loss:  -0.423 | PDE Loss:  -0.9273 | Function Loss:  -1.5861\n",
      "Total loss:  -0.4234 | PDE Loss:  -0.9267 | Function Loss:  -1.587\n",
      "Total loss:  -0.4235 | PDE Loss:  -0.9267 | Function Loss:  -1.5872\n",
      "Total loss:  -0.4237 | PDE Loss:  -0.9269 | Function Loss:  -1.5873\n",
      "Total loss:  -0.4238 | PDE Loss:  -0.9273 | Function Loss:  -1.5874\n",
      "Total loss:  -0.424 | PDE Loss:  -0.9278 | Function Loss:  -1.5874\n",
      "Total loss:  -0.4242 | PDE Loss:  -0.9283 | Function Loss:  -1.5874\n",
      "Total loss:  -0.4244 | PDE Loss:  -0.9286 | Function Loss:  -1.5875\n",
      "Total loss:  -0.4245 | PDE Loss:  -0.9288 | Function Loss:  -1.5876\n",
      "Total loss:  -0.4247 | PDE Loss:  -0.9284 | Function Loss:  -1.588\n",
      "Total loss:  -0.4248 | PDE Loss:  -0.9285 | Function Loss:  -1.5882\n",
      "Total loss:  -0.4249 | PDE Loss:  -0.9283 | Function Loss:  -1.5885\n",
      "Total loss:  -0.4251 | PDE Loss:  -0.9283 | Function Loss:  -1.5887\n",
      "Total loss:  -0.4253 | PDE Loss:  -0.9281 | Function Loss:  -1.5891\n",
      "Total loss:  -0.4254 | PDE Loss:  -0.928 | Function Loss:  -1.5893\n",
      "Total loss:  -0.4255 | PDE Loss:  -0.9279 | Function Loss:  -1.5895\n",
      "Total loss:  -0.4256 | PDE Loss:  -0.9281 | Function Loss:  -1.5895\n",
      "Total loss:  -0.4257 | PDE Loss:  -0.9284 | Function Loss:  -1.5895\n",
      "Total loss:  -0.4258 | PDE Loss:  -0.9288 | Function Loss:  -1.5895\n",
      "Total loss:  -0.4259 | PDE Loss:  -0.9291 | Function Loss:  -1.5896\n",
      "Total loss:  -0.4262 | PDE Loss:  -0.9298 | Function Loss:  -1.5896\n",
      "Total loss:  -0.4263 | PDE Loss:  -0.9303 | Function Loss:  -1.5896\n",
      "Total loss:  -0.4265 | PDE Loss:  -0.9311 | Function Loss:  -1.5895\n",
      "Total loss:  -0.4267 | PDE Loss:  -0.9314 | Function Loss:  -1.5896\n",
      "Total loss:  -0.4268 | PDE Loss:  -0.9321 | Function Loss:  -1.5895\n",
      "Total loss:  -0.427 | PDE Loss:  -0.9324 | Function Loss:  -1.5895\n",
      "Total loss:  -0.4271 | PDE Loss:  -0.9324 | Function Loss:  -1.5898\n",
      "Total loss:  -0.4273 | PDE Loss:  -0.9332 | Function Loss:  -1.5896\n",
      "Total loss:  -0.4274 | PDE Loss:  -0.9337 | Function Loss:  -1.5896\n",
      "Total loss:  -0.4275 | PDE Loss:  -0.9343 | Function Loss:  -1.5896\n",
      "Total loss:  -0.4277 | PDE Loss:  -0.9348 | Function Loss:  -1.5895\n",
      "Total loss:  -0.4278 | PDE Loss:  -0.935 | Function Loss:  -1.5895\n",
      "Total loss:  -0.4278 | PDE Loss:  -0.9353 | Function Loss:  -1.5895\n",
      "Total loss:  -0.4279 | PDE Loss:  -0.9354 | Function Loss:  -1.5896\n",
      "Total loss:  -0.428 | PDE Loss:  -0.9354 | Function Loss:  -1.5897\n",
      "Total loss:  -0.4281 | PDE Loss:  -0.9354 | Function Loss:  -1.5898\n",
      "Total loss:  -0.4282 | PDE Loss:  -0.9352 | Function Loss:  -1.5901\n",
      "Total loss:  -0.4284 | PDE Loss:  -0.9347 | Function Loss:  -1.5906\n",
      "Total loss:  -0.4285 | PDE Loss:  -0.934 | Function Loss:  -1.591\n",
      "Total loss:  -0.4286 | PDE Loss:  -0.9337 | Function Loss:  -1.5914\n",
      "Total loss:  -0.4288 | PDE Loss:  -0.9335 | Function Loss:  -1.5917\n",
      "Total loss:  -0.4289 | PDE Loss:  -0.9334 | Function Loss:  -1.5919\n",
      "Total loss:  -0.429 | PDE Loss:  -0.9331 | Function Loss:  -1.5922\n",
      "Total loss:  -0.4291 | PDE Loss:  -0.9331 | Function Loss:  -1.5923\n",
      "Total loss:  -0.4292 | PDE Loss:  -0.9329 | Function Loss:  -1.5926\n",
      "Total loss:  -0.4293 | PDE Loss:  -0.933 | Function Loss:  -1.5926\n",
      "Total loss:  -0.4293 | PDE Loss:  -0.9331 | Function Loss:  -1.5927\n",
      "Total loss:  -0.4294 | PDE Loss:  -0.9332 | Function Loss:  -1.5927\n",
      "Total loss:  -0.4295 | PDE Loss:  -0.9336 | Function Loss:  -1.5926\n",
      "Total loss:  -0.4295 | PDE Loss:  -0.9339 | Function Loss:  -1.5926\n",
      "Total loss:  -0.4296 | PDE Loss:  -0.9342 | Function Loss:  -1.5925\n",
      "Total loss:  -0.4296 | PDE Loss:  -0.9345 | Function Loss:  -1.5925\n",
      "Total loss:  -0.4297 | PDE Loss:  -0.9348 | Function Loss:  -1.5925\n",
      "Total loss:  -0.4298 | PDE Loss:  -0.9352 | Function Loss:  -1.5924\n",
      "Total loss:  -0.43 | PDE Loss:  -0.9354 | Function Loss:  -1.5926\n",
      "Total loss:  -0.4301 | PDE Loss:  -0.9358 | Function Loss:  -1.5926\n",
      "Total loss:  -0.4302 | PDE Loss:  -0.936 | Function Loss:  -1.5927\n",
      "Total loss:  -0.4304 | PDE Loss:  -0.9363 | Function Loss:  -1.5927\n",
      "Total loss:  -0.4305 | PDE Loss:  -0.9365 | Function Loss:  -1.5928\n",
      "Total loss:  -0.4307 | PDE Loss:  -0.9366 | Function Loss:  -1.593\n",
      "Total loss:  -0.4308 | PDE Loss:  -0.9366 | Function Loss:  -1.5933\n",
      "Total loss:  -0.431 | PDE Loss:  -0.9367 | Function Loss:  -1.5934\n",
      "Total loss:  -0.4311 | PDE Loss:  -0.9365 | Function Loss:  -1.5937\n",
      "Total loss:  -0.4312 | PDE Loss:  -0.9367 | Function Loss:  -1.5938\n",
      "Total loss:  -0.4314 | PDE Loss:  -0.937 | Function Loss:  -1.5939\n",
      "Total loss:  -0.4316 | PDE Loss:  -0.9371 | Function Loss:  -1.5941\n",
      "Total loss:  -0.4318 | PDE Loss:  -0.9377 | Function Loss:  -1.5942\n",
      "Total loss:  -0.432 | PDE Loss:  -0.938 | Function Loss:  -1.5943\n",
      "Total loss:  -0.4322 | PDE Loss:  -0.9386 | Function Loss:  -1.5943\n",
      "Total loss:  -0.4324 | PDE Loss:  -0.9392 | Function Loss:  -1.5944\n",
      "Total loss:  -0.4326 | PDE Loss:  -0.94 | Function Loss:  -1.5943\n",
      "Total loss:  -0.4328 | PDE Loss:  -0.9404 | Function Loss:  -1.5945\n",
      "Total loss:  -0.433 | PDE Loss:  -0.9408 | Function Loss:  -1.5945\n",
      "Total loss:  -0.4331 | PDE Loss:  -0.941 | Function Loss:  -1.5946\n",
      "Total loss:  -0.4333 | PDE Loss:  -0.9411 | Function Loss:  -1.5948\n",
      "Total loss:  -0.4335 | PDE Loss:  -0.9412 | Function Loss:  -1.595\n",
      "Total loss:  -0.4336 | PDE Loss:  -0.9411 | Function Loss:  -1.5952\n",
      "Total loss:  -0.4337 | PDE Loss:  -0.9411 | Function Loss:  -1.5955\n",
      "Total loss:  -0.4339 | PDE Loss:  -0.9408 | Function Loss:  -1.5959\n",
      "Total loss:  -0.4341 | PDE Loss:  -0.9406 | Function Loss:  -1.5962\n",
      "Total loss:  -0.4343 | PDE Loss:  -0.9406 | Function Loss:  -1.5965\n",
      "Total loss:  -0.4345 | PDE Loss:  -0.9406 | Function Loss:  -1.5968\n",
      "Total loss:  -0.4347 | PDE Loss:  -0.9408 | Function Loss:  -1.597\n",
      "Total loss:  -0.4348 | PDE Loss:  -0.9408 | Function Loss:  -1.5971\n",
      "Total loss:  -0.435 | PDE Loss:  -0.9413 | Function Loss:  -1.5972\n",
      "Total loss:  -0.4352 | PDE Loss:  -0.9417 | Function Loss:  -1.5973\n",
      "Total loss:  -0.4353 | PDE Loss:  -0.942 | Function Loss:  -1.5974\n",
      "Total loss:  -0.4354 | PDE Loss:  -0.942 | Function Loss:  -1.5975\n",
      "Total loss:  -0.4356 | PDE Loss:  -0.9419 | Function Loss:  -1.5978\n",
      "Total loss:  -0.4357 | PDE Loss:  -0.9417 | Function Loss:  -1.5981\n",
      "Total loss:  -0.4359 | PDE Loss:  -0.9413 | Function Loss:  -1.5985\n",
      "Total loss:  -0.436 | PDE Loss:  -0.9407 | Function Loss:  -1.599\n",
      "Total loss:  -0.4362 | PDE Loss:  -0.9402 | Function Loss:  -1.5994\n",
      "Total loss:  -0.4363 | PDE Loss:  -0.9395 | Function Loss:  -1.5999\n",
      "Total loss:  -0.4364 | PDE Loss:  -0.9393 | Function Loss:  -1.6002\n",
      "Total loss:  -0.4365 | PDE Loss:  -0.9394 | Function Loss:  -1.6003\n",
      "Total loss:  -0.4367 | PDE Loss:  -0.9396 | Function Loss:  -1.6004\n",
      "Total loss:  -0.4368 | PDE Loss:  -0.9402 | Function Loss:  -1.6003\n",
      "Total loss:  -0.4369 | PDE Loss:  -0.9406 | Function Loss:  -1.6003\n",
      "Total loss:  -0.437 | PDE Loss:  -0.9413 | Function Loss:  -1.6001\n",
      "Total loss:  -0.4372 | PDE Loss:  -0.9419 | Function Loss:  -1.6\n",
      "Total loss:  -0.4373 | PDE Loss:  -0.9423 | Function Loss:  -1.6\n",
      "Total loss:  -0.4374 | PDE Loss:  -0.9426 | Function Loss:  -1.6001\n",
      "Total loss:  -0.4375 | PDE Loss:  -0.9427 | Function Loss:  -1.6002\n",
      "Total loss:  -0.4376 | PDE Loss:  -0.9424 | Function Loss:  -1.6005\n",
      "Total loss:  -0.4377 | PDE Loss:  -0.9422 | Function Loss:  -1.6007\n",
      "Total loss:  -0.4378 | PDE Loss:  -0.9417 | Function Loss:  -1.6011\n",
      "Total loss:  -0.4379 | PDE Loss:  -0.9413 | Function Loss:  -1.6014\n",
      "Total loss:  -0.4379 | PDE Loss:  -0.941 | Function Loss:  -1.6016\n",
      "Total loss:  -0.438 | PDE Loss:  -0.9408 | Function Loss:  -1.6017\n",
      "Total loss:  -0.438 | PDE Loss:  -0.9409 | Function Loss:  -1.6018\n",
      "Total loss:  -0.4381 | PDE Loss:  -0.9412 | Function Loss:  -1.6018\n",
      "Total loss:  -0.4382 | PDE Loss:  -0.9419 | Function Loss:  -1.6016\n",
      "Total loss:  -0.4383 | PDE Loss:  -0.9426 | Function Loss:  -1.6014\n",
      "Total loss:  -0.4384 | PDE Loss:  -0.9437 | Function Loss:  -1.601\n",
      "Total loss:  -0.4384 | PDE Loss:  -0.9443 | Function Loss:  -1.6008\n",
      "Total loss:  -0.4385 | PDE Loss:  -0.9447 | Function Loss:  -1.6007\n",
      "Total loss:  -0.4385 | PDE Loss:  -0.945 | Function Loss:  -1.6007\n",
      "Total loss:  -0.4386 | PDE Loss:  -0.9452 | Function Loss:  -1.6006\n",
      "Total loss:  -0.4386 | PDE Loss:  -0.9453 | Function Loss:  -1.6006\n",
      "Total loss:  -0.4387 | PDE Loss:  -0.9454 | Function Loss:  -1.6007\n",
      "Total loss:  -0.4388 | PDE Loss:  -0.9456 | Function Loss:  -1.6007\n",
      "Total loss:  -0.4389 | PDE Loss:  -0.9456 | Function Loss:  -1.601\n",
      "Total loss:  -0.4391 | PDE Loss:  -0.9461 | Function Loss:  -1.601\n",
      "Total loss:  -0.4394 | PDE Loss:  -0.9469 | Function Loss:  -1.601\n",
      "Total loss:  -0.4397 | PDE Loss:  -0.9482 | Function Loss:  -1.601\n",
      "Total loss:  -0.44 | PDE Loss:  -0.9491 | Function Loss:  -1.601\n",
      "Total loss:  -0.4403 | PDE Loss:  -0.9503 | Function Loss:  -1.6009\n",
      "Total loss:  -0.4406 | PDE Loss:  -0.9507 | Function Loss:  -1.6011\n",
      "Total loss:  -0.4408 | PDE Loss:  -0.9514 | Function Loss:  -1.601\n",
      "Total loss:  -0.441 | PDE Loss:  -0.9521 | Function Loss:  -1.601\n",
      "Total loss:  -0.4412 | PDE Loss:  -0.9528 | Function Loss:  -1.601\n",
      "Total loss:  -0.4414 | PDE Loss:  -0.9536 | Function Loss:  -1.601\n",
      "Total loss:  -0.4416 | PDE Loss:  -0.9541 | Function Loss:  -1.6011\n",
      "Total loss:  -0.4418 | PDE Loss:  -0.9545 | Function Loss:  -1.6012\n",
      "Total loss:  -0.4421 | PDE Loss:  -0.9548 | Function Loss:  -1.6014\n",
      "Total loss:  -0.4423 | PDE Loss:  -0.9551 | Function Loss:  -1.6016\n",
      "Total loss:  -0.4425 | PDE Loss:  -0.9551 | Function Loss:  -1.6019\n",
      "Total loss:  -0.4427 | PDE Loss:  -0.955 | Function Loss:  -1.6022\n",
      "Total loss:  -0.4429 | PDE Loss:  -0.9553 | Function Loss:  -1.6023\n",
      "Total loss:  -0.4431 | PDE Loss:  -0.9552 | Function Loss:  -1.6026\n",
      "Total loss:  -0.4432 | PDE Loss:  -0.9553 | Function Loss:  -1.6028\n",
      "Total loss:  -0.4434 | PDE Loss:  -0.955 | Function Loss:  -1.6031\n",
      "Total loss:  -0.4435 | PDE Loss:  -0.9548 | Function Loss:  -1.6034\n",
      "Total loss:  -0.4436 | PDE Loss:  -0.9547 | Function Loss:  -1.6037\n",
      "Total loss:  -0.4438 | PDE Loss:  -0.9543 | Function Loss:  -1.6041\n",
      "Total loss:  -0.4439 | PDE Loss:  -0.9539 | Function Loss:  -1.6045\n",
      "Total loss:  -0.4441 | PDE Loss:  -0.9533 | Function Loss:  -1.6049\n",
      "Total loss:  -0.4442 | PDE Loss:  -0.9528 | Function Loss:  -1.6053\n",
      "Total loss:  -0.4443 | PDE Loss:  -0.9523 | Function Loss:  -1.6058\n",
      "Total loss:  -0.4444 | PDE Loss:  -0.9516 | Function Loss:  -1.6062\n",
      "Total loss:  -0.4446 | PDE Loss:  -0.9509 | Function Loss:  -1.6067\n",
      "Total loss:  -0.4447 | PDE Loss:  -0.9502 | Function Loss:  -1.6072\n",
      "Total loss:  -0.4449 | PDE Loss:  -0.9494 | Function Loss:  -1.6078\n",
      "Total loss:  -0.4451 | PDE Loss:  -0.9487 | Function Loss:  -1.6085\n",
      "Total loss:  -0.4453 | PDE Loss:  -0.9476 | Function Loss:  -1.6094\n",
      "Total loss:  -0.4455 | PDE Loss:  -0.9471 | Function Loss:  -1.6099\n",
      "Total loss:  -0.4457 | PDE Loss:  -0.947 | Function Loss:  -1.6102\n",
      "Total loss:  -0.4459 | PDE Loss:  -0.9473 | Function Loss:  -1.6103\n",
      "Total loss:  -0.4461 | PDE Loss:  -0.948 | Function Loss:  -1.6103\n",
      "Total loss:  -0.4462 | PDE Loss:  -0.9488 | Function Loss:  -1.6101\n",
      "Total loss:  -0.4464 | PDE Loss:  -0.9501 | Function Loss:  -1.6097\n",
      "Total loss:  -0.4466 | PDE Loss:  -0.9512 | Function Loss:  -1.6096\n",
      "Total loss:  -0.4467 | PDE Loss:  -0.9524 | Function Loss:  -1.6092\n",
      "Total loss:  -0.4469 | PDE Loss:  -0.9531 | Function Loss:  -1.6091\n",
      "Total loss:  -0.447 | PDE Loss:  -0.9538 | Function Loss:  -1.609\n",
      "Total loss:  -0.4471 | PDE Loss:  -0.9543 | Function Loss:  -1.609\n",
      "Total loss:  -0.4473 | PDE Loss:  -0.9546 | Function Loss:  -1.609\n",
      "Total loss:  -0.4474 | PDE Loss:  -0.9553 | Function Loss:  -1.6089\n",
      "Total loss:  -0.4476 | PDE Loss:  -0.9558 | Function Loss:  -1.6089\n",
      "Total loss:  -0.4477 | PDE Loss:  -0.9563 | Function Loss:  -1.6089\n",
      "Total loss:  -0.4478 | PDE Loss:  -0.9567 | Function Loss:  -1.6089\n",
      "Total loss:  -0.4479 | PDE Loss:  -0.9569 | Function Loss:  -1.6089\n",
      "Total loss:  -0.4479 | PDE Loss:  -0.9571 | Function Loss:  -1.6088\n",
      "Total loss:  -0.448 | PDE Loss:  -0.9572 | Function Loss:  -1.6089\n",
      "Total loss:  -0.4481 | PDE Loss:  -0.9577 | Function Loss:  -1.6088\n",
      "Total loss:  -0.4483 | PDE Loss:  -0.9578 | Function Loss:  -1.609\n",
      "Total loss:  -0.4484 | PDE Loss:  -0.958 | Function Loss:  -1.6091\n",
      "Total loss:  -0.4486 | PDE Loss:  -0.9585 | Function Loss:  -1.6092\n",
      "Total loss:  -0.4488 | PDE Loss:  -0.9589 | Function Loss:  -1.6093\n",
      "Total loss:  -0.4491 | PDE Loss:  -0.9597 | Function Loss:  -1.6094\n",
      "Total loss:  -0.4494 | PDE Loss:  -0.9602 | Function Loss:  -1.6096\n",
      "Total loss:  -0.4496 | PDE Loss:  -0.9606 | Function Loss:  -1.6097\n",
      "Total loss:  -0.4499 | PDE Loss:  -0.9609 | Function Loss:  -1.61\n",
      "Total loss:  -0.4501 | PDE Loss:  -0.9611 | Function Loss:  -1.6102\n",
      "Total loss:  -0.4503 | PDE Loss:  -0.9613 | Function Loss:  -1.6104\n",
      "Total loss:  -0.4506 | PDE Loss:  -0.9617 | Function Loss:  -1.6106\n",
      "Total loss:  -0.4508 | PDE Loss:  -0.9617 | Function Loss:  -1.6109\n",
      "Total loss:  -0.4509 | PDE Loss:  -0.9621 | Function Loss:  -1.6109\n",
      "Total loss:  -0.4511 | PDE Loss:  -0.9626 | Function Loss:  -1.611\n",
      "Total loss:  -0.4513 | PDE Loss:  -0.9633 | Function Loss:  -1.6109\n",
      "Total loss:  -0.4515 | PDE Loss:  -0.9637 | Function Loss:  -1.611\n",
      "Total loss:  -0.4516 | PDE Loss:  -0.9641 | Function Loss:  -1.611\n",
      "Total loss:  -0.4517 | PDE Loss:  -0.9644 | Function Loss:  -1.611\n",
      "Total loss:  -0.4518 | PDE Loss:  -0.9647 | Function Loss:  -1.611\n",
      "Total loss:  -0.452 | PDE Loss:  -0.9651 | Function Loss:  -1.6111\n",
      "Total loss:  -0.4521 | PDE Loss:  -0.9654 | Function Loss:  -1.6112\n",
      "Total loss:  -0.4523 | PDE Loss:  -0.9658 | Function Loss:  -1.6113\n",
      "Total loss:  -0.4525 | PDE Loss:  -0.966 | Function Loss:  -1.6115\n",
      "Total loss:  -0.4526 | PDE Loss:  -0.9661 | Function Loss:  -1.6116\n",
      "Total loss:  -0.4527 | PDE Loss:  -0.9663 | Function Loss:  -1.6117\n",
      "Total loss:  -0.4529 | PDE Loss:  -0.9665 | Function Loss:  -1.6118\n",
      "Total loss:  -0.4531 | PDE Loss:  -0.9668 | Function Loss:  -1.612\n",
      "Total loss:  -0.4532 | PDE Loss:  -0.9671 | Function Loss:  -1.612\n",
      "Total loss:  -0.4534 | PDE Loss:  -0.9675 | Function Loss:  -1.6121\n",
      "Total loss:  -0.4536 | PDE Loss:  -0.9689 | Function Loss:  -1.6119\n",
      "Total loss:  -0.4539 | PDE Loss:  -0.9694 | Function Loss:  -1.612\n",
      "Total loss:  -0.4541 | PDE Loss:  -0.9697 | Function Loss:  -1.6121\n",
      "Total loss:  -0.4542 | PDE Loss:  -0.9702 | Function Loss:  -1.6121\n",
      "Total loss:  -0.4544 | PDE Loss:  -0.9699 | Function Loss:  -1.6124\n",
      "Total loss:  -0.4545 | PDE Loss:  -0.97 | Function Loss:  -1.6126\n",
      "Total loss:  -0.4546 | PDE Loss:  -0.97 | Function Loss:  -1.6128\n",
      "Total loss:  -0.4548 | PDE Loss:  -0.9699 | Function Loss:  -1.6131\n",
      "Total loss:  -0.455 | PDE Loss:  -0.9696 | Function Loss:  -1.6134\n",
      "Total loss:  -0.4551 | PDE Loss:  -0.9693 | Function Loss:  -1.6138\n",
      "Total loss:  -0.4553 | PDE Loss:  -0.969 | Function Loss:  -1.6142\n",
      "Total loss:  -0.4554 | PDE Loss:  -0.9687 | Function Loss:  -1.6145\n",
      "Total loss:  -0.4555 | PDE Loss:  -0.9686 | Function Loss:  -1.6147\n",
      "Total loss:  -0.4556 | PDE Loss:  -0.9686 | Function Loss:  -1.6148\n",
      "Total loss:  -0.4557 | PDE Loss:  -0.9687 | Function Loss:  -1.6148\n",
      "Total loss:  -0.4558 | PDE Loss:  -0.9691 | Function Loss:  -1.6148\n",
      "Total loss:  -0.4559 | PDE Loss:  -0.9695 | Function Loss:  -1.6148\n",
      "Total loss:  -0.456 | PDE Loss:  -0.9702 | Function Loss:  -1.6147\n",
      "Total loss:  -0.4561 | PDE Loss:  -0.9708 | Function Loss:  -1.6146\n",
      "Total loss:  -0.4563 | PDE Loss:  -0.9712 | Function Loss:  -1.6147\n",
      "Total loss:  -0.4565 | PDE Loss:  -0.9719 | Function Loss:  -1.6147\n",
      "Total loss:  -0.4569 | PDE Loss:  -0.9727 | Function Loss:  -1.6148\n",
      "Total loss:  -0.4572 | PDE Loss:  -0.9734 | Function Loss:  -1.6149\n",
      "Total loss:  -0.4574 | PDE Loss:  -0.974 | Function Loss:  -1.6151\n",
      "Total loss:  -0.4576 | PDE Loss:  -0.9742 | Function Loss:  -1.6153\n",
      "Total loss:  -0.4579 | PDE Loss:  -0.9745 | Function Loss:  -1.6155\n",
      "Total loss:  -0.4581 | PDE Loss:  -0.9746 | Function Loss:  -1.6157\n",
      "Total loss:  -0.4582 | PDE Loss:  -0.9748 | Function Loss:  -1.6159\n",
      "Total loss:  -0.4584 | PDE Loss:  -0.9749 | Function Loss:  -1.616\n",
      "Total loss:  -0.4585 | PDE Loss:  -0.9751 | Function Loss:  -1.6162\n",
      "Total loss:  -0.4587 | PDE Loss:  -0.9753 | Function Loss:  -1.6163\n",
      "Total loss:  -0.4589 | PDE Loss:  -0.9754 | Function Loss:  -1.6165\n",
      "Total loss:  -0.459 | PDE Loss:  -0.9749 | Function Loss:  -1.6169\n",
      "Total loss:  -0.4592 | PDE Loss:  -0.9749 | Function Loss:  -1.6172\n",
      "Total loss:  -0.4593 | PDE Loss:  -0.9746 | Function Loss:  -1.6175\n",
      "Total loss:  -0.4595 | PDE Loss:  -0.974 | Function Loss:  -1.618\n",
      "Total loss:  -0.4596 | PDE Loss:  -0.9734 | Function Loss:  -1.6185\n",
      "Total loss:  -0.4598 | PDE Loss:  -0.9724 | Function Loss:  -1.6192\n",
      "Total loss:  -0.46 | PDE Loss:  -0.9714 | Function Loss:  -1.6198\n",
      "Total loss:  -0.4601 | PDE Loss:  -0.9706 | Function Loss:  -1.6204\n",
      "Total loss:  -0.4602 | PDE Loss:  -0.9703 | Function Loss:  -1.6207\n",
      "Total loss:  -0.4603 | PDE Loss:  -0.9705 | Function Loss:  -1.6208\n",
      "Total loss:  -0.4604 | PDE Loss:  -0.9707 | Function Loss:  -1.6208\n",
      "Total loss:  -0.4605 | PDE Loss:  -0.9711 | Function Loss:  -1.6207\n",
      "Total loss:  -0.4605 | PDE Loss:  -0.9715 | Function Loss:  -1.6206\n",
      "Total loss:  -0.4606 | PDE Loss:  -0.9717 | Function Loss:  -1.6206\n",
      "Total loss:  -0.4607 | PDE Loss:  -0.9722 | Function Loss:  -1.6205\n",
      "Total loss:  -0.4608 | PDE Loss:  -0.9728 | Function Loss:  -1.6205\n",
      "Total loss:  -0.4609 | PDE Loss:  -0.9733 | Function Loss:  -1.6204\n",
      "Total loss:  -0.461 | PDE Loss:  -0.9736 | Function Loss:  -1.6204\n",
      "Total loss:  -0.4612 | PDE Loss:  -0.9737 | Function Loss:  -1.6205\n",
      "Total loss:  -0.4612 | PDE Loss:  -0.9737 | Function Loss:  -1.6207\n",
      "Total loss:  -0.4613 | PDE Loss:  -0.9736 | Function Loss:  -1.6209\n",
      "Total loss:  -0.4614 | PDE Loss:  -0.9735 | Function Loss:  -1.621\n",
      "Total loss:  -0.4615 | PDE Loss:  -0.9735 | Function Loss:  -1.6212\n",
      "Total loss:  -0.4617 | PDE Loss:  -0.9734 | Function Loss:  -1.6214\n",
      "Total loss:  -0.4618 | PDE Loss:  -0.9738 | Function Loss:  -1.6215\n",
      "Total loss:  -0.4619 | PDE Loss:  -0.9742 | Function Loss:  -1.6215\n",
      "Total loss:  -0.4621 | PDE Loss:  -0.975 | Function Loss:  -1.6214\n",
      "Total loss:  -0.4623 | PDE Loss:  -0.9758 | Function Loss:  -1.6212\n",
      "Total loss:  -0.4624 | PDE Loss:  -0.9764 | Function Loss:  -1.6211\n",
      "Total loss:  -0.4625 | PDE Loss:  -0.977 | Function Loss:  -1.621\n",
      "Total loss:  -0.4626 | PDE Loss:  -0.9775 | Function Loss:  -1.621\n",
      "Total loss:  -0.4627 | PDE Loss:  -0.9777 | Function Loss:  -1.6211\n",
      "Total loss:  -0.4629 | PDE Loss:  -0.9781 | Function Loss:  -1.6211\n",
      "Total loss:  -0.463 | PDE Loss:  -0.9782 | Function Loss:  -1.6213\n",
      "Total loss:  -0.4632 | PDE Loss:  -0.9781 | Function Loss:  -1.6216\n",
      "Total loss:  -0.4634 | PDE Loss:  -0.9779 | Function Loss:  -1.6219\n",
      "Total loss:  -0.4635 | PDE Loss:  -0.9776 | Function Loss:  -1.6222\n",
      "Total loss:  -0.4637 | PDE Loss:  -0.9774 | Function Loss:  -1.6225\n",
      "Total loss:  -0.4638 | PDE Loss:  -0.9773 | Function Loss:  -1.6229\n",
      "Total loss:  -0.464 | PDE Loss:  -0.9773 | Function Loss:  -1.6231\n",
      "Total loss:  -0.4642 | PDE Loss:  -0.9758 | Function Loss:  -1.624\n",
      "Total loss:  -0.4644 | PDE Loss:  -0.9764 | Function Loss:  -1.624\n",
      "Total loss:  -0.4645 | PDE Loss:  -0.9772 | Function Loss:  -1.6239\n",
      "Total loss:  -0.4647 | PDE Loss:  -0.9779 | Function Loss:  -1.6238\n",
      "Total loss:  -0.4649 | PDE Loss:  -0.9787 | Function Loss:  -1.6237\n",
      "Total loss:  -0.465 | PDE Loss:  -0.9792 | Function Loss:  -1.6237\n",
      "Total loss:  -0.4652 | PDE Loss:  -0.9795 | Function Loss:  -1.6238\n",
      "Total loss:  -0.4653 | PDE Loss:  -0.9797 | Function Loss:  -1.6239\n",
      "Total loss:  -0.4655 | PDE Loss:  -0.9796 | Function Loss:  -1.6243\n",
      "Total loss:  -0.4657 | PDE Loss:  -0.9795 | Function Loss:  -1.6245\n",
      "Total loss:  -0.4658 | PDE Loss:  -0.9791 | Function Loss:  -1.6249\n",
      "Total loss:  -0.4659 | PDE Loss:  -0.9789 | Function Loss:  -1.6251\n",
      "Total loss:  -0.466 | PDE Loss:  -0.9786 | Function Loss:  -1.6253\n",
      "Total loss:  -0.4661 | PDE Loss:  -0.9785 | Function Loss:  -1.6255\n",
      "Total loss:  -0.4662 | PDE Loss:  -0.9784 | Function Loss:  -1.6258\n",
      "Total loss:  -0.4664 | PDE Loss:  -0.9788 | Function Loss:  -1.6259\n",
      "Total loss:  -0.4666 | PDE Loss:  -0.9786 | Function Loss:  -1.6263\n",
      "Total loss:  -0.4667 | PDE Loss:  -0.9789 | Function Loss:  -1.6263\n",
      "Total loss:  -0.4669 | PDE Loss:  -0.9795 | Function Loss:  -1.6263\n",
      "Total loss:  -0.4671 | PDE Loss:  -0.98 | Function Loss:  -1.6263\n",
      "Total loss:  -0.4672 | PDE Loss:  -0.9808 | Function Loss:  -1.6262\n",
      "Total loss:  -0.4673 | PDE Loss:  -0.9813 | Function Loss:  -1.6261\n",
      "Total loss:  -0.4675 | PDE Loss:  -0.982 | Function Loss:  -1.626\n",
      "Total loss:  -0.4676 | PDE Loss:  -0.9825 | Function Loss:  -1.626\n",
      "Total loss:  -0.4678 | PDE Loss:  -0.9831 | Function Loss:  -1.626\n",
      "Total loss:  -0.4679 | PDE Loss:  -0.9834 | Function Loss:  -1.626\n",
      "Total loss:  -0.4681 | PDE Loss:  -0.9837 | Function Loss:  -1.6261\n",
      "Total loss:  -0.4682 | PDE Loss:  -0.9838 | Function Loss:  -1.6263\n",
      "Total loss:  -0.4683 | PDE Loss:  -0.9838 | Function Loss:  -1.6264\n",
      "Total loss:  -0.4685 | PDE Loss:  -0.9837 | Function Loss:  -1.6267\n",
      "Total loss:  -0.4687 | PDE Loss:  -0.9842 | Function Loss:  -1.6268\n",
      "Total loss:  -0.469 | PDE Loss:  -0.9839 | Function Loss:  -1.6273\n",
      "Total loss:  -0.4691 | PDE Loss:  -0.984 | Function Loss:  -1.6275\n",
      "Total loss:  -0.4693 | PDE Loss:  -0.9841 | Function Loss:  -1.6278\n",
      "Total loss:  -0.4695 | PDE Loss:  -0.9845 | Function Loss:  -1.6279\n",
      "Total loss:  -0.4696 | PDE Loss:  -0.9845 | Function Loss:  -1.628\n",
      "Total loss:  -0.4697 | PDE Loss:  -0.9845 | Function Loss:  -1.6281\n",
      "Total loss:  -0.4698 | PDE Loss:  -0.9844 | Function Loss:  -1.6283\n",
      "Total loss:  -0.4699 | PDE Loss:  -0.9842 | Function Loss:  -1.6286\n",
      "Total loss:  -0.47 | PDE Loss:  -0.9839 | Function Loss:  -1.6289\n",
      "Total loss:  -0.4702 | PDE Loss:  -0.9834 | Function Loss:  -1.6293\n",
      "Total loss:  -0.4703 | PDE Loss:  -0.983 | Function Loss:  -1.6297\n",
      "Total loss:  -0.4705 | PDE Loss:  -0.9824 | Function Loss:  -1.6302\n",
      "Total loss:  -0.4707 | PDE Loss:  -0.9822 | Function Loss:  -1.6305\n",
      "Total loss:  -0.4707 | PDE Loss:  -0.9818 | Function Loss:  -1.6307\n",
      "Total loss:  -0.4708 | PDE Loss:  -0.9821 | Function Loss:  -1.6307\n",
      "Total loss:  -0.4709 | PDE Loss:  -0.9824 | Function Loss:  -1.6307\n",
      "Total loss:  -0.471 | PDE Loss:  -0.9828 | Function Loss:  -1.6307\n",
      "Total loss:  -0.471 | PDE Loss:  -0.9835 | Function Loss:  -1.6305\n",
      "Total loss:  -0.4711 | PDE Loss:  -0.9842 | Function Loss:  -1.6303\n",
      "Total loss:  -0.4712 | PDE Loss:  -0.9851 | Function Loss:  -1.63\n",
      "Total loss:  -0.4713 | PDE Loss:  -0.9861 | Function Loss:  -1.6297\n",
      "Total loss:  -0.4714 | PDE Loss:  -0.9872 | Function Loss:  -1.6294\n",
      "Total loss:  -0.4716 | PDE Loss:  -0.9883 | Function Loss:  -1.6291\n",
      "Total loss:  -0.4717 | PDE Loss:  -0.989 | Function Loss:  -1.629\n",
      "Total loss:  -0.4718 | PDE Loss:  -0.9895 | Function Loss:  -1.6289\n",
      "Total loss:  -0.4719 | PDE Loss:  -0.9899 | Function Loss:  -1.6289\n",
      "Total loss:  -0.472 | PDE Loss:  -0.99 | Function Loss:  -1.6291\n",
      "Total loss:  -0.4722 | PDE Loss:  -0.9901 | Function Loss:  -1.6292\n",
      "Total loss:  -0.4723 | PDE Loss:  -0.9901 | Function Loss:  -1.6294\n",
      "Total loss:  -0.4725 | PDE Loss:  -0.9902 | Function Loss:  -1.6296\n",
      "Total loss:  -0.4726 | PDE Loss:  -0.9904 | Function Loss:  -1.6297\n",
      "Total loss:  -0.4728 | PDE Loss:  -0.9909 | Function Loss:  -1.6298\n",
      "Total loss:  -0.4729 | PDE Loss:  -0.9913 | Function Loss:  -1.6298\n",
      "Total loss:  -0.473 | PDE Loss:  -0.9918 | Function Loss:  -1.6297\n",
      "Total loss:  -0.4731 | PDE Loss:  -0.993 | Function Loss:  -1.6294\n",
      "Total loss:  -0.4732 | PDE Loss:  -0.9931 | Function Loss:  -1.6294\n",
      "Total loss:  -0.4733 | PDE Loss:  -0.9937 | Function Loss:  -1.6293\n",
      "Total loss:  -0.4735 | PDE Loss:  -0.9943 | Function Loss:  -1.6292\n",
      "Total loss:  -0.4736 | PDE Loss:  -0.9947 | Function Loss:  -1.6293\n",
      "Total loss:  -0.4737 | PDE Loss:  -0.9948 | Function Loss:  -1.6294\n",
      "Total loss:  -0.4739 | PDE Loss:  -0.9948 | Function Loss:  -1.6296\n",
      "Total loss:  -0.474 | PDE Loss:  -0.9947 | Function Loss:  -1.6299\n",
      "Total loss:  -0.4741 | PDE Loss:  -0.9944 | Function Loss:  -1.6302\n",
      "Total loss:  -0.4742 | PDE Loss:  -0.9942 | Function Loss:  -1.6304\n",
      "Total loss:  -0.4744 | PDE Loss:  -0.9955 | Function Loss:  -1.6301\n",
      "Total loss:  -0.4746 | PDE Loss:  -0.9958 | Function Loss:  -1.6302\n",
      "Total loss:  -0.4747 | PDE Loss:  -0.9964 | Function Loss:  -1.6301\n",
      "Total loss:  -0.4749 | PDE Loss:  -0.9969 | Function Loss:  -1.6302\n",
      "Total loss:  -0.475 | PDE Loss:  -0.9975 | Function Loss:  -1.6301\n",
      "Total loss:  -0.4751 | PDE Loss:  -0.998 | Function Loss:  -1.63\n",
      "Total loss:  -0.4752 | PDE Loss:  -0.9984 | Function Loss:  -1.63\n",
      "Total loss:  -0.4754 | PDE Loss:  -0.9987 | Function Loss:  -1.6301\n",
      "Total loss:  -0.4754 | PDE Loss:  -0.9988 | Function Loss:  -1.6301\n",
      "Total loss:  -0.4755 | PDE Loss:  -0.9989 | Function Loss:  -1.6302\n",
      "Total loss:  -0.4757 | PDE Loss:  -0.9988 | Function Loss:  -1.6305\n",
      "Total loss:  -0.4757 | PDE Loss:  -0.9984 | Function Loss:  -1.6307\n",
      "Total loss:  -0.4758 | PDE Loss:  -0.998 | Function Loss:  -1.631\n",
      "Total loss:  -0.4759 | PDE Loss:  -0.9975 | Function Loss:  -1.6314\n",
      "Total loss:  -0.476 | PDE Loss:  -0.997 | Function Loss:  -1.6317\n",
      "Total loss:  -0.4761 | PDE Loss:  -0.9966 | Function Loss:  -1.6321\n",
      "Total loss:  -0.4763 | PDE Loss:  -0.996 | Function Loss:  -1.6325\n",
      "Total loss:  -0.4765 | PDE Loss:  -0.9959 | Function Loss:  -1.6329\n",
      "Total loss:  -0.4767 | PDE Loss:  -0.9964 | Function Loss:  -1.633\n",
      "Total loss:  -0.4768 | PDE Loss:  -0.9966 | Function Loss:  -1.6331\n",
      "Total loss:  -0.4771 | PDE Loss:  -0.9976 | Function Loss:  -1.633\n",
      "Total loss:  -0.4772 | PDE Loss:  -0.9988 | Function Loss:  -1.6327\n",
      "Total loss:  -0.4774 | PDE Loss:  -1.0001 | Function Loss:  -1.6324\n",
      "Total loss:  -0.4775 | PDE Loss:  -1.0012 | Function Loss:  -1.6321\n",
      "Total loss:  -0.4777 | PDE Loss:  -1.0023 | Function Loss:  -1.6319\n",
      "Total loss:  -0.4779 | PDE Loss:  -1.0033 | Function Loss:  -1.6317\n",
      "Total loss:  -0.4781 | PDE Loss:  -1.0041 | Function Loss:  -1.6317\n",
      "Total loss:  -0.4783 | PDE Loss:  -1.0044 | Function Loss:  -1.6319\n",
      "Total loss:  -0.4785 | PDE Loss:  -1.0043 | Function Loss:  -1.6321\n",
      "Total loss:  -0.4786 | PDE Loss:  -1.004 | Function Loss:  -1.6325\n",
      "Total loss:  -0.4787 | PDE Loss:  -1.0036 | Function Loss:  -1.6328\n",
      "Total loss:  -0.4789 | PDE Loss:  -1.0034 | Function Loss:  -1.6331\n",
      "Total loss:  -0.4791 | PDE Loss:  -1.0032 | Function Loss:  -1.6335\n",
      "Total loss:  -0.4792 | PDE Loss:  -1.0026 | Function Loss:  -1.6339\n",
      "Total loss:  -0.4794 | PDE Loss:  -1.0025 | Function Loss:  -1.6342\n",
      "Total loss:  -0.4795 | PDE Loss:  -1.0029 | Function Loss:  -1.6341\n",
      "Total loss:  -0.4796 | PDE Loss:  -1.0036 | Function Loss:  -1.634\n",
      "Total loss:  -0.4797 | PDE Loss:  -1.0041 | Function Loss:  -1.634\n",
      "Total loss:  -0.4799 | PDE Loss:  -1.0043 | Function Loss:  -1.6341\n",
      "Total loss:  -0.48 | PDE Loss:  -1.0042 | Function Loss:  -1.6343\n",
      "Total loss:  -0.4801 | PDE Loss:  -1.0039 | Function Loss:  -1.6346\n",
      "Total loss:  -0.4802 | PDE Loss:  -1.0035 | Function Loss:  -1.6349\n",
      "Total loss:  -0.4803 | PDE Loss:  -1.0033 | Function Loss:  -1.6352\n",
      "Total loss:  -0.4805 | PDE Loss:  -1.003 | Function Loss:  -1.6356\n",
      "Total loss:  -0.4806 | PDE Loss:  -1.0032 | Function Loss:  -1.6356\n",
      "Total loss:  -0.4809 | PDE Loss:  -1.0027 | Function Loss:  -1.6362\n",
      "Total loss:  -0.481 | PDE Loss:  -1.0028 | Function Loss:  -1.6364\n",
      "Total loss:  -0.4812 | PDE Loss:  -1.0031 | Function Loss:  -1.6366\n",
      "Total loss:  -0.4814 | PDE Loss:  -1.0036 | Function Loss:  -1.6365\n",
      "Total loss:  -0.4815 | PDE Loss:  -1.0039 | Function Loss:  -1.6366\n",
      "Total loss:  -0.4815 | PDE Loss:  -1.0041 | Function Loss:  -1.6366\n",
      "Total loss:  -0.4816 | PDE Loss:  -1.0041 | Function Loss:  -1.6367\n",
      "Total loss:  -0.4817 | PDE Loss:  -1.0039 | Function Loss:  -1.6369\n",
      "Total loss:  -0.4818 | PDE Loss:  -1.0035 | Function Loss:  -1.6372\n",
      "Total loss:  -0.4819 | PDE Loss:  -1.0028 | Function Loss:  -1.6377\n",
      "Total loss:  -0.482 | PDE Loss:  -1.002 | Function Loss:  -1.6381\n",
      "Total loss:  -0.4821 | PDE Loss:  -1.0012 | Function Loss:  -1.6386\n",
      "Total loss:  -0.4821 | PDE Loss:  -1.0006 | Function Loss:  -1.6389\n",
      "Total loss:  -0.4822 | PDE Loss:  -0.9999 | Function Loss:  -1.6394\n",
      "Total loss:  -0.4823 | PDE Loss:  -0.9997 | Function Loss:  -1.6396\n",
      "Total loss:  -0.4825 | PDE Loss:  -0.9983 | Function Loss:  -1.6404\n",
      "Total loss:  -0.4825 | PDE Loss:  -0.9984 | Function Loss:  -1.6405\n",
      "Total loss:  -0.4826 | PDE Loss:  -0.9987 | Function Loss:  -1.6404\n",
      "Total loss:  -0.4827 | PDE Loss:  -0.9991 | Function Loss:  -1.6404\n",
      "Total loss:  -0.4828 | PDE Loss:  -0.9993 | Function Loss:  -1.6405\n",
      "Total loss:  -0.4829 | PDE Loss:  -0.9993 | Function Loss:  -1.6406\n",
      "Total loss:  -0.4831 | PDE Loss:  -0.9991 | Function Loss:  -1.6409\n",
      "Total loss:  -0.4832 | PDE Loss:  -0.9985 | Function Loss:  -1.6414\n",
      "Total loss:  -0.4834 | PDE Loss:  -0.9975 | Function Loss:  -1.6421\n",
      "Total loss:  -0.4835 | PDE Loss:  -0.997 | Function Loss:  -1.6425\n",
      "Total loss:  -0.4837 | PDE Loss:  -0.9961 | Function Loss:  -1.6431\n",
      "Total loss:  -0.4838 | PDE Loss:  -0.9957 | Function Loss:  -1.6434\n",
      "Total loss:  -0.4838 | PDE Loss:  -0.9954 | Function Loss:  -1.6437\n",
      "Total loss:  -0.4839 | PDE Loss:  -0.9954 | Function Loss:  -1.6438\n",
      "Total loss:  -0.484 | PDE Loss:  -0.9955 | Function Loss:  -1.6438\n",
      "Total loss:  -0.484 | PDE Loss:  -0.9954 | Function Loss:  -1.644\n",
      "Total loss:  -0.4841 | PDE Loss:  -0.9954 | Function Loss:  -1.6441\n",
      "Total loss:  -0.4842 | PDE Loss:  -0.9954 | Function Loss:  -1.6442\n",
      "Total loss:  -0.4843 | PDE Loss:  -0.9955 | Function Loss:  -1.6443\n",
      "Total loss:  -0.4844 | PDE Loss:  -0.9957 | Function Loss:  -1.6444\n",
      "Total loss:  -0.4846 | PDE Loss:  -0.9959 | Function Loss:  -1.6445\n",
      "Total loss:  -0.4845 | PDE Loss:  -0.995 | Function Loss:  -1.6448\n",
      "Total loss:  -0.4847 | PDE Loss:  -0.9957 | Function Loss:  -1.6448\n",
      "Total loss:  -0.4848 | PDE Loss:  -0.9961 | Function Loss:  -1.6448\n",
      "Total loss:  -0.485 | PDE Loss:  -0.9965 | Function Loss:  -1.6448\n",
      "Total loss:  -0.4851 | PDE Loss:  -0.9971 | Function Loss:  -1.6447\n",
      "Total loss:  -0.4852 | PDE Loss:  -0.9973 | Function Loss:  -1.6448\n",
      "Total loss:  -0.4853 | PDE Loss:  -0.9975 | Function Loss:  -1.6449\n",
      "Total loss:  -0.4854 | PDE Loss:  -0.9975 | Function Loss:  -1.645\n",
      "Total loss:  -0.4855 | PDE Loss:  -0.9975 | Function Loss:  -1.6451\n",
      "Total loss:  -0.4856 | PDE Loss:  -0.9973 | Function Loss:  -1.6453\n",
      "Total loss:  -0.4857 | PDE Loss:  -0.9974 | Function Loss:  -1.6455\n",
      "Total loss:  -0.4858 | PDE Loss:  -0.9967 | Function Loss:  -1.646\n",
      "Total loss:  -0.4859 | PDE Loss:  -0.9968 | Function Loss:  -1.646\n",
      "Total loss:  -0.486 | PDE Loss:  -0.9969 | Function Loss:  -1.6461\n",
      "Total loss:  -0.4861 | PDE Loss:  -0.997 | Function Loss:  -1.6462\n",
      "Total loss:  -0.4862 | PDE Loss:  -0.9967 | Function Loss:  -1.6465\n",
      "Total loss:  -0.4863 | PDE Loss:  -0.9969 | Function Loss:  -1.6465\n",
      "Total loss:  -0.4863 | PDE Loss:  -0.9971 | Function Loss:  -1.6466\n",
      "Total loss:  -0.4864 | PDE Loss:  -0.9972 | Function Loss:  -1.6466\n",
      "Total loss:  -0.4865 | PDE Loss:  -0.9972 | Function Loss:  -1.6467\n",
      "Total loss:  -0.4865 | PDE Loss:  -0.9972 | Function Loss:  -1.6467\n",
      "Total loss:  -0.4866 | PDE Loss:  -0.9969 | Function Loss:  -1.647\n",
      "Total loss:  -0.4867 | PDE Loss:  -0.9971 | Function Loss:  -1.647\n",
      "Total loss:  -0.4867 | PDE Loss:  -0.9972 | Function Loss:  -1.647\n",
      "Total loss:  -0.4868 | PDE Loss:  -0.9973 | Function Loss:  -1.6471\n",
      "Total loss:  -0.4869 | PDE Loss:  -0.9975 | Function Loss:  -1.6471\n",
      "Total loss:  -0.487 | PDE Loss:  -0.9978 | Function Loss:  -1.6471\n",
      "Total loss:  -0.4871 | PDE Loss:  -0.9983 | Function Loss:  -1.647\n",
      "Total loss:  -0.4872 | PDE Loss:  -0.9987 | Function Loss:  -1.647\n",
      "Total loss:  -0.4872 | PDE Loss:  -0.999 | Function Loss:  -1.647\n",
      "Total loss:  -0.4873 | PDE Loss:  -0.9987 | Function Loss:  -1.6473\n",
      "Total loss:  -0.4874 | PDE Loss:  -0.9987 | Function Loss:  -1.6474\n",
      "Total loss:  -0.4876 | PDE Loss:  -0.9989 | Function Loss:  -1.6476\n",
      "Total loss:  -0.4877 | PDE Loss:  -0.9987 | Function Loss:  -1.6478\n",
      "Total loss:  -0.4878 | PDE Loss:  -0.9988 | Function Loss:  -1.6479\n",
      "Total loss:  -0.4879 | PDE Loss:  -0.9989 | Function Loss:  -1.6479\n",
      "Total loss:  -0.4879 | PDE Loss:  -0.9978 | Function Loss:  -1.6485\n",
      "Total loss:  -0.488 | PDE Loss:  -0.998 | Function Loss:  -1.6485\n",
      "Total loss:  -0.4881 | PDE Loss:  -0.9983 | Function Loss:  -1.6485\n",
      "Total loss:  -0.4881 | PDE Loss:  -0.9986 | Function Loss:  -1.6484\n",
      "Total loss:  -0.4882 | PDE Loss:  -0.999 | Function Loss:  -1.6484\n",
      "Total loss:  -0.4883 | PDE Loss:  -0.9995 | Function Loss:  -1.6483\n",
      "Total loss:  -0.4884 | PDE Loss:  -0.9999 | Function Loss:  -1.6483\n",
      "Total loss:  -0.4886 | PDE Loss:  -1.0005 | Function Loss:  -1.6483\n",
      "Total loss:  -0.4887 | PDE Loss:  -1.0008 | Function Loss:  -1.6483\n",
      "Total loss:  -0.4888 | PDE Loss:  -1.001 | Function Loss:  -1.6484\n",
      "Total loss:  -0.4889 | PDE Loss:  -1.0013 | Function Loss:  -1.6484\n",
      "Total loss:  -0.489 | PDE Loss:  -1.0012 | Function Loss:  -1.6486\n",
      "Total loss:  -0.4891 | PDE Loss:  -1.0013 | Function Loss:  -1.6487\n",
      "Total loss:  -0.4892 | PDE Loss:  -1.0012 | Function Loss:  -1.6488\n",
      "Total loss:  -0.4893 | PDE Loss:  -1.0012 | Function Loss:  -1.6489\n",
      "Total loss:  -0.4894 | PDE Loss:  -1.0011 | Function Loss:  -1.6492\n",
      "Total loss:  -0.4896 | PDE Loss:  -1.0014 | Function Loss:  -1.6493\n",
      "Total loss:  -0.4896 | PDE Loss:  -1.0011 | Function Loss:  -1.6495\n",
      "Total loss:  -0.4897 | PDE Loss:  -1.0015 | Function Loss:  -1.6495\n",
      "Total loss:  -0.4898 | PDE Loss:  -1.002 | Function Loss:  -1.6494\n",
      "Total loss:  -0.4899 | PDE Loss:  -1.0024 | Function Loss:  -1.6493\n",
      "Total loss:  -0.49 | PDE Loss:  -1.0028 | Function Loss:  -1.6492\n",
      "Total loss:  -0.49 | PDE Loss:  -1.0031 | Function Loss:  -1.6492\n",
      "Total loss:  -0.4901 | PDE Loss:  -1.0035 | Function Loss:  -1.6492\n",
      "Total loss:  -0.4902 | PDE Loss:  -1.0037 | Function Loss:  -1.6492\n",
      "Total loss:  -0.4904 | PDE Loss:  -1.0042 | Function Loss:  -1.6492\n",
      "Total loss:  -0.4905 | PDE Loss:  -1.004 | Function Loss:  -1.6495\n",
      "Total loss:  -0.4906 | PDE Loss:  -1.0039 | Function Loss:  -1.6497\n",
      "Total loss:  -0.4908 | PDE Loss:  -1.0037 | Function Loss:  -1.65\n",
      "Total loss:  -0.4909 | PDE Loss:  -1.0035 | Function Loss:  -1.6503\n",
      "Total loss:  -0.491 | PDE Loss:  -1.0033 | Function Loss:  -1.6505\n",
      "Total loss:  -0.4911 | PDE Loss:  -1.0034 | Function Loss:  -1.6507\n",
      "Total loss:  -0.4912 | PDE Loss:  -1.0032 | Function Loss:  -1.6509\n",
      "Total loss:  -0.4913 | PDE Loss:  -1.0034 | Function Loss:  -1.651\n",
      "Total loss:  -0.4915 | PDE Loss:  -1.0035 | Function Loss:  -1.6511\n",
      "Total loss:  -0.4916 | PDE Loss:  -1.0037 | Function Loss:  -1.6512\n",
      "Total loss:  -0.4917 | PDE Loss:  -1.0041 | Function Loss:  -1.6512\n",
      "Total loss:  -0.4918 | PDE Loss:  -1.0044 | Function Loss:  -1.6512\n",
      "Total loss:  -0.4919 | PDE Loss:  -1.005 | Function Loss:  -1.6511\n",
      "Total loss:  -0.4921 | PDE Loss:  -1.0053 | Function Loss:  -1.6512\n",
      "Total loss:  -0.4921 | PDE Loss:  -1.0055 | Function Loss:  -1.6512\n",
      "Total loss:  -0.4922 | PDE Loss:  -1.0057 | Function Loss:  -1.6512\n",
      "Total loss:  -0.4923 | PDE Loss:  -1.0058 | Function Loss:  -1.6513\n",
      "Total loss:  -0.4923 | PDE Loss:  -1.0058 | Function Loss:  -1.6513\n",
      "Total loss:  -0.4924 | PDE Loss:  -1.0057 | Function Loss:  -1.6514\n",
      "Total loss:  -0.4924 | PDE Loss:  -1.0057 | Function Loss:  -1.6515\n",
      "Total loss:  -0.4925 | PDE Loss:  -1.0056 | Function Loss:  -1.6517\n",
      "Total loss:  -0.4926 | PDE Loss:  -1.0056 | Function Loss:  -1.6518\n",
      "Total loss:  -0.4928 | PDE Loss:  -1.0057 | Function Loss:  -1.652\n",
      "Total loss:  -0.4929 | PDE Loss:  -1.0057 | Function Loss:  -1.6522\n",
      "Total loss:  -0.493 | PDE Loss:  -1.0059 | Function Loss:  -1.6523\n",
      "Total loss:  -0.4931 | PDE Loss:  -1.0059 | Function Loss:  -1.6524\n",
      "Total loss:  -0.4932 | PDE Loss:  -1.006 | Function Loss:  -1.6524\n",
      "Total loss:  -0.4933 | PDE Loss:  -1.0061 | Function Loss:  -1.6525\n",
      "Total loss:  -0.4933 | PDE Loss:  -1.0062 | Function Loss:  -1.6526\n",
      "Total loss:  -0.4934 | PDE Loss:  -1.0062 | Function Loss:  -1.6527\n",
      "Total loss:  -0.4934 | PDE Loss:  -1.0055 | Function Loss:  -1.6529\n",
      "Total loss:  -0.4935 | PDE Loss:  -1.006 | Function Loss:  -1.6529\n",
      "Total loss:  -0.4936 | PDE Loss:  -1.0059 | Function Loss:  -1.6531\n",
      "Total loss:  -0.4937 | PDE Loss:  -1.0058 | Function Loss:  -1.6533\n",
      "Total loss:  -0.4938 | PDE Loss:  -1.0055 | Function Loss:  -1.6536\n",
      "Total loss:  -0.4939 | PDE Loss:  -1.0053 | Function Loss:  -1.6538\n",
      "Total loss:  -0.494 | PDE Loss:  -1.0051 | Function Loss:  -1.6541\n",
      "Total loss:  -0.4942 | PDE Loss:  -1.005 | Function Loss:  -1.6544\n",
      "Total loss:  -0.4944 | PDE Loss:  -1.005 | Function Loss:  -1.6546\n",
      "Total loss:  -0.4946 | PDE Loss:  -1.0051 | Function Loss:  -1.6549\n",
      "Total loss:  -0.4948 | PDE Loss:  -1.0055 | Function Loss:  -1.655\n",
      "Total loss:  -0.495 | PDE Loss:  -1.0058 | Function Loss:  -1.6551\n",
      "Total loss:  -0.4951 | PDE Loss:  -1.0065 | Function Loss:  -1.6551\n",
      "Total loss:  -0.4953 | PDE Loss:  -1.0071 | Function Loss:  -1.6551\n",
      "Total loss:  -0.4954 | PDE Loss:  -1.0078 | Function Loss:  -1.6549\n",
      "Total loss:  -0.4956 | PDE Loss:  -1.0084 | Function Loss:  -1.6549\n",
      "Total loss:  -0.4957 | PDE Loss:  -1.0094 | Function Loss:  -1.6546\n",
      "Total loss:  -0.4958 | PDE Loss:  -1.0099 | Function Loss:  -1.6545\n",
      "Total loss:  -0.4959 | PDE Loss:  -1.0105 | Function Loss:  -1.6544\n",
      "Total loss:  -0.496 | PDE Loss:  -1.0112 | Function Loss:  -1.6542\n",
      "Total loss:  -0.4961 | PDE Loss:  -1.0117 | Function Loss:  -1.6541\n",
      "Total loss:  -0.4962 | PDE Loss:  -1.0123 | Function Loss:  -1.654\n",
      "Total loss:  -0.4963 | PDE Loss:  -1.0128 | Function Loss:  -1.654\n",
      "Total loss:  -0.4965 | PDE Loss:  -1.0134 | Function Loss:  -1.654\n",
      "Total loss:  -0.4967 | PDE Loss:  -1.0137 | Function Loss:  -1.6541\n",
      "Total loss:  -0.4969 | PDE Loss:  -1.0138 | Function Loss:  -1.6543\n",
      "Total loss:  -0.497 | PDE Loss:  -1.0139 | Function Loss:  -1.6545\n",
      "Total loss:  -0.4972 | PDE Loss:  -1.0142 | Function Loss:  -1.6547\n",
      "Total loss:  -0.4974 | PDE Loss:  -1.0144 | Function Loss:  -1.6549\n",
      "Total loss:  -0.4976 | PDE Loss:  -1.0145 | Function Loss:  -1.655\n",
      "Total loss:  -0.4977 | PDE Loss:  -1.0146 | Function Loss:  -1.6552\n",
      "Total loss:  -0.4978 | PDE Loss:  -1.0148 | Function Loss:  -1.6553\n",
      "Total loss:  -0.4979 | PDE Loss:  -1.0149 | Function Loss:  -1.6554\n",
      "Total loss:  -0.498 | PDE Loss:  -1.0151 | Function Loss:  -1.6555\n",
      "Total loss:  -0.4982 | PDE Loss:  -1.0154 | Function Loss:  -1.6556\n",
      "Total loss:  -0.4984 | PDE Loss:  -1.0162 | Function Loss:  -1.6556\n",
      "Total loss:  -0.4986 | PDE Loss:  -1.0162 | Function Loss:  -1.6559\n",
      "Total loss:  -0.4988 | PDE Loss:  -1.0166 | Function Loss:  -1.6559\n",
      "Total loss:  -0.4989 | PDE Loss:  -1.0166 | Function Loss:  -1.6561\n",
      "Total loss:  -0.4991 | PDE Loss:  -1.0168 | Function Loss:  -1.6562\n",
      "Total loss:  -0.4992 | PDE Loss:  -1.0169 | Function Loss:  -1.6563\n",
      "Total loss:  -0.4993 | PDE Loss:  -1.0172 | Function Loss:  -1.6564\n",
      "Total loss:  -0.4995 | PDE Loss:  -1.0173 | Function Loss:  -1.6565\n",
      "Total loss:  -0.4996 | PDE Loss:  -1.0174 | Function Loss:  -1.6567\n",
      "Total loss:  -0.4998 | PDE Loss:  -1.0178 | Function Loss:  -1.6567\n",
      "Total loss:  -0.4999 | PDE Loss:  -1.0179 | Function Loss:  -1.6569\n",
      "Total loss:  -0.5 | PDE Loss:  -1.0184 | Function Loss:  -1.6569\n",
      "Total loss:  -0.5002 | PDE Loss:  -1.0188 | Function Loss:  -1.657\n",
      "Total loss:  -0.5003 | PDE Loss:  -1.0192 | Function Loss:  -1.657\n",
      "Total loss:  -0.5005 | PDE Loss:  -1.0195 | Function Loss:  -1.657\n",
      "Total loss:  -0.5006 | PDE Loss:  -1.0198 | Function Loss:  -1.657\n",
      "Total loss:  -0.5007 | PDE Loss:  -1.0201 | Function Loss:  -1.6571\n",
      "Total loss:  -0.5008 | PDE Loss:  -1.0204 | Function Loss:  -1.6571\n",
      "Total loss:  -0.5009 | PDE Loss:  -1.0206 | Function Loss:  -1.6572\n",
      "Total loss:  -0.501 | PDE Loss:  -1.0211 | Function Loss:  -1.6571\n",
      "Total loss:  -0.5011 | PDE Loss:  -1.0215 | Function Loss:  -1.6571\n",
      "Total loss:  -0.5012 | PDE Loss:  -1.022 | Function Loss:  -1.657\n",
      "Total loss:  -0.5013 | PDE Loss:  -1.0225 | Function Loss:  -1.6569\n",
      "Total loss:  -0.5014 | PDE Loss:  -1.023 | Function Loss:  -1.6568\n",
      "Total loss:  -0.5015 | PDE Loss:  -1.0235 | Function Loss:  -1.6567\n",
      "Total loss:  -0.5016 | PDE Loss:  -1.0241 | Function Loss:  -1.6566\n",
      "Total loss:  -0.5017 | PDE Loss:  -1.0244 | Function Loss:  -1.6566\n",
      "Total loss:  -0.5018 | PDE Loss:  -1.0248 | Function Loss:  -1.6567\n",
      "Total loss:  -0.5019 | PDE Loss:  -1.025 | Function Loss:  -1.6568\n",
      "Total loss:  -0.502 | PDE Loss:  -1.025 | Function Loss:  -1.6569\n",
      "Total loss:  -0.5021 | PDE Loss:  -1.025 | Function Loss:  -1.657\n",
      "Total loss:  -0.5022 | PDE Loss:  -1.0248 | Function Loss:  -1.6573\n",
      "Total loss:  -0.5023 | PDE Loss:  -1.0246 | Function Loss:  -1.6574\n",
      "Total loss:  -0.5024 | PDE Loss:  -1.0246 | Function Loss:  -1.6575\n",
      "Total loss:  -0.5025 | PDE Loss:  -1.0246 | Function Loss:  -1.6577\n",
      "Total loss:  -0.5026 | PDE Loss:  -1.0247 | Function Loss:  -1.6578\n",
      "Total loss:  -0.5027 | PDE Loss:  -1.0249 | Function Loss:  -1.6578\n",
      "Total loss:  -0.5028 | PDE Loss:  -1.0252 | Function Loss:  -1.6579\n",
      "Total loss:  -0.5029 | PDE Loss:  -1.0261 | Function Loss:  -1.6577\n",
      "Total loss:  -0.5031 | PDE Loss:  -1.0265 | Function Loss:  -1.6577\n",
      "Total loss:  -0.5032 | PDE Loss:  -1.0278 | Function Loss:  -1.6573\n",
      "Total loss:  -0.5033 | PDE Loss:  -1.0288 | Function Loss:  -1.657\n",
      "Total loss:  -0.5034 | PDE Loss:  -1.0286 | Function Loss:  -1.6572\n",
      "Total loss:  -0.5035 | PDE Loss:  -1.0287 | Function Loss:  -1.6574\n",
      "Total loss:  -0.5036 | PDE Loss:  -1.0289 | Function Loss:  -1.6575\n",
      "Total loss:  -0.5037 | PDE Loss:  -1.0292 | Function Loss:  -1.6575\n",
      "Total loss:  -0.5038 | PDE Loss:  -1.0296 | Function Loss:  -1.6574\n",
      "Total loss:  -0.5038 | PDE Loss:  -1.0303 | Function Loss:  -1.6572\n",
      "Total loss:  -0.504 | PDE Loss:  -1.0313 | Function Loss:  -1.657\n",
      "Total loss:  -0.5041 | PDE Loss:  -1.0322 | Function Loss:  -1.6568\n",
      "Total loss:  -0.5044 | PDE Loss:  -1.0342 | Function Loss:  -1.6563\n",
      "Total loss:  -0.5045 | PDE Loss:  -1.0351 | Function Loss:  -1.6562\n",
      "Total loss:  -0.5047 | PDE Loss:  -1.0358 | Function Loss:  -1.6561\n",
      "Total loss:  -0.5049 | PDE Loss:  -1.0364 | Function Loss:  -1.6561\n",
      "Total loss:  -0.505 | PDE Loss:  -1.0369 | Function Loss:  -1.6561\n",
      "Total loss:  -0.5051 | PDE Loss:  -1.037 | Function Loss:  -1.6562\n",
      "Total loss:  -0.5051 | PDE Loss:  -1.0371 | Function Loss:  -1.6562\n",
      "Total loss:  -0.5052 | PDE Loss:  -1.037 | Function Loss:  -1.6564\n",
      "Total loss:  -0.5053 | PDE Loss:  -1.037 | Function Loss:  -1.6565\n",
      "Total loss:  -0.5054 | PDE Loss:  -1.0371 | Function Loss:  -1.6566\n",
      "Total loss:  -0.5055 | PDE Loss:  -1.0375 | Function Loss:  -1.6566\n",
      "Total loss:  -0.5056 | PDE Loss:  -1.0376 | Function Loss:  -1.6567\n",
      "Total loss:  -0.5057 | PDE Loss:  -1.0387 | Function Loss:  -1.6564\n",
      "Total loss:  -0.5058 | PDE Loss:  -1.039 | Function Loss:  -1.6564\n",
      "Total loss:  -0.5059 | PDE Loss:  -1.0391 | Function Loss:  -1.6565\n",
      "Total loss:  -0.506 | PDE Loss:  -1.0393 | Function Loss:  -1.6566\n",
      "Total loss:  -0.5061 | PDE Loss:  -1.0394 | Function Loss:  -1.6567\n",
      "Total loss:  -0.5062 | PDE Loss:  -1.0396 | Function Loss:  -1.6567\n",
      "Total loss:  -0.5063 | PDE Loss:  -1.0398 | Function Loss:  -1.6568\n",
      "Total loss:  -0.5064 | PDE Loss:  -1.04 | Function Loss:  -1.6568\n",
      "Total loss:  -0.5065 | PDE Loss:  -1.0404 | Function Loss:  -1.6568\n",
      "Total loss:  -0.5066 | PDE Loss:  -1.0409 | Function Loss:  -1.6567\n",
      "Total loss:  -0.5067 | PDE Loss:  -1.0415 | Function Loss:  -1.6566\n",
      "Total loss:  -0.5069 | PDE Loss:  -1.0421 | Function Loss:  -1.6566\n",
      "Total loss:  -0.507 | PDE Loss:  -1.0426 | Function Loss:  -1.6566\n",
      "Total loss:  -0.5071 | PDE Loss:  -1.0429 | Function Loss:  -1.6566\n",
      "Total loss:  -0.5073 | PDE Loss:  -1.0431 | Function Loss:  -1.6568\n",
      "Total loss:  -0.5075 | PDE Loss:  -1.0434 | Function Loss:  -1.6569\n",
      "Total loss:  -0.5076 | PDE Loss:  -1.0432 | Function Loss:  -1.6572\n",
      "Total loss:  -0.5078 | PDE Loss:  -1.0433 | Function Loss:  -1.6574\n",
      "Total loss:  -0.5079 | PDE Loss:  -1.0433 | Function Loss:  -1.6576\n",
      "Total loss:  -0.5081 | PDE Loss:  -1.0433 | Function Loss:  -1.6577\n",
      "Total loss:  -0.5082 | PDE Loss:  -1.0435 | Function Loss:  -1.6579\n",
      "Total loss:  -0.5084 | PDE Loss:  -1.0438 | Function Loss:  -1.658\n",
      "Total loss:  -0.5084 | PDE Loss:  -1.044 | Function Loss:  -1.658\n",
      "Total loss:  -0.5086 | PDE Loss:  -1.0448 | Function Loss:  -1.6579\n",
      "Total loss:  -0.5087 | PDE Loss:  -1.0456 | Function Loss:  -1.6577\n",
      "Total loss:  -0.5089 | PDE Loss:  -1.0465 | Function Loss:  -1.6576\n",
      "Total loss:  -0.509 | PDE Loss:  -1.0475 | Function Loss:  -1.6574\n",
      "Total loss:  -0.5092 | PDE Loss:  -1.0479 | Function Loss:  -1.6574\n",
      "Total loss:  -0.5092 | PDE Loss:  -1.0483 | Function Loss:  -1.6574\n",
      "Total loss:  -0.5093 | PDE Loss:  -1.0485 | Function Loss:  -1.6575\n",
      "Total loss:  -0.5094 | PDE Loss:  -1.0484 | Function Loss:  -1.6576\n",
      "Total loss:  -0.5095 | PDE Loss:  -1.0481 | Function Loss:  -1.6578\n",
      "Total loss:  -0.5096 | PDE Loss:  -1.0478 | Function Loss:  -1.6581\n",
      "Total loss:  -0.5097 | PDE Loss:  -1.0474 | Function Loss:  -1.6583\n",
      "Total loss:  -0.5098 | PDE Loss:  -1.0469 | Function Loss:  -1.6587\n",
      "Total loss:  -0.5099 | PDE Loss:  -1.0469 | Function Loss:  -1.6589\n",
      "Total loss:  -0.51 | PDE Loss:  -1.0459 | Function Loss:  -1.6594\n",
      "Total loss:  -0.5101 | PDE Loss:  -1.0463 | Function Loss:  -1.6594\n",
      "Total loss:  -0.5102 | PDE Loss:  -1.0462 | Function Loss:  -1.6595\n",
      "Total loss:  -0.5103 | PDE Loss:  -1.0466 | Function Loss:  -1.6595\n",
      "Total loss:  -0.5104 | PDE Loss:  -1.0472 | Function Loss:  -1.6595\n",
      "Total loss:  -0.5105 | PDE Loss:  -1.0478 | Function Loss:  -1.6594\n",
      "Total loss:  -0.5106 | PDE Loss:  -1.0486 | Function Loss:  -1.6592\n",
      "Total loss:  -0.5108 | PDE Loss:  -1.0498 | Function Loss:  -1.659\n",
      "Total loss:  -0.511 | PDE Loss:  -1.05 | Function Loss:  -1.6592\n",
      "Total loss:  -0.5112 | PDE Loss:  -1.0503 | Function Loss:  -1.6593\n",
      "Total loss:  -0.5114 | PDE Loss:  -1.0502 | Function Loss:  -1.6596\n",
      "Total loss:  -0.5116 | PDE Loss:  -1.0498 | Function Loss:  -1.66\n",
      "Total loss:  -0.5117 | PDE Loss:  -1.0492 | Function Loss:  -1.6604\n",
      "Total loss:  -0.5117 | PDE Loss:  -1.049 | Function Loss:  -1.6606\n",
      "Total loss:  -0.5117 | PDE Loss:  -1.0472 | Function Loss:  -1.6614\n",
      "Total loss:  -0.5118 | PDE Loss:  -1.0474 | Function Loss:  -1.6614\n",
      "Total loss:  -0.5119 | PDE Loss:  -1.0477 | Function Loss:  -1.6614\n",
      "Total loss:  -0.512 | PDE Loss:  -1.0478 | Function Loss:  -1.6615\n",
      "Total loss:  -0.5121 | PDE Loss:  -1.0479 | Function Loss:  -1.6615\n",
      "Total loss:  -0.5121 | PDE Loss:  -1.0479 | Function Loss:  -1.6616\n",
      "Total loss:  -0.5122 | PDE Loss:  -1.048 | Function Loss:  -1.6617\n",
      "Total loss:  -0.5123 | PDE Loss:  -1.0485 | Function Loss:  -1.6616\n",
      "Total loss:  -0.5124 | PDE Loss:  -1.0486 | Function Loss:  -1.6617\n",
      "Total loss:  -0.5125 | PDE Loss:  -1.0487 | Function Loss:  -1.6618\n",
      "Total loss:  -0.5127 | PDE Loss:  -1.0489 | Function Loss:  -1.662\n",
      "Total loss:  -0.5128 | PDE Loss:  -1.0493 | Function Loss:  -1.662\n",
      "Total loss:  -0.513 | PDE Loss:  -1.0496 | Function Loss:  -1.6621\n",
      "Total loss:  -0.5131 | PDE Loss:  -1.0499 | Function Loss:  -1.6622\n",
      "Total loss:  -0.5132 | PDE Loss:  -1.0503 | Function Loss:  -1.6622\n",
      "Total loss:  -0.5133 | PDE Loss:  -1.0506 | Function Loss:  -1.6622\n",
      "Total loss:  -0.5134 | PDE Loss:  -1.0509 | Function Loss:  -1.6622\n",
      "Total loss:  -0.5135 | PDE Loss:  -1.051 | Function Loss:  -1.6623\n",
      "Total loss:  -0.5136 | PDE Loss:  -1.0512 | Function Loss:  -1.6623\n",
      "Total loss:  -0.5136 | PDE Loss:  -1.0512 | Function Loss:  -1.6624\n",
      "Total loss:  -0.5137 | PDE Loss:  -1.0511 | Function Loss:  -1.6625\n",
      "Total loss:  -0.5138 | PDE Loss:  -1.0509 | Function Loss:  -1.6627\n",
      "Total loss:  -0.5139 | PDE Loss:  -1.0506 | Function Loss:  -1.663\n",
      "Total loss:  -0.514 | PDE Loss:  -1.0504 | Function Loss:  -1.6632\n",
      "Total loss:  -0.514 | PDE Loss:  -1.0502 | Function Loss:  -1.6634\n",
      "Total loss:  -0.5141 | PDE Loss:  -1.0499 | Function Loss:  -1.6636\n",
      "Total loss:  -0.5141 | PDE Loss:  -1.05 | Function Loss:  -1.6636\n",
      "Total loss:  -0.5142 | PDE Loss:  -1.0502 | Function Loss:  -1.6636\n",
      "Total loss:  -0.5144 | PDE Loss:  -1.0508 | Function Loss:  -1.6636\n",
      "Total loss:  -0.5146 | PDE Loss:  -1.0516 | Function Loss:  -1.6635\n",
      "Total loss:  -0.5148 | PDE Loss:  -1.0525 | Function Loss:  -1.6635\n",
      "Total loss:  -0.5149 | PDE Loss:  -1.0534 | Function Loss:  -1.6633\n",
      "Total loss:  -0.5151 | PDE Loss:  -1.0543 | Function Loss:  -1.6632\n",
      "Total loss:  -0.5153 | PDE Loss:  -1.0552 | Function Loss:  -1.6631\n",
      "Total loss:  -0.5154 | PDE Loss:  -1.0559 | Function Loss:  -1.663\n",
      "Total loss:  -0.5156 | PDE Loss:  -1.0564 | Function Loss:  -1.663\n",
      "Total loss:  -0.5157 | PDE Loss:  -1.0567 | Function Loss:  -1.6631\n",
      "Total loss:  -0.5159 | PDE Loss:  -1.0567 | Function Loss:  -1.6633\n",
      "Total loss:  -0.5161 | PDE Loss:  -1.0566 | Function Loss:  -1.6636\n",
      "Total loss:  -0.5162 | PDE Loss:  -1.0564 | Function Loss:  -1.6639\n",
      "Total loss:  -0.5163 | PDE Loss:  -1.0561 | Function Loss:  -1.6642\n",
      "Total loss:  -0.5164 | PDE Loss:  -1.0561 | Function Loss:  -1.6643\n",
      "Total loss:  -0.5165 | PDE Loss:  -1.0563 | Function Loss:  -1.6644\n",
      "Total loss:  -0.5141 | PDE Loss:  -1.0505 | Function Loss:  -1.6633\n",
      "Total loss:  -0.5167 | PDE Loss:  -1.0562 | Function Loss:  -1.6646\n",
      "Total loss:  -0.5168 | PDE Loss:  -1.0566 | Function Loss:  -1.6646\n",
      "Total loss:  -0.517 | PDE Loss:  -1.0575 | Function Loss:  -1.6646\n",
      "Total loss:  -0.5172 | PDE Loss:  -1.0584 | Function Loss:  -1.6645\n",
      "Total loss:  -0.5174 | PDE Loss:  -1.0592 | Function Loss:  -1.6644\n",
      "Total loss:  -0.5175 | PDE Loss:  -1.0598 | Function Loss:  -1.6644\n",
      "Total loss:  -0.5177 | PDE Loss:  -1.0603 | Function Loss:  -1.6644\n",
      "Total loss:  -0.5179 | PDE Loss:  -1.0608 | Function Loss:  -1.6645\n",
      "Total loss:  -0.5181 | PDE Loss:  -1.0608 | Function Loss:  -1.6648\n",
      "Total loss:  -0.5183 | PDE Loss:  -1.0609 | Function Loss:  -1.665\n",
      "Total loss:  -0.5184 | PDE Loss:  -1.0612 | Function Loss:  -1.665\n",
      "Total loss:  -0.5186 | PDE Loss:  -1.0615 | Function Loss:  -1.6652\n",
      "Total loss:  -0.5187 | PDE Loss:  -1.062 | Function Loss:  -1.6651\n",
      "Total loss:  -0.5188 | PDE Loss:  -1.0624 | Function Loss:  -1.6652\n",
      "Total loss:  -0.5189 | PDE Loss:  -1.0628 | Function Loss:  -1.6651\n",
      "Total loss:  -0.519 | PDE Loss:  -1.0633 | Function Loss:  -1.6651\n",
      "Total loss:  -0.5192 | PDE Loss:  -1.0637 | Function Loss:  -1.6651\n",
      "Total loss:  -0.5194 | PDE Loss:  -1.0646 | Function Loss:  -1.6651\n",
      "Total loss:  -0.5196 | PDE Loss:  -1.0647 | Function Loss:  -1.6653\n",
      "Total loss:  -0.5198 | PDE Loss:  -1.0656 | Function Loss:  -1.6652\n",
      "Total loss:  -0.5199 | PDE Loss:  -1.0661 | Function Loss:  -1.6652\n",
      "Total loss:  -0.5202 | PDE Loss:  -1.0667 | Function Loss:  -1.6653\n",
      "Total loss:  -0.5204 | PDE Loss:  -1.067 | Function Loss:  -1.6655\n",
      "Total loss:  -0.5206 | PDE Loss:  -1.0677 | Function Loss:  -1.6654\n",
      "Total loss:  -0.5208 | PDE Loss:  -1.0678 | Function Loss:  -1.6657\n",
      "Total loss:  -0.5209 | PDE Loss:  -1.0681 | Function Loss:  -1.6658\n",
      "Total loss:  -0.521 | PDE Loss:  -1.0685 | Function Loss:  -1.6658\n",
      "Total loss:  -0.5212 | PDE Loss:  -1.0688 | Function Loss:  -1.6659\n",
      "Total loss:  -0.5213 | PDE Loss:  -1.0691 | Function Loss:  -1.6659\n",
      "Total loss:  -0.5214 | PDE Loss:  -1.0694 | Function Loss:  -1.666\n",
      "Total loss:  -0.5215 | PDE Loss:  -1.0696 | Function Loss:  -1.6661\n",
      "Total loss:  -0.5217 | PDE Loss:  -1.0696 | Function Loss:  -1.6663\n",
      "Total loss:  -0.5218 | PDE Loss:  -1.0694 | Function Loss:  -1.6665\n",
      "Total loss:  -0.5219 | PDE Loss:  -1.0689 | Function Loss:  -1.6669\n",
      "Total loss:  -0.522 | PDE Loss:  -1.0685 | Function Loss:  -1.6672\n",
      "Total loss:  -0.5221 | PDE Loss:  -1.0681 | Function Loss:  -1.6675\n",
      "Total loss:  -0.5222 | PDE Loss:  -1.0678 | Function Loss:  -1.6678\n",
      "Total loss:  -0.5223 | PDE Loss:  -1.0675 | Function Loss:  -1.668\n",
      "Total loss:  -0.5224 | PDE Loss:  -1.0673 | Function Loss:  -1.6683\n",
      "Total loss:  -0.5225 | PDE Loss:  -1.0674 | Function Loss:  -1.6684\n",
      "Total loss:  -0.5226 | PDE Loss:  -1.0676 | Function Loss:  -1.6684\n",
      "Total loss:  -0.5227 | PDE Loss:  -1.068 | Function Loss:  -1.6684\n",
      "Total loss:  -0.5228 | PDE Loss:  -1.0682 | Function Loss:  -1.6684\n",
      "Total loss:  -0.5228 | PDE Loss:  -1.0698 | Function Loss:  -1.6678\n",
      "Total loss:  -0.5229 | PDE Loss:  -1.0696 | Function Loss:  -1.668\n",
      "Total loss:  -0.523 | PDE Loss:  -1.0694 | Function Loss:  -1.6681\n",
      "Total loss:  -0.523 | PDE Loss:  -1.0692 | Function Loss:  -1.6683\n",
      "Total loss:  -0.5231 | PDE Loss:  -1.0691 | Function Loss:  -1.6684\n",
      "Total loss:  -0.5232 | PDE Loss:  -1.069 | Function Loss:  -1.6686\n",
      "Total loss:  -0.5233 | PDE Loss:  -1.0688 | Function Loss:  -1.6688\n",
      "Total loss:  -0.5234 | PDE Loss:  -1.0687 | Function Loss:  -1.669\n",
      "Total loss:  -0.5235 | PDE Loss:  -1.0686 | Function Loss:  -1.6692\n",
      "Total loss:  -0.5236 | PDE Loss:  -1.0685 | Function Loss:  -1.6694\n",
      "Total loss:  -0.5237 | PDE Loss:  -1.0685 | Function Loss:  -1.6696\n",
      "Total loss:  -0.5239 | PDE Loss:  -1.0685 | Function Loss:  -1.6697\n",
      "Total loss:  -0.524 | PDE Loss:  -1.0684 | Function Loss:  -1.6699\n",
      "Total loss:  -0.524 | PDE Loss:  -1.0684 | Function Loss:  -1.67\n",
      "Total loss:  -0.5242 | PDE Loss:  -1.0685 | Function Loss:  -1.6702\n",
      "Total loss:  -0.5243 | PDE Loss:  -1.0685 | Function Loss:  -1.6704\n",
      "Total loss:  -0.5245 | PDE Loss:  -1.0685 | Function Loss:  -1.6706\n",
      "Total loss:  -0.5246 | PDE Loss:  -1.0685 | Function Loss:  -1.6708\n",
      "Total loss:  -0.5247 | PDE Loss:  -1.0683 | Function Loss:  -1.671\n",
      "Total loss:  -0.5247 | PDE Loss:  -1.0683 | Function Loss:  -1.6711\n",
      "Total loss:  -0.5249 | PDE Loss:  -1.0681 | Function Loss:  -1.6713\n",
      "Total loss:  -0.525 | PDE Loss:  -1.0679 | Function Loss:  -1.6715\n",
      "Total loss:  -0.5251 | PDE Loss:  -1.0677 | Function Loss:  -1.6718\n",
      "Total loss:  -0.5251 | PDE Loss:  -1.0674 | Function Loss:  -1.672\n",
      "Total loss:  -0.5252 | PDE Loss:  -1.067 | Function Loss:  -1.6723\n",
      "Total loss:  -0.5253 | PDE Loss:  -1.0665 | Function Loss:  -1.6726\n",
      "Total loss:  -0.5254 | PDE Loss:  -1.066 | Function Loss:  -1.6729\n",
      "Total loss:  -0.5255 | PDE Loss:  -1.0655 | Function Loss:  -1.6733\n",
      "Total loss:  -0.5256 | PDE Loss:  -1.0651 | Function Loss:  -1.6735\n",
      "Total loss:  -0.5257 | PDE Loss:  -1.0646 | Function Loss:  -1.6738\n",
      "Total loss:  -0.5257 | PDE Loss:  -1.0642 | Function Loss:  -1.6741\n",
      "Total loss:  -0.5258 | PDE Loss:  -1.0639 | Function Loss:  -1.6744\n",
      "Total loss:  -0.526 | PDE Loss:  -1.0639 | Function Loss:  -1.6746\n",
      "Total loss:  -0.5261 | PDE Loss:  -1.0634 | Function Loss:  -1.6749\n",
      "Total loss:  -0.5262 | PDE Loss:  -1.0637 | Function Loss:  -1.675\n",
      "Total loss:  -0.5263 | PDE Loss:  -1.0639 | Function Loss:  -1.6751\n",
      "Total loss:  -0.5265 | PDE Loss:  -1.0642 | Function Loss:  -1.6752\n",
      "Total loss:  -0.5266 | PDE Loss:  -1.0645 | Function Loss:  -1.6753\n",
      "Total loss:  -0.5265 | PDE Loss:  -1.0634 | Function Loss:  -1.6755\n",
      "Total loss:  -0.5267 | PDE Loss:  -1.0643 | Function Loss:  -1.6755\n",
      "Total loss:  -0.5268 | PDE Loss:  -1.0643 | Function Loss:  -1.6756\n",
      "Total loss:  -0.527 | PDE Loss:  -1.0645 | Function Loss:  -1.6758\n",
      "Total loss:  -0.5271 | PDE Loss:  -1.0644 | Function Loss:  -1.676\n",
      "Total loss:  -0.5273 | PDE Loss:  -1.0641 | Function Loss:  -1.6764\n",
      "Total loss:  -0.5274 | PDE Loss:  -1.0635 | Function Loss:  -1.6768\n",
      "Total loss:  -0.5275 | PDE Loss:  -1.0634 | Function Loss:  -1.6769\n",
      "Total loss:  -0.5276 | PDE Loss:  -1.0634 | Function Loss:  -1.6771\n",
      "Total loss:  -0.5277 | PDE Loss:  -1.0635 | Function Loss:  -1.6771\n",
      "Total loss:  -0.5277 | PDE Loss:  -1.0636 | Function Loss:  -1.6772\n",
      "Total loss:  -0.5278 | PDE Loss:  -1.0639 | Function Loss:  -1.6772\n",
      "Total loss:  -0.5279 | PDE Loss:  -1.064 | Function Loss:  -1.6773\n",
      "Total loss:  -0.528 | PDE Loss:  -1.0641 | Function Loss:  -1.6773\n",
      "Total loss:  -0.5281 | PDE Loss:  -1.064 | Function Loss:  -1.6775\n",
      "Total loss:  -0.5281 | PDE Loss:  -1.0639 | Function Loss:  -1.6776\n",
      "Total loss:  -0.5282 | PDE Loss:  -1.0639 | Function Loss:  -1.6777\n",
      "Total loss:  -0.5282 | PDE Loss:  -1.0637 | Function Loss:  -1.6779\n",
      "Total loss:  -0.5283 | PDE Loss:  -1.0636 | Function Loss:  -1.678\n",
      "Total loss:  -0.5284 | PDE Loss:  -1.0635 | Function Loss:  -1.6782\n",
      "Total loss:  -0.5285 | PDE Loss:  -1.0642 | Function Loss:  -1.6781\n",
      "Total loss:  -0.5287 | PDE Loss:  -1.064 | Function Loss:  -1.6784\n",
      "Total loss:  -0.5288 | PDE Loss:  -1.0643 | Function Loss:  -1.6784\n",
      "Total loss:  -0.5289 | PDE Loss:  -1.0648 | Function Loss:  -1.6784\n",
      "Total loss:  -0.5291 | PDE Loss:  -1.0649 | Function Loss:  -1.6786\n",
      "Total loss:  -0.5293 | PDE Loss:  -1.0654 | Function Loss:  -1.6786\n",
      "Total loss:  -0.5294 | PDE Loss:  -1.0659 | Function Loss:  -1.6785\n",
      "Total loss:  -0.5295 | PDE Loss:  -1.0662 | Function Loss:  -1.6786\n",
      "Total loss:  -0.5296 | PDE Loss:  -1.0664 | Function Loss:  -1.6786\n",
      "Total loss:  -0.5297 | PDE Loss:  -1.0667 | Function Loss:  -1.6787\n",
      "Total loss:  -0.5298 | PDE Loss:  -1.0669 | Function Loss:  -1.6788\n",
      "Total loss:  -0.5299 | PDE Loss:  -1.0669 | Function Loss:  -1.6789\n",
      "Total loss:  -0.53 | PDE Loss:  -1.0669 | Function Loss:  -1.6791\n",
      "Total loss:  -0.5301 | PDE Loss:  -1.0666 | Function Loss:  -1.6793\n",
      "Total loss:  -0.5302 | PDE Loss:  -1.0664 | Function Loss:  -1.6795\n",
      "Total loss:  -0.5303 | PDE Loss:  -1.0661 | Function Loss:  -1.6797\n",
      "Total loss:  -0.5303 | PDE Loss:  -1.0659 | Function Loss:  -1.6799\n",
      "Total loss:  -0.5304 | PDE Loss:  -1.0658 | Function Loss:  -1.68\n",
      "Total loss:  -0.5305 | PDE Loss:  -1.0659 | Function Loss:  -1.6801\n",
      "Total loss:  -0.5305 | PDE Loss:  -1.0661 | Function Loss:  -1.6801\n",
      "Total loss:  -0.5306 | PDE Loss:  -1.066 | Function Loss:  -1.6802\n",
      "Total loss:  -0.5307 | PDE Loss:  -1.0665 | Function Loss:  -1.6802\n",
      "Total loss:  -0.5307 | PDE Loss:  -1.0669 | Function Loss:  -1.6801\n",
      "Total loss:  -0.5308 | PDE Loss:  -1.0675 | Function Loss:  -1.68\n",
      "Total loss:  -0.5309 | PDE Loss:  -1.0681 | Function Loss:  -1.6798\n",
      "Total loss:  -0.531 | PDE Loss:  -1.0684 | Function Loss:  -1.6799\n",
      "Total loss:  -0.5311 | PDE Loss:  -1.0687 | Function Loss:  -1.6798\n",
      "Total loss:  -0.5312 | PDE Loss:  -1.0688 | Function Loss:  -1.6799\n",
      "Total loss:  -0.5312 | PDE Loss:  -1.0688 | Function Loss:  -1.68\n",
      "Total loss:  -0.5313 | PDE Loss:  -1.0688 | Function Loss:  -1.6801\n",
      "Total loss:  -0.5313 | PDE Loss:  -1.0687 | Function Loss:  -1.6802\n",
      "Total loss:  -0.5314 | PDE Loss:  -1.0687 | Function Loss:  -1.6803\n",
      "Total loss:  -0.5315 | PDE Loss:  -1.0688 | Function Loss:  -1.6803\n",
      "Total loss:  -0.5316 | PDE Loss:  -1.069 | Function Loss:  -1.6804\n",
      "Total loss:  -0.5318 | PDE Loss:  -1.0693 | Function Loss:  -1.6805\n",
      "Total loss:  -0.5319 | PDE Loss:  -1.0698 | Function Loss:  -1.6805\n",
      "Total loss:  -0.532 | PDE Loss:  -1.07 | Function Loss:  -1.6806\n",
      "Total loss:  -0.5321 | PDE Loss:  -1.0703 | Function Loss:  -1.6806\n",
      "Total loss:  -0.5322 | PDE Loss:  -1.0704 | Function Loss:  -1.6807\n",
      "Total loss:  -0.5323 | PDE Loss:  -1.0706 | Function Loss:  -1.6807\n",
      "Total loss:  -0.5323 | PDE Loss:  -1.0706 | Function Loss:  -1.6808\n",
      "Total loss:  -0.5324 | PDE Loss:  -1.0706 | Function Loss:  -1.6809\n",
      "Total loss:  -0.5325 | PDE Loss:  -1.0706 | Function Loss:  -1.681\n",
      "Total loss:  -0.5326 | PDE Loss:  -1.0704 | Function Loss:  -1.6813\n",
      "Total loss:  -0.5327 | PDE Loss:  -1.0701 | Function Loss:  -1.6815\n",
      "Total loss:  -0.5328 | PDE Loss:  -1.0698 | Function Loss:  -1.6818\n",
      "Total loss:  -0.5329 | PDE Loss:  -1.0696 | Function Loss:  -1.682\n",
      "Total loss:  -0.5329 | PDE Loss:  -1.0694 | Function Loss:  -1.6821\n",
      "Total loss:  -0.5329 | PDE Loss:  -1.0692 | Function Loss:  -1.6822\n",
      "Total loss:  -0.533 | PDE Loss:  -1.0692 | Function Loss:  -1.6824\n",
      "Total loss:  -0.5331 | PDE Loss:  -1.0694 | Function Loss:  -1.6824\n",
      "Total loss:  -0.5332 | PDE Loss:  -1.0697 | Function Loss:  -1.6824\n",
      "Total loss:  -0.5333 | PDE Loss:  -1.0701 | Function Loss:  -1.6824\n",
      "Total loss:  -0.5334 | PDE Loss:  -1.0704 | Function Loss:  -1.6823\n",
      "Total loss:  -0.5335 | PDE Loss:  -1.0709 | Function Loss:  -1.6823\n",
      "Total loss:  -0.5336 | PDE Loss:  -1.071 | Function Loss:  -1.6824\n",
      "Total loss:  -0.5337 | PDE Loss:  -1.0713 | Function Loss:  -1.6824\n",
      "Total loss:  -0.5338 | PDE Loss:  -1.0716 | Function Loss:  -1.6825\n",
      "Total loss:  -0.5339 | PDE Loss:  -1.0717 | Function Loss:  -1.6826\n",
      "Total loss:  -0.5341 | PDE Loss:  -1.0715 | Function Loss:  -1.6829\n",
      "Total loss:  -0.5342 | PDE Loss:  -1.0714 | Function Loss:  -1.6831\n",
      "Total loss:  -0.5343 | PDE Loss:  -1.0709 | Function Loss:  -1.6835\n",
      "Total loss:  -0.5344 | PDE Loss:  -1.0709 | Function Loss:  -1.6836\n",
      "Total loss:  -0.5345 | PDE Loss:  -1.0707 | Function Loss:  -1.6838\n",
      "Total loss:  -0.5346 | PDE Loss:  -1.0705 | Function Loss:  -1.684\n",
      "Total loss:  -0.5347 | PDE Loss:  -1.0707 | Function Loss:  -1.6841\n",
      "Total loss:  -0.5348 | PDE Loss:  -1.0705 | Function Loss:  -1.6844\n",
      "Total loss:  -0.5349 | PDE Loss:  -1.0707 | Function Loss:  -1.6844\n",
      "Total loss:  -0.535 | PDE Loss:  -1.0711 | Function Loss:  -1.6843\n",
      "Total loss:  -0.5351 | PDE Loss:  -1.0714 | Function Loss:  -1.6844\n",
      "Total loss:  -0.5352 | PDE Loss:  -1.0718 | Function Loss:  -1.6843\n",
      "Total loss:  -0.5353 | PDE Loss:  -1.072 | Function Loss:  -1.6844\n",
      "Total loss:  -0.5354 | PDE Loss:  -1.0723 | Function Loss:  -1.6844\n",
      "Total loss:  -0.5355 | PDE Loss:  -1.0724 | Function Loss:  -1.6845\n",
      "Total loss:  -0.5356 | PDE Loss:  -1.0727 | Function Loss:  -1.6845\n",
      "Total loss:  -0.5357 | PDE Loss:  -1.0726 | Function Loss:  -1.6846\n",
      "Total loss:  -0.5357 | PDE Loss:  -1.0726 | Function Loss:  -1.6848\n",
      "Total loss:  -0.5358 | PDE Loss:  -1.0726 | Function Loss:  -1.6849\n",
      "Total loss:  -0.5359 | PDE Loss:  -1.0726 | Function Loss:  -1.6851\n",
      "Total loss:  -0.536 | PDE Loss:  -1.0726 | Function Loss:  -1.6852\n",
      "Total loss:  -0.5361 | PDE Loss:  -1.0726 | Function Loss:  -1.6853\n",
      "Total loss:  -0.5362 | PDE Loss:  -1.0726 | Function Loss:  -1.6854\n",
      "Total loss:  -0.5363 | PDE Loss:  -1.0729 | Function Loss:  -1.6854\n",
      "Total loss:  -0.5364 | PDE Loss:  -1.0727 | Function Loss:  -1.6856\n",
      "Total loss:  -0.5365 | PDE Loss:  -1.0725 | Function Loss:  -1.6859\n",
      "Total loss:  -0.5366 | PDE Loss:  -1.0725 | Function Loss:  -1.686\n",
      "Total loss:  -0.5367 | PDE Loss:  -1.0723 | Function Loss:  -1.6862\n",
      "Total loss:  -0.5367 | PDE Loss:  -1.0722 | Function Loss:  -1.6864\n",
      "Total loss:  -0.5368 | PDE Loss:  -1.0721 | Function Loss:  -1.6865\n",
      "Total loss:  -0.5369 | PDE Loss:  -1.072 | Function Loss:  -1.6866\n",
      "Total loss:  -0.5369 | PDE Loss:  -1.0719 | Function Loss:  -1.6867\n",
      "Total loss:  -0.537 | PDE Loss:  -1.0719 | Function Loss:  -1.6869\n",
      "Total loss:  -0.5371 | PDE Loss:  -1.0717 | Function Loss:  -1.6871\n",
      "Total loss:  -0.5372 | PDE Loss:  -1.0716 | Function Loss:  -1.6872\n",
      "Total loss:  -0.5372 | PDE Loss:  -1.0714 | Function Loss:  -1.6874\n",
      "Total loss:  -0.5373 | PDE Loss:  -1.0714 | Function Loss:  -1.6875\n",
      "Total loss:  -0.5374 | PDE Loss:  -1.0714 | Function Loss:  -1.6876\n",
      "Total loss:  -0.5374 | PDE Loss:  -1.0715 | Function Loss:  -1.6876\n",
      "Total loss:  -0.5375 | PDE Loss:  -1.0714 | Function Loss:  -1.6878\n",
      "Total loss:  -0.5376 | PDE Loss:  -1.0714 | Function Loss:  -1.6878\n",
      "Total loss:  -0.5376 | PDE Loss:  -1.0712 | Function Loss:  -1.688\n",
      "Total loss:  -0.5377 | PDE Loss:  -1.071 | Function Loss:  -1.6882\n",
      "Total loss:  -0.5378 | PDE Loss:  -1.0704 | Function Loss:  -1.6886\n",
      "Total loss:  -0.5379 | PDE Loss:  -1.07 | Function Loss:  -1.6889\n",
      "Total loss:  -0.538 | PDE Loss:  -1.0694 | Function Loss:  -1.6893\n",
      "Total loss:  -0.5381 | PDE Loss:  -1.0687 | Function Loss:  -1.6898\n",
      "Total loss:  -0.5383 | PDE Loss:  -1.0683 | Function Loss:  -1.6902\n",
      "Total loss:  -0.5384 | PDE Loss:  -1.0679 | Function Loss:  -1.6905\n",
      "Total loss:  -0.5386 | PDE Loss:  -1.0678 | Function Loss:  -1.6908\n",
      "Total loss:  -0.5387 | PDE Loss:  -1.0671 | Function Loss:  -1.6913\n",
      "Total loss:  -0.5389 | PDE Loss:  -1.067 | Function Loss:  -1.6915\n",
      "Total loss:  -0.539 | PDE Loss:  -1.0668 | Function Loss:  -1.6919\n",
      "Total loss:  -0.5393 | PDE Loss:  -1.0662 | Function Loss:  -1.6925\n",
      "Total loss:  -0.5395 | PDE Loss:  -1.0655 | Function Loss:  -1.6931\n",
      "Total loss:  -0.5397 | PDE Loss:  -1.0645 | Function Loss:  -1.6938\n",
      "Total loss:  -0.5399 | PDE Loss:  -1.0637 | Function Loss:  -1.6944\n",
      "Total loss:  -0.5401 | PDE Loss:  -1.0628 | Function Loss:  -1.695\n",
      "Total loss:  -0.5403 | PDE Loss:  -1.0625 | Function Loss:  -1.6955\n",
      "Total loss:  -0.5403 | PDE Loss:  -1.0579 | Function Loss:  -1.6975\n",
      "Total loss:  -0.5406 | PDE Loss:  -1.0593 | Function Loss:  -1.6973\n",
      "Total loss:  -0.5408 | PDE Loss:  -1.0606 | Function Loss:  -1.6971\n",
      "Total loss:  -0.541 | PDE Loss:  -1.0616 | Function Loss:  -1.6969\n",
      "Total loss:  -0.5411 | PDE Loss:  -1.0625 | Function Loss:  -1.6967\n",
      "Total loss:  -0.5412 | PDE Loss:  -1.0631 | Function Loss:  -1.6966\n",
      "Total loss:  -0.5414 | PDE Loss:  -1.0639 | Function Loss:  -1.6964\n",
      "Total loss:  -0.5414 | PDE Loss:  -1.0642 | Function Loss:  -1.6964\n",
      "Total loss:  -0.5415 | PDE Loss:  -1.0646 | Function Loss:  -1.6964\n",
      "Total loss:  -0.5416 | PDE Loss:  -1.0646 | Function Loss:  -1.6964\n",
      "Total loss:  -0.5417 | PDE Loss:  -1.0647 | Function Loss:  -1.6966\n",
      "Total loss:  -0.5418 | PDE Loss:  -1.0648 | Function Loss:  -1.6966\n",
      "Total loss:  -0.5418 | PDE Loss:  -1.0646 | Function Loss:  -1.6968\n",
      "Total loss:  -0.5419 | PDE Loss:  -1.0647 | Function Loss:  -1.6969\n",
      "Total loss:  -0.542 | PDE Loss:  -1.0648 | Function Loss:  -1.6969\n",
      "Total loss:  -0.5421 | PDE Loss:  -1.0647 | Function Loss:  -1.6971\n",
      "Total loss:  -0.5421 | PDE Loss:  -1.0649 | Function Loss:  -1.6971\n",
      "Total loss:  -0.5422 | PDE Loss:  -1.0651 | Function Loss:  -1.697\n",
      "Total loss:  -0.5422 | PDE Loss:  -1.0653 | Function Loss:  -1.697\n",
      "Total loss:  -0.5423 | PDE Loss:  -1.0656 | Function Loss:  -1.697\n",
      "Total loss:  -0.5423 | PDE Loss:  -1.0657 | Function Loss:  -1.697\n",
      "Total loss:  -0.5424 | PDE Loss:  -1.066 | Function Loss:  -1.6969\n",
      "Total loss:  -0.5424 | PDE Loss:  -1.0664 | Function Loss:  -1.6969\n",
      "Total loss:  -0.5425 | PDE Loss:  -1.0667 | Function Loss:  -1.6968\n",
      "Total loss:  -0.5425 | PDE Loss:  -1.067 | Function Loss:  -1.6968\n",
      "Total loss:  -0.5426 | PDE Loss:  -1.0673 | Function Loss:  -1.6967\n",
      "Total loss:  -0.5426 | PDE Loss:  -1.0675 | Function Loss:  -1.6967\n",
      "Total loss:  -0.5427 | PDE Loss:  -1.0677 | Function Loss:  -1.6967\n",
      "Total loss:  -0.5428 | PDE Loss:  -1.0677 | Function Loss:  -1.6968\n",
      "Total loss:  -0.5429 | PDE Loss:  -1.0677 | Function Loss:  -1.6969\n",
      "Total loss:  -0.5429 | PDE Loss:  -1.0677 | Function Loss:  -1.697\n",
      "Total loss:  -0.543 | PDE Loss:  -1.0676 | Function Loss:  -1.6971\n",
      "Total loss:  -0.5431 | PDE Loss:  -1.0671 | Function Loss:  -1.6975\n",
      "Total loss:  -0.5432 | PDE Loss:  -1.0669 | Function Loss:  -1.6977\n",
      "Total loss:  -0.5432 | PDE Loss:  -1.0665 | Function Loss:  -1.6979\n",
      "Total loss:  -0.5433 | PDE Loss:  -1.0664 | Function Loss:  -1.6981\n",
      "Total loss:  -0.5434 | PDE Loss:  -1.0662 | Function Loss:  -1.6983\n",
      "Total loss:  -0.5435 | PDE Loss:  -1.0663 | Function Loss:  -1.6984\n",
      "Total loss:  -0.5436 | PDE Loss:  -1.0665 | Function Loss:  -1.6986\n",
      "Total loss:  -0.5437 | PDE Loss:  -1.0667 | Function Loss:  -1.6986\n",
      "Total loss:  -0.5439 | PDE Loss:  -1.0675 | Function Loss:  -1.6985\n",
      "Total loss:  -0.5441 | PDE Loss:  -1.0685 | Function Loss:  -1.6983\n",
      "Total loss:  -0.5442 | PDE Loss:  -1.0696 | Function Loss:  -1.698\n",
      "Total loss:  -0.5443 | PDE Loss:  -1.0705 | Function Loss:  -1.6978\n",
      "Total loss:  -0.5444 | PDE Loss:  -1.0715 | Function Loss:  -1.6975\n",
      "Total loss:  -0.5445 | PDE Loss:  -1.0722 | Function Loss:  -1.6973\n",
      "Total loss:  -0.5446 | PDE Loss:  -1.0728 | Function Loss:  -1.6972\n",
      "Total loss:  -0.5446 | PDE Loss:  -1.0732 | Function Loss:  -1.6971\n",
      "Total loss:  -0.5447 | PDE Loss:  -1.0736 | Function Loss:  -1.6971\n",
      "Total loss:  -0.5448 | PDE Loss:  -1.0736 | Function Loss:  -1.6972\n",
      "Total loss:  -0.5448 | PDE Loss:  -1.0737 | Function Loss:  -1.6972\n",
      "Total loss:  -0.5449 | PDE Loss:  -1.0737 | Function Loss:  -1.6973\n",
      "Total loss:  -0.545 | PDE Loss:  -1.0737 | Function Loss:  -1.6974\n",
      "Total loss:  -0.5451 | PDE Loss:  -1.0737 | Function Loss:  -1.6975\n",
      "Total loss:  -0.5451 | PDE Loss:  -1.0736 | Function Loss:  -1.6976\n",
      "Total loss:  -0.5452 | PDE Loss:  -1.0737 | Function Loss:  -1.6977\n",
      "Total loss:  -0.5453 | PDE Loss:  -1.0737 | Function Loss:  -1.6978\n",
      "Total loss:  -0.5454 | PDE Loss:  -1.0739 | Function Loss:  -1.6979\n",
      "Total loss:  -0.5455 | PDE Loss:  -1.074 | Function Loss:  -1.698\n",
      "Total loss:  -0.5456 | PDE Loss:  -1.0739 | Function Loss:  -1.6982\n",
      "Total loss:  -0.5457 | PDE Loss:  -1.0739 | Function Loss:  -1.6983\n",
      "Total loss:  -0.5458 | PDE Loss:  -1.074 | Function Loss:  -1.6984\n",
      "Total loss:  -0.5459 | PDE Loss:  -1.0738 | Function Loss:  -1.6987\n",
      "Total loss:  -0.546 | PDE Loss:  -1.0738 | Function Loss:  -1.6988\n",
      "Total loss:  -0.5461 | PDE Loss:  -1.0738 | Function Loss:  -1.699\n",
      "Total loss:  -0.5462 | PDE Loss:  -1.0739 | Function Loss:  -1.6991\n",
      "Total loss:  -0.5464 | PDE Loss:  -1.074 | Function Loss:  -1.6992\n",
      "Total loss:  -0.5465 | PDE Loss:  -1.0741 | Function Loss:  -1.6993\n",
      "Total loss:  -0.5466 | PDE Loss:  -1.0744 | Function Loss:  -1.6994\n",
      "Total loss:  -0.5467 | PDE Loss:  -1.0744 | Function Loss:  -1.6995\n",
      "Total loss:  -0.5468 | PDE Loss:  -1.0743 | Function Loss:  -1.6997\n",
      "Total loss:  -0.5468 | PDE Loss:  -1.0744 | Function Loss:  -1.6998\n",
      "Total loss:  -0.5469 | PDE Loss:  -1.0743 | Function Loss:  -1.6999\n",
      "Total loss:  -0.547 | PDE Loss:  -1.0743 | Function Loss:  -1.7\n",
      "Total loss:  -0.5471 | PDE Loss:  -1.0742 | Function Loss:  -1.7002\n",
      "Total loss:  -0.5472 | PDE Loss:  -1.0741 | Function Loss:  -1.7004\n",
      "Total loss:  -0.5474 | PDE Loss:  -1.0741 | Function Loss:  -1.7006\n",
      "Total loss:  -0.5474 | PDE Loss:  -1.0735 | Function Loss:  -1.7009\n",
      "Total loss:  -0.5475 | PDE Loss:  -1.0738 | Function Loss:  -1.701\n",
      "Total loss:  -0.5476 | PDE Loss:  -1.074 | Function Loss:  -1.701\n",
      "Total loss:  -0.5477 | PDE Loss:  -1.0742 | Function Loss:  -1.701\n",
      "Total loss:  -0.5477 | PDE Loss:  -1.0744 | Function Loss:  -1.701\n",
      "Total loss:  -0.5478 | PDE Loss:  -1.0746 | Function Loss:  -1.701\n",
      "Total loss:  -0.5478 | PDE Loss:  -1.0749 | Function Loss:  -1.7009\n",
      "Total loss:  -0.5479 | PDE Loss:  -1.0753 | Function Loss:  -1.7009\n",
      "Total loss:  -0.548 | PDE Loss:  -1.0756 | Function Loss:  -1.7009\n",
      "Total loss:  -0.548 | PDE Loss:  -1.0758 | Function Loss:  -1.7008\n",
      "Total loss:  -0.5481 | PDE Loss:  -1.0761 | Function Loss:  -1.7008\n",
      "Total loss:  -0.5482 | PDE Loss:  -1.0763 | Function Loss:  -1.7009\n",
      "Total loss:  -0.5482 | PDE Loss:  -1.0764 | Function Loss:  -1.7009\n",
      "Total loss:  -0.5483 | PDE Loss:  -1.0765 | Function Loss:  -1.7009\n",
      "Total loss:  -0.5484 | PDE Loss:  -1.0767 | Function Loss:  -1.7009\n",
      "Total loss:  -0.5484 | PDE Loss:  -1.0769 | Function Loss:  -1.701\n",
      "Total loss:  -0.5486 | PDE Loss:  -1.0773 | Function Loss:  -1.701\n",
      "Total loss:  -0.5487 | PDE Loss:  -1.0776 | Function Loss:  -1.701\n",
      "Total loss:  -0.5487 | PDE Loss:  -1.078 | Function Loss:  -1.701\n",
      "Total loss:  -0.5488 | PDE Loss:  -1.0783 | Function Loss:  -1.701\n",
      "Total loss:  -0.5489 | PDE Loss:  -1.0787 | Function Loss:  -1.7009\n",
      "Total loss:  -0.549 | PDE Loss:  -1.0789 | Function Loss:  -1.7009\n",
      "Total loss:  -0.549 | PDE Loss:  -1.0792 | Function Loss:  -1.7009\n",
      "Total loss:  -0.5491 | PDE Loss:  -1.0793 | Function Loss:  -1.7009\n",
      "Total loss:  -0.5492 | PDE Loss:  -1.0794 | Function Loss:  -1.701\n",
      "Total loss:  -0.5492 | PDE Loss:  -1.0793 | Function Loss:  -1.7011\n",
      "Total loss:  -0.5493 | PDE Loss:  -1.0793 | Function Loss:  -1.7012\n",
      "Total loss:  -0.5494 | PDE Loss:  -1.0793 | Function Loss:  -1.7014\n",
      "Total loss:  -0.5495 | PDE Loss:  -1.0781 | Function Loss:  -1.702\n",
      "Total loss:  -0.5497 | PDE Loss:  -1.0782 | Function Loss:  -1.7021\n",
      "Total loss:  -0.5497 | PDE Loss:  -1.0781 | Function Loss:  -1.7023\n",
      "Total loss:  -0.5499 | PDE Loss:  -1.078 | Function Loss:  -1.7025\n",
      "Total loss:  -0.55 | PDE Loss:  -1.0779 | Function Loss:  -1.7028\n",
      "Total loss:  -0.55 | PDE Loss:  -1.0759 | Function Loss:  -1.7037\n",
      "Total loss:  -0.5501 | PDE Loss:  -1.0765 | Function Loss:  -1.7036\n",
      "Total loss:  -0.5502 | PDE Loss:  -1.0768 | Function Loss:  -1.7035\n",
      "Total loss:  -0.5503 | PDE Loss:  -1.0771 | Function Loss:  -1.7036\n",
      "Total loss:  -0.5504 | PDE Loss:  -1.0771 | Function Loss:  -1.7036\n",
      "Total loss:  -0.5504 | PDE Loss:  -1.077 | Function Loss:  -1.7038\n",
      "Total loss:  -0.5505 | PDE Loss:  -1.0768 | Function Loss:  -1.704\n",
      "Total loss:  -0.5506 | PDE Loss:  -1.0766 | Function Loss:  -1.7042\n",
      "Total loss:  -0.5507 | PDE Loss:  -1.0762 | Function Loss:  -1.7044\n",
      "Total loss:  -0.5508 | PDE Loss:  -1.076 | Function Loss:  -1.7046\n",
      "Total loss:  -0.5508 | PDE Loss:  -1.076 | Function Loss:  -1.7048\n",
      "Total loss:  -0.5509 | PDE Loss:  -1.076 | Function Loss:  -1.7049\n",
      "Total loss:  -0.5509 | PDE Loss:  -1.0761 | Function Loss:  -1.7049\n",
      "Total loss:  -0.551 | PDE Loss:  -1.0762 | Function Loss:  -1.7049\n",
      "Total loss:  -0.551 | PDE Loss:  -1.0763 | Function Loss:  -1.7049\n",
      "Total loss:  -0.5511 | PDE Loss:  -1.0766 | Function Loss:  -1.7049\n",
      "Total loss:  -0.5512 | PDE Loss:  -1.0757 | Function Loss:  -1.7054\n",
      "Total loss:  -0.5513 | PDE Loss:  -1.0762 | Function Loss:  -1.7053\n",
      "Total loss:  -0.5514 | PDE Loss:  -1.0767 | Function Loss:  -1.7053\n",
      "Total loss:  -0.5515 | PDE Loss:  -1.077 | Function Loss:  -1.7053\n",
      "Total loss:  -0.5516 | PDE Loss:  -1.0771 | Function Loss:  -1.7053\n",
      "Total loss:  -0.5517 | PDE Loss:  -1.077 | Function Loss:  -1.7055\n",
      "Total loss:  -0.5517 | PDE Loss:  -1.0771 | Function Loss:  -1.7056\n",
      "Total loss:  -0.5518 | PDE Loss:  -1.0768 | Function Loss:  -1.7058\n",
      "Total loss:  -0.5518 | PDE Loss:  -1.0765 | Function Loss:  -1.706\n",
      "Total loss:  -0.5519 | PDE Loss:  -1.0762 | Function Loss:  -1.7062\n",
      "Total loss:  -0.5519 | PDE Loss:  -1.076 | Function Loss:  -1.7063\n",
      "Total loss:  -0.552 | PDE Loss:  -1.0759 | Function Loss:  -1.7065\n",
      "Total loss:  -0.552 | PDE Loss:  -1.076 | Function Loss:  -1.7065\n",
      "Total loss:  -0.5521 | PDE Loss:  -1.0762 | Function Loss:  -1.7065\n",
      "Total loss:  -0.5522 | PDE Loss:  -1.0763 | Function Loss:  -1.7065\n",
      "Total loss:  -0.5522 | PDE Loss:  -1.0767 | Function Loss:  -1.7064\n",
      "Total loss:  -0.5523 | PDE Loss:  -1.0772 | Function Loss:  -1.7063\n",
      "Total loss:  -0.5524 | PDE Loss:  -1.0777 | Function Loss:  -1.7062\n",
      "Total loss:  -0.5524 | PDE Loss:  -1.0782 | Function Loss:  -1.7061\n",
      "Total loss:  -0.5525 | PDE Loss:  -1.0785 | Function Loss:  -1.7061\n",
      "Total loss:  -0.5526 | PDE Loss:  -1.0788 | Function Loss:  -1.7061\n",
      "Total loss:  -0.5527 | PDE Loss:  -1.0796 | Function Loss:  -1.7059\n",
      "Total loss:  -0.5528 | PDE Loss:  -1.0791 | Function Loss:  -1.7062\n",
      "Total loss:  -0.5529 | PDE Loss:  -1.0794 | Function Loss:  -1.7062\n",
      "Total loss:  -0.553 | PDE Loss:  -1.0794 | Function Loss:  -1.7063\n",
      "Total loss:  -0.553 | PDE Loss:  -1.0795 | Function Loss:  -1.7064\n",
      "Total loss:  -0.5531 | PDE Loss:  -1.0794 | Function Loss:  -1.7066\n",
      "Total loss:  -0.5532 | PDE Loss:  -1.0795 | Function Loss:  -1.7066\n",
      "Total loss:  -0.5532 | PDE Loss:  -1.079 | Function Loss:  -1.7069\n",
      "Total loss:  -0.5533 | PDE Loss:  -1.0794 | Function Loss:  -1.7068\n",
      "Total loss:  -0.5534 | PDE Loss:  -1.08 | Function Loss:  -1.7067\n",
      "Total loss:  -0.5535 | PDE Loss:  -1.0807 | Function Loss:  -1.7065\n",
      "Total loss:  -0.5535 | PDE Loss:  -1.0814 | Function Loss:  -1.7063\n",
      "Total loss:  -0.5536 | PDE Loss:  -1.0821 | Function Loss:  -1.7061\n",
      "Total loss:  -0.5537 | PDE Loss:  -1.0826 | Function Loss:  -1.706\n",
      "Total loss:  -0.5538 | PDE Loss:  -1.083 | Function Loss:  -1.706\n",
      "Total loss:  -0.5539 | PDE Loss:  -1.0835 | Function Loss:  -1.706\n",
      "Total loss:  -0.554 | PDE Loss:  -1.0833 | Function Loss:  -1.7062\n",
      "Total loss:  -0.5541 | PDE Loss:  -1.0834 | Function Loss:  -1.7063\n",
      "Total loss:  -0.5542 | PDE Loss:  -1.0832 | Function Loss:  -1.7064\n",
      "Total loss:  -0.5542 | PDE Loss:  -1.0831 | Function Loss:  -1.7066\n",
      "Total loss:  -0.5543 | PDE Loss:  -1.0829 | Function Loss:  -1.7068\n",
      "Total loss:  -0.5544 | PDE Loss:  -1.0827 | Function Loss:  -1.7069\n",
      "Total loss:  -0.5544 | PDE Loss:  -1.0822 | Function Loss:  -1.7072\n",
      "Total loss:  -0.5545 | PDE Loss:  -1.0822 | Function Loss:  -1.7074\n",
      "Total loss:  -0.5546 | PDE Loss:  -1.0822 | Function Loss:  -1.7075\n",
      "Total loss:  -0.5547 | PDE Loss:  -1.0825 | Function Loss:  -1.7075\n",
      "Total loss:  -0.5548 | PDE Loss:  -1.0826 | Function Loss:  -1.7076\n",
      "Total loss:  -0.5549 | PDE Loss:  -1.0829 | Function Loss:  -1.7076\n",
      "Total loss:  -0.555 | PDE Loss:  -1.0831 | Function Loss:  -1.7077\n",
      "Total loss:  -0.5551 | PDE Loss:  -1.0832 | Function Loss:  -1.7078\n",
      "Total loss:  -0.5552 | PDE Loss:  -1.0833 | Function Loss:  -1.7079\n",
      "Total loss:  -0.5553 | PDE Loss:  -1.0832 | Function Loss:  -1.708\n",
      "Total loss:  -0.5554 | PDE Loss:  -1.0831 | Function Loss:  -1.7082\n",
      "Total loss:  -0.5554 | PDE Loss:  -1.083 | Function Loss:  -1.7083\n",
      "Total loss:  -0.5555 | PDE Loss:  -1.0827 | Function Loss:  -1.7085\n",
      "Total loss:  -0.5555 | PDE Loss:  -1.0826 | Function Loss:  -1.7086\n",
      "Total loss:  -0.5555 | PDE Loss:  -1.0815 | Function Loss:  -1.7091\n",
      "Total loss:  -0.5556 | PDE Loss:  -1.0817 | Function Loss:  -1.7091\n",
      "Total loss:  -0.5556 | PDE Loss:  -1.0819 | Function Loss:  -1.7091\n",
      "Total loss:  -0.5557 | PDE Loss:  -1.0821 | Function Loss:  -1.7091\n",
      "Total loss:  -0.5558 | PDE Loss:  -1.0822 | Function Loss:  -1.7091\n",
      "Total loss:  -0.5558 | PDE Loss:  -1.0823 | Function Loss:  -1.7092\n",
      "Total loss:  -0.5559 | PDE Loss:  -1.0825 | Function Loss:  -1.7092\n",
      "Total loss:  -0.5559 | PDE Loss:  -1.0824 | Function Loss:  -1.7093\n",
      "Total loss:  -0.556 | PDE Loss:  -1.0824 | Function Loss:  -1.7093\n",
      "Total loss:  -0.556 | PDE Loss:  -1.0825 | Function Loss:  -1.7093\n",
      "Total loss:  -0.556 | PDE Loss:  -1.0825 | Function Loss:  -1.7094\n",
      "Total loss:  -0.556 | PDE Loss:  -1.0825 | Function Loss:  -1.7094\n",
      "Total loss:  -0.5561 | PDE Loss:  -1.0825 | Function Loss:  -1.7094\n",
      "Total loss:  -0.5561 | PDE Loss:  -1.0826 | Function Loss:  -1.7094\n",
      "Total loss:  -0.5561 | PDE Loss:  -1.0826 | Function Loss:  -1.7095\n",
      "Total loss:  -0.5562 | PDE Loss:  -1.0828 | Function Loss:  -1.7095\n",
      "Total loss:  -0.5563 | PDE Loss:  -1.0828 | Function Loss:  -1.7096\n",
      "Total loss:  -0.5564 | PDE Loss:  -1.0829 | Function Loss:  -1.7097\n",
      "Total loss:  -0.5565 | PDE Loss:  -1.083 | Function Loss:  -1.7098\n",
      "Total loss:  -0.5565 | PDE Loss:  -1.0827 | Function Loss:  -1.71\n",
      "Total loss:  -0.5566 | PDE Loss:  -1.0829 | Function Loss:  -1.71\n",
      "Total loss:  -0.5567 | PDE Loss:  -1.0833 | Function Loss:  -1.71\n",
      "Total loss:  -0.5568 | PDE Loss:  -1.0833 | Function Loss:  -1.7102\n",
      "Total loss:  -0.5569 | PDE Loss:  -1.0837 | Function Loss:  -1.7102\n",
      "Total loss:  -0.557 | PDE Loss:  -1.0841 | Function Loss:  -1.7101\n",
      "Total loss:  -0.5571 | PDE Loss:  -1.0842 | Function Loss:  -1.7102\n",
      "Total loss:  -0.5572 | PDE Loss:  -1.0844 | Function Loss:  -1.7103\n",
      "Total loss:  -0.5573 | PDE Loss:  -1.0846 | Function Loss:  -1.7104\n",
      "Total loss:  -0.5574 | PDE Loss:  -1.0846 | Function Loss:  -1.7105\n",
      "Total loss:  -0.5575 | PDE Loss:  -1.0848 | Function Loss:  -1.7105\n",
      "Total loss:  -0.5575 | PDE Loss:  -1.085 | Function Loss:  -1.7105\n",
      "Total loss:  -0.5576 | PDE Loss:  -1.0856 | Function Loss:  -1.7103\n",
      "Total loss:  -0.5577 | PDE Loss:  -1.0861 | Function Loss:  -1.7102\n",
      "Total loss:  -0.5578 | PDE Loss:  -1.0869 | Function Loss:  -1.71\n",
      "Total loss:  -0.5579 | PDE Loss:  -1.0875 | Function Loss:  -1.7099\n",
      "Total loss:  -0.558 | PDE Loss:  -1.0884 | Function Loss:  -1.7097\n",
      "Total loss:  -0.5581 | PDE Loss:  -1.0889 | Function Loss:  -1.7097\n",
      "Total loss:  -0.5582 | PDE Loss:  -1.0895 | Function Loss:  -1.7096\n",
      "Total loss:  -0.5583 | PDE Loss:  -1.09 | Function Loss:  -1.7095\n",
      "Total loss:  -0.5584 | PDE Loss:  -1.09 | Function Loss:  -1.7096\n",
      "Total loss:  -0.5585 | PDE Loss:  -1.0899 | Function Loss:  -1.7098\n",
      "Total loss:  -0.5585 | PDE Loss:  -1.0896 | Function Loss:  -1.71\n",
      "Total loss:  -0.5586 | PDE Loss:  -1.0897 | Function Loss:  -1.71\n",
      "Total loss:  -0.5587 | PDE Loss:  -1.0897 | Function Loss:  -1.7101\n",
      "Total loss:  -0.5588 | PDE Loss:  -1.09 | Function Loss:  -1.7101\n",
      "Total loss:  -0.5589 | PDE Loss:  -1.0907 | Function Loss:  -1.7101\n",
      "Total loss:  -0.559 | PDE Loss:  -1.0913 | Function Loss:  -1.71\n",
      "Total loss:  -0.5584 | PDE Loss:  -1.0929 | Function Loss:  -1.7085\n",
      "Total loss:  -0.5591 | PDE Loss:  -1.0919 | Function Loss:  -1.7098\n",
      "Total loss:  -0.5592 | PDE Loss:  -1.0921 | Function Loss:  -1.7098\n",
      "Total loss:  -0.5593 | PDE Loss:  -1.0927 | Function Loss:  -1.7097\n",
      "Total loss:  -0.5594 | PDE Loss:  -1.0931 | Function Loss:  -1.7097\n",
      "Total loss:  -0.5594 | PDE Loss:  -1.0935 | Function Loss:  -1.7096\n",
      "Total loss:  -0.5595 | PDE Loss:  -1.0937 | Function Loss:  -1.7097\n",
      "Total loss:  -0.5596 | PDE Loss:  -1.0939 | Function Loss:  -1.7097\n",
      "Total loss:  -0.5597 | PDE Loss:  -1.0941 | Function Loss:  -1.7097\n",
      "Total loss:  -0.5598 | PDE Loss:  -1.0941 | Function Loss:  -1.7098\n",
      "Total loss:  -0.5599 | PDE Loss:  -1.0946 | Function Loss:  -1.7098\n",
      "Total loss:  -0.5599 | PDE Loss:  -1.0946 | Function Loss:  -1.7099\n",
      "Total loss:  -0.56 | PDE Loss:  -1.0946 | Function Loss:  -1.71\n",
      "Total loss:  -0.5601 | PDE Loss:  -1.0953 | Function Loss:  -1.7098\n",
      "Total loss:  -0.5602 | PDE Loss:  -1.0954 | Function Loss:  -1.7099\n",
      "Total loss:  -0.5603 | PDE Loss:  -1.0957 | Function Loss:  -1.7099\n",
      "Total loss:  -0.5604 | PDE Loss:  -1.0962 | Function Loss:  -1.7098\n",
      "Total loss:  -0.5605 | PDE Loss:  -1.097 | Function Loss:  -1.7097\n",
      "Total loss:  -0.5606 | PDE Loss:  -1.0975 | Function Loss:  -1.7096\n",
      "Total loss:  -0.5607 | PDE Loss:  -1.0981 | Function Loss:  -1.7095\n",
      "Total loss:  -0.5608 | PDE Loss:  -1.0989 | Function Loss:  -1.7093\n",
      "Total loss:  -0.5609 | PDE Loss:  -1.0994 | Function Loss:  -1.7093\n",
      "Total loss:  -0.561 | PDE Loss:  -1.0998 | Function Loss:  -1.7092\n",
      "Total loss:  -0.5611 | PDE Loss:  -1.1004 | Function Loss:  -1.7092\n",
      "Total loss:  -0.5612 | PDE Loss:  -1.1007 | Function Loss:  -1.7092\n",
      "Total loss:  -0.5613 | PDE Loss:  -1.1006 | Function Loss:  -1.7093\n",
      "Total loss:  -0.5614 | PDE Loss:  -1.1007 | Function Loss:  -1.7094\n",
      "Total loss:  -0.5614 | PDE Loss:  -1.1006 | Function Loss:  -1.7096\n",
      "Total loss:  -0.5616 | PDE Loss:  -1.1006 | Function Loss:  -1.7097\n",
      "Total loss:  -0.5616 | PDE Loss:  -1.1006 | Function Loss:  -1.7098\n",
      "Total loss:  -0.5617 | PDE Loss:  -1.1006 | Function Loss:  -1.7099\n",
      "Total loss:  -0.5618 | PDE Loss:  -1.1006 | Function Loss:  -1.7101\n",
      "Total loss:  -0.5619 | PDE Loss:  -1.1006 | Function Loss:  -1.7102\n",
      "Total loss:  -0.562 | PDE Loss:  -1.1006 | Function Loss:  -1.7103\n",
      "Total loss:  -0.5621 | PDE Loss:  -1.1006 | Function Loss:  -1.7104\n",
      "Total loss:  -0.5621 | PDE Loss:  -1.1006 | Function Loss:  -1.7105\n",
      "Total loss:  -0.5622 | PDE Loss:  -1.1007 | Function Loss:  -1.7106\n",
      "Total loss:  -0.5622 | PDE Loss:  -1.1007 | Function Loss:  -1.7106\n",
      "Total loss:  -0.5623 | PDE Loss:  -1.1007 | Function Loss:  -1.7107\n",
      "Total loss:  -0.5623 | PDE Loss:  -1.1007 | Function Loss:  -1.7108\n",
      "Total loss:  -0.5624 | PDE Loss:  -1.1006 | Function Loss:  -1.7109\n",
      "Total loss:  -0.5625 | PDE Loss:  -1.1007 | Function Loss:  -1.711\n",
      "Total loss:  -0.5626 | PDE Loss:  -1.1008 | Function Loss:  -1.711\n",
      "Total loss:  -0.5627 | PDE Loss:  -1.1011 | Function Loss:  -1.7111\n",
      "Total loss:  -0.5628 | PDE Loss:  -1.1017 | Function Loss:  -1.7111\n",
      "Total loss:  -0.563 | PDE Loss:  -1.1027 | Function Loss:  -1.7109\n",
      "Total loss:  -0.5631 | PDE Loss:  -1.1033 | Function Loss:  -1.7108\n",
      "Total loss:  -0.5632 | PDE Loss:  -1.1036 | Function Loss:  -1.7108\n",
      "Total loss:  -0.5633 | PDE Loss:  -1.1041 | Function Loss:  -1.7107\n",
      "Total loss:  -0.5634 | PDE Loss:  -1.1046 | Function Loss:  -1.7107\n",
      "Total loss:  -0.5635 | PDE Loss:  -1.1052 | Function Loss:  -1.7106\n",
      "Total loss:  -0.5636 | PDE Loss:  -1.1054 | Function Loss:  -1.7106\n",
      "Total loss:  -0.5637 | PDE Loss:  -1.1057 | Function Loss:  -1.7107\n",
      "Total loss:  -0.5638 | PDE Loss:  -1.106 | Function Loss:  -1.7107\n",
      "Total loss:  -0.5639 | PDE Loss:  -1.1063 | Function Loss:  -1.7107\n",
      "Total loss:  -0.5638 | PDE Loss:  -1.1073 | Function Loss:  -1.7101\n",
      "Total loss:  -0.564 | PDE Loss:  -1.1069 | Function Loss:  -1.7105\n",
      "Total loss:  -0.564 | PDE Loss:  -1.1071 | Function Loss:  -1.7106\n",
      "Total loss:  -0.5641 | PDE Loss:  -1.1073 | Function Loss:  -1.7106\n",
      "Total loss:  -0.5642 | PDE Loss:  -1.1075 | Function Loss:  -1.7106\n",
      "Total loss:  -0.5643 | PDE Loss:  -1.1078 | Function Loss:  -1.7106\n",
      "Total loss:  -0.5643 | PDE Loss:  -1.108 | Function Loss:  -1.7106\n",
      "Total loss:  -0.5645 | PDE Loss:  -1.1089 | Function Loss:  -1.7104\n",
      "Total loss:  -0.5645 | PDE Loss:  -1.1094 | Function Loss:  -1.7103\n",
      "Total loss:  -0.5646 | PDE Loss:  -1.1101 | Function Loss:  -1.7102\n",
      "Total loss:  -0.5647 | PDE Loss:  -1.1106 | Function Loss:  -1.7101\n",
      "Total loss:  -0.5648 | PDE Loss:  -1.1112 | Function Loss:  -1.7101\n",
      "Total loss:  -0.5649 | PDE Loss:  -1.1118 | Function Loss:  -1.71\n",
      "Total loss:  -0.5651 | PDE Loss:  -1.1121 | Function Loss:  -1.71\n",
      "Total loss:  -0.5651 | PDE Loss:  -1.1124 | Function Loss:  -1.71\n",
      "Total loss:  -0.5652 | PDE Loss:  -1.1123 | Function Loss:  -1.7101\n",
      "Total loss:  -0.5653 | PDE Loss:  -1.1123 | Function Loss:  -1.7103\n",
      "Total loss:  -0.5654 | PDE Loss:  -1.1118 | Function Loss:  -1.7106\n",
      "Total loss:  -0.5655 | PDE Loss:  -1.1116 | Function Loss:  -1.7108\n",
      "Total loss:  -0.5655 | PDE Loss:  -1.1113 | Function Loss:  -1.711\n",
      "Total loss:  -0.5656 | PDE Loss:  -1.1109 | Function Loss:  -1.7112\n",
      "Total loss:  -0.5657 | PDE Loss:  -1.1106 | Function Loss:  -1.7115\n",
      "Total loss:  -0.5658 | PDE Loss:  -1.1102 | Function Loss:  -1.7118\n",
      "Total loss:  -0.5659 | PDE Loss:  -1.1097 | Function Loss:  -1.7121\n",
      "Total loss:  -0.5653 | PDE Loss:  -1.1076 | Function Loss:  -1.7122\n",
      "Total loss:  -0.5659 | PDE Loss:  -1.1095 | Function Loss:  -1.7123\n",
      "Total loss:  -0.566 | PDE Loss:  -1.1093 | Function Loss:  -1.7124\n",
      "Total loss:  -0.5661 | PDE Loss:  -1.1092 | Function Loss:  -1.7126\n",
      "Total loss:  -0.5662 | PDE Loss:  -1.1091 | Function Loss:  -1.7128\n",
      "Total loss:  -0.5663 | PDE Loss:  -1.1091 | Function Loss:  -1.7129\n",
      "Total loss:  -0.5664 | PDE Loss:  -1.109 | Function Loss:  -1.7131\n",
      "Total loss:  -0.5665 | PDE Loss:  -1.1091 | Function Loss:  -1.7132\n",
      "Total loss:  -0.5666 | PDE Loss:  -1.1089 | Function Loss:  -1.7134\n",
      "Total loss:  -0.5666 | PDE Loss:  -1.1094 | Function Loss:  -1.7133\n",
      "Total loss:  -0.5667 | PDE Loss:  -1.1093 | Function Loss:  -1.7134\n",
      "Total loss:  -0.5667 | PDE Loss:  -1.1093 | Function Loss:  -1.7135\n",
      "Total loss:  -0.5668 | PDE Loss:  -1.1093 | Function Loss:  -1.7135\n",
      "Total loss:  -0.5668 | PDE Loss:  -1.1094 | Function Loss:  -1.7136\n",
      "Total loss:  -0.5669 | PDE Loss:  -1.1095 | Function Loss:  -1.7136\n",
      "Total loss:  -0.5669 | PDE Loss:  -1.1096 | Function Loss:  -1.7136\n",
      "Total loss:  -0.567 | PDE Loss:  -1.1097 | Function Loss:  -1.7137\n",
      "Total loss:  -0.5671 | PDE Loss:  -1.11 | Function Loss:  -1.7136\n",
      "Total loss:  -0.5672 | PDE Loss:  -1.11 | Function Loss:  -1.7138\n",
      "Total loss:  -0.5672 | PDE Loss:  -1.11 | Function Loss:  -1.7138\n",
      "Total loss:  -0.5673 | PDE Loss:  -1.1101 | Function Loss:  -1.714\n",
      "Total loss:  -0.5674 | PDE Loss:  -1.1101 | Function Loss:  -1.7141\n",
      "Total loss:  -0.5675 | PDE Loss:  -1.11 | Function Loss:  -1.7142\n",
      "Total loss:  -0.5676 | PDE Loss:  -1.1101 | Function Loss:  -1.7143\n",
      "Total loss:  -0.5677 | PDE Loss:  -1.1101 | Function Loss:  -1.7145\n",
      "Total loss:  -0.5678 | PDE Loss:  -1.1104 | Function Loss:  -1.7145\n",
      "Total loss:  -0.5679 | PDE Loss:  -1.1109 | Function Loss:  -1.7145\n",
      "Total loss:  -0.568 | PDE Loss:  -1.1117 | Function Loss:  -1.7143\n",
      "Total loss:  -0.5681 | PDE Loss:  -1.1124 | Function Loss:  -1.7142\n",
      "Total loss:  -0.5682 | PDE Loss:  -1.1137 | Function Loss:  -1.7138\n",
      "Total loss:  -0.5683 | PDE Loss:  -1.1145 | Function Loss:  -1.7136\n",
      "Total loss:  -0.5684 | PDE Loss:  -1.1154 | Function Loss:  -1.7133\n",
      "Total loss:  -0.5684 | PDE Loss:  -1.1158 | Function Loss:  -1.7133\n",
      "Total loss:  -0.5685 | PDE Loss:  -1.1161 | Function Loss:  -1.7132\n",
      "Total loss:  -0.5686 | PDE Loss:  -1.1163 | Function Loss:  -1.7133\n",
      "Total loss:  -0.5687 | PDE Loss:  -1.1162 | Function Loss:  -1.7134\n",
      "Total loss:  -0.5687 | PDE Loss:  -1.1164 | Function Loss:  -1.7134\n",
      "Total loss:  -0.5688 | PDE Loss:  -1.1165 | Function Loss:  -1.7135\n",
      "Total loss:  -0.5689 | PDE Loss:  -1.1165 | Function Loss:  -1.7136\n",
      "Total loss:  -0.569 | PDE Loss:  -1.1165 | Function Loss:  -1.7137\n",
      "Total loss:  -0.569 | PDE Loss:  -1.1166 | Function Loss:  -1.7137\n",
      "Total loss:  -0.5691 | PDE Loss:  -1.1166 | Function Loss:  -1.7138\n",
      "Total loss:  -0.5691 | PDE Loss:  -1.1172 | Function Loss:  -1.7137\n",
      "Total loss:  -0.5692 | PDE Loss:  -1.1173 | Function Loss:  -1.7137\n",
      "Total loss:  -0.5693 | PDE Loss:  -1.1176 | Function Loss:  -1.7137\n",
      "Total loss:  -0.5693 | PDE Loss:  -1.118 | Function Loss:  -1.7136\n",
      "Total loss:  -0.5694 | PDE Loss:  -1.1182 | Function Loss:  -1.7137\n",
      "Total loss:  -0.5695 | PDE Loss:  -1.1183 | Function Loss:  -1.7138\n",
      "Total loss:  -0.5697 | PDE Loss:  -1.1183 | Function Loss:  -1.7139\n",
      "Total loss:  -0.5698 | PDE Loss:  -1.1181 | Function Loss:  -1.7142\n",
      "Total loss:  -0.5699 | PDE Loss:  -1.1172 | Function Loss:  -1.7148\n",
      "Total loss:  -0.5701 | PDE Loss:  -1.1166 | Function Loss:  -1.7152\n",
      "Total loss:  -0.5702 | PDE Loss:  -1.116 | Function Loss:  -1.7157\n",
      "Total loss:  -0.5704 | PDE Loss:  -1.1153 | Function Loss:  -1.7162\n",
      "Total loss:  -0.5705 | PDE Loss:  -1.115 | Function Loss:  -1.7165\n",
      "Total loss:  -0.5706 | PDE Loss:  -1.1148 | Function Loss:  -1.7167\n",
      "Total loss:  -0.5707 | PDE Loss:  -1.1149 | Function Loss:  -1.7168\n",
      "Total loss:  -0.5708 | PDE Loss:  -1.1151 | Function Loss:  -1.7168\n",
      "Total loss:  -0.5709 | PDE Loss:  -1.1153 | Function Loss:  -1.7168\n",
      "Total loss:  -0.571 | PDE Loss:  -1.1157 | Function Loss:  -1.7168\n",
      "Total loss:  -0.5711 | PDE Loss:  -1.1161 | Function Loss:  -1.7168\n",
      "Total loss:  -0.5712 | PDE Loss:  -1.1165 | Function Loss:  -1.7168\n",
      "Total loss:  -0.5713 | PDE Loss:  -1.1166 | Function Loss:  -1.7169\n",
      "Total loss:  -0.5714 | PDE Loss:  -1.1167 | Function Loss:  -1.717\n",
      "Total loss:  -0.5715 | PDE Loss:  -1.1167 | Function Loss:  -1.7172\n",
      "Total loss:  -0.5716 | PDE Loss:  -1.1166 | Function Loss:  -1.7174\n",
      "Total loss:  -0.5716 | PDE Loss:  -1.1163 | Function Loss:  -1.7175\n",
      "Total loss:  -0.5717 | PDE Loss:  -1.1163 | Function Loss:  -1.7176\n",
      "Total loss:  -0.5718 | PDE Loss:  -1.1163 | Function Loss:  -1.7177\n",
      "Total loss:  -0.5718 | PDE Loss:  -1.1164 | Function Loss:  -1.7178\n",
      "Total loss:  -0.5719 | PDE Loss:  -1.1167 | Function Loss:  -1.7178\n",
      "Total loss:  -0.572 | PDE Loss:  -1.1172 | Function Loss:  -1.7177\n",
      "Total loss:  -0.5721 | PDE Loss:  -1.1177 | Function Loss:  -1.7177\n",
      "Total loss:  -0.5722 | PDE Loss:  -1.118 | Function Loss:  -1.7176\n",
      "Total loss:  -0.5723 | PDE Loss:  -1.1185 | Function Loss:  -1.7175\n",
      "Total loss:  -0.5723 | PDE Loss:  -1.1189 | Function Loss:  -1.7175\n",
      "Total loss:  -0.5724 | PDE Loss:  -1.1191 | Function Loss:  -1.7174\n",
      "Total loss:  -0.5724 | PDE Loss:  -1.1189 | Function Loss:  -1.7176\n",
      "Total loss:  -0.5725 | PDE Loss:  -1.1188 | Function Loss:  -1.7177\n",
      "Total loss:  -0.5725 | PDE Loss:  -1.1182 | Function Loss:  -1.7181\n",
      "Total loss:  -0.5727 | PDE Loss:  -1.1175 | Function Loss:  -1.7185\n",
      "Total loss:  -0.5728 | PDE Loss:  -1.1165 | Function Loss:  -1.7191\n",
      "Total loss:  -0.5729 | PDE Loss:  -1.1157 | Function Loss:  -1.7195\n",
      "Total loss:  -0.573 | PDE Loss:  -1.1152 | Function Loss:  -1.7199\n",
      "Total loss:  -0.5731 | PDE Loss:  -1.1146 | Function Loss:  -1.7202\n",
      "Total loss:  -0.5731 | PDE Loss:  -1.1144 | Function Loss:  -1.7204\n",
      "Total loss:  -0.5732 | PDE Loss:  -1.1144 | Function Loss:  -1.7205\n",
      "Total loss:  -0.5732 | PDE Loss:  -1.113 | Function Loss:  -1.7211\n",
      "Total loss:  -0.5733 | PDE Loss:  -1.1137 | Function Loss:  -1.7209\n",
      "Total loss:  -0.5734 | PDE Loss:  -1.1145 | Function Loss:  -1.7207\n",
      "Total loss:  -0.5734 | PDE Loss:  -1.1151 | Function Loss:  -1.7205\n",
      "Total loss:  -0.5735 | PDE Loss:  -1.1156 | Function Loss:  -1.7204\n",
      "Total loss:  -0.5735 | PDE Loss:  -1.116 | Function Loss:  -1.7203\n",
      "Total loss:  -0.5735 | PDE Loss:  -1.1162 | Function Loss:  -1.7202\n",
      "Total loss:  -0.5736 | PDE Loss:  -1.1167 | Function Loss:  -1.7201\n",
      "Total loss:  -0.5736 | PDE Loss:  -1.1168 | Function Loss:  -1.7201\n",
      "Total loss:  -0.5736 | PDE Loss:  -1.1169 | Function Loss:  -1.7201\n",
      "Total loss:  -0.5737 | PDE Loss:  -1.1171 | Function Loss:  -1.7201\n",
      "Total loss:  -0.5738 | PDE Loss:  -1.1173 | Function Loss:  -1.7201\n",
      "Total loss:  -0.5738 | PDE Loss:  -1.1174 | Function Loss:  -1.7201\n",
      "Total loss:  -0.5738 | PDE Loss:  -1.1175 | Function Loss:  -1.7202\n",
      "Total loss:  -0.5739 | PDE Loss:  -1.1176 | Function Loss:  -1.7202\n",
      "Total loss:  -0.5739 | PDE Loss:  -1.1177 | Function Loss:  -1.7202\n",
      "Total loss:  -0.574 | PDE Loss:  -1.1179 | Function Loss:  -1.7202\n",
      "Total loss:  -0.574 | PDE Loss:  -1.118 | Function Loss:  -1.7202\n",
      "Total loss:  -0.5741 | PDE Loss:  -1.1181 | Function Loss:  -1.7202\n",
      "Total loss:  -0.5741 | PDE Loss:  -1.1182 | Function Loss:  -1.7202\n",
      "Total loss:  -0.5741 | PDE Loss:  -1.1182 | Function Loss:  -1.7202\n",
      "Total loss:  -0.5742 | PDE Loss:  -1.1183 | Function Loss:  -1.7203\n",
      "Total loss:  -0.5742 | PDE Loss:  -1.1183 | Function Loss:  -1.7203\n",
      "Total loss:  -0.5743 | PDE Loss:  -1.1183 | Function Loss:  -1.7204\n",
      "Total loss:  -0.5743 | PDE Loss:  -1.1182 | Function Loss:  -1.7206\n",
      "Total loss:  -0.5744 | PDE Loss:  -1.1181 | Function Loss:  -1.7207\n",
      "Total loss:  -0.5745 | PDE Loss:  -1.1178 | Function Loss:  -1.7209\n",
      "Total loss:  -0.5745 | PDE Loss:  -1.1176 | Function Loss:  -1.7211\n",
      "Total loss:  -0.5746 | PDE Loss:  -1.1175 | Function Loss:  -1.7212\n",
      "Total loss:  -0.5746 | PDE Loss:  -1.1174 | Function Loss:  -1.7213\n",
      "Total loss:  -0.5747 | PDE Loss:  -1.1174 | Function Loss:  -1.7213\n",
      "Total loss:  -0.5747 | PDE Loss:  -1.1174 | Function Loss:  -1.7214\n",
      "Total loss:  -0.5748 | PDE Loss:  -1.1175 | Function Loss:  -1.7214\n",
      "Total loss:  -0.5748 | PDE Loss:  -1.1176 | Function Loss:  -1.7215\n",
      "Total loss:  -0.5749 | PDE Loss:  -1.1175 | Function Loss:  -1.7216\n",
      "Total loss:  -0.575 | PDE Loss:  -1.1179 | Function Loss:  -1.7216\n",
      "Total loss:  -0.5751 | PDE Loss:  -1.1178 | Function Loss:  -1.7217\n",
      "Total loss:  -0.5752 | PDE Loss:  -1.1178 | Function Loss:  -1.7219\n",
      "Total loss:  -0.5752 | PDE Loss:  -1.1177 | Function Loss:  -1.722\n",
      "Total loss:  -0.5753 | PDE Loss:  -1.1177 | Function Loss:  -1.7221\n",
      "Total loss:  -0.5753 | PDE Loss:  -1.1176 | Function Loss:  -1.7222\n",
      "Total loss:  -0.5754 | PDE Loss:  -1.1175 | Function Loss:  -1.7223\n",
      "Total loss:  -0.5755 | PDE Loss:  -1.1173 | Function Loss:  -1.7225\n",
      "Total loss:  -0.5756 | PDE Loss:  -1.1173 | Function Loss:  -1.7226\n",
      "Total loss:  -0.5757 | PDE Loss:  -1.1163 | Function Loss:  -1.7232\n",
      "Total loss:  -0.5757 | PDE Loss:  -1.1163 | Function Loss:  -1.7233\n",
      "Total loss:  -0.5758 | PDE Loss:  -1.1165 | Function Loss:  -1.7233\n",
      "Total loss:  -0.5759 | PDE Loss:  -1.1166 | Function Loss:  -1.7234\n",
      "Total loss:  -0.576 | PDE Loss:  -1.1167 | Function Loss:  -1.7234\n",
      "Total loss:  -0.576 | PDE Loss:  -1.1168 | Function Loss:  -1.7234\n",
      "Total loss:  -0.5761 | PDE Loss:  -1.1169 | Function Loss:  -1.7235\n",
      "Total loss:  -0.5762 | PDE Loss:  -1.1169 | Function Loss:  -1.7236\n",
      "Total loss:  -0.5763 | PDE Loss:  -1.1172 | Function Loss:  -1.7237\n",
      "Total loss:  -0.5764 | PDE Loss:  -1.1175 | Function Loss:  -1.7238\n",
      "Total loss:  -0.5766 | PDE Loss:  -1.1179 | Function Loss:  -1.7238\n",
      "Total loss:  -0.5767 | PDE Loss:  -1.1183 | Function Loss:  -1.7239\n",
      "Total loss:  -0.5769 | PDE Loss:  -1.1188 | Function Loss:  -1.7239\n",
      "Total loss:  -0.577 | PDE Loss:  -1.1191 | Function Loss:  -1.7239\n",
      "Total loss:  -0.5771 | PDE Loss:  -1.1192 | Function Loss:  -1.724\n",
      "Total loss:  -0.5772 | PDE Loss:  -1.1195 | Function Loss:  -1.724\n",
      "Total loss:  -0.5773 | PDE Loss:  -1.1194 | Function Loss:  -1.7242\n",
      "Total loss:  -0.5773 | PDE Loss:  -1.1194 | Function Loss:  -1.7242\n",
      "Total loss:  -0.5774 | PDE Loss:  -1.1194 | Function Loss:  -1.7243\n",
      "Total loss:  -0.5774 | PDE Loss:  -1.1192 | Function Loss:  -1.7245\n",
      "Total loss:  -0.5775 | PDE Loss:  -1.1192 | Function Loss:  -1.7246\n",
      "Total loss:  -0.5776 | PDE Loss:  -1.1191 | Function Loss:  -1.7247\n",
      "Total loss:  -0.5777 | PDE Loss:  -1.1191 | Function Loss:  -1.7248\n",
      "Total loss:  -0.5777 | PDE Loss:  -1.1191 | Function Loss:  -1.7249\n",
      "Total loss:  -0.5769 | PDE Loss:  -1.1178 | Function Loss:  -1.7243\n",
      "Total loss:  -0.5777 | PDE Loss:  -1.1191 | Function Loss:  -1.7249\n",
      "Total loss:  -0.5778 | PDE Loss:  -1.1193 | Function Loss:  -1.7249\n",
      "Total loss:  -0.5779 | PDE Loss:  -1.1196 | Function Loss:  -1.725\n",
      "Total loss:  -0.578 | PDE Loss:  -1.1199 | Function Loss:  -1.725\n",
      "Total loss:  -0.5781 | PDE Loss:  -1.1204 | Function Loss:  -1.7249\n",
      "Total loss:  -0.5782 | PDE Loss:  -1.1205 | Function Loss:  -1.725\n",
      "Total loss:  -0.5783 | PDE Loss:  -1.1205 | Function Loss:  -1.7251\n",
      "Total loss:  -0.5783 | PDE Loss:  -1.1205 | Function Loss:  -1.7252\n",
      "Total loss:  -0.5784 | PDE Loss:  -1.1204 | Function Loss:  -1.7253\n",
      "Total loss:  -0.5785 | PDE Loss:  -1.1204 | Function Loss:  -1.7255\n",
      "Total loss:  -0.5785 | PDE Loss:  -1.1203 | Function Loss:  -1.7256\n",
      "Total loss:  -0.5786 | PDE Loss:  -1.1202 | Function Loss:  -1.7257\n",
      "Total loss:  -0.5786 | PDE Loss:  -1.1203 | Function Loss:  -1.7257\n",
      "Total loss:  -0.5787 | PDE Loss:  -1.1206 | Function Loss:  -1.7257\n",
      "Total loss:  -0.5788 | PDE Loss:  -1.121 | Function Loss:  -1.7256\n",
      "Total loss:  -0.5788 | PDE Loss:  -1.1213 | Function Loss:  -1.7255\n",
      "Total loss:  -0.5788 | PDE Loss:  -1.1217 | Function Loss:  -1.7255\n",
      "Total loss:  -0.5789 | PDE Loss:  -1.122 | Function Loss:  -1.7254\n",
      "Total loss:  -0.5789 | PDE Loss:  -1.1223 | Function Loss:  -1.7253\n",
      "Total loss:  -0.5789 | PDE Loss:  -1.1228 | Function Loss:  -1.7251\n",
      "Total loss:  -0.5787 | PDE Loss:  -1.1233 | Function Loss:  -1.7247\n",
      "Total loss:  -0.579 | PDE Loss:  -1.123 | Function Loss:  -1.7251\n",
      "Total loss:  -0.579 | PDE Loss:  -1.1233 | Function Loss:  -1.725\n",
      "Total loss:  -0.5791 | PDE Loss:  -1.1237 | Function Loss:  -1.725\n",
      "Total loss:  -0.5791 | PDE Loss:  -1.1239 | Function Loss:  -1.7249\n",
      "Total loss:  -0.5792 | PDE Loss:  -1.124 | Function Loss:  -1.725\n",
      "Total loss:  -0.5792 | PDE Loss:  -1.124 | Function Loss:  -1.7251\n",
      "Total loss:  -0.5793 | PDE Loss:  -1.1239 | Function Loss:  -1.7252\n",
      "Total loss:  -0.5793 | PDE Loss:  -1.1237 | Function Loss:  -1.7253\n",
      "Total loss:  -0.5794 | PDE Loss:  -1.1243 | Function Loss:  -1.7251\n",
      "Total loss:  -0.5794 | PDE Loss:  -1.1239 | Function Loss:  -1.7254\n",
      "Total loss:  -0.5794 | PDE Loss:  -1.1235 | Function Loss:  -1.7256\n",
      "Total loss:  -0.5795 | PDE Loss:  -1.1233 | Function Loss:  -1.7257\n",
      "Total loss:  -0.5795 | PDE Loss:  -1.1231 | Function Loss:  -1.7258\n",
      "Total loss:  -0.5795 | PDE Loss:  -1.123 | Function Loss:  -1.7259\n",
      "Total loss:  -0.5795 | PDE Loss:  -1.123 | Function Loss:  -1.7259\n",
      "Total loss:  -0.5796 | PDE Loss:  -1.123 | Function Loss:  -1.7259\n",
      "Total loss:  -0.5796 | PDE Loss:  -1.123 | Function Loss:  -1.7259\n",
      "Total loss:  -0.5796 | PDE Loss:  -1.1231 | Function Loss:  -1.726\n",
      "Total loss:  -0.5796 | PDE Loss:  -1.1231 | Function Loss:  -1.726\n",
      "Total loss:  -0.5797 | PDE Loss:  -1.1231 | Function Loss:  -1.726\n",
      "Total loss:  -0.5797 | PDE Loss:  -1.1231 | Function Loss:  -1.7261\n",
      "Total loss:  -0.5797 | PDE Loss:  -1.1231 | Function Loss:  -1.7262\n",
      "Total loss:  -0.5798 | PDE Loss:  -1.1231 | Function Loss:  -1.7262\n",
      "Total loss:  -0.5799 | PDE Loss:  -1.1226 | Function Loss:  -1.7265\n",
      "Total loss:  -0.5799 | PDE Loss:  -1.123 | Function Loss:  -1.7265\n",
      "Total loss:  -0.58 | PDE Loss:  -1.123 | Function Loss:  -1.7265\n",
      "Total loss:  -0.58 | PDE Loss:  -1.1232 | Function Loss:  -1.7265\n",
      "Total loss:  -0.58 | PDE Loss:  -1.1232 | Function Loss:  -1.7265\n",
      "Total loss:  -0.5801 | PDE Loss:  -1.1234 | Function Loss:  -1.7265\n",
      "Total loss:  -0.5801 | PDE Loss:  -1.1236 | Function Loss:  -1.7265\n",
      "Total loss:  -0.5802 | PDE Loss:  -1.1238 | Function Loss:  -1.7265\n",
      "Total loss:  -0.5802 | PDE Loss:  -1.1241 | Function Loss:  -1.7264\n",
      "Total loss:  -0.5802 | PDE Loss:  -1.1244 | Function Loss:  -1.7264\n",
      "Total loss:  -0.5803 | PDE Loss:  -1.1246 | Function Loss:  -1.7263\n",
      "Total loss:  -0.5803 | PDE Loss:  -1.1246 | Function Loss:  -1.7263\n",
      "Total loss:  -0.5803 | PDE Loss:  -1.1247 | Function Loss:  -1.7263\n",
      "Total loss:  -0.5804 | PDE Loss:  -1.1247 | Function Loss:  -1.7264\n",
      "Total loss:  -0.5804 | PDE Loss:  -1.1247 | Function Loss:  -1.7265\n",
      "Total loss:  -0.5805 | PDE Loss:  -1.1245 | Function Loss:  -1.7267\n",
      "Total loss:  -0.5806 | PDE Loss:  -1.1245 | Function Loss:  -1.7268\n",
      "Total loss:  -0.5807 | PDE Loss:  -1.1244 | Function Loss:  -1.7269\n",
      "Total loss:  -0.5808 | PDE Loss:  -1.1245 | Function Loss:  -1.727\n",
      "Total loss:  -0.5808 | PDE Loss:  -1.1245 | Function Loss:  -1.7271\n",
      "Total loss:  -0.5805 | PDE Loss:  -1.1257 | Function Loss:  -1.7262\n",
      "Total loss:  -0.5809 | PDE Loss:  -1.1249 | Function Loss:  -1.727\n",
      "Total loss:  -0.5809 | PDE Loss:  -1.1249 | Function Loss:  -1.7271\n",
      "Total loss:  -0.581 | PDE Loss:  -1.1251 | Function Loss:  -1.7271\n",
      "Total loss:  -0.5811 | PDE Loss:  -1.1254 | Function Loss:  -1.7271\n",
      "Total loss:  -0.5812 | PDE Loss:  -1.1258 | Function Loss:  -1.7271\n",
      "Total loss:  -0.5812 | PDE Loss:  -1.1263 | Function Loss:  -1.727\n",
      "Total loss:  -0.5813 | PDE Loss:  -1.1265 | Function Loss:  -1.727\n",
      "Total loss:  -0.5814 | PDE Loss:  -1.1267 | Function Loss:  -1.727\n",
      "Total loss:  -0.5815 | PDE Loss:  -1.1267 | Function Loss:  -1.7271\n",
      "Total loss:  -0.5816 | PDE Loss:  -1.1267 | Function Loss:  -1.7273\n",
      "Total loss:  -0.5817 | PDE Loss:  -1.1264 | Function Loss:  -1.7275\n",
      "Total loss:  -0.5818 | PDE Loss:  -1.126 | Function Loss:  -1.7278\n",
      "Total loss:  -0.5819 | PDE Loss:  -1.1254 | Function Loss:  -1.7282\n",
      "Total loss:  -0.582 | PDE Loss:  -1.1247 | Function Loss:  -1.7287\n",
      "Total loss:  -0.5821 | PDE Loss:  -1.1243 | Function Loss:  -1.729\n",
      "Total loss:  -0.5822 | PDE Loss:  -1.1241 | Function Loss:  -1.7292\n",
      "Total loss:  -0.5823 | PDE Loss:  -1.1239 | Function Loss:  -1.7294\n",
      "Total loss:  -0.5824 | PDE Loss:  -1.1239 | Function Loss:  -1.7295\n",
      "Total loss:  -0.5825 | PDE Loss:  -1.1241 | Function Loss:  -1.7296\n",
      "Total loss:  -0.5826 | PDE Loss:  -1.1239 | Function Loss:  -1.7299\n",
      "Total loss:  -0.5827 | PDE Loss:  -1.1243 | Function Loss:  -1.7299\n",
      "Total loss:  -0.5829 | PDE Loss:  -1.1248 | Function Loss:  -1.7299\n",
      "Total loss:  -0.5829 | PDE Loss:  -1.1265 | Function Loss:  -1.7292\n",
      "Total loss:  -0.583 | PDE Loss:  -1.1263 | Function Loss:  -1.7294\n",
      "Total loss:  -0.583 | PDE Loss:  -1.126 | Function Loss:  -1.7296\n",
      "Total loss:  -0.5831 | PDE Loss:  -1.1258 | Function Loss:  -1.7298\n",
      "Total loss:  -0.5831 | PDE Loss:  -1.1256 | Function Loss:  -1.7299\n",
      "Total loss:  -0.5832 | PDE Loss:  -1.1254 | Function Loss:  -1.7301\n",
      "Total loss:  -0.5833 | PDE Loss:  -1.1252 | Function Loss:  -1.7303\n",
      "Total loss:  -0.5834 | PDE Loss:  -1.1252 | Function Loss:  -1.7304\n",
      "Total loss:  -0.5835 | PDE Loss:  -1.1252 | Function Loss:  -1.7306\n",
      "Total loss:  -0.5836 | PDE Loss:  -1.1254 | Function Loss:  -1.7306\n",
      "Total loss:  -0.5837 | PDE Loss:  -1.1261 | Function Loss:  -1.7305\n",
      "Total loss:  -0.5838 | PDE Loss:  -1.1268 | Function Loss:  -1.7303\n",
      "Total loss:  -0.5838 | PDE Loss:  -1.1273 | Function Loss:  -1.7302\n",
      "Total loss:  -0.5839 | PDE Loss:  -1.1278 | Function Loss:  -1.7301\n",
      "Total loss:  -0.584 | PDE Loss:  -1.1281 | Function Loss:  -1.7301\n",
      "Total loss:  -0.5841 | PDE Loss:  -1.1283 | Function Loss:  -1.7301\n",
      "Total loss:  -0.5841 | PDE Loss:  -1.1283 | Function Loss:  -1.7302\n",
      "Total loss:  -0.5842 | PDE Loss:  -1.1283 | Function Loss:  -1.7303\n",
      "Total loss:  -0.5842 | PDE Loss:  -1.1283 | Function Loss:  -1.7304\n",
      "Total loss:  -0.5843 | PDE Loss:  -1.1282 | Function Loss:  -1.7305\n",
      "Total loss:  -0.5844 | PDE Loss:  -1.1283 | Function Loss:  -1.7306\n",
      "Total loss:  -0.5844 | PDE Loss:  -1.1284 | Function Loss:  -1.7306\n",
      "Total loss:  -0.5845 | PDE Loss:  -1.1289 | Function Loss:  -1.7305\n",
      "Total loss:  -0.5846 | PDE Loss:  -1.129 | Function Loss:  -1.7305\n",
      "Total loss:  -0.5846 | PDE Loss:  -1.1295 | Function Loss:  -1.7304\n",
      "Total loss:  -0.5847 | PDE Loss:  -1.13 | Function Loss:  -1.7303\n",
      "Total loss:  -0.5848 | PDE Loss:  -1.1305 | Function Loss:  -1.7302\n",
      "Total loss:  -0.5848 | PDE Loss:  -1.1308 | Function Loss:  -1.7302\n",
      "Total loss:  -0.5849 | PDE Loss:  -1.1309 | Function Loss:  -1.7302\n",
      "Total loss:  -0.5849 | PDE Loss:  -1.131 | Function Loss:  -1.7303\n",
      "Total loss:  -0.5849 | PDE Loss:  -1.1307 | Function Loss:  -1.7303\n",
      "Total loss:  -0.5849 | PDE Loss:  -1.1309 | Function Loss:  -1.7303\n",
      "Total loss:  -0.585 | PDE Loss:  -1.1308 | Function Loss:  -1.7304\n",
      "Total loss:  -0.585 | PDE Loss:  -1.1306 | Function Loss:  -1.7305\n",
      "Total loss:  -0.5851 | PDE Loss:  -1.1304 | Function Loss:  -1.7307\n",
      "Total loss:  -0.5851 | PDE Loss:  -1.1302 | Function Loss:  -1.7308\n",
      "Total loss:  -0.5852 | PDE Loss:  -1.1302 | Function Loss:  -1.7309\n",
      "Total loss:  -0.5852 | PDE Loss:  -1.1302 | Function Loss:  -1.731\n",
      "Total loss:  -0.5852 | PDE Loss:  -1.1306 | Function Loss:  -1.7309\n",
      "Total loss:  -0.5853 | PDE Loss:  -1.1306 | Function Loss:  -1.7309\n",
      "Total loss:  -0.5853 | PDE Loss:  -1.1308 | Function Loss:  -1.7309\n",
      "Total loss:  -0.5853 | PDE Loss:  -1.131 | Function Loss:  -1.7308\n",
      "Total loss:  -0.5853 | PDE Loss:  -1.1313 | Function Loss:  -1.7307\n",
      "Total loss:  -0.5854 | PDE Loss:  -1.1316 | Function Loss:  -1.7306\n",
      "Total loss:  -0.5854 | PDE Loss:  -1.1318 | Function Loss:  -1.7306\n",
      "Total loss:  -0.5854 | PDE Loss:  -1.132 | Function Loss:  -1.7305\n",
      "Total loss:  -0.5855 | PDE Loss:  -1.1319 | Function Loss:  -1.7307\n",
      "Total loss:  -0.5855 | PDE Loss:  -1.1321 | Function Loss:  -1.7306\n",
      "Total loss:  -0.5856 | PDE Loss:  -1.1321 | Function Loss:  -1.7307\n",
      "Total loss:  -0.5856 | PDE Loss:  -1.1321 | Function Loss:  -1.7308\n",
      "Total loss:  -0.5856 | PDE Loss:  -1.1319 | Function Loss:  -1.7309\n",
      "Total loss:  -0.5857 | PDE Loss:  -1.1318 | Function Loss:  -1.731\n",
      "Total loss:  -0.5857 | PDE Loss:  -1.1317 | Function Loss:  -1.731\n",
      "Total loss:  -0.5858 | PDE Loss:  -1.1318 | Function Loss:  -1.7311\n",
      "Total loss:  -0.5859 | PDE Loss:  -1.1319 | Function Loss:  -1.7312\n",
      "Total loss:  -0.586 | PDE Loss:  -1.1322 | Function Loss:  -1.7312\n",
      "Total loss:  -0.586 | PDE Loss:  -1.1326 | Function Loss:  -1.7312\n",
      "Total loss:  -0.5862 | PDE Loss:  -1.1332 | Function Loss:  -1.7311\n",
      "Total loss:  -0.5863 | PDE Loss:  -1.1339 | Function Loss:  -1.7311\n",
      "Total loss:  -0.5864 | PDE Loss:  -1.1344 | Function Loss:  -1.731\n",
      "Total loss:  -0.5865 | PDE Loss:  -1.135 | Function Loss:  -1.7309\n",
      "Total loss:  -0.5866 | PDE Loss:  -1.1353 | Function Loss:  -1.7309\n",
      "Total loss:  -0.5867 | PDE Loss:  -1.1354 | Function Loss:  -1.7309\n",
      "Total loss:  -0.5868 | PDE Loss:  -1.1352 | Function Loss:  -1.7312\n",
      "Total loss:  -0.5868 | PDE Loss:  -1.1352 | Function Loss:  -1.7313\n",
      "Total loss:  -0.5869 | PDE Loss:  -1.135 | Function Loss:  -1.7315\n",
      "Total loss:  -0.587 | PDE Loss:  -1.1347 | Function Loss:  -1.7317\n",
      "Total loss:  -0.5871 | PDE Loss:  -1.1344 | Function Loss:  -1.7319\n",
      "Total loss:  -0.5871 | PDE Loss:  -1.1341 | Function Loss:  -1.732\n",
      "Total loss:  -0.5871 | PDE Loss:  -1.1344 | Function Loss:  -1.732\n",
      "Total loss:  -0.5871 | PDE Loss:  -1.1342 | Function Loss:  -1.7321\n",
      "Total loss:  -0.5872 | PDE Loss:  -1.1341 | Function Loss:  -1.7321\n",
      "Total loss:  -0.5872 | PDE Loss:  -1.1341 | Function Loss:  -1.7322\n",
      "Total loss:  -0.5873 | PDE Loss:  -1.1342 | Function Loss:  -1.7323\n",
      "Total loss:  -0.5873 | PDE Loss:  -1.1342 | Function Loss:  -1.7323\n",
      "Total loss:  -0.5873 | PDE Loss:  -1.1343 | Function Loss:  -1.7323\n",
      "Total loss:  -0.5873 | PDE Loss:  -1.1344 | Function Loss:  -1.7323\n",
      "Total loss:  -0.5874 | PDE Loss:  -1.1345 | Function Loss:  -1.7323\n",
      "Total loss:  -0.5874 | PDE Loss:  -1.1347 | Function Loss:  -1.7323\n",
      "Total loss:  -0.5875 | PDE Loss:  -1.1347 | Function Loss:  -1.7324\n",
      "Total loss:  -0.5876 | PDE Loss:  -1.1351 | Function Loss:  -1.7324\n",
      "Total loss:  -0.5877 | PDE Loss:  -1.1352 | Function Loss:  -1.7324\n",
      "Total loss:  -0.5877 | PDE Loss:  -1.1353 | Function Loss:  -1.7325\n",
      "Total loss:  -0.5878 | PDE Loss:  -1.1356 | Function Loss:  -1.7325\n",
      "Total loss:  -0.5879 | PDE Loss:  -1.1357 | Function Loss:  -1.7326\n",
      "Total loss:  -0.588 | PDE Loss:  -1.1358 | Function Loss:  -1.7326\n",
      "Total loss:  -0.5881 | PDE Loss:  -1.1359 | Function Loss:  -1.7327\n",
      "Total loss:  -0.5881 | PDE Loss:  -1.1355 | Function Loss:  -1.7329\n",
      "Total loss:  -0.5882 | PDE Loss:  -1.1359 | Function Loss:  -1.7329\n",
      "Total loss:  -0.5883 | PDE Loss:  -1.136 | Function Loss:  -1.733\n",
      "Total loss:  -0.5884 | PDE Loss:  -1.1362 | Function Loss:  -1.733\n",
      "Total loss:  -0.5885 | PDE Loss:  -1.1362 | Function Loss:  -1.7331\n",
      "Total loss:  -0.5885 | PDE Loss:  -1.1361 | Function Loss:  -1.7333\n",
      "Total loss:  -0.5886 | PDE Loss:  -1.136 | Function Loss:  -1.7334\n",
      "Total loss:  -0.5887 | PDE Loss:  -1.1352 | Function Loss:  -1.7339\n",
      "Total loss:  -0.5889 | PDE Loss:  -1.1352 | Function Loss:  -1.7341\n",
      "Total loss:  -0.5889 | PDE Loss:  -1.1352 | Function Loss:  -1.7342\n",
      "Total loss:  -0.5891 | PDE Loss:  -1.1353 | Function Loss:  -1.7343\n",
      "Total loss:  -0.5892 | PDE Loss:  -1.1352 | Function Loss:  -1.7345\n",
      "Total loss:  -0.5888 | PDE Loss:  -1.1372 | Function Loss:  -1.7332\n",
      "Total loss:  -0.5892 | PDE Loss:  -1.1358 | Function Loss:  -1.7343\n",
      "Total loss:  -0.5892 | PDE Loss:  -1.1357 | Function Loss:  -1.7344\n",
      "Total loss:  -0.5893 | PDE Loss:  -1.1355 | Function Loss:  -1.7346\n",
      "Total loss:  -0.5894 | PDE Loss:  -1.1353 | Function Loss:  -1.7348\n",
      "Total loss:  -0.5896 | PDE Loss:  -1.1352 | Function Loss:  -1.7351\n",
      "Total loss:  -0.5897 | PDE Loss:  -1.1352 | Function Loss:  -1.7352\n",
      "Total loss:  -0.5898 | PDE Loss:  -1.1354 | Function Loss:  -1.7353\n",
      "Total loss:  -0.5897 | PDE Loss:  -1.1341 | Function Loss:  -1.7356\n",
      "Total loss:  -0.5898 | PDE Loss:  -1.1351 | Function Loss:  -1.7355\n",
      "Total loss:  -0.5899 | PDE Loss:  -1.1353 | Function Loss:  -1.7355\n",
      "Total loss:  -0.59 | PDE Loss:  -1.1356 | Function Loss:  -1.7355\n",
      "Total loss:  -0.59 | PDE Loss:  -1.1358 | Function Loss:  -1.7355\n",
      "Total loss:  -0.5901 | PDE Loss:  -1.1361 | Function Loss:  -1.7355\n",
      "Total loss:  -0.5902 | PDE Loss:  -1.1362 | Function Loss:  -1.7355\n",
      "Total loss:  -0.5902 | PDE Loss:  -1.1363 | Function Loss:  -1.7356\n",
      "Total loss:  -0.5903 | PDE Loss:  -1.1359 | Function Loss:  -1.7359\n",
      "Total loss:  -0.5904 | PDE Loss:  -1.1362 | Function Loss:  -1.7358\n",
      "Total loss:  -0.5904 | PDE Loss:  -1.1362 | Function Loss:  -1.7359\n",
      "Total loss:  -0.5905 | PDE Loss:  -1.1363 | Function Loss:  -1.736\n",
      "Total loss:  -0.5906 | PDE Loss:  -1.1364 | Function Loss:  -1.7361\n",
      "Total loss:  -0.5907 | PDE Loss:  -1.1366 | Function Loss:  -1.7361\n",
      "Total loss:  -0.5908 | PDE Loss:  -1.1371 | Function Loss:  -1.736\n",
      "Total loss:  -0.5909 | PDE Loss:  -1.1378 | Function Loss:  -1.7359\n",
      "Total loss:  -0.591 | PDE Loss:  -1.1385 | Function Loss:  -1.7358\n",
      "Total loss:  -0.5911 | PDE Loss:  -1.1392 | Function Loss:  -1.7356\n",
      "Total loss:  -0.5912 | PDE Loss:  -1.1398 | Function Loss:  -1.7355\n",
      "Total loss:  -0.5912 | PDE Loss:  -1.1403 | Function Loss:  -1.7353\n",
      "Total loss:  -0.5913 | PDE Loss:  -1.1408 | Function Loss:  -1.7352\n",
      "Total loss:  -0.5913 | PDE Loss:  -1.1413 | Function Loss:  -1.7351\n",
      "Total loss:  -0.5914 | PDE Loss:  -1.1418 | Function Loss:  -1.7351\n",
      "Total loss:  -0.5915 | PDE Loss:  -1.142 | Function Loss:  -1.7351\n",
      "Total loss:  -0.5916 | PDE Loss:  -1.1422 | Function Loss:  -1.7351\n",
      "Total loss:  -0.5917 | PDE Loss:  -1.1422 | Function Loss:  -1.7352\n",
      "Total loss:  -0.5917 | PDE Loss:  -1.1421 | Function Loss:  -1.7354\n",
      "Total loss:  -0.5918 | PDE Loss:  -1.1417 | Function Loss:  -1.7356\n",
      "Total loss:  -0.5919 | PDE Loss:  -1.1415 | Function Loss:  -1.7358\n",
      "Total loss:  -0.5919 | PDE Loss:  -1.1412 | Function Loss:  -1.736\n",
      "Total loss:  -0.592 | PDE Loss:  -1.141 | Function Loss:  -1.7361\n",
      "Total loss:  -0.592 | PDE Loss:  -1.1407 | Function Loss:  -1.7364\n",
      "Total loss:  -0.5921 | PDE Loss:  -1.1406 | Function Loss:  -1.7365\n",
      "Total loss:  -0.5922 | PDE Loss:  -1.1406 | Function Loss:  -1.7366\n",
      "Total loss:  -0.5922 | PDE Loss:  -1.1407 | Function Loss:  -1.7366\n",
      "Total loss:  -0.5923 | PDE Loss:  -1.1408 | Function Loss:  -1.7367\n",
      "Total loss:  -0.5924 | PDE Loss:  -1.1411 | Function Loss:  -1.7367\n",
      "Total loss:  -0.5924 | PDE Loss:  -1.1413 | Function Loss:  -1.7367\n",
      "Total loss:  -0.5925 | PDE Loss:  -1.1416 | Function Loss:  -1.7367\n",
      "Total loss:  -0.5926 | PDE Loss:  -1.142 | Function Loss:  -1.7366\n",
      "Total loss:  -0.5927 | PDE Loss:  -1.1422 | Function Loss:  -1.7366\n",
      "Total loss:  -0.5927 | PDE Loss:  -1.1424 | Function Loss:  -1.7367\n",
      "Total loss:  -0.5928 | PDE Loss:  -1.1424 | Function Loss:  -1.7368\n",
      "Total loss:  -0.5929 | PDE Loss:  -1.1424 | Function Loss:  -1.7369\n",
      "Total loss:  -0.5929 | PDE Loss:  -1.1424 | Function Loss:  -1.7369\n",
      "Total loss:  -0.593 | PDE Loss:  -1.1428 | Function Loss:  -1.7369\n",
      "Total loss:  -0.5931 | PDE Loss:  -1.1427 | Function Loss:  -1.737\n",
      "Total loss:  -0.5931 | PDE Loss:  -1.1431 | Function Loss:  -1.7369\n",
      "Total loss:  -0.5932 | PDE Loss:  -1.143 | Function Loss:  -1.7371\n",
      "Total loss:  -0.5933 | PDE Loss:  -1.1432 | Function Loss:  -1.7371\n",
      "Total loss:  -0.5934 | PDE Loss:  -1.1436 | Function Loss:  -1.7371\n",
      "Total loss:  -0.5935 | PDE Loss:  -1.1439 | Function Loss:  -1.737\n",
      "Total loss:  -0.5935 | PDE Loss:  -1.1457 | Function Loss:  -1.7364\n",
      "Total loss:  -0.5936 | PDE Loss:  -1.1457 | Function Loss:  -1.7366\n",
      "Total loss:  -0.5937 | PDE Loss:  -1.1456 | Function Loss:  -1.7367\n",
      "Total loss:  -0.5938 | PDE Loss:  -1.1457 | Function Loss:  -1.7368\n",
      "Total loss:  -0.5939 | PDE Loss:  -1.1461 | Function Loss:  -1.7368\n",
      "Total loss:  -0.594 | PDE Loss:  -1.1466 | Function Loss:  -1.7367\n",
      "Total loss:  -0.5941 | PDE Loss:  -1.1472 | Function Loss:  -1.7367\n",
      "Total loss:  -0.5942 | PDE Loss:  -1.1481 | Function Loss:  -1.7365\n",
      "Total loss:  -0.594 | PDE Loss:  -1.1496 | Function Loss:  -1.7356\n",
      "Total loss:  -0.5943 | PDE Loss:  -1.1487 | Function Loss:  -1.7363\n",
      "Total loss:  -0.5944 | PDE Loss:  -1.1493 | Function Loss:  -1.7363\n",
      "Total loss:  -0.5945 | PDE Loss:  -1.1498 | Function Loss:  -1.7362\n",
      "Total loss:  -0.5946 | PDE Loss:  -1.1502 | Function Loss:  -1.7363\n",
      "Total loss:  -0.5947 | PDE Loss:  -1.1505 | Function Loss:  -1.7363\n",
      "Total loss:  -0.5948 | PDE Loss:  -1.1506 | Function Loss:  -1.7363\n",
      "Total loss:  -0.5949 | PDE Loss:  -1.1506 | Function Loss:  -1.7364\n",
      "Total loss:  -0.595 | PDE Loss:  -1.1504 | Function Loss:  -1.7366\n",
      "Total loss:  -0.595 | PDE Loss:  -1.1502 | Function Loss:  -1.7368\n",
      "Total loss:  -0.5951 | PDE Loss:  -1.1492 | Function Loss:  -1.7373\n",
      "Total loss:  -0.5952 | PDE Loss:  -1.1492 | Function Loss:  -1.7374\n",
      "Total loss:  -0.5953 | PDE Loss:  -1.1493 | Function Loss:  -1.7375\n",
      "Total loss:  -0.5953 | PDE Loss:  -1.1494 | Function Loss:  -1.7375\n",
      "Total loss:  -0.5954 | PDE Loss:  -1.1493 | Function Loss:  -1.7377\n",
      "Total loss:  -0.5955 | PDE Loss:  -1.1494 | Function Loss:  -1.7377\n",
      "Total loss:  -0.5955 | PDE Loss:  -1.1493 | Function Loss:  -1.7378\n",
      "Total loss:  -0.5955 | PDE Loss:  -1.1493 | Function Loss:  -1.7378\n",
      "Total loss:  -0.5956 | PDE Loss:  -1.1493 | Function Loss:  -1.7379\n",
      "Total loss:  -0.5956 | PDE Loss:  -1.1493 | Function Loss:  -1.738\n",
      "Total loss:  -0.5957 | PDE Loss:  -1.1492 | Function Loss:  -1.7381\n",
      "Total loss:  -0.5957 | PDE Loss:  -1.1493 | Function Loss:  -1.7381\n",
      "Total loss:  -0.5958 | PDE Loss:  -1.1495 | Function Loss:  -1.7381\n",
      "Total loss:  -0.5959 | PDE Loss:  -1.1497 | Function Loss:  -1.7381\n",
      "Total loss:  -0.5959 | PDE Loss:  -1.1499 | Function Loss:  -1.7382\n",
      "Total loss:  -0.596 | PDE Loss:  -1.1502 | Function Loss:  -1.7382\n",
      "Total loss:  -0.5961 | PDE Loss:  -1.1504 | Function Loss:  -1.7382\n",
      "Total loss:  -0.5961 | PDE Loss:  -1.1506 | Function Loss:  -1.7382\n",
      "Total loss:  -0.5962 | PDE Loss:  -1.1507 | Function Loss:  -1.7382\n",
      "Total loss:  -0.5962 | PDE Loss:  -1.1509 | Function Loss:  -1.7382\n",
      "Total loss:  -0.5963 | PDE Loss:  -1.1509 | Function Loss:  -1.7383\n",
      "Total loss:  -0.5963 | PDE Loss:  -1.151 | Function Loss:  -1.7383\n",
      "Total loss:  -0.5964 | PDE Loss:  -1.1509 | Function Loss:  -1.7384\n",
      "Total loss:  -0.5964 | PDE Loss:  -1.1509 | Function Loss:  -1.7385\n",
      "Total loss:  -0.5965 | PDE Loss:  -1.151 | Function Loss:  -1.7385\n",
      "Total loss:  -0.5965 | PDE Loss:  -1.1509 | Function Loss:  -1.7386\n",
      "Total loss:  -0.5966 | PDE Loss:  -1.1509 | Function Loss:  -1.7387\n",
      "Total loss:  -0.5967 | PDE Loss:  -1.1511 | Function Loss:  -1.7387\n",
      "Total loss:  -0.5967 | PDE Loss:  -1.1513 | Function Loss:  -1.7388\n",
      "Total loss:  -0.5968 | PDE Loss:  -1.1518 | Function Loss:  -1.7386\n",
      "Total loss:  -0.5969 | PDE Loss:  -1.152 | Function Loss:  -1.7387\n",
      "Total loss:  -0.5969 | PDE Loss:  -1.1524 | Function Loss:  -1.7386\n",
      "Total loss:  -0.597 | PDE Loss:  -1.1527 | Function Loss:  -1.7385\n",
      "Total loss:  -0.5971 | PDE Loss:  -1.1531 | Function Loss:  -1.7385\n",
      "Total loss:  -0.5972 | PDE Loss:  -1.1533 | Function Loss:  -1.7386\n",
      "Total loss:  -0.5972 | PDE Loss:  -1.1534 | Function Loss:  -1.7386\n",
      "Total loss:  -0.5973 | PDE Loss:  -1.1532 | Function Loss:  -1.7388\n",
      "Total loss:  -0.5974 | PDE Loss:  -1.1529 | Function Loss:  -1.7391\n",
      "Total loss:  -0.5975 | PDE Loss:  -1.1525 | Function Loss:  -1.7393\n",
      "Total loss:  -0.5976 | PDE Loss:  -1.1519 | Function Loss:  -1.7397\n",
      "Total loss:  -0.5977 | PDE Loss:  -1.1511 | Function Loss:  -1.7401\n",
      "Total loss:  -0.5978 | PDE Loss:  -1.1504 | Function Loss:  -1.7405\n",
      "Total loss:  -0.5979 | PDE Loss:  -1.1499 | Function Loss:  -1.7409\n",
      "Total loss:  -0.598 | PDE Loss:  -1.1493 | Function Loss:  -1.7413\n",
      "Total loss:  -0.5981 | PDE Loss:  -1.1492 | Function Loss:  -1.7415\n",
      "Total loss:  -0.5983 | PDE Loss:  -1.1486 | Function Loss:  -1.7419\n",
      "Total loss:  -0.5984 | PDE Loss:  -1.1486 | Function Loss:  -1.7421\n",
      "Total loss:  -0.5985 | PDE Loss:  -1.1486 | Function Loss:  -1.7422\n",
      "Total loss:  -0.5986 | PDE Loss:  -1.149 | Function Loss:  -1.7422\n",
      "Total loss:  -0.5987 | PDE Loss:  -1.1492 | Function Loss:  -1.7422\n",
      "Total loss:  -0.5988 | PDE Loss:  -1.1495 | Function Loss:  -1.7423\n",
      "Total loss:  -0.5989 | PDE Loss:  -1.1495 | Function Loss:  -1.7425\n",
      "Total loss:  -0.5991 | PDE Loss:  -1.1494 | Function Loss:  -1.7427\n",
      "Total loss:  -0.5992 | PDE Loss:  -1.1489 | Function Loss:  -1.7431\n",
      "Total loss:  -0.5993 | PDE Loss:  -1.1486 | Function Loss:  -1.7434\n",
      "Total loss:  -0.5994 | PDE Loss:  -1.148 | Function Loss:  -1.7437\n",
      "Total loss:  -0.5995 | PDE Loss:  -1.1476 | Function Loss:  -1.744\n",
      "Total loss:  -0.5995 | PDE Loss:  -1.1471 | Function Loss:  -1.7442\n",
      "Total loss:  -0.5996 | PDE Loss:  -1.1468 | Function Loss:  -1.7445\n",
      "Total loss:  -0.5997 | PDE Loss:  -1.1466 | Function Loss:  -1.7446\n",
      "Total loss:  -0.5997 | PDE Loss:  -1.1466 | Function Loss:  -1.7448\n",
      "Total loss:  -0.5998 | PDE Loss:  -1.1469 | Function Loss:  -1.7447\n",
      "Total loss:  -0.5999 | PDE Loss:  -1.147 | Function Loss:  -1.7448\n",
      "Total loss:  -0.5999 | PDE Loss:  -1.1472 | Function Loss:  -1.7448\n",
      "Total loss:  -0.6 | PDE Loss:  -1.1478 | Function Loss:  -1.7447\n",
      "Total loss:  -0.6001 | PDE Loss:  -1.1482 | Function Loss:  -1.7446\n",
      "Total loss:  -0.6001 | PDE Loss:  -1.1485 | Function Loss:  -1.7445\n",
      "Total loss:  -0.6002 | PDE Loss:  -1.1484 | Function Loss:  -1.7447\n",
      "Total loss:  -0.6004 | PDE Loss:  -1.149 | Function Loss:  -1.7447\n",
      "Total loss:  -0.6005 | PDE Loss:  -1.1492 | Function Loss:  -1.7448\n",
      "Total loss:  -0.6007 | PDE Loss:  -1.149 | Function Loss:  -1.7451\n",
      "Total loss:  -0.6008 | PDE Loss:  -1.1486 | Function Loss:  -1.7455\n",
      "Total loss:  -0.601 | PDE Loss:  -1.1483 | Function Loss:  -1.7458\n",
      "Total loss:  -0.601 | PDE Loss:  -1.1479 | Function Loss:  -1.7461\n",
      "Total loss:  -0.6011 | PDE Loss:  -1.1477 | Function Loss:  -1.7462\n",
      "Total loss:  -0.6012 | PDE Loss:  -1.1475 | Function Loss:  -1.7464\n",
      "Total loss:  -0.6012 | PDE Loss:  -1.1475 | Function Loss:  -1.7465\n",
      "Total loss:  -0.6013 | PDE Loss:  -1.1476 | Function Loss:  -1.7465\n",
      "Total loss:  -0.6013 | PDE Loss:  -1.1478 | Function Loss:  -1.7465\n",
      "Total loss:  -0.6011 | PDE Loss:  -1.1463 | Function Loss:  -1.7468\n",
      "Total loss:  -0.6013 | PDE Loss:  -1.1477 | Function Loss:  -1.7466\n",
      "Total loss:  -0.6014 | PDE Loss:  -1.1478 | Function Loss:  -1.7466\n",
      "Total loss:  -0.6015 | PDE Loss:  -1.1481 | Function Loss:  -1.7466\n",
      "Total loss:  -0.6016 | PDE Loss:  -1.1484 | Function Loss:  -1.7466\n",
      "Total loss:  -0.6016 | PDE Loss:  -1.1486 | Function Loss:  -1.7466\n",
      "Total loss:  -0.6017 | PDE Loss:  -1.1491 | Function Loss:  -1.7465\n",
      "Total loss:  -0.6018 | PDE Loss:  -1.1491 | Function Loss:  -1.7467\n",
      "Total loss:  -0.6019 | PDE Loss:  -1.1492 | Function Loss:  -1.7467\n",
      "Total loss:  -0.602 | PDE Loss:  -1.1488 | Function Loss:  -1.747\n",
      "Total loss:  -0.6021 | PDE Loss:  -1.1488 | Function Loss:  -1.7472\n",
      "Total loss:  -0.6022 | PDE Loss:  -1.1487 | Function Loss:  -1.7474\n",
      "Total loss:  -0.6023 | PDE Loss:  -1.1486 | Function Loss:  -1.7476\n",
      "Total loss:  -0.6024 | PDE Loss:  -1.1485 | Function Loss:  -1.7478\n",
      "Total loss:  -0.6025 | PDE Loss:  -1.1484 | Function Loss:  -1.7479\n",
      "Total loss:  -0.6026 | PDE Loss:  -1.1484 | Function Loss:  -1.748\n",
      "Total loss:  -0.6027 | PDE Loss:  -1.1483 | Function Loss:  -1.7482\n",
      "Total loss:  -0.6027 | PDE Loss:  -1.1483 | Function Loss:  -1.7483\n",
      "Total loss:  -0.6028 | PDE Loss:  -1.1483 | Function Loss:  -1.7484\n",
      "Total loss:  -0.6029 | PDE Loss:  -1.1484 | Function Loss:  -1.7485\n",
      "Total loss:  -0.603 | PDE Loss:  -1.1483 | Function Loss:  -1.7486\n",
      "Total loss:  -0.6031 | PDE Loss:  -1.1484 | Function Loss:  -1.7487\n",
      "Total loss:  -0.6031 | PDE Loss:  -1.1483 | Function Loss:  -1.7488\n",
      "Total loss:  -0.6032 | PDE Loss:  -1.1484 | Function Loss:  -1.7489\n",
      "Total loss:  -0.6032 | PDE Loss:  -1.1482 | Function Loss:  -1.749\n",
      "Total loss:  -0.6033 | PDE Loss:  -1.1482 | Function Loss:  -1.7491\n",
      "Total loss:  -0.6034 | PDE Loss:  -1.1481 | Function Loss:  -1.7492\n",
      "Total loss:  -0.6034 | PDE Loss:  -1.1482 | Function Loss:  -1.7493\n",
      "Total loss:  -0.6035 | PDE Loss:  -1.1482 | Function Loss:  -1.7494\n",
      "Total loss:  -0.6037 | PDE Loss:  -1.1485 | Function Loss:  -1.7495\n",
      "Total loss:  -0.6036 | PDE Loss:  -1.1474 | Function Loss:  -1.7498\n",
      "Total loss:  -0.6037 | PDE Loss:  -1.1482 | Function Loss:  -1.7496\n",
      "Total loss:  -0.6038 | PDE Loss:  -1.1486 | Function Loss:  -1.7496\n",
      "Total loss:  -0.6039 | PDE Loss:  -1.1491 | Function Loss:  -1.7496\n",
      "Total loss:  -0.604 | PDE Loss:  -1.1494 | Function Loss:  -1.7496\n",
      "Total loss:  -0.6041 | PDE Loss:  -1.1501 | Function Loss:  -1.7495\n",
      "Total loss:  -0.6042 | PDE Loss:  -1.15 | Function Loss:  -1.7497\n",
      "Total loss:  -0.6043 | PDE Loss:  -1.1502 | Function Loss:  -1.7497\n",
      "Total loss:  -0.6043 | PDE Loss:  -1.1502 | Function Loss:  -1.7498\n",
      "Total loss:  -0.6044 | PDE Loss:  -1.1501 | Function Loss:  -1.7499\n",
      "Total loss:  -0.6045 | PDE Loss:  -1.1501 | Function Loss:  -1.75\n",
      "Total loss:  -0.6046 | PDE Loss:  -1.15 | Function Loss:  -1.7501\n",
      "Total loss:  -0.6046 | PDE Loss:  -1.1501 | Function Loss:  -1.7502\n",
      "Total loss:  -0.6047 | PDE Loss:  -1.1501 | Function Loss:  -1.7503\n",
      "Total loss:  -0.6048 | PDE Loss:  -1.1503 | Function Loss:  -1.7503\n",
      "Total loss:  -0.6049 | PDE Loss:  -1.1506 | Function Loss:  -1.7504\n",
      "Total loss:  -0.6049 | PDE Loss:  -1.1509 | Function Loss:  -1.7503\n",
      "Total loss:  -0.605 | PDE Loss:  -1.1513 | Function Loss:  -1.7503\n",
      "Total loss:  -0.6051 | PDE Loss:  -1.1518 | Function Loss:  -1.7502\n",
      "Total loss:  -0.6052 | PDE Loss:  -1.1524 | Function Loss:  -1.7501\n",
      "Total loss:  -0.6053 | PDE Loss:  -1.1527 | Function Loss:  -1.7501\n",
      "Total loss:  -0.6054 | PDE Loss:  -1.1531 | Function Loss:  -1.7501\n",
      "Total loss:  -0.6055 | PDE Loss:  -1.1532 | Function Loss:  -1.7501\n",
      "Total loss:  -0.6055 | PDE Loss:  -1.1531 | Function Loss:  -1.7502\n",
      "Total loss:  -0.6056 | PDE Loss:  -1.1531 | Function Loss:  -1.7503\n",
      "Total loss:  -0.6056 | PDE Loss:  -1.1529 | Function Loss:  -1.7504\n",
      "Total loss:  -0.6056 | PDE Loss:  -1.1529 | Function Loss:  -1.7505\n",
      "Total loss:  -0.6057 | PDE Loss:  -1.1528 | Function Loss:  -1.7506\n",
      "Total loss:  -0.6057 | PDE Loss:  -1.153 | Function Loss:  -1.7506\n",
      "Total loss:  -0.6058 | PDE Loss:  -1.1528 | Function Loss:  -1.7507\n",
      "Total loss:  -0.6059 | PDE Loss:  -1.153 | Function Loss:  -1.7507\n",
      "Total loss:  -0.6059 | PDE Loss:  -1.1532 | Function Loss:  -1.7508\n",
      "Total loss:  -0.606 | PDE Loss:  -1.1533 | Function Loss:  -1.7508\n",
      "Total loss:  -0.606 | PDE Loss:  -1.1535 | Function Loss:  -1.7508\n",
      "Total loss:  -0.6061 | PDE Loss:  -1.1535 | Function Loss:  -1.7509\n",
      "Total loss:  -0.6062 | PDE Loss:  -1.1535 | Function Loss:  -1.751\n",
      "Total loss:  -0.6063 | PDE Loss:  -1.1534 | Function Loss:  -1.7512\n",
      "Total loss:  -0.6064 | PDE Loss:  -1.1533 | Function Loss:  -1.7513\n",
      "Total loss:  -0.6064 | PDE Loss:  -1.1526 | Function Loss:  -1.7518\n",
      "Total loss:  -0.6065 | PDE Loss:  -1.1525 | Function Loss:  -1.7519\n",
      "Total loss:  -0.6066 | PDE Loss:  -1.1525 | Function Loss:  -1.7521\n",
      "Total loss:  -0.6068 | PDE Loss:  -1.1525 | Function Loss:  -1.7523\n",
      "Total loss:  -0.6069 | PDE Loss:  -1.1526 | Function Loss:  -1.7524\n",
      "Total loss:  -0.6071 | PDE Loss:  -1.1527 | Function Loss:  -1.7526\n",
      "Total loss:  -0.6072 | PDE Loss:  -1.1528 | Function Loss:  -1.7527\n",
      "Total loss:  -0.6073 | PDE Loss:  -1.1531 | Function Loss:  -1.7528\n",
      "Total loss:  -0.6075 | PDE Loss:  -1.1535 | Function Loss:  -1.7528\n",
      "Total loss:  -0.6076 | PDE Loss:  -1.154 | Function Loss:  -1.7528\n",
      "Total loss:  -0.6077 | PDE Loss:  -1.154 | Function Loss:  -1.753\n",
      "Total loss:  -0.6078 | PDE Loss:  -1.1541 | Function Loss:  -1.753\n",
      "Total loss:  -0.6078 | PDE Loss:  -1.154 | Function Loss:  -1.7531\n",
      "Total loss:  -0.608 | PDE Loss:  -1.1536 | Function Loss:  -1.7535\n",
      "Total loss:  -0.6081 | PDE Loss:  -1.1534 | Function Loss:  -1.7537\n",
      "Total loss:  -0.6082 | PDE Loss:  -1.1532 | Function Loss:  -1.754\n",
      "Total loss:  -0.6083 | PDE Loss:  -1.1531 | Function Loss:  -1.7542\n",
      "Total loss:  -0.6084 | PDE Loss:  -1.153 | Function Loss:  -1.7543\n",
      "Total loss:  -0.6085 | PDE Loss:  -1.1534 | Function Loss:  -1.7543\n",
      "Total loss:  -0.6086 | PDE Loss:  -1.1539 | Function Loss:  -1.7543\n",
      "Total loss:  -0.6088 | PDE Loss:  -1.1545 | Function Loss:  -1.7543\n",
      "Total loss:  -0.6089 | PDE Loss:  -1.1553 | Function Loss:  -1.7541\n",
      "Total loss:  -0.6091 | PDE Loss:  -1.1562 | Function Loss:  -1.754\n",
      "Total loss:  -0.6092 | PDE Loss:  -1.1568 | Function Loss:  -1.7539\n",
      "Total loss:  -0.6092 | PDE Loss:  -1.1573 | Function Loss:  -1.7538\n",
      "Total loss:  -0.6093 | PDE Loss:  -1.1577 | Function Loss:  -1.7537\n",
      "Total loss:  -0.6094 | PDE Loss:  -1.1579 | Function Loss:  -1.7537\n",
      "Total loss:  -0.6094 | PDE Loss:  -1.1581 | Function Loss:  -1.7537\n",
      "Total loss:  -0.6095 | PDE Loss:  -1.158 | Function Loss:  -1.7539\n",
      "Total loss:  -0.6096 | PDE Loss:  -1.158 | Function Loss:  -1.754\n",
      "Total loss:  -0.6097 | PDE Loss:  -1.1578 | Function Loss:  -1.7542\n",
      "Total loss:  -0.6098 | PDE Loss:  -1.1576 | Function Loss:  -1.7544\n",
      "Total loss:  -0.6099 | PDE Loss:  -1.1576 | Function Loss:  -1.7545\n",
      "Total loss:  -0.61 | PDE Loss:  -1.1563 | Function Loss:  -1.7553\n",
      "Total loss:  -0.6101 | PDE Loss:  -1.1563 | Function Loss:  -1.7554\n",
      "Total loss:  -0.6102 | PDE Loss:  -1.1565 | Function Loss:  -1.7555\n",
      "Total loss:  -0.6104 | PDE Loss:  -1.1568 | Function Loss:  -1.7556\n",
      "Total loss:  -0.6106 | PDE Loss:  -1.157 | Function Loss:  -1.7558\n",
      "Total loss:  -0.6107 | PDE Loss:  -1.1571 | Function Loss:  -1.7559\n",
      "Total loss:  -0.6109 | PDE Loss:  -1.1573 | Function Loss:  -1.756\n",
      "Total loss:  -0.611 | PDE Loss:  -1.1573 | Function Loss:  -1.7563\n",
      "Total loss:  -0.6112 | PDE Loss:  -1.1569 | Function Loss:  -1.7566\n",
      "Total loss:  -0.6112 | PDE Loss:  -1.1569 | Function Loss:  -1.7567\n",
      "Total loss:  -0.6113 | PDE Loss:  -1.1568 | Function Loss:  -1.7569\n",
      "Total loss:  -0.6114 | PDE Loss:  -1.1567 | Function Loss:  -1.7571\n",
      "Total loss:  -0.6115 | PDE Loss:  -1.1569 | Function Loss:  -1.7572\n",
      "Total loss:  -0.6117 | PDE Loss:  -1.157 | Function Loss:  -1.7573\n",
      "Total loss:  -0.6118 | PDE Loss:  -1.1573 | Function Loss:  -1.7573\n",
      "Total loss:  -0.612 | PDE Loss:  -1.1573 | Function Loss:  -1.7576\n",
      "Total loss:  -0.6121 | PDE Loss:  -1.1572 | Function Loss:  -1.7579\n",
      "Total loss:  -0.6123 | PDE Loss:  -1.1574 | Function Loss:  -1.758\n",
      "Total loss:  -0.6124 | PDE Loss:  -1.1574 | Function Loss:  -1.7582\n",
      "Total loss:  -0.6125 | PDE Loss:  -1.1576 | Function Loss:  -1.7582\n",
      "Total loss:  -0.6126 | PDE Loss:  -1.1578 | Function Loss:  -1.7583\n",
      "Total loss:  -0.6127 | PDE Loss:  -1.158 | Function Loss:  -1.7583\n",
      "Total loss:  -0.6128 | PDE Loss:  -1.158 | Function Loss:  -1.7584\n",
      "Total loss:  -0.6128 | PDE Loss:  -1.1587 | Function Loss:  -1.7582\n",
      "Total loss:  -0.6129 | PDE Loss:  -1.1583 | Function Loss:  -1.7585\n",
      "Total loss:  -0.613 | PDE Loss:  -1.1583 | Function Loss:  -1.7586\n",
      "Total loss:  -0.6131 | PDE Loss:  -1.1583 | Function Loss:  -1.7588\n",
      "Total loss:  -0.6132 | PDE Loss:  -1.1584 | Function Loss:  -1.7589\n",
      "Total loss:  -0.6133 | PDE Loss:  -1.1585 | Function Loss:  -1.7589\n",
      "Total loss:  -0.6133 | PDE Loss:  -1.1587 | Function Loss:  -1.7589\n",
      "Total loss:  -0.6134 | PDE Loss:  -1.1592 | Function Loss:  -1.7589\n",
      "Total loss:  -0.6135 | PDE Loss:  -1.1588 | Function Loss:  -1.7592\n",
      "Total loss:  -0.6137 | PDE Loss:  -1.1599 | Function Loss:  -1.7589\n",
      "Total loss:  -0.6137 | PDE Loss:  -1.1605 | Function Loss:  -1.7588\n",
      "Total loss:  -0.6138 | PDE Loss:  -1.1613 | Function Loss:  -1.7586\n",
      "Total loss:  -0.6139 | PDE Loss:  -1.1617 | Function Loss:  -1.7586\n",
      "Total loss:  -0.614 | PDE Loss:  -1.162 | Function Loss:  -1.7585\n",
      "Total loss:  -0.614 | PDE Loss:  -1.1621 | Function Loss:  -1.7586\n",
      "Total loss:  -0.6141 | PDE Loss:  -1.162 | Function Loss:  -1.7586\n",
      "Total loss:  -0.6142 | PDE Loss:  -1.1619 | Function Loss:  -1.7588\n",
      "Total loss:  -0.6143 | PDE Loss:  -1.1617 | Function Loss:  -1.7591\n",
      "Total loss:  -0.6144 | PDE Loss:  -1.1613 | Function Loss:  -1.7594\n",
      "Total loss:  -0.6145 | PDE Loss:  -1.1608 | Function Loss:  -1.7598\n",
      "Total loss:  -0.6146 | PDE Loss:  -1.1608 | Function Loss:  -1.7599\n",
      "Total loss:  -0.6147 | PDE Loss:  -1.1608 | Function Loss:  -1.76\n",
      "Total loss:  -0.6148 | PDE Loss:  -1.1611 | Function Loss:  -1.76\n",
      "Total loss:  -0.6148 | PDE Loss:  -1.1611 | Function Loss:  -1.76\n",
      "Total loss:  -0.6148 | PDE Loss:  -1.1612 | Function Loss:  -1.76\n",
      "Total loss:  -0.6149 | PDE Loss:  -1.1614 | Function Loss:  -1.7601\n",
      "Total loss:  -0.6151 | PDE Loss:  -1.162 | Function Loss:  -1.7601\n",
      "Total loss:  -0.6152 | PDE Loss:  -1.1622 | Function Loss:  -1.7602\n",
      "Total loss:  -0.6154 | PDE Loss:  -1.1625 | Function Loss:  -1.7603\n",
      "Total loss:  -0.6156 | PDE Loss:  -1.1626 | Function Loss:  -1.7605\n",
      "Total loss:  -0.6157 | PDE Loss:  -1.1628 | Function Loss:  -1.7606\n",
      "Total loss:  -0.6158 | PDE Loss:  -1.1628 | Function Loss:  -1.7608\n",
      "Total loss:  -0.6159 | PDE Loss:  -1.1629 | Function Loss:  -1.7608\n",
      "Total loss:  -0.616 | PDE Loss:  -1.1631 | Function Loss:  -1.761\n",
      "Total loss:  -0.6162 | PDE Loss:  -1.1634 | Function Loss:  -1.761\n",
      "Total loss:  -0.6163 | PDE Loss:  -1.1639 | Function Loss:  -1.7611\n",
      "Total loss:  -0.6165 | PDE Loss:  -1.1644 | Function Loss:  -1.7611\n",
      "Total loss:  -0.6166 | PDE Loss:  -1.1649 | Function Loss:  -1.7611\n",
      "Total loss:  -0.6167 | PDE Loss:  -1.1651 | Function Loss:  -1.7612\n",
      "Total loss:  -0.6168 | PDE Loss:  -1.1652 | Function Loss:  -1.7612\n",
      "Total loss:  -0.6169 | PDE Loss:  -1.1652 | Function Loss:  -1.7613\n",
      "Total loss:  -0.6169 | PDE Loss:  -1.1652 | Function Loss:  -1.7614\n",
      "Total loss:  -0.617 | PDE Loss:  -1.165 | Function Loss:  -1.7616\n",
      "Total loss:  -0.6172 | PDE Loss:  -1.1647 | Function Loss:  -1.7619\n",
      "Total loss:  -0.6173 | PDE Loss:  -1.164 | Function Loss:  -1.7623\n",
      "Total loss:  -0.6174 | PDE Loss:  -1.164 | Function Loss:  -1.7625\n",
      "Total loss:  -0.6174 | PDE Loss:  -1.1638 | Function Loss:  -1.7626\n",
      "Total loss:  -0.6175 | PDE Loss:  -1.1638 | Function Loss:  -1.7627\n",
      "Total loss:  -0.6176 | PDE Loss:  -1.1638 | Function Loss:  -1.7628\n",
      "Total loss:  -0.6177 | PDE Loss:  -1.1639 | Function Loss:  -1.7629\n",
      "Total loss:  -0.6177 | PDE Loss:  -1.1641 | Function Loss:  -1.763\n",
      "Total loss:  -0.6179 | PDE Loss:  -1.1646 | Function Loss:  -1.7629\n",
      "Total loss:  -0.6179 | PDE Loss:  -1.1648 | Function Loss:  -1.763\n",
      "Total loss:  -0.618 | PDE Loss:  -1.1651 | Function Loss:  -1.7629\n",
      "Total loss:  -0.6181 | PDE Loss:  -1.1656 | Function Loss:  -1.7629\n",
      "Total loss:  -0.6182 | PDE Loss:  -1.1661 | Function Loss:  -1.7629\n",
      "Total loss:  -0.6183 | PDE Loss:  -1.1664 | Function Loss:  -1.7628\n",
      "Total loss:  -0.6184 | PDE Loss:  -1.1671 | Function Loss:  -1.7627\n",
      "Total loss:  -0.6185 | PDE Loss:  -1.1673 | Function Loss:  -1.7628\n",
      "Total loss:  -0.6186 | PDE Loss:  -1.1676 | Function Loss:  -1.7628\n",
      "Total loss:  -0.6187 | PDE Loss:  -1.1676 | Function Loss:  -1.7629\n",
      "Total loss:  -0.6187 | PDE Loss:  -1.1676 | Function Loss:  -1.763\n",
      "Total loss:  -0.6188 | PDE Loss:  -1.1676 | Function Loss:  -1.7631\n",
      "Total loss:  -0.6189 | PDE Loss:  -1.1675 | Function Loss:  -1.7632\n",
      "Total loss:  -0.619 | PDE Loss:  -1.1676 | Function Loss:  -1.7633\n",
      "Total loss:  -0.619 | PDE Loss:  -1.1667 | Function Loss:  -1.7637\n",
      "Total loss:  -0.6191 | PDE Loss:  -1.1669 | Function Loss:  -1.7638\n",
      "Total loss:  -0.6192 | PDE Loss:  -1.1673 | Function Loss:  -1.7638\n",
      "Total loss:  -0.6193 | PDE Loss:  -1.1678 | Function Loss:  -1.7637\n",
      "Total loss:  -0.6194 | PDE Loss:  -1.1683 | Function Loss:  -1.7637\n",
      "Total loss:  -0.6195 | PDE Loss:  -1.1687 | Function Loss:  -1.7636\n",
      "Total loss:  -0.6196 | PDE Loss:  -1.1693 | Function Loss:  -1.7635\n",
      "Total loss:  -0.6197 | PDE Loss:  -1.1698 | Function Loss:  -1.7634\n",
      "Total loss:  -0.6197 | PDE Loss:  -1.1703 | Function Loss:  -1.7633\n",
      "Total loss:  -0.6198 | PDE Loss:  -1.1708 | Function Loss:  -1.7632\n",
      "Total loss:  -0.6199 | PDE Loss:  -1.1714 | Function Loss:  -1.7631\n",
      "Total loss:  -0.62 | PDE Loss:  -1.1719 | Function Loss:  -1.7631\n",
      "Total loss:  -0.6201 | PDE Loss:  -1.1723 | Function Loss:  -1.763\n",
      "Total loss:  -0.6202 | PDE Loss:  -1.1726 | Function Loss:  -1.7631\n",
      "Total loss:  -0.6204 | PDE Loss:  -1.1731 | Function Loss:  -1.763\n",
      "Total loss:  -0.6204 | PDE Loss:  -1.1734 | Function Loss:  -1.7631\n",
      "Total loss:  -0.6206 | PDE Loss:  -1.1737 | Function Loss:  -1.7631\n",
      "Total loss:  -0.6207 | PDE Loss:  -1.174 | Function Loss:  -1.7632\n",
      "Total loss:  -0.6208 | PDE Loss:  -1.1742 | Function Loss:  -1.7632\n",
      "Total loss:  -0.6208 | PDE Loss:  -1.1743 | Function Loss:  -1.7633\n",
      "Total loss:  -0.6209 | PDE Loss:  -1.1745 | Function Loss:  -1.7633\n",
      "Total loss:  -0.621 | PDE Loss:  -1.1748 | Function Loss:  -1.7633\n",
      "Total loss:  -0.6211 | PDE Loss:  -1.1749 | Function Loss:  -1.7635\n",
      "Total loss:  -0.6212 | PDE Loss:  -1.1752 | Function Loss:  -1.7635\n",
      "Total loss:  -0.6214 | PDE Loss:  -1.1757 | Function Loss:  -1.7634\n",
      "Total loss:  -0.6214 | PDE Loss:  -1.176 | Function Loss:  -1.7634\n",
      "Total loss:  -0.6215 | PDE Loss:  -1.1764 | Function Loss:  -1.7634\n",
      "Total loss:  -0.6216 | PDE Loss:  -1.1767 | Function Loss:  -1.7634\n",
      "Total loss:  -0.6216 | PDE Loss:  -1.177 | Function Loss:  -1.7633\n",
      "Total loss:  -0.6217 | PDE Loss:  -1.1773 | Function Loss:  -1.7633\n",
      "Total loss:  -0.6218 | PDE Loss:  -1.1777 | Function Loss:  -1.7632\n",
      "Total loss:  -0.6218 | PDE Loss:  -1.1782 | Function Loss:  -1.7631\n",
      "Total loss:  -0.6219 | PDE Loss:  -1.1787 | Function Loss:  -1.7631\n",
      "Total loss:  -0.6221 | PDE Loss:  -1.1795 | Function Loss:  -1.763\n",
      "Total loss:  -0.6221 | PDE Loss:  -1.1799 | Function Loss:  -1.7629\n",
      "Total loss:  -0.6223 | PDE Loss:  -1.1806 | Function Loss:  -1.7628\n",
      "Total loss:  -0.6224 | PDE Loss:  -1.1807 | Function Loss:  -1.7629\n",
      "Total loss:  -0.6225 | PDE Loss:  -1.1805 | Function Loss:  -1.7631\n",
      "Total loss:  -0.6226 | PDE Loss:  -1.1802 | Function Loss:  -1.7634\n",
      "Total loss:  -0.6227 | PDE Loss:  -1.1799 | Function Loss:  -1.7636\n",
      "Total loss:  -0.6228 | PDE Loss:  -1.1796 | Function Loss:  -1.7639\n",
      "Total loss:  -0.6229 | PDE Loss:  -1.1794 | Function Loss:  -1.7641\n",
      "Total loss:  -0.623 | PDE Loss:  -1.1792 | Function Loss:  -1.7644\n",
      "Total loss:  -0.6231 | PDE Loss:  -1.1789 | Function Loss:  -1.7647\n",
      "Total loss:  -0.6232 | PDE Loss:  -1.179 | Function Loss:  -1.7648\n",
      "Total loss:  -0.6233 | PDE Loss:  -1.1791 | Function Loss:  -1.7648\n",
      "Total loss:  -0.6234 | PDE Loss:  -1.1795 | Function Loss:  -1.7648\n",
      "Total loss:  -0.6235 | PDE Loss:  -1.1797 | Function Loss:  -1.7649\n",
      "Total loss:  -0.6236 | PDE Loss:  -1.1799 | Function Loss:  -1.7649\n",
      "Total loss:  -0.6236 | PDE Loss:  -1.18 | Function Loss:  -1.7649\n",
      "Total loss:  -0.6237 | PDE Loss:  -1.18 | Function Loss:  -1.765\n",
      "Total loss:  -0.6237 | PDE Loss:  -1.1799 | Function Loss:  -1.7651\n",
      "Total loss:  -0.6238 | PDE Loss:  -1.1797 | Function Loss:  -1.7653\n",
      "Total loss:  -0.6239 | PDE Loss:  -1.1795 | Function Loss:  -1.7655\n",
      "Total loss:  -0.624 | PDE Loss:  -1.1791 | Function Loss:  -1.7657\n",
      "Total loss:  -0.624 | PDE Loss:  -1.1789 | Function Loss:  -1.7659\n",
      "Total loss:  -0.6241 | PDE Loss:  -1.1788 | Function Loss:  -1.766\n",
      "Total loss:  -0.6242 | PDE Loss:  -1.1786 | Function Loss:  -1.7662\n",
      "Total loss:  -0.6243 | PDE Loss:  -1.1786 | Function Loss:  -1.7664\n",
      "Total loss:  -0.6243 | PDE Loss:  -1.1786 | Function Loss:  -1.7665\n",
      "Total loss:  -0.6244 | PDE Loss:  -1.1787 | Function Loss:  -1.7665\n",
      "Total loss:  -0.6245 | PDE Loss:  -1.1789 | Function Loss:  -1.7666\n",
      "Total loss:  -0.6246 | PDE Loss:  -1.1791 | Function Loss:  -1.7666\n",
      "Total loss:  -0.6247 | PDE Loss:  -1.1794 | Function Loss:  -1.7666\n",
      "Total loss:  -0.6248 | PDE Loss:  -1.1799 | Function Loss:  -1.7666\n",
      "Total loss:  -0.6249 | PDE Loss:  -1.1801 | Function Loss:  -1.7666\n",
      "Total loss:  -0.625 | PDE Loss:  -1.1804 | Function Loss:  -1.7667\n",
      "Total loss:  -0.6251 | PDE Loss:  -1.1805 | Function Loss:  -1.7667\n",
      "Total loss:  -0.6251 | PDE Loss:  -1.1806 | Function Loss:  -1.7668\n",
      "Total loss:  -0.6252 | PDE Loss:  -1.1807 | Function Loss:  -1.7668\n",
      "Total loss:  -0.6253 | PDE Loss:  -1.1811 | Function Loss:  -1.7668\n",
      "Total loss:  -0.6254 | PDE Loss:  -1.1811 | Function Loss:  -1.767\n",
      "Total loss:  -0.6252 | PDE Loss:  -1.1828 | Function Loss:  -1.766\n",
      "Total loss:  -0.6255 | PDE Loss:  -1.1819 | Function Loss:  -1.7668\n",
      "Total loss:  -0.6255 | PDE Loss:  -1.182 | Function Loss:  -1.7668\n",
      "Total loss:  -0.6256 | PDE Loss:  -1.1821 | Function Loss:  -1.7669\n",
      "Total loss:  -0.6257 | PDE Loss:  -1.1824 | Function Loss:  -1.7669\n",
      "Total loss:  -0.6258 | PDE Loss:  -1.1824 | Function Loss:  -1.767\n",
      "Total loss:  -0.6258 | PDE Loss:  -1.1825 | Function Loss:  -1.767\n",
      "Total loss:  -0.6259 | PDE Loss:  -1.1825 | Function Loss:  -1.7671\n",
      "Total loss:  -0.6259 | PDE Loss:  -1.1825 | Function Loss:  -1.7671\n",
      "Total loss:  -0.6259 | PDE Loss:  -1.1823 | Function Loss:  -1.7672\n",
      "Total loss:  -0.626 | PDE Loss:  -1.182 | Function Loss:  -1.7674\n",
      "Total loss:  -0.6261 | PDE Loss:  -1.1818 | Function Loss:  -1.7676\n",
      "Total loss:  -0.6261 | PDE Loss:  -1.1816 | Function Loss:  -1.7678\n",
      "Total loss:  -0.6262 | PDE Loss:  -1.1813 | Function Loss:  -1.768\n",
      "Total loss:  -0.6262 | PDE Loss:  -1.181 | Function Loss:  -1.7682\n",
      "Total loss:  -0.6263 | PDE Loss:  -1.1806 | Function Loss:  -1.7684\n",
      "Total loss:  -0.6264 | PDE Loss:  -1.1802 | Function Loss:  -1.7687\n",
      "Total loss:  -0.6265 | PDE Loss:  -1.1791 | Function Loss:  -1.7692\n",
      "Total loss:  -0.6266 | PDE Loss:  -1.179 | Function Loss:  -1.7694\n",
      "Total loss:  -0.6267 | PDE Loss:  -1.1789 | Function Loss:  -1.7696\n",
      "Total loss:  -0.6268 | PDE Loss:  -1.179 | Function Loss:  -1.7697\n",
      "Total loss:  -0.6269 | PDE Loss:  -1.1792 | Function Loss:  -1.7698\n",
      "Total loss:  -0.627 | PDE Loss:  -1.1795 | Function Loss:  -1.7698\n",
      "Total loss:  -0.6271 | PDE Loss:  -1.18 | Function Loss:  -1.7698\n",
      "Total loss:  -0.6272 | PDE Loss:  -1.1804 | Function Loss:  -1.7698\n",
      "Total loss:  -0.6273 | PDE Loss:  -1.1808 | Function Loss:  -1.7697\n",
      "Total loss:  -0.6274 | PDE Loss:  -1.1809 | Function Loss:  -1.7698\n",
      "Total loss:  -0.6275 | PDE Loss:  -1.1811 | Function Loss:  -1.7698\n",
      "Total loss:  -0.6275 | PDE Loss:  -1.181 | Function Loss:  -1.77\n",
      "Total loss:  -0.6276 | PDE Loss:  -1.181 | Function Loss:  -1.7701\n",
      "Total loss:  -0.6277 | PDE Loss:  -1.1808 | Function Loss:  -1.7702\n",
      "Total loss:  -0.6277 | PDE Loss:  -1.1805 | Function Loss:  -1.7704\n",
      "Total loss:  -0.6278 | PDE Loss:  -1.1802 | Function Loss:  -1.7706\n",
      "Total loss:  -0.6279 | PDE Loss:  -1.1799 | Function Loss:  -1.7709\n",
      "Total loss:  -0.628 | PDE Loss:  -1.1797 | Function Loss:  -1.7711\n",
      "Total loss:  -0.628 | PDE Loss:  -1.1792 | Function Loss:  -1.7713\n",
      "Total loss:  -0.6281 | PDE Loss:  -1.1793 | Function Loss:  -1.7714\n",
      "Total loss:  -0.6282 | PDE Loss:  -1.1795 | Function Loss:  -1.7715\n",
      "Total loss:  -0.6283 | PDE Loss:  -1.1798 | Function Loss:  -1.7715\n",
      "Total loss:  -0.6283 | PDE Loss:  -1.1801 | Function Loss:  -1.7714\n",
      "Total loss:  -0.6284 | PDE Loss:  -1.1805 | Function Loss:  -1.7713\n",
      "Total loss:  -0.6285 | PDE Loss:  -1.181 | Function Loss:  -1.7713\n",
      "Total loss:  -0.6286 | PDE Loss:  -1.1816 | Function Loss:  -1.7712\n",
      "Total loss:  -0.6287 | PDE Loss:  -1.1817 | Function Loss:  -1.7713\n",
      "Total loss:  -0.6287 | PDE Loss:  -1.183 | Function Loss:  -1.7708\n",
      "Total loss:  -0.6288 | PDE Loss:  -1.1825 | Function Loss:  -1.7711\n",
      "Total loss:  -0.6288 | PDE Loss:  -1.1824 | Function Loss:  -1.7712\n",
      "Total loss:  -0.6289 | PDE Loss:  -1.1823 | Function Loss:  -1.7713\n",
      "Total loss:  -0.629 | PDE Loss:  -1.1823 | Function Loss:  -1.7714\n",
      "Total loss:  -0.629 | PDE Loss:  -1.1824 | Function Loss:  -1.7715\n",
      "Total loss:  -0.6291 | PDE Loss:  -1.1824 | Function Loss:  -1.7715\n",
      "Total loss:  -0.6291 | PDE Loss:  -1.1827 | Function Loss:  -1.7715\n",
      "Total loss:  -0.6292 | PDE Loss:  -1.1824 | Function Loss:  -1.7717\n",
      "Total loss:  -0.6293 | PDE Loss:  -1.1827 | Function Loss:  -1.7717\n",
      "Total loss:  -0.6293 | PDE Loss:  -1.183 | Function Loss:  -1.7717\n",
      "Total loss:  -0.6294 | PDE Loss:  -1.1833 | Function Loss:  -1.7716\n",
      "Total loss:  -0.6294 | PDE Loss:  -1.1834 | Function Loss:  -1.7716\n",
      "Total loss:  -0.6295 | PDE Loss:  -1.1835 | Function Loss:  -1.7717\n",
      "Total loss:  -0.6295 | PDE Loss:  -1.1834 | Function Loss:  -1.7718\n",
      "Total loss:  -0.6296 | PDE Loss:  -1.1835 | Function Loss:  -1.7719\n",
      "Total loss:  -0.6298 | PDE Loss:  -1.1832 | Function Loss:  -1.7722\n",
      "Total loss:  -0.6299 | PDE Loss:  -1.1832 | Function Loss:  -1.7723\n",
      "Total loss:  -0.63 | PDE Loss:  -1.183 | Function Loss:  -1.7726\n",
      "Total loss:  -0.6301 | PDE Loss:  -1.183 | Function Loss:  -1.7727\n",
      "Total loss:  -0.6302 | PDE Loss:  -1.183 | Function Loss:  -1.7729\n",
      "Total loss:  -0.6303 | PDE Loss:  -1.1831 | Function Loss:  -1.773\n",
      "Total loss:  -0.6304 | PDE Loss:  -1.1832 | Function Loss:  -1.7731\n",
      "Total loss:  -0.6305 | PDE Loss:  -1.1833 | Function Loss:  -1.7733\n",
      "Total loss:  -0.6307 | PDE Loss:  -1.1835 | Function Loss:  -1.7734\n",
      "Total loss:  -0.6308 | PDE Loss:  -1.1833 | Function Loss:  -1.7736\n",
      "Total loss:  -0.6309 | PDE Loss:  -1.1833 | Function Loss:  -1.7738\n",
      "Total loss:  -0.631 | PDE Loss:  -1.1831 | Function Loss:  -1.774\n",
      "Total loss:  -0.6312 | PDE Loss:  -1.183 | Function Loss:  -1.7742\n",
      "Total loss:  -0.6313 | PDE Loss:  -1.1826 | Function Loss:  -1.7745\n",
      "Total loss:  -0.6314 | PDE Loss:  -1.1825 | Function Loss:  -1.7747\n",
      "Total loss:  -0.6314 | PDE Loss:  -1.1822 | Function Loss:  -1.7749\n",
      "Total loss:  -0.6315 | PDE Loss:  -1.1821 | Function Loss:  -1.7751\n",
      "Total loss:  -0.6316 | PDE Loss:  -1.1821 | Function Loss:  -1.7752\n",
      "Total loss:  -0.6317 | PDE Loss:  -1.1822 | Function Loss:  -1.7752\n",
      "Total loss:  -0.6317 | PDE Loss:  -1.1819 | Function Loss:  -1.7754\n",
      "Total loss:  -0.6317 | PDE Loss:  -1.1821 | Function Loss:  -1.7754\n",
      "Total loss:  -0.6318 | PDE Loss:  -1.1823 | Function Loss:  -1.7753\n",
      "Total loss:  -0.6318 | PDE Loss:  -1.1825 | Function Loss:  -1.7753\n",
      "Total loss:  -0.6319 | PDE Loss:  -1.1826 | Function Loss:  -1.7754\n",
      "Total loss:  -0.6319 | PDE Loss:  -1.1827 | Function Loss:  -1.7754\n",
      "Total loss:  -0.632 | PDE Loss:  -1.182 | Function Loss:  -1.7758\n",
      "Total loss:  -0.632 | PDE Loss:  -1.1822 | Function Loss:  -1.7758\n",
      "Total loss:  -0.6321 | PDE Loss:  -1.1823 | Function Loss:  -1.7758\n",
      "Total loss:  -0.6322 | PDE Loss:  -1.1824 | Function Loss:  -1.7759\n",
      "Total loss:  -0.6323 | PDE Loss:  -1.1824 | Function Loss:  -1.776\n",
      "Total loss:  -0.6323 | PDE Loss:  -1.1825 | Function Loss:  -1.7761\n",
      "Total loss:  -0.6324 | PDE Loss:  -1.1826 | Function Loss:  -1.7761\n",
      "Total loss:  -0.6324 | PDE Loss:  -1.1826 | Function Loss:  -1.776\n",
      "Total loss:  -0.6325 | PDE Loss:  -1.1828 | Function Loss:  -1.7761\n",
      "Total loss:  -0.6326 | PDE Loss:  -1.1829 | Function Loss:  -1.7762\n",
      "Total loss:  -0.6326 | PDE Loss:  -1.1831 | Function Loss:  -1.7762\n",
      "Total loss:  -0.6327 | PDE Loss:  -1.1836 | Function Loss:  -1.7762\n",
      "Total loss:  -0.6328 | PDE Loss:  -1.1838 | Function Loss:  -1.7762\n",
      "Total loss:  -0.6328 | PDE Loss:  -1.1842 | Function Loss:  -1.7761\n",
      "Total loss:  -0.6329 | PDE Loss:  -1.1844 | Function Loss:  -1.7761\n",
      "Total loss:  -0.6329 | PDE Loss:  -1.1845 | Function Loss:  -1.7761\n",
      "Total loss:  -0.633 | PDE Loss:  -1.1845 | Function Loss:  -1.7762\n",
      "Total loss:  -0.633 | PDE Loss:  -1.1844 | Function Loss:  -1.7763\n",
      "Total loss:  -0.6331 | PDE Loss:  -1.1842 | Function Loss:  -1.7764\n",
      "Total loss:  -0.6331 | PDE Loss:  -1.1839 | Function Loss:  -1.7766\n",
      "Total loss:  -0.6331 | PDE Loss:  -1.1835 | Function Loss:  -1.7768\n",
      "Total loss:  -0.6332 | PDE Loss:  -1.183 | Function Loss:  -1.7771\n",
      "Total loss:  -0.6333 | PDE Loss:  -1.1826 | Function Loss:  -1.7773\n",
      "Total loss:  -0.6333 | PDE Loss:  -1.1823 | Function Loss:  -1.7775\n",
      "Total loss:  -0.6334 | PDE Loss:  -1.1822 | Function Loss:  -1.7777\n",
      "Total loss:  -0.6335 | PDE Loss:  -1.1823 | Function Loss:  -1.7777\n",
      "Total loss:  -0.6335 | PDE Loss:  -1.1824 | Function Loss:  -1.7777\n",
      "Total loss:  -0.6336 | PDE Loss:  -1.1827 | Function Loss:  -1.7777\n",
      "Total loss:  -0.6336 | PDE Loss:  -1.1831 | Function Loss:  -1.7776\n",
      "Total loss:  -0.6336 | PDE Loss:  -1.1836 | Function Loss:  -1.7774\n",
      "Total loss:  -0.6337 | PDE Loss:  -1.1839 | Function Loss:  -1.7773\n",
      "Total loss:  -0.6337 | PDE Loss:  -1.1841 | Function Loss:  -1.7773\n",
      "Total loss:  -0.6337 | PDE Loss:  -1.1842 | Function Loss:  -1.7773\n",
      "Total loss:  -0.6338 | PDE Loss:  -1.1842 | Function Loss:  -1.7774\n",
      "Total loss:  -0.6338 | PDE Loss:  -1.1835 | Function Loss:  -1.7777\n",
      "Total loss:  -0.6338 | PDE Loss:  -1.1835 | Function Loss:  -1.7777\n",
      "Total loss:  -0.6339 | PDE Loss:  -1.1833 | Function Loss:  -1.7779\n",
      "Total loss:  -0.6339 | PDE Loss:  -1.1829 | Function Loss:  -1.7781\n",
      "Total loss:  -0.6339 | PDE Loss:  -1.1825 | Function Loss:  -1.7783\n",
      "Total loss:  -0.634 | PDE Loss:  -1.1821 | Function Loss:  -1.7785\n",
      "Total loss:  -0.634 | PDE Loss:  -1.1817 | Function Loss:  -1.7787\n",
      "Total loss:  -0.6341 | PDE Loss:  -1.1814 | Function Loss:  -1.7789\n",
      "Total loss:  -0.6341 | PDE Loss:  -1.1812 | Function Loss:  -1.779\n",
      "Total loss:  -0.6342 | PDE Loss:  -1.181 | Function Loss:  -1.7792\n",
      "Total loss:  -0.6342 | PDE Loss:  -1.1811 | Function Loss:  -1.7793\n",
      "Total loss:  -0.6343 | PDE Loss:  -1.1811 | Function Loss:  -1.7793\n",
      "Total loss:  -0.6343 | PDE Loss:  -1.1812 | Function Loss:  -1.7793\n",
      "Total loss:  -0.6344 | PDE Loss:  -1.1813 | Function Loss:  -1.7794\n",
      "Total loss:  -0.6344 | PDE Loss:  -1.1814 | Function Loss:  -1.7794\n",
      "Total loss:  -0.6345 | PDE Loss:  -1.1814 | Function Loss:  -1.7795\n",
      "Total loss:  -0.6346 | PDE Loss:  -1.1819 | Function Loss:  -1.7794\n",
      "Total loss:  -0.6347 | PDE Loss:  -1.1812 | Function Loss:  -1.7798\n",
      "Total loss:  -0.6347 | PDE Loss:  -1.1817 | Function Loss:  -1.7797\n",
      "Total loss:  -0.6348 | PDE Loss:  -1.1823 | Function Loss:  -1.7796\n",
      "Total loss:  -0.6349 | PDE Loss:  -1.1826 | Function Loss:  -1.7795\n",
      "Total loss:  -0.6349 | PDE Loss:  -1.1828 | Function Loss:  -1.7795\n",
      "Total loss:  -0.6349 | PDE Loss:  -1.1828 | Function Loss:  -1.7796\n",
      "Total loss:  -0.635 | PDE Loss:  -1.1828 | Function Loss:  -1.7796\n",
      "Total loss:  -0.6351 | PDE Loss:  -1.1827 | Function Loss:  -1.7798\n",
      "Total loss:  -0.6352 | PDE Loss:  -1.1825 | Function Loss:  -1.78\n",
      "Total loss:  -0.6352 | PDE Loss:  -1.182 | Function Loss:  -1.7802\n",
      "Total loss:  -0.6353 | PDE Loss:  -1.1821 | Function Loss:  -1.7804\n",
      "Total loss:  -0.6354 | PDE Loss:  -1.1822 | Function Loss:  -1.7805\n",
      "Total loss:  -0.6355 | PDE Loss:  -1.1822 | Function Loss:  -1.7805\n",
      "Total loss:  -0.6355 | PDE Loss:  -1.1823 | Function Loss:  -1.7805\n",
      "Total loss:  -0.6355 | PDE Loss:  -1.1824 | Function Loss:  -1.7806\n",
      "Total loss:  -0.6356 | PDE Loss:  -1.1825 | Function Loss:  -1.7806\n",
      "Total loss:  -0.6356 | PDE Loss:  -1.1827 | Function Loss:  -1.7806\n",
      "Total loss:  -0.6357 | PDE Loss:  -1.1829 | Function Loss:  -1.7806\n",
      "Total loss:  -0.6358 | PDE Loss:  -1.183 | Function Loss:  -1.7807\n",
      "Total loss:  -0.6359 | PDE Loss:  -1.1823 | Function Loss:  -1.7811\n",
      "Total loss:  -0.636 | PDE Loss:  -1.1827 | Function Loss:  -1.781\n",
      "Total loss:  -0.636 | PDE Loss:  -1.183 | Function Loss:  -1.781\n",
      "Total loss:  -0.6361 | PDE Loss:  -1.1835 | Function Loss:  -1.7809\n",
      "Total loss:  -0.6362 | PDE Loss:  -1.1839 | Function Loss:  -1.7809\n",
      "Total loss:  -0.6363 | PDE Loss:  -1.1844 | Function Loss:  -1.7808\n",
      "Total loss:  -0.6364 | PDE Loss:  -1.1848 | Function Loss:  -1.7808\n",
      "Total loss:  -0.6364 | PDE Loss:  -1.185 | Function Loss:  -1.7808\n",
      "Total loss:  -0.6365 | PDE Loss:  -1.1852 | Function Loss:  -1.7808\n",
      "Total loss:  -0.6365 | PDE Loss:  -1.1853 | Function Loss:  -1.7808\n",
      "Total loss:  -0.6366 | PDE Loss:  -1.1854 | Function Loss:  -1.7808\n",
      "Total loss:  -0.6366 | PDE Loss:  -1.1853 | Function Loss:  -1.781\n",
      "Total loss:  -0.6367 | PDE Loss:  -1.1854 | Function Loss:  -1.781\n",
      "Total loss:  -0.6368 | PDE Loss:  -1.1854 | Function Loss:  -1.7811\n",
      "Total loss:  -0.6369 | PDE Loss:  -1.1854 | Function Loss:  -1.7812\n",
      "Total loss:  -0.637 | PDE Loss:  -1.1853 | Function Loss:  -1.7814\n",
      "Total loss:  -0.637 | PDE Loss:  -1.1853 | Function Loss:  -1.7815\n",
      "Total loss:  -0.6371 | PDE Loss:  -1.1853 | Function Loss:  -1.7815\n",
      "Total loss:  -0.6371 | PDE Loss:  -1.1854 | Function Loss:  -1.7816\n",
      "Total loss:  -0.6372 | PDE Loss:  -1.1855 | Function Loss:  -1.7816\n",
      "Total loss:  -0.6373 | PDE Loss:  -1.1858 | Function Loss:  -1.7816\n",
      "Total loss:  -0.6374 | PDE Loss:  -1.186 | Function Loss:  -1.7817\n",
      "Total loss:  -0.6374 | PDE Loss:  -1.1865 | Function Loss:  -1.7816\n",
      "Total loss:  -0.6375 | PDE Loss:  -1.1868 | Function Loss:  -1.7816\n",
      "Total loss:  -0.6376 | PDE Loss:  -1.1869 | Function Loss:  -1.7816\n",
      "Total loss:  -0.6376 | PDE Loss:  -1.1871 | Function Loss:  -1.7817\n",
      "Total loss:  -0.6377 | PDE Loss:  -1.1871 | Function Loss:  -1.7817\n",
      "Total loss:  -0.6378 | PDE Loss:  -1.1871 | Function Loss:  -1.7818\n",
      "Total loss:  -0.6379 | PDE Loss:  -1.187 | Function Loss:  -1.782\n",
      "Total loss:  -0.6379 | PDE Loss:  -1.187 | Function Loss:  -1.7821\n",
      "Total loss:  -0.638 | PDE Loss:  -1.187 | Function Loss:  -1.7822\n",
      "Total loss:  -0.6381 | PDE Loss:  -1.1871 | Function Loss:  -1.7823\n",
      "Total loss:  -0.6382 | PDE Loss:  -1.1871 | Function Loss:  -1.7824\n",
      "Total loss:  -0.6383 | PDE Loss:  -1.1874 | Function Loss:  -1.7824\n",
      "Total loss:  -0.6383 | PDE Loss:  -1.1878 | Function Loss:  -1.7823\n",
      "Total loss:  -0.6384 | PDE Loss:  -1.188 | Function Loss:  -1.7823\n",
      "Total loss:  -0.6384 | PDE Loss:  -1.1883 | Function Loss:  -1.7823\n",
      "Total loss:  -0.6385 | PDE Loss:  -1.1886 | Function Loss:  -1.7822\n",
      "Total loss:  -0.6385 | PDE Loss:  -1.189 | Function Loss:  -1.7821\n",
      "Total loss:  -0.6385 | PDE Loss:  -1.1895 | Function Loss:  -1.7819\n",
      "Total loss:  -0.6386 | PDE Loss:  -1.1902 | Function Loss:  -1.7817\n",
      "Total loss:  -0.6386 | PDE Loss:  -1.1908 | Function Loss:  -1.7816\n",
      "Total loss:  -0.6387 | PDE Loss:  -1.1915 | Function Loss:  -1.7814\n",
      "Total loss:  -0.6387 | PDE Loss:  -1.1918 | Function Loss:  -1.7813\n",
      "Total loss:  -0.6388 | PDE Loss:  -1.1937 | Function Loss:  -1.7807\n",
      "Total loss:  -0.6388 | PDE Loss:  -1.1932 | Function Loss:  -1.7809\n",
      "Total loss:  -0.6389 | PDE Loss:  -1.1927 | Function Loss:  -1.7812\n",
      "Total loss:  -0.6389 | PDE Loss:  -1.1922 | Function Loss:  -1.7814\n",
      "Total loss:  -0.639 | PDE Loss:  -1.1918 | Function Loss:  -1.7816\n",
      "Total loss:  -0.639 | PDE Loss:  -1.1915 | Function Loss:  -1.7818\n",
      "Total loss:  -0.6388 | PDE Loss:  -1.1922 | Function Loss:  -1.7812\n",
      "Total loss:  -0.6391 | PDE Loss:  -1.1917 | Function Loss:  -1.7818\n",
      "Total loss:  -0.6391 | PDE Loss:  -1.1916 | Function Loss:  -1.7819\n",
      "Total loss:  -0.6392 | PDE Loss:  -1.1916 | Function Loss:  -1.782\n",
      "Total loss:  -0.6392 | PDE Loss:  -1.1918 | Function Loss:  -1.782\n",
      "Total loss:  -0.6393 | PDE Loss:  -1.1921 | Function Loss:  -1.782\n",
      "Total loss:  -0.6394 | PDE Loss:  -1.1924 | Function Loss:  -1.7819\n",
      "Total loss:  -0.6394 | PDE Loss:  -1.1927 | Function Loss:  -1.7819\n",
      "Total loss:  -0.6395 | PDE Loss:  -1.1929 | Function Loss:  -1.7819\n",
      "Total loss:  -0.6395 | PDE Loss:  -1.193 | Function Loss:  -1.7819\n",
      "Total loss:  -0.6396 | PDE Loss:  -1.1929 | Function Loss:  -1.7821\n",
      "Total loss:  -0.6397 | PDE Loss:  -1.1929 | Function Loss:  -1.7823\n",
      "Total loss:  -0.6398 | PDE Loss:  -1.1923 | Function Loss:  -1.7826\n",
      "Total loss:  -0.6399 | PDE Loss:  -1.1919 | Function Loss:  -1.7829\n",
      "Total loss:  -0.6399 | PDE Loss:  -1.1914 | Function Loss:  -1.7831\n",
      "Total loss:  -0.64 | PDE Loss:  -1.1911 | Function Loss:  -1.7833\n",
      "Total loss:  -0.6401 | PDE Loss:  -1.1912 | Function Loss:  -1.7834\n",
      "Total loss:  -0.6401 | PDE Loss:  -1.1913 | Function Loss:  -1.7834\n",
      "Total loss:  -0.6402 | PDE Loss:  -1.1915 | Function Loss:  -1.7835\n",
      "Total loss:  -0.6403 | PDE Loss:  -1.1917 | Function Loss:  -1.7835\n",
      "Total loss:  -0.6403 | PDE Loss:  -1.1918 | Function Loss:  -1.7835\n",
      "Total loss:  -0.6404 | PDE Loss:  -1.192 | Function Loss:  -1.7835\n",
      "Total loss:  -0.6404 | PDE Loss:  -1.192 | Function Loss:  -1.7836\n",
      "Total loss:  -0.6405 | PDE Loss:  -1.1921 | Function Loss:  -1.7837\n",
      "Total loss:  -0.6406 | PDE Loss:  -1.1921 | Function Loss:  -1.7838\n",
      "Total loss:  -0.6407 | PDE Loss:  -1.1922 | Function Loss:  -1.7839\n",
      "Total loss:  -0.6408 | PDE Loss:  -1.1922 | Function Loss:  -1.784\n",
      "Total loss:  -0.6409 | PDE Loss:  -1.1924 | Function Loss:  -1.784\n",
      "Total loss:  -0.641 | PDE Loss:  -1.1923 | Function Loss:  -1.7842\n",
      "Total loss:  -0.641 | PDE Loss:  -1.1923 | Function Loss:  -1.7843\n",
      "Total loss:  -0.6411 | PDE Loss:  -1.1923 | Function Loss:  -1.7844\n",
      "Total loss:  -0.6412 | PDE Loss:  -1.1922 | Function Loss:  -1.7845\n",
      "Total loss:  -0.6412 | PDE Loss:  -1.1922 | Function Loss:  -1.7846\n",
      "Total loss:  -0.6413 | PDE Loss:  -1.1922 | Function Loss:  -1.7847\n",
      "Total loss:  -0.6413 | PDE Loss:  -1.1921 | Function Loss:  -1.7848\n",
      "Total loss:  -0.6414 | PDE Loss:  -1.192 | Function Loss:  -1.7849\n",
      "Total loss:  -0.6414 | PDE Loss:  -1.1921 | Function Loss:  -1.7849\n",
      "Total loss:  -0.6414 | PDE Loss:  -1.192 | Function Loss:  -1.785\n",
      "Total loss:  -0.6415 | PDE Loss:  -1.1924 | Function Loss:  -1.7849\n",
      "Total loss:  -0.6415 | PDE Loss:  -1.1921 | Function Loss:  -1.785\n",
      "Total loss:  -0.6415 | PDE Loss:  -1.1919 | Function Loss:  -1.7852\n",
      "Total loss:  -0.6416 | PDE Loss:  -1.1916 | Function Loss:  -1.7854\n",
      "Total loss:  -0.6416 | PDE Loss:  -1.1914 | Function Loss:  -1.7855\n",
      "Total loss:  -0.6417 | PDE Loss:  -1.1913 | Function Loss:  -1.7856\n",
      "Total loss:  -0.6418 | PDE Loss:  -1.1914 | Function Loss:  -1.7857\n",
      "Total loss:  -0.6418 | PDE Loss:  -1.1916 | Function Loss:  -1.7857\n",
      "Total loss:  -0.6419 | PDE Loss:  -1.192 | Function Loss:  -1.7857\n",
      "Total loss:  -0.642 | PDE Loss:  -1.1924 | Function Loss:  -1.7856\n",
      "Total loss:  -0.6421 | PDE Loss:  -1.193 | Function Loss:  -1.7855\n",
      "Total loss:  -0.6421 | PDE Loss:  -1.1935 | Function Loss:  -1.7854\n",
      "Total loss:  -0.6422 | PDE Loss:  -1.194 | Function Loss:  -1.7853\n",
      "Total loss:  -0.6423 | PDE Loss:  -1.1946 | Function Loss:  -1.7852\n",
      "Total loss:  -0.6424 | PDE Loss:  -1.1947 | Function Loss:  -1.7852\n",
      "Total loss:  -0.6424 | PDE Loss:  -1.1948 | Function Loss:  -1.7853\n",
      "Total loss:  -0.6425 | PDE Loss:  -1.1947 | Function Loss:  -1.7854\n",
      "Total loss:  -0.6425 | PDE Loss:  -1.1945 | Function Loss:  -1.7855\n",
      "Total loss:  -0.6426 | PDE Loss:  -1.1942 | Function Loss:  -1.7857\n",
      "Total loss:  -0.6427 | PDE Loss:  -1.1942 | Function Loss:  -1.7858\n",
      "Total loss:  -0.6427 | PDE Loss:  -1.194 | Function Loss:  -1.786\n",
      "Total loss:  -0.6428 | PDE Loss:  -1.1938 | Function Loss:  -1.7862\n",
      "Total loss:  -0.6429 | PDE Loss:  -1.1938 | Function Loss:  -1.7863\n",
      "Total loss:  -0.6429 | PDE Loss:  -1.1938 | Function Loss:  -1.7864\n",
      "Total loss:  -0.643 | PDE Loss:  -1.194 | Function Loss:  -1.7864\n",
      "Total loss:  -0.643 | PDE Loss:  -1.1943 | Function Loss:  -1.7863\n",
      "Total loss:  -0.6431 | PDE Loss:  -1.1947 | Function Loss:  -1.7863\n",
      "Total loss:  -0.6432 | PDE Loss:  -1.1943 | Function Loss:  -1.7865\n",
      "Total loss:  -0.6433 | PDE Loss:  -1.1947 | Function Loss:  -1.7865\n",
      "Total loss:  -0.6434 | PDE Loss:  -1.1952 | Function Loss:  -1.7865\n",
      "Total loss:  -0.6436 | PDE Loss:  -1.1956 | Function Loss:  -1.7866\n",
      "Total loss:  -0.6437 | PDE Loss:  -1.1957 | Function Loss:  -1.7867\n",
      "Total loss:  -0.6438 | PDE Loss:  -1.1956 | Function Loss:  -1.7869\n",
      "Total loss:  -0.6439 | PDE Loss:  -1.1955 | Function Loss:  -1.787\n",
      "Total loss:  -0.644 | PDE Loss:  -1.1957 | Function Loss:  -1.7871\n",
      "Total loss:  -0.644 | PDE Loss:  -1.1956 | Function Loss:  -1.7872\n",
      "Total loss:  -0.6441 | PDE Loss:  -1.1956 | Function Loss:  -1.7873\n",
      "Total loss:  -0.6441 | PDE Loss:  -1.1956 | Function Loss:  -1.7873\n",
      "Total loss:  -0.6441 | PDE Loss:  -1.1956 | Function Loss:  -1.7873\n",
      "Total loss:  -0.6442 | PDE Loss:  -1.1957 | Function Loss:  -1.7873\n",
      "Total loss:  -0.6442 | PDE Loss:  -1.1958 | Function Loss:  -1.7874\n",
      "Total loss:  -0.6442 | PDE Loss:  -1.1959 | Function Loss:  -1.7874\n",
      "Total loss:  -0.6443 | PDE Loss:  -1.1959 | Function Loss:  -1.7874\n",
      "Total loss:  -0.6443 | PDE Loss:  -1.1959 | Function Loss:  -1.7875\n",
      "Total loss:  -0.6443 | PDE Loss:  -1.1955 | Function Loss:  -1.7877\n",
      "Total loss:  -0.6444 | PDE Loss:  -1.1956 | Function Loss:  -1.7877\n",
      "Total loss:  -0.6444 | PDE Loss:  -1.1954 | Function Loss:  -1.7878\n",
      "Total loss:  -0.6444 | PDE Loss:  -1.1952 | Function Loss:  -1.7879\n",
      "Total loss:  -0.6445 | PDE Loss:  -1.195 | Function Loss:  -1.788\n",
      "Total loss:  -0.6445 | PDE Loss:  -1.1949 | Function Loss:  -1.7881\n",
      "Total loss:  -0.6445 | PDE Loss:  -1.1949 | Function Loss:  -1.7881\n",
      "Total loss:  -0.6445 | PDE Loss:  -1.1948 | Function Loss:  -1.7882\n",
      "Total loss:  -0.6446 | PDE Loss:  -1.1949 | Function Loss:  -1.7882\n",
      "Total loss:  -0.6444 | PDE Loss:  -1.1933 | Function Loss:  -1.7886\n",
      "Total loss:  -0.6446 | PDE Loss:  -1.1946 | Function Loss:  -1.7883\n",
      "Total loss:  -0.6446 | PDE Loss:  -1.1948 | Function Loss:  -1.7883\n",
      "Total loss:  -0.6447 | PDE Loss:  -1.195 | Function Loss:  -1.7883\n",
      "Total loss:  -0.6447 | PDE Loss:  -1.1953 | Function Loss:  -1.7883\n",
      "Total loss:  -0.6448 | PDE Loss:  -1.1956 | Function Loss:  -1.7882\n",
      "Total loss:  -0.6448 | PDE Loss:  -1.1958 | Function Loss:  -1.7882\n",
      "Total loss:  -0.6448 | PDE Loss:  -1.196 | Function Loss:  -1.7882\n",
      "Total loss:  -0.6449 | PDE Loss:  -1.1961 | Function Loss:  -1.7882\n",
      "Total loss:  -0.6449 | PDE Loss:  -1.1962 | Function Loss:  -1.7882\n",
      "Total loss:  -0.645 | PDE Loss:  -1.1962 | Function Loss:  -1.7882\n",
      "Total loss:  -0.645 | PDE Loss:  -1.196 | Function Loss:  -1.7885\n",
      "Total loss:  -0.6449 | PDE Loss:  -1.1947 | Function Loss:  -1.7888\n",
      "Total loss:  -0.6451 | PDE Loss:  -1.1957 | Function Loss:  -1.7886\n",
      "Total loss:  -0.6451 | PDE Loss:  -1.1956 | Function Loss:  -1.7887\n",
      "Total loss:  -0.6452 | PDE Loss:  -1.1953 | Function Loss:  -1.789\n",
      "Total loss:  -0.6453 | PDE Loss:  -1.1951 | Function Loss:  -1.7891\n",
      "Total loss:  -0.6453 | PDE Loss:  -1.1949 | Function Loss:  -1.7892\n",
      "Total loss:  -0.6454 | PDE Loss:  -1.1949 | Function Loss:  -1.7893\n",
      "Total loss:  -0.6454 | PDE Loss:  -1.195 | Function Loss:  -1.7893\n",
      "Total loss:  -0.6454 | PDE Loss:  -1.195 | Function Loss:  -1.7894\n",
      "Total loss:  -0.6455 | PDE Loss:  -1.1952 | Function Loss:  -1.7894\n",
      "Total loss:  -0.6456 | PDE Loss:  -1.1955 | Function Loss:  -1.7894\n",
      "Total loss:  -0.6456 | PDE Loss:  -1.1957 | Function Loss:  -1.7894\n",
      "Total loss:  -0.6457 | PDE Loss:  -1.196 | Function Loss:  -1.7893\n",
      "Total loss:  -0.6457 | PDE Loss:  -1.1962 | Function Loss:  -1.7893\n",
      "Total loss:  -0.6458 | PDE Loss:  -1.1964 | Function Loss:  -1.7893\n",
      "Total loss:  -0.6458 | PDE Loss:  -1.1964 | Function Loss:  -1.7894\n",
      "Total loss:  -0.6459 | PDE Loss:  -1.1964 | Function Loss:  -1.7894\n",
      "Total loss:  -0.6459 | PDE Loss:  -1.1962 | Function Loss:  -1.7896\n",
      "Total loss:  -0.646 | PDE Loss:  -1.196 | Function Loss:  -1.7897\n",
      "Total loss:  -0.646 | PDE Loss:  -1.1957 | Function Loss:  -1.7899\n",
      "Total loss:  -0.6456 | PDE Loss:  -1.193 | Function Loss:  -1.7905\n",
      "Total loss:  -0.6461 | PDE Loss:  -1.1954 | Function Loss:  -1.7901\n",
      "Total loss:  -0.6461 | PDE Loss:  -1.1951 | Function Loss:  -1.7903\n",
      "Total loss:  -0.6462 | PDE Loss:  -1.1948 | Function Loss:  -1.7905\n",
      "Total loss:  -0.6463 | PDE Loss:  -1.1947 | Function Loss:  -1.7907\n",
      "Total loss:  -0.6463 | PDE Loss:  -1.1938 | Function Loss:  -1.791\n",
      "Total loss:  -0.6463 | PDE Loss:  -1.194 | Function Loss:  -1.791\n",
      "Total loss:  -0.6464 | PDE Loss:  -1.1942 | Function Loss:  -1.791\n",
      "Total loss:  -0.6464 | PDE Loss:  -1.1944 | Function Loss:  -1.791\n",
      "Total loss:  -0.6464 | PDE Loss:  -1.1947 | Function Loss:  -1.7909\n",
      "Total loss:  -0.6465 | PDE Loss:  -1.1949 | Function Loss:  -1.7909\n",
      "Total loss:  -0.6465 | PDE Loss:  -1.1951 | Function Loss:  -1.7908\n",
      "Total loss:  -0.6465 | PDE Loss:  -1.1951 | Function Loss:  -1.7909\n",
      "Total loss:  -0.6466 | PDE Loss:  -1.1953 | Function Loss:  -1.7909\n",
      "Total loss:  -0.6467 | PDE Loss:  -1.1955 | Function Loss:  -1.7909\n",
      "Total loss:  -0.6468 | PDE Loss:  -1.1957 | Function Loss:  -1.791\n",
      "Total loss:  -0.6468 | PDE Loss:  -1.1957 | Function Loss:  -1.7911\n",
      "Total loss:  -0.6469 | PDE Loss:  -1.1957 | Function Loss:  -1.7912\n",
      "Total loss:  -0.6469 | PDE Loss:  -1.1953 | Function Loss:  -1.7913\n",
      "Total loss:  -0.6469 | PDE Loss:  -1.1956 | Function Loss:  -1.7913\n",
      "Total loss:  -0.647 | PDE Loss:  -1.1956 | Function Loss:  -1.7913\n",
      "Total loss:  -0.647 | PDE Loss:  -1.1955 | Function Loss:  -1.7914\n",
      "Total loss:  -0.6471 | PDE Loss:  -1.1956 | Function Loss:  -1.7915\n",
      "Total loss:  -0.6472 | PDE Loss:  -1.1955 | Function Loss:  -1.7916\n",
      "Total loss:  -0.6473 | PDE Loss:  -1.1956 | Function Loss:  -1.7917\n",
      "Total loss:  -0.6474 | PDE Loss:  -1.1957 | Function Loss:  -1.7918\n",
      "Total loss:  -0.6475 | PDE Loss:  -1.1957 | Function Loss:  -1.792\n",
      "Total loss:  -0.6476 | PDE Loss:  -1.1958 | Function Loss:  -1.792\n",
      "Total loss:  -0.6477 | PDE Loss:  -1.1958 | Function Loss:  -1.7922\n",
      "Total loss:  -0.6478 | PDE Loss:  -1.1961 | Function Loss:  -1.7922\n",
      "Total loss:  -0.6479 | PDE Loss:  -1.1957 | Function Loss:  -1.7925\n",
      "Total loss:  -0.6479 | PDE Loss:  -1.1957 | Function Loss:  -1.7926\n",
      "Total loss:  -0.648 | PDE Loss:  -1.1957 | Function Loss:  -1.7927\n",
      "Total loss:  -0.6481 | PDE Loss:  -1.1954 | Function Loss:  -1.7929\n",
      "Total loss:  -0.6481 | PDE Loss:  -1.1951 | Function Loss:  -1.7931\n",
      "Total loss:  -0.6482 | PDE Loss:  -1.1946 | Function Loss:  -1.7933\n",
      "Total loss:  -0.6482 | PDE Loss:  -1.1946 | Function Loss:  -1.7934\n",
      "Total loss:  -0.6483 | PDE Loss:  -1.1945 | Function Loss:  -1.7935\n",
      "Total loss:  -0.6483 | PDE Loss:  -1.1944 | Function Loss:  -1.7936\n",
      "Total loss:  -0.6484 | PDE Loss:  -1.1944 | Function Loss:  -1.7937\n",
      "Total loss:  -0.6484 | PDE Loss:  -1.1943 | Function Loss:  -1.7938\n",
      "Total loss:  -0.6485 | PDE Loss:  -1.1943 | Function Loss:  -1.7939\n",
      "Total loss:  -0.6486 | PDE Loss:  -1.1942 | Function Loss:  -1.794\n",
      "Total loss:  -0.6486 | PDE Loss:  -1.1943 | Function Loss:  -1.7941\n",
      "Total loss:  -0.6487 | PDE Loss:  -1.1943 | Function Loss:  -1.7942\n",
      "Total loss:  -0.6487 | PDE Loss:  -1.1944 | Function Loss:  -1.7943\n",
      "Total loss:  -0.6488 | PDE Loss:  -1.1944 | Function Loss:  -1.7943\n",
      "Total loss:  -0.6489 | PDE Loss:  -1.1946 | Function Loss:  -1.7943\n",
      "Total loss:  -0.6489 | PDE Loss:  -1.1947 | Function Loss:  -1.7943\n",
      "Total loss:  -0.649 | PDE Loss:  -1.195 | Function Loss:  -1.7943\n",
      "Total loss:  -0.649 | PDE Loss:  -1.1952 | Function Loss:  -1.7943\n",
      "Total loss:  -0.649 | PDE Loss:  -1.1954 | Function Loss:  -1.7942\n",
      "Total loss:  -0.6491 | PDE Loss:  -1.1955 | Function Loss:  -1.7942\n",
      "Total loss:  -0.6491 | PDE Loss:  -1.1957 | Function Loss:  -1.7942\n",
      "Total loss:  -0.6491 | PDE Loss:  -1.1958 | Function Loss:  -1.7942\n",
      "Total loss:  -0.6491 | PDE Loss:  -1.1959 | Function Loss:  -1.7942\n",
      "Total loss:  -0.6492 | PDE Loss:  -1.196 | Function Loss:  -1.7942\n",
      "Total loss:  -0.6492 | PDE Loss:  -1.1961 | Function Loss:  -1.7942\n",
      "Total loss:  -0.6493 | PDE Loss:  -1.196 | Function Loss:  -1.7944\n",
      "Total loss:  -0.6493 | PDE Loss:  -1.1966 | Function Loss:  -1.7942\n",
      "Total loss:  -0.6494 | PDE Loss:  -1.1965 | Function Loss:  -1.7943\n",
      "Total loss:  -0.6495 | PDE Loss:  -1.1963 | Function Loss:  -1.7945\n",
      "Total loss:  -0.6495 | PDE Loss:  -1.1962 | Function Loss:  -1.7946\n",
      "Total loss:  -0.6496 | PDE Loss:  -1.1961 | Function Loss:  -1.7947\n",
      "Total loss:  -0.6496 | PDE Loss:  -1.1961 | Function Loss:  -1.7948\n",
      "Total loss:  -0.6497 | PDE Loss:  -1.1962 | Function Loss:  -1.7949\n",
      "Total loss:  -0.6498 | PDE Loss:  -1.1963 | Function Loss:  -1.7949\n",
      "Total loss:  -0.6498 | PDE Loss:  -1.1966 | Function Loss:  -1.7949\n",
      "Total loss:  -0.6499 | PDE Loss:  -1.197 | Function Loss:  -1.7949\n",
      "Total loss:  -0.65 | PDE Loss:  -1.1983 | Function Loss:  -1.7945\n",
      "Total loss:  -0.6501 | PDE Loss:  -1.1987 | Function Loss:  -1.7945\n",
      "Total loss:  -0.6502 | PDE Loss:  -1.1991 | Function Loss:  -1.7945\n",
      "Total loss:  -0.6503 | PDE Loss:  -1.1996 | Function Loss:  -1.7944\n",
      "Total loss:  -0.6504 | PDE Loss:  -1.2005 | Function Loss:  -1.7942\n",
      "Total loss:  -0.6505 | PDE Loss:  -1.2008 | Function Loss:  -1.7941\n",
      "Total loss:  -0.6505 | PDE Loss:  -1.2012 | Function Loss:  -1.794\n",
      "Total loss:  -0.6506 | PDE Loss:  -1.2019 | Function Loss:  -1.7939\n",
      "Total loss:  -0.6508 | PDE Loss:  -1.2027 | Function Loss:  -1.7938\n",
      "Total loss:  -0.6509 | PDE Loss:  -1.2034 | Function Loss:  -1.7937\n",
      "Total loss:  -0.651 | PDE Loss:  -1.2041 | Function Loss:  -1.7936\n",
      "Total loss:  -0.6511 | PDE Loss:  -1.2044 | Function Loss:  -1.7935\n",
      "Total loss:  -0.6511 | PDE Loss:  -1.2048 | Function Loss:  -1.7935\n",
      "Total loss:  -0.6512 | PDE Loss:  -1.2048 | Function Loss:  -1.7936\n",
      "Total loss:  -0.6512 | PDE Loss:  -1.2048 | Function Loss:  -1.7936\n",
      "Total loss:  -0.6513 | PDE Loss:  -1.2047 | Function Loss:  -1.7937\n",
      "Total loss:  -0.6513 | PDE Loss:  -1.2046 | Function Loss:  -1.7938\n",
      "Total loss:  -0.6513 | PDE Loss:  -1.2044 | Function Loss:  -1.7939\n",
      "Total loss:  -0.6514 | PDE Loss:  -1.2043 | Function Loss:  -1.794\n",
      "Total loss:  -0.6514 | PDE Loss:  -1.2041 | Function Loss:  -1.7942\n",
      "Total loss:  -0.6515 | PDE Loss:  -1.2041 | Function Loss:  -1.7942\n",
      "Total loss:  -0.6515 | PDE Loss:  -1.2041 | Function Loss:  -1.7943\n",
      "Total loss:  -0.6515 | PDE Loss:  -1.2042 | Function Loss:  -1.7943\n",
      "Total loss:  -0.6516 | PDE Loss:  -1.2042 | Function Loss:  -1.7944\n",
      "Total loss:  -0.6517 | PDE Loss:  -1.2043 | Function Loss:  -1.7944\n",
      "Total loss:  -0.6517 | PDE Loss:  -1.2039 | Function Loss:  -1.7947\n",
      "Total loss:  -0.6518 | PDE Loss:  -1.2041 | Function Loss:  -1.7947\n",
      "Total loss:  -0.6518 | PDE Loss:  -1.204 | Function Loss:  -1.7948\n",
      "Total loss:  -0.6519 | PDE Loss:  -1.2039 | Function Loss:  -1.7949\n",
      "Total loss:  -0.652 | PDE Loss:  -1.2039 | Function Loss:  -1.795\n",
      "Total loss:  -0.652 | PDE Loss:  -1.2039 | Function Loss:  -1.7951\n",
      "Total loss:  -0.6521 | PDE Loss:  -1.2038 | Function Loss:  -1.7952\n",
      "Total loss:  -0.6521 | PDE Loss:  -1.2038 | Function Loss:  -1.7952\n",
      "Total loss:  -0.6522 | PDE Loss:  -1.2038 | Function Loss:  -1.7953\n",
      "Total loss:  -0.6523 | PDE Loss:  -1.2039 | Function Loss:  -1.7954\n",
      "Total loss:  -0.6523 | PDE Loss:  -1.2039 | Function Loss:  -1.7955\n",
      "Total loss:  -0.6513 | PDE Loss:  -1.205 | Function Loss:  -1.7937\n",
      "Total loss:  -0.6524 | PDE Loss:  -1.2042 | Function Loss:  -1.7954\n",
      "Total loss:  -0.6525 | PDE Loss:  -1.2041 | Function Loss:  -1.7956\n",
      "Total loss:  -0.6525 | PDE Loss:  -1.2041 | Function Loss:  -1.7957\n",
      "Total loss:  -0.6526 | PDE Loss:  -1.2043 | Function Loss:  -1.7958\n",
      "Total loss:  -0.6527 | PDE Loss:  -1.2045 | Function Loss:  -1.7958\n",
      "Total loss:  -0.6528 | PDE Loss:  -1.2047 | Function Loss:  -1.7958\n",
      "Total loss:  -0.6529 | PDE Loss:  -1.2051 | Function Loss:  -1.7958\n",
      "Total loss:  -0.653 | PDE Loss:  -1.2054 | Function Loss:  -1.7958\n",
      "Total loss:  -0.6531 | PDE Loss:  -1.2058 | Function Loss:  -1.7958\n",
      "Total loss:  -0.6531 | PDE Loss:  -1.2061 | Function Loss:  -1.7958\n",
      "Total loss:  -0.6532 | PDE Loss:  -1.2064 | Function Loss:  -1.7958\n",
      "Total loss:  -0.6533 | PDE Loss:  -1.2066 | Function Loss:  -1.7958\n",
      "Total loss:  -0.6533 | PDE Loss:  -1.2067 | Function Loss:  -1.7958\n",
      "Total loss:  -0.6535 | PDE Loss:  -1.2068 | Function Loss:  -1.7959\n",
      "Total loss:  -0.6535 | PDE Loss:  -1.2068 | Function Loss:  -1.796\n",
      "Total loss:  -0.6536 | PDE Loss:  -1.2066 | Function Loss:  -1.7962\n",
      "Total loss:  -0.6536 | PDE Loss:  -1.2065 | Function Loss:  -1.7963\n",
      "Total loss:  -0.6537 | PDE Loss:  -1.2064 | Function Loss:  -1.7964\n",
      "Total loss:  -0.6537 | PDE Loss:  -1.2063 | Function Loss:  -1.7964\n",
      "Total loss:  -0.6537 | PDE Loss:  -1.2063 | Function Loss:  -1.7965\n",
      "Total loss:  -0.6538 | PDE Loss:  -1.2065 | Function Loss:  -1.7965\n",
      "Total loss:  -0.6538 | PDE Loss:  -1.2067 | Function Loss:  -1.7965\n",
      "Total loss:  -0.6538 | PDE Loss:  -1.2073 | Function Loss:  -1.7962\n",
      "Total loss:  -0.6539 | PDE Loss:  -1.207 | Function Loss:  -1.7964\n",
      "Total loss:  -0.6539 | PDE Loss:  -1.2073 | Function Loss:  -1.7964\n",
      "Total loss:  -0.654 | PDE Loss:  -1.2076 | Function Loss:  -1.7964\n",
      "Total loss:  -0.6541 | PDE Loss:  -1.2083 | Function Loss:  -1.7962\n",
      "Total loss:  -0.6542 | PDE Loss:  -1.2086 | Function Loss:  -1.7962\n",
      "Total loss:  -0.6542 | PDE Loss:  -1.209 | Function Loss:  -1.7962\n",
      "Total loss:  -0.6543 | PDE Loss:  -1.2093 | Function Loss:  -1.7962\n",
      "Total loss:  -0.6544 | PDE Loss:  -1.2094 | Function Loss:  -1.7963\n",
      "Total loss:  -0.6545 | PDE Loss:  -1.2095 | Function Loss:  -1.7964\n",
      "Total loss:  -0.6546 | PDE Loss:  -1.2094 | Function Loss:  -1.7965\n",
      "Total loss:  -0.6547 | PDE Loss:  -1.2093 | Function Loss:  -1.7967\n",
      "Total loss:  -0.6548 | PDE Loss:  -1.2092 | Function Loss:  -1.7969\n",
      "Total loss:  -0.6549 | PDE Loss:  -1.2089 | Function Loss:  -1.7971\n",
      "Total loss:  -0.655 | PDE Loss:  -1.209 | Function Loss:  -1.7972\n",
      "Total loss:  -0.655 | PDE Loss:  -1.2086 | Function Loss:  -1.7974\n",
      "Total loss:  -0.6551 | PDE Loss:  -1.2085 | Function Loss:  -1.7975\n",
      "Total loss:  -0.6552 | PDE Loss:  -1.208 | Function Loss:  -1.7979\n",
      "Total loss:  -0.6553 | PDE Loss:  -1.2078 | Function Loss:  -1.7981\n",
      "Total loss:  -0.6553 | PDE Loss:  -1.2076 | Function Loss:  -1.7983\n",
      "Total loss:  -0.6554 | PDE Loss:  -1.2074 | Function Loss:  -1.7984\n",
      "Total loss:  -0.6554 | PDE Loss:  -1.2073 | Function Loss:  -1.7985\n",
      "Total loss:  -0.6555 | PDE Loss:  -1.2073 | Function Loss:  -1.7985\n",
      "Total loss:  -0.6555 | PDE Loss:  -1.2073 | Function Loss:  -1.7986\n",
      "Total loss:  -0.6556 | PDE Loss:  -1.2073 | Function Loss:  -1.7987\n",
      "Total loss:  -0.6557 | PDE Loss:  -1.207 | Function Loss:  -1.799\n",
      "Total loss:  -0.6558 | PDE Loss:  -1.2073 | Function Loss:  -1.799\n",
      "Total loss:  -0.6559 | PDE Loss:  -1.2075 | Function Loss:  -1.799\n",
      "Total loss:  -0.6559 | PDE Loss:  -1.2078 | Function Loss:  -1.799\n",
      "Total loss:  -0.656 | PDE Loss:  -1.2079 | Function Loss:  -1.7991\n",
      "Total loss:  -0.6561 | PDE Loss:  -1.2081 | Function Loss:  -1.7991\n",
      "Total loss:  -0.6562 | PDE Loss:  -1.2083 | Function Loss:  -1.7991\n",
      "Total loss:  -0.6563 | PDE Loss:  -1.2083 | Function Loss:  -1.7993\n",
      "Total loss:  -0.6564 | PDE Loss:  -1.2081 | Function Loss:  -1.7994\n",
      "Total loss:  -0.6565 | PDE Loss:  -1.2079 | Function Loss:  -1.7997\n",
      "Total loss:  -0.6565 | PDE Loss:  -1.2076 | Function Loss:  -1.7999\n",
      "Total loss:  -0.6566 | PDE Loss:  -1.2072 | Function Loss:  -1.8002\n",
      "Total loss:  -0.6567 | PDE Loss:  -1.2068 | Function Loss:  -1.8005\n",
      "Total loss:  -0.6568 | PDE Loss:  -1.2064 | Function Loss:  -1.8007\n",
      "Total loss:  -0.6569 | PDE Loss:  -1.2062 | Function Loss:  -1.8009\n",
      "Total loss:  -0.6569 | PDE Loss:  -1.2061 | Function Loss:  -1.801\n",
      "Total loss:  -0.657 | PDE Loss:  -1.206 | Function Loss:  -1.8012\n",
      "Total loss:  -0.657 | PDE Loss:  -1.206 | Function Loss:  -1.8012\n",
      "Total loss:  -0.6571 | PDE Loss:  -1.2059 | Function Loss:  -1.8014\n",
      "Total loss:  -0.6572 | PDE Loss:  -1.2063 | Function Loss:  -1.8014\n",
      "Total loss:  -0.6573 | PDE Loss:  -1.2062 | Function Loss:  -1.8015\n",
      "Total loss:  -0.6573 | PDE Loss:  -1.2062 | Function Loss:  -1.8015\n",
      "Total loss:  -0.6573 | PDE Loss:  -1.206 | Function Loss:  -1.8017\n",
      "Total loss:  -0.6574 | PDE Loss:  -1.206 | Function Loss:  -1.8017\n",
      "Total loss:  -0.6574 | PDE Loss:  -1.206 | Function Loss:  -1.8017\n",
      "Total loss:  -0.6574 | PDE Loss:  -1.206 | Function Loss:  -1.8018\n",
      "Total loss:  -0.6575 | PDE Loss:  -1.206 | Function Loss:  -1.8018\n",
      "Total loss:  -0.6575 | PDE Loss:  -1.206 | Function Loss:  -1.8019\n",
      "Total loss:  -0.6576 | PDE Loss:  -1.206 | Function Loss:  -1.802\n",
      "Total loss:  -0.6576 | PDE Loss:  -1.2058 | Function Loss:  -1.8021\n",
      "Total loss:  -0.6577 | PDE Loss:  -1.2057 | Function Loss:  -1.8023\n",
      "Total loss:  -0.6578 | PDE Loss:  -1.2055 | Function Loss:  -1.8025\n",
      "Total loss:  -0.6579 | PDE Loss:  -1.2052 | Function Loss:  -1.8028\n",
      "Total loss:  -0.658 | PDE Loss:  -1.2047 | Function Loss:  -1.8031\n",
      "Total loss:  -0.6581 | PDE Loss:  -1.2046 | Function Loss:  -1.8033\n",
      "Total loss:  -0.6581 | PDE Loss:  -1.2045 | Function Loss:  -1.8034\n",
      "Total loss:  -0.6582 | PDE Loss:  -1.2046 | Function Loss:  -1.8034\n",
      "Total loss:  -0.6583 | PDE Loss:  -1.2046 | Function Loss:  -1.8035\n",
      "Total loss:  -0.6583 | PDE Loss:  -1.2047 | Function Loss:  -1.8035\n",
      "Total loss:  -0.6584 | PDE Loss:  -1.2048 | Function Loss:  -1.8036\n",
      "Total loss:  -0.6584 | PDE Loss:  -1.205 | Function Loss:  -1.8036\n",
      "Total loss:  -0.6585 | PDE Loss:  -1.2051 | Function Loss:  -1.8036\n",
      "Total loss:  -0.6585 | PDE Loss:  -1.2053 | Function Loss:  -1.8036\n",
      "Total loss:  -0.6586 | PDE Loss:  -1.2055 | Function Loss:  -1.8036\n",
      "Total loss:  -0.6586 | PDE Loss:  -1.2057 | Function Loss:  -1.8036\n",
      "Total loss:  -0.6587 | PDE Loss:  -1.2062 | Function Loss:  -1.8035\n",
      "Total loss:  -0.6588 | PDE Loss:  -1.2063 | Function Loss:  -1.8036\n",
      "Total loss:  -0.6589 | PDE Loss:  -1.2066 | Function Loss:  -1.8036\n",
      "Total loss:  -0.659 | PDE Loss:  -1.2065 | Function Loss:  -1.8038\n",
      "Total loss:  -0.6591 | PDE Loss:  -1.2066 | Function Loss:  -1.8038\n",
      "Total loss:  -0.6592 | PDE Loss:  -1.2061 | Function Loss:  -1.8042\n",
      "Total loss:  -0.6593 | PDE Loss:  -1.2063 | Function Loss:  -1.8042\n",
      "Total loss:  -0.6593 | PDE Loss:  -1.2066 | Function Loss:  -1.8042\n",
      "Total loss:  -0.6594 | PDE Loss:  -1.2067 | Function Loss:  -1.8042\n",
      "Total loss:  -0.6595 | PDE Loss:  -1.2067 | Function Loss:  -1.8044\n",
      "Total loss:  -0.6596 | PDE Loss:  -1.2067 | Function Loss:  -1.8045\n",
      "Total loss:  -0.6596 | PDE Loss:  -1.2067 | Function Loss:  -1.8046\n",
      "Total loss:  -0.6597 | PDE Loss:  -1.2068 | Function Loss:  -1.8046\n",
      "Total loss:  -0.6597 | PDE Loss:  -1.2069 | Function Loss:  -1.8047\n",
      "Total loss:  -0.6598 | PDE Loss:  -1.2071 | Function Loss:  -1.8047\n",
      "Total loss:  -0.6598 | PDE Loss:  -1.2072 | Function Loss:  -1.8047\n",
      "Total loss:  -0.6599 | PDE Loss:  -1.2074 | Function Loss:  -1.8047\n",
      "Total loss:  -0.66 | PDE Loss:  -1.208 | Function Loss:  -1.8045\n",
      "Total loss:  -0.66 | PDE Loss:  -1.2081 | Function Loss:  -1.8046\n",
      "Total loss:  -0.6601 | PDE Loss:  -1.2083 | Function Loss:  -1.8046\n",
      "Total loss:  -0.6602 | PDE Loss:  -1.2083 | Function Loss:  -1.8047\n",
      "Total loss:  -0.6603 | PDE Loss:  -1.2085 | Function Loss:  -1.8048\n",
      "Total loss:  -0.6604 | PDE Loss:  -1.2086 | Function Loss:  -1.8048\n",
      "Total loss:  -0.6605 | PDE Loss:  -1.2089 | Function Loss:  -1.8049\n",
      "Total loss:  -0.6606 | PDE Loss:  -1.209 | Function Loss:  -1.805\n",
      "Total loss:  -0.6607 | PDE Loss:  -1.2095 | Function Loss:  -1.805\n",
      "Total loss:  -0.6608 | PDE Loss:  -1.2098 | Function Loss:  -1.805\n",
      "Total loss:  -0.6609 | PDE Loss:  -1.2099 | Function Loss:  -1.8051\n",
      "Total loss:  -0.661 | PDE Loss:  -1.2101 | Function Loss:  -1.8051\n",
      "Total loss:  -0.6611 | PDE Loss:  -1.2103 | Function Loss:  -1.8051\n",
      "Total loss:  -0.6611 | PDE Loss:  -1.2105 | Function Loss:  -1.8051\n",
      "Total loss:  -0.6611 | PDE Loss:  -1.2106 | Function Loss:  -1.8051\n",
      "Total loss:  -0.6612 | PDE Loss:  -1.2107 | Function Loss:  -1.8052\n",
      "Total loss:  -0.6613 | PDE Loss:  -1.2105 | Function Loss:  -1.8054\n",
      "Total loss:  -0.6614 | PDE Loss:  -1.2105 | Function Loss:  -1.8055\n",
      "Total loss:  -0.6615 | PDE Loss:  -1.2095 | Function Loss:  -1.806\n",
      "Total loss:  -0.6615 | PDE Loss:  -1.2096 | Function Loss:  -1.8061\n",
      "Total loss:  -0.6616 | PDE Loss:  -1.2097 | Function Loss:  -1.8061\n",
      "Total loss:  -0.6616 | PDE Loss:  -1.2097 | Function Loss:  -1.8061\n",
      "Total loss:  -0.6616 | PDE Loss:  -1.2098 | Function Loss:  -1.8061\n",
      "Total loss:  -0.6616 | PDE Loss:  -1.21 | Function Loss:  -1.8061\n",
      "Total loss:  -0.6617 | PDE Loss:  -1.2101 | Function Loss:  -1.8061\n",
      "Total loss:  -0.6617 | PDE Loss:  -1.2103 | Function Loss:  -1.806\n",
      "Total loss:  -0.6618 | PDE Loss:  -1.2107 | Function Loss:  -1.806\n",
      "Total loss:  -0.6619 | PDE Loss:  -1.2111 | Function Loss:  -1.8059\n",
      "Total loss:  -0.6619 | PDE Loss:  -1.2113 | Function Loss:  -1.8059\n",
      "Total loss:  -0.662 | PDE Loss:  -1.2114 | Function Loss:  -1.806\n",
      "Total loss:  -0.6621 | PDE Loss:  -1.2116 | Function Loss:  -1.8061\n",
      "Total loss:  -0.6622 | PDE Loss:  -1.2117 | Function Loss:  -1.8061\n",
      "Total loss:  -0.6623 | PDE Loss:  -1.2116 | Function Loss:  -1.8063\n",
      "Total loss:  -0.6624 | PDE Loss:  -1.2118 | Function Loss:  -1.8064\n",
      "Total loss:  -0.6625 | PDE Loss:  -1.2119 | Function Loss:  -1.8065\n",
      "Total loss:  -0.6626 | PDE Loss:  -1.2121 | Function Loss:  -1.8065\n",
      "Total loss:  -0.6627 | PDE Loss:  -1.2125 | Function Loss:  -1.8065\n",
      "Total loss:  -0.6627 | PDE Loss:  -1.2127 | Function Loss:  -1.8065\n",
      "Total loss:  -0.6628 | PDE Loss:  -1.2129 | Function Loss:  -1.8066\n",
      "Total loss:  -0.6629 | PDE Loss:  -1.2133 | Function Loss:  -1.8065\n",
      "Total loss:  -0.663 | PDE Loss:  -1.2135 | Function Loss:  -1.8066\n",
      "Total loss:  -0.663 | PDE Loss:  -1.2138 | Function Loss:  -1.8065\n",
      "Total loss:  -0.6631 | PDE Loss:  -1.2139 | Function Loss:  -1.8065\n",
      "Total loss:  -0.6631 | PDE Loss:  -1.214 | Function Loss:  -1.8066\n",
      "Total loss:  -0.6632 | PDE Loss:  -1.2141 | Function Loss:  -1.8066\n",
      "Total loss:  -0.6632 | PDE Loss:  -1.2141 | Function Loss:  -1.8066\n",
      "Total loss:  -0.6633 | PDE Loss:  -1.2142 | Function Loss:  -1.8067\n",
      "Total loss:  -0.6633 | PDE Loss:  -1.2142 | Function Loss:  -1.8068\n",
      "Total loss:  -0.6634 | PDE Loss:  -1.2141 | Function Loss:  -1.8069\n",
      "Total loss:  -0.6635 | PDE Loss:  -1.214 | Function Loss:  -1.8071\n",
      "Total loss:  -0.6636 | PDE Loss:  -1.2138 | Function Loss:  -1.8073\n",
      "Total loss:  -0.6636 | PDE Loss:  -1.2132 | Function Loss:  -1.8076\n",
      "Total loss:  -0.6637 | PDE Loss:  -1.2136 | Function Loss:  -1.8075\n",
      "Total loss:  -0.6637 | PDE Loss:  -1.2135 | Function Loss:  -1.8076\n",
      "Total loss:  -0.6638 | PDE Loss:  -1.2134 | Function Loss:  -1.8077\n",
      "Total loss:  -0.6638 | PDE Loss:  -1.2132 | Function Loss:  -1.8078\n",
      "Total loss:  -0.6638 | PDE Loss:  -1.2131 | Function Loss:  -1.8079\n",
      "Total loss:  -0.6638 | PDE Loss:  -1.2131 | Function Loss:  -1.8079\n",
      "Total loss:  -0.6639 | PDE Loss:  -1.213 | Function Loss:  -1.808\n",
      "Total loss:  -0.6639 | PDE Loss:  -1.213 | Function Loss:  -1.808\n",
      "Total loss:  -0.6639 | PDE Loss:  -1.2129 | Function Loss:  -1.8081\n",
      "Total loss:  -0.664 | PDE Loss:  -1.2129 | Function Loss:  -1.8082\n",
      "Total loss:  -0.664 | PDE Loss:  -1.2129 | Function Loss:  -1.8082\n",
      "Total loss:  -0.664 | PDE Loss:  -1.2131 | Function Loss:  -1.8082\n",
      "Total loss:  -0.6641 | PDE Loss:  -1.2132 | Function Loss:  -1.8082\n",
      "Total loss:  -0.6641 | PDE Loss:  -1.2134 | Function Loss:  -1.8081\n",
      "Total loss:  -0.6641 | PDE Loss:  -1.2136 | Function Loss:  -1.8081\n",
      "Total loss:  -0.6642 | PDE Loss:  -1.2138 | Function Loss:  -1.8081\n",
      "Total loss:  -0.6642 | PDE Loss:  -1.214 | Function Loss:  -1.808\n",
      "Total loss:  -0.6642 | PDE Loss:  -1.2142 | Function Loss:  -1.808\n",
      "Total loss:  -0.6643 | PDE Loss:  -1.2144 | Function Loss:  -1.8081\n",
      "Total loss:  -0.6644 | PDE Loss:  -1.2144 | Function Loss:  -1.8082\n",
      "Total loss:  -0.6644 | PDE Loss:  -1.2144 | Function Loss:  -1.8082\n",
      "Total loss:  -0.6645 | PDE Loss:  -1.2143 | Function Loss:  -1.8084\n",
      "Total loss:  -0.6646 | PDE Loss:  -1.2142 | Function Loss:  -1.8085\n",
      "Total loss:  -0.6646 | PDE Loss:  -1.2141 | Function Loss:  -1.8086\n",
      "Total loss:  -0.6646 | PDE Loss:  -1.2141 | Function Loss:  -1.8087\n",
      "Total loss:  -0.6647 | PDE Loss:  -1.2141 | Function Loss:  -1.8087\n",
      "Total loss:  -0.6647 | PDE Loss:  -1.2141 | Function Loss:  -1.8087\n",
      "Total loss:  -0.6648 | PDE Loss:  -1.2142 | Function Loss:  -1.8088\n",
      "Total loss:  -0.6648 | PDE Loss:  -1.2145 | Function Loss:  -1.8087\n",
      "Total loss:  -0.6649 | PDE Loss:  -1.2147 | Function Loss:  -1.8087\n",
      "Total loss:  -0.6649 | PDE Loss:  -1.2153 | Function Loss:  -1.8086\n",
      "Total loss:  -0.665 | PDE Loss:  -1.2154 | Function Loss:  -1.8086\n",
      "Total loss:  -0.6651 | PDE Loss:  -1.2158 | Function Loss:  -1.8086\n",
      "Total loss:  -0.6651 | PDE Loss:  -1.2162 | Function Loss:  -1.8085\n",
      "Total loss:  -0.6652 | PDE Loss:  -1.2165 | Function Loss:  -1.8085\n",
      "Total loss:  -0.6653 | PDE Loss:  -1.2168 | Function Loss:  -1.8085\n",
      "Total loss:  -0.6653 | PDE Loss:  -1.217 | Function Loss:  -1.8085\n",
      "Total loss:  -0.6654 | PDE Loss:  -1.2171 | Function Loss:  -1.8085\n",
      "Total loss:  -0.6655 | PDE Loss:  -1.2171 | Function Loss:  -1.8086\n",
      "Total loss:  -0.6655 | PDE Loss:  -1.2172 | Function Loss:  -1.8086\n",
      "Total loss:  -0.6656 | PDE Loss:  -1.2172 | Function Loss:  -1.8087\n",
      "Total loss:  -0.6657 | PDE Loss:  -1.2173 | Function Loss:  -1.8088\n",
      "Total loss:  -0.6657 | PDE Loss:  -1.2172 | Function Loss:  -1.809\n",
      "Total loss:  -0.6658 | PDE Loss:  -1.2174 | Function Loss:  -1.809\n",
      "Total loss:  -0.6659 | PDE Loss:  -1.2174 | Function Loss:  -1.8091\n",
      "Total loss:  -0.666 | PDE Loss:  -1.2176 | Function Loss:  -1.8091\n",
      "Total loss:  -0.6661 | PDE Loss:  -1.218 | Function Loss:  -1.8091\n",
      "Total loss:  -0.6662 | PDE Loss:  -1.2181 | Function Loss:  -1.8092\n",
      "Total loss:  -0.6663 | PDE Loss:  -1.2181 | Function Loss:  -1.8094\n",
      "Total loss:  -0.6664 | PDE Loss:  -1.2184 | Function Loss:  -1.8094\n",
      "Total loss:  -0.6666 | PDE Loss:  -1.2184 | Function Loss:  -1.8096\n",
      "Total loss:  -0.6666 | PDE Loss:  -1.2184 | Function Loss:  -1.8097\n",
      "Total loss:  -0.6667 | PDE Loss:  -1.2183 | Function Loss:  -1.8099\n",
      "Total loss:  -0.6668 | PDE Loss:  -1.2181 | Function Loss:  -1.8101\n",
      "Total loss:  -0.6668 | PDE Loss:  -1.2179 | Function Loss:  -1.8102\n",
      "Total loss:  -0.6668 | PDE Loss:  -1.2178 | Function Loss:  -1.8103\n",
      "Total loss:  -0.6669 | PDE Loss:  -1.2177 | Function Loss:  -1.8103\n",
      "Total loss:  -0.6669 | PDE Loss:  -1.2176 | Function Loss:  -1.8104\n",
      "Total loss:  -0.667 | PDE Loss:  -1.2175 | Function Loss:  -1.8105\n",
      "Total loss:  -0.667 | PDE Loss:  -1.2179 | Function Loss:  -1.8104\n",
      "Total loss:  -0.6671 | PDE Loss:  -1.2178 | Function Loss:  -1.8105\n",
      "Total loss:  -0.6671 | PDE Loss:  -1.2179 | Function Loss:  -1.8106\n",
      "Total loss:  -0.6672 | PDE Loss:  -1.218 | Function Loss:  -1.8106\n",
      "Total loss:  -0.6672 | PDE Loss:  -1.2183 | Function Loss:  -1.8106\n",
      "Total loss:  -0.6673 | PDE Loss:  -1.2185 | Function Loss:  -1.8106\n",
      "Total loss:  -0.6673 | PDE Loss:  -1.2187 | Function Loss:  -1.8105\n",
      "Total loss:  -0.6673 | PDE Loss:  -1.2189 | Function Loss:  -1.8105\n",
      "Total loss:  -0.6674 | PDE Loss:  -1.219 | Function Loss:  -1.8105\n",
      "Total loss:  -0.6674 | PDE Loss:  -1.219 | Function Loss:  -1.8105\n",
      "Total loss:  -0.6674 | PDE Loss:  -1.2191 | Function Loss:  -1.8105\n",
      "Total loss:  -0.6674 | PDE Loss:  -1.2191 | Function Loss:  -1.8105\n",
      "Total loss:  -0.6675 | PDE Loss:  -1.2192 | Function Loss:  -1.8106\n",
      "Total loss:  -0.6675 | PDE Loss:  -1.2192 | Function Loss:  -1.8106\n",
      "Total loss:  -0.6675 | PDE Loss:  -1.2193 | Function Loss:  -1.8106\n",
      "Total loss:  -0.6676 | PDE Loss:  -1.2195 | Function Loss:  -1.8106\n",
      "Total loss:  -0.6676 | PDE Loss:  -1.2198 | Function Loss:  -1.8105\n",
      "Total loss:  -0.6677 | PDE Loss:  -1.2199 | Function Loss:  -1.8106\n",
      "Total loss:  -0.6677 | PDE Loss:  -1.2202 | Function Loss:  -1.8106\n",
      "Total loss:  -0.6678 | PDE Loss:  -1.2204 | Function Loss:  -1.8106\n",
      "Total loss:  -0.6679 | PDE Loss:  -1.2207 | Function Loss:  -1.8106\n",
      "Total loss:  -0.668 | PDE Loss:  -1.2212 | Function Loss:  -1.8106\n",
      "Total loss:  -0.6681 | PDE Loss:  -1.2215 | Function Loss:  -1.8106\n",
      "Total loss:  -0.6682 | PDE Loss:  -1.2218 | Function Loss:  -1.8106\n",
      "Total loss:  -0.6683 | PDE Loss:  -1.2221 | Function Loss:  -1.8106\n",
      "Total loss:  -0.6683 | PDE Loss:  -1.2223 | Function Loss:  -1.8106\n",
      "Total loss:  -0.6684 | PDE Loss:  -1.2224 | Function Loss:  -1.8106\n",
      "Total loss:  -0.6684 | PDE Loss:  -1.2225 | Function Loss:  -1.8106\n",
      "Total loss:  -0.6685 | PDE Loss:  -1.2227 | Function Loss:  -1.8106\n",
      "Total loss:  -0.6685 | PDE Loss:  -1.2229 | Function Loss:  -1.8106\n",
      "Total loss:  -0.6686 | PDE Loss:  -1.223 | Function Loss:  -1.8106\n",
      "Total loss:  -0.6686 | PDE Loss:  -1.2232 | Function Loss:  -1.8106\n",
      "Total loss:  -0.6687 | PDE Loss:  -1.223 | Function Loss:  -1.8108\n",
      "Total loss:  -0.6687 | PDE Loss:  -1.2231 | Function Loss:  -1.8108\n",
      "Total loss:  -0.6687 | PDE Loss:  -1.223 | Function Loss:  -1.8109\n",
      "Total loss:  -0.6688 | PDE Loss:  -1.2229 | Function Loss:  -1.811\n",
      "Total loss:  -0.6688 | PDE Loss:  -1.2228 | Function Loss:  -1.811\n",
      "Total loss:  -0.6688 | PDE Loss:  -1.2228 | Function Loss:  -1.8111\n",
      "Total loss:  -0.6689 | PDE Loss:  -1.2228 | Function Loss:  -1.8111\n",
      "Total loss:  -0.6689 | PDE Loss:  -1.2229 | Function Loss:  -1.8111\n",
      "Total loss:  -0.6689 | PDE Loss:  -1.223 | Function Loss:  -1.8111\n",
      "Total loss:  -0.669 | PDE Loss:  -1.2232 | Function Loss:  -1.8111\n",
      "Total loss:  -0.669 | PDE Loss:  -1.2237 | Function Loss:  -1.8109\n",
      "Total loss:  -0.669 | PDE Loss:  -1.2235 | Function Loss:  -1.811\n",
      "Total loss:  -0.669 | PDE Loss:  -1.2237 | Function Loss:  -1.811\n",
      "Total loss:  -0.6691 | PDE Loss:  -1.2239 | Function Loss:  -1.811\n",
      "Total loss:  -0.6691 | PDE Loss:  -1.224 | Function Loss:  -1.811\n",
      "Total loss:  -0.6691 | PDE Loss:  -1.224 | Function Loss:  -1.811\n",
      "Total loss:  -0.6692 | PDE Loss:  -1.2239 | Function Loss:  -1.8111\n",
      "Total loss:  -0.6692 | PDE Loss:  -1.2237 | Function Loss:  -1.8112\n",
      "Total loss:  -0.6692 | PDE Loss:  -1.2235 | Function Loss:  -1.8114\n",
      "Total loss:  -0.6693 | PDE Loss:  -1.2232 | Function Loss:  -1.8115\n",
      "Total loss:  -0.6693 | PDE Loss:  -1.2229 | Function Loss:  -1.8117\n",
      "Total loss:  -0.6694 | PDE Loss:  -1.2226 | Function Loss:  -1.8119\n",
      "Total loss:  -0.6694 | PDE Loss:  -1.2225 | Function Loss:  -1.812\n",
      "Total loss:  -0.6695 | PDE Loss:  -1.2225 | Function Loss:  -1.8121\n",
      "Total loss:  -0.6695 | PDE Loss:  -1.2224 | Function Loss:  -1.8122\n",
      "Total loss:  -0.6696 | PDE Loss:  -1.2226 | Function Loss:  -1.8121\n",
      "Total loss:  -0.6696 | PDE Loss:  -1.2226 | Function Loss:  -1.8122\n",
      "Total loss:  -0.6696 | PDE Loss:  -1.2228 | Function Loss:  -1.8122\n",
      "Total loss:  -0.6697 | PDE Loss:  -1.2233 | Function Loss:  -1.8121\n",
      "Total loss:  -0.6698 | PDE Loss:  -1.2231 | Function Loss:  -1.8123\n",
      "Total loss:  -0.6699 | PDE Loss:  -1.2237 | Function Loss:  -1.8121\n",
      "Total loss:  -0.6699 | PDE Loss:  -1.2243 | Function Loss:  -1.812\n",
      "Total loss:  -0.67 | PDE Loss:  -1.2248 | Function Loss:  -1.8119\n",
      "Total loss:  -0.67 | PDE Loss:  -1.2252 | Function Loss:  -1.8118\n",
      "Total loss:  -0.6701 | PDE Loss:  -1.2256 | Function Loss:  -1.8117\n",
      "Total loss:  -0.6701 | PDE Loss:  -1.2261 | Function Loss:  -1.8116\n",
      "Total loss:  -0.6701 | PDE Loss:  -1.2263 | Function Loss:  -1.8115\n",
      "Total loss:  -0.6702 | PDE Loss:  -1.2265 | Function Loss:  -1.8115\n",
      "Total loss:  -0.6702 | PDE Loss:  -1.2265 | Function Loss:  -1.8115\n",
      "Total loss:  -0.6703 | PDE Loss:  -1.2267 | Function Loss:  -1.8115\n",
      "Total loss:  -0.6703 | PDE Loss:  -1.2268 | Function Loss:  -1.8116\n",
      "Total loss:  -0.6703 | PDE Loss:  -1.2268 | Function Loss:  -1.8116\n",
      "Total loss:  -0.6704 | PDE Loss:  -1.2268 | Function Loss:  -1.8117\n",
      "Total loss:  -0.6704 | PDE Loss:  -1.2267 | Function Loss:  -1.8118\n",
      "Total loss:  -0.6705 | PDE Loss:  -1.2267 | Function Loss:  -1.8118\n",
      "Total loss:  -0.6705 | PDE Loss:  -1.2267 | Function Loss:  -1.8119\n",
      "Total loss:  -0.6705 | PDE Loss:  -1.2267 | Function Loss:  -1.8119\n",
      "Total loss:  -0.6706 | PDE Loss:  -1.2268 | Function Loss:  -1.812\n",
      "Total loss:  -0.6707 | PDE Loss:  -1.227 | Function Loss:  -1.812\n",
      "Total loss:  -0.6707 | PDE Loss:  -1.2271 | Function Loss:  -1.812\n",
      "Total loss:  -0.6708 | PDE Loss:  -1.2276 | Function Loss:  -1.812\n",
      "Total loss:  -0.6709 | PDE Loss:  -1.2277 | Function Loss:  -1.812\n",
      "Total loss:  -0.6709 | PDE Loss:  -1.2279 | Function Loss:  -1.812\n",
      "Total loss:  -0.671 | PDE Loss:  -1.2281 | Function Loss:  -1.812\n",
      "Total loss:  -0.671 | PDE Loss:  -1.2284 | Function Loss:  -1.812\n",
      "Total loss:  -0.6711 | PDE Loss:  -1.2286 | Function Loss:  -1.812\n",
      "Total loss:  -0.6711 | PDE Loss:  -1.2288 | Function Loss:  -1.8119\n",
      "Total loss:  -0.6712 | PDE Loss:  -1.2291 | Function Loss:  -1.8119\n",
      "Total loss:  -0.6712 | PDE Loss:  -1.2294 | Function Loss:  -1.8118\n",
      "Total loss:  -0.6713 | PDE Loss:  -1.2299 | Function Loss:  -1.8118\n",
      "Total loss:  -0.6714 | PDE Loss:  -1.2303 | Function Loss:  -1.8117\n",
      "Total loss:  -0.6714 | PDE Loss:  -1.2306 | Function Loss:  -1.8116\n",
      "Total loss:  -0.6715 | PDE Loss:  -1.2308 | Function Loss:  -1.8116\n",
      "Total loss:  -0.6715 | PDE Loss:  -1.231 | Function Loss:  -1.8116\n",
      "Total loss:  -0.6714 | PDE Loss:  -1.2295 | Function Loss:  -1.8121\n",
      "Total loss:  -0.6715 | PDE Loss:  -1.2306 | Function Loss:  -1.8118\n",
      "Total loss:  -0.6716 | PDE Loss:  -1.2307 | Function Loss:  -1.8118\n",
      "Total loss:  -0.6716 | PDE Loss:  -1.2308 | Function Loss:  -1.8119\n",
      "Total loss:  -0.6717 | PDE Loss:  -1.2309 | Function Loss:  -1.8119\n",
      "Total loss:  -0.6717 | PDE Loss:  -1.231 | Function Loss:  -1.8119\n",
      "Total loss:  -0.6718 | PDE Loss:  -1.2313 | Function Loss:  -1.8118\n",
      "Total loss:  -0.6718 | PDE Loss:  -1.2313 | Function Loss:  -1.8119\n",
      "Total loss:  -0.6719 | PDE Loss:  -1.2313 | Function Loss:  -1.812\n",
      "Total loss:  -0.6719 | PDE Loss:  -1.2314 | Function Loss:  -1.812\n",
      "Total loss:  -0.672 | PDE Loss:  -1.2316 | Function Loss:  -1.812\n",
      "Total loss:  -0.672 | PDE Loss:  -1.2317 | Function Loss:  -1.8121\n",
      "Total loss:  -0.6721 | PDE Loss:  -1.2319 | Function Loss:  -1.8121\n",
      "Total loss:  -0.6721 | PDE Loss:  -1.2319 | Function Loss:  -1.8121\n",
      "Total loss:  -0.6721 | PDE Loss:  -1.2319 | Function Loss:  -1.8121\n",
      "Total loss:  -0.6722 | PDE Loss:  -1.2321 | Function Loss:  -1.8121\n",
      "Total loss:  -0.6722 | PDE Loss:  -1.2322 | Function Loss:  -1.8122\n",
      "Total loss:  -0.6723 | PDE Loss:  -1.2323 | Function Loss:  -1.8122\n",
      "Total loss:  -0.6723 | PDE Loss:  -1.2323 | Function Loss:  -1.8123\n",
      "Total loss:  -0.6724 | PDE Loss:  -1.2323 | Function Loss:  -1.8123\n",
      "Total loss:  -0.6724 | PDE Loss:  -1.2323 | Function Loss:  -1.8124\n",
      "Total loss:  -0.6724 | PDE Loss:  -1.2322 | Function Loss:  -1.8124\n",
      "Total loss:  -0.6725 | PDE Loss:  -1.2322 | Function Loss:  -1.8125\n",
      "Total loss:  -0.6726 | PDE Loss:  -1.232 | Function Loss:  -1.8127\n",
      "Total loss:  -0.6726 | PDE Loss:  -1.232 | Function Loss:  -1.8127\n",
      "Total loss:  -0.6726 | PDE Loss:  -1.232 | Function Loss:  -1.8128\n",
      "Total loss:  -0.6727 | PDE Loss:  -1.232 | Function Loss:  -1.8129\n",
      "Total loss:  -0.6728 | PDE Loss:  -1.232 | Function Loss:  -1.8129\n",
      "Total loss:  -0.6728 | PDE Loss:  -1.2321 | Function Loss:  -1.813\n",
      "Total loss:  -0.6729 | PDE Loss:  -1.2322 | Function Loss:  -1.8131\n",
      "Total loss:  -0.673 | PDE Loss:  -1.232 | Function Loss:  -1.8132\n",
      "Total loss:  -0.673 | PDE Loss:  -1.232 | Function Loss:  -1.8133\n",
      "Total loss:  -0.6731 | PDE Loss:  -1.232 | Function Loss:  -1.8134\n",
      "Total loss:  -0.6731 | PDE Loss:  -1.232 | Function Loss:  -1.8134\n",
      "Total loss:  -0.6731 | PDE Loss:  -1.232 | Function Loss:  -1.8135\n",
      "Total loss:  -0.6731 | PDE Loss:  -1.232 | Function Loss:  -1.8135\n",
      "Total loss:  -0.6732 | PDE Loss:  -1.2321 | Function Loss:  -1.8135\n",
      "Total loss:  -0.6732 | PDE Loss:  -1.2323 | Function Loss:  -1.8135\n",
      "Total loss:  -0.6732 | PDE Loss:  -1.2325 | Function Loss:  -1.8134\n",
      "Total loss:  -0.6733 | PDE Loss:  -1.2331 | Function Loss:  -1.8133\n",
      "Total loss:  -0.6733 | PDE Loss:  -1.2332 | Function Loss:  -1.8133\n",
      "Total loss:  -0.6734 | PDE Loss:  -1.2336 | Function Loss:  -1.8132\n",
      "Total loss:  -0.6734 | PDE Loss:  -1.234 | Function Loss:  -1.8131\n",
      "Total loss:  -0.6735 | PDE Loss:  -1.2342 | Function Loss:  -1.8131\n",
      "Total loss:  -0.6735 | PDE Loss:  -1.2346 | Function Loss:  -1.813\n",
      "Total loss:  -0.6735 | PDE Loss:  -1.235 | Function Loss:  -1.8129\n",
      "Total loss:  -0.6735 | PDE Loss:  -1.2351 | Function Loss:  -1.8129\n",
      "Total loss:  -0.6736 | PDE Loss:  -1.2353 | Function Loss:  -1.8129\n",
      "Total loss:  -0.6737 | PDE Loss:  -1.2354 | Function Loss:  -1.8129\n",
      "Total loss:  -0.6737 | PDE Loss:  -1.2356 | Function Loss:  -1.8129\n",
      "Total loss:  -0.6738 | PDE Loss:  -1.2357 | Function Loss:  -1.8129\n",
      "Total loss:  -0.6738 | PDE Loss:  -1.2359 | Function Loss:  -1.8129\n",
      "Total loss:  -0.6738 | PDE Loss:  -1.2358 | Function Loss:  -1.813\n",
      "Total loss:  -0.6739 | PDE Loss:  -1.236 | Function Loss:  -1.813\n",
      "Total loss:  -0.674 | PDE Loss:  -1.2362 | Function Loss:  -1.813\n",
      "Total loss:  -0.674 | PDE Loss:  -1.2365 | Function Loss:  -1.813\n",
      "Total loss:  -0.6741 | PDE Loss:  -1.2367 | Function Loss:  -1.813\n",
      "Total loss:  -0.6741 | PDE Loss:  -1.237 | Function Loss:  -1.813\n",
      "Total loss:  -0.6742 | PDE Loss:  -1.2373 | Function Loss:  -1.813\n",
      "Total loss:  -0.6742 | PDE Loss:  -1.2375 | Function Loss:  -1.8129\n",
      "Total loss:  -0.6742 | PDE Loss:  -1.2381 | Function Loss:  -1.8127\n",
      "Total loss:  -0.6743 | PDE Loss:  -1.2378 | Function Loss:  -1.8128\n",
      "Total loss:  -0.6743 | PDE Loss:  -1.238 | Function Loss:  -1.8128\n",
      "Total loss:  -0.6743 | PDE Loss:  -1.2382 | Function Loss:  -1.8128\n",
      "Total loss:  -0.6744 | PDE Loss:  -1.2384 | Function Loss:  -1.8128\n",
      "Total loss:  -0.6744 | PDE Loss:  -1.2385 | Function Loss:  -1.8128\n",
      "Total loss:  -0.6745 | PDE Loss:  -1.2386 | Function Loss:  -1.8128\n",
      "Total loss:  -0.6745 | PDE Loss:  -1.2387 | Function Loss:  -1.8128\n",
      "Total loss:  -0.6746 | PDE Loss:  -1.2387 | Function Loss:  -1.8129\n",
      "Total loss:  -0.6746 | PDE Loss:  -1.2383 | Function Loss:  -1.8131\n",
      "Total loss:  -0.6746 | PDE Loss:  -1.2383 | Function Loss:  -1.8132\n",
      "Total loss:  -0.6747 | PDE Loss:  -1.2382 | Function Loss:  -1.8133\n",
      "Total loss:  -0.6748 | PDE Loss:  -1.238 | Function Loss:  -1.8135\n",
      "Total loss:  -0.6748 | PDE Loss:  -1.2378 | Function Loss:  -1.8136\n",
      "Total loss:  -0.6749 | PDE Loss:  -1.2375 | Function Loss:  -1.8138\n",
      "Total loss:  -0.6749 | PDE Loss:  -1.2372 | Function Loss:  -1.814\n",
      "Total loss:  -0.675 | PDE Loss:  -1.2369 | Function Loss:  -1.8142\n",
      "Total loss:  -0.675 | PDE Loss:  -1.2363 | Function Loss:  -1.8145\n",
      "Total loss:  -0.6751 | PDE Loss:  -1.2363 | Function Loss:  -1.8145\n",
      "Total loss:  -0.6751 | PDE Loss:  -1.2364 | Function Loss:  -1.8145\n",
      "Total loss:  -0.6751 | PDE Loss:  -1.2365 | Function Loss:  -1.8145\n",
      "Total loss:  -0.6751 | PDE Loss:  -1.2366 | Function Loss:  -1.8145\n",
      "Total loss:  -0.6751 | PDE Loss:  -1.2365 | Function Loss:  -1.8145\n",
      "Total loss:  -0.6752 | PDE Loss:  -1.2364 | Function Loss:  -1.8146\n",
      "Total loss:  -0.6752 | PDE Loss:  -1.2363 | Function Loss:  -1.8147\n",
      "Total loss:  -0.6753 | PDE Loss:  -1.2362 | Function Loss:  -1.8148\n",
      "Total loss:  -0.6753 | PDE Loss:  -1.236 | Function Loss:  -1.8149\n",
      "Total loss:  -0.6753 | PDE Loss:  -1.2359 | Function Loss:  -1.815\n",
      "Total loss:  -0.6753 | PDE Loss:  -1.2358 | Function Loss:  -1.8151\n",
      "Total loss:  -0.6754 | PDE Loss:  -1.2357 | Function Loss:  -1.8152\n",
      "Total loss:  -0.6754 | PDE Loss:  -1.2356 | Function Loss:  -1.8152\n",
      "Total loss:  -0.6754 | PDE Loss:  -1.2352 | Function Loss:  -1.8154\n",
      "Total loss:  -0.6755 | PDE Loss:  -1.2353 | Function Loss:  -1.8154\n",
      "Total loss:  -0.6755 | PDE Loss:  -1.2356 | Function Loss:  -1.8154\n",
      "Total loss:  -0.6755 | PDE Loss:  -1.2358 | Function Loss:  -1.8154\n",
      "Total loss:  -0.6756 | PDE Loss:  -1.236 | Function Loss:  -1.8153\n",
      "Total loss:  -0.6756 | PDE Loss:  -1.2362 | Function Loss:  -1.8153\n",
      "Total loss:  -0.6757 | PDE Loss:  -1.2363 | Function Loss:  -1.8154\n",
      "Total loss:  -0.6754 | PDE Loss:  -1.2367 | Function Loss:  -1.8149\n",
      "Total loss:  -0.6757 | PDE Loss:  -1.2366 | Function Loss:  -1.8153\n",
      "Total loss:  -0.6758 | PDE Loss:  -1.2366 | Function Loss:  -1.8154\n",
      "Total loss:  -0.6758 | PDE Loss:  -1.2368 | Function Loss:  -1.8154\n",
      "Total loss:  -0.6759 | PDE Loss:  -1.2369 | Function Loss:  -1.8155\n",
      "Total loss:  -0.676 | PDE Loss:  -1.2371 | Function Loss:  -1.8155\n",
      "Total loss:  -0.6761 | PDE Loss:  -1.2373 | Function Loss:  -1.8155\n",
      "Total loss:  -0.6761 | PDE Loss:  -1.2375 | Function Loss:  -1.8155\n",
      "Total loss:  -0.6762 | PDE Loss:  -1.2376 | Function Loss:  -1.8155\n",
      "Total loss:  -0.6762 | PDE Loss:  -1.2378 | Function Loss:  -1.8155\n",
      "Total loss:  -0.6763 | PDE Loss:  -1.238 | Function Loss:  -1.8155\n",
      "Total loss:  -0.6764 | PDE Loss:  -1.2385 | Function Loss:  -1.8155\n",
      "Total loss:  -0.6764 | PDE Loss:  -1.2385 | Function Loss:  -1.8156\n",
      "Total loss:  -0.6765 | PDE Loss:  -1.2386 | Function Loss:  -1.8156\n",
      "Total loss:  -0.6766 | PDE Loss:  -1.2388 | Function Loss:  -1.8157\n",
      "Total loss:  -0.6766 | PDE Loss:  -1.2389 | Function Loss:  -1.8157\n",
      "Total loss:  -0.6767 | PDE Loss:  -1.2388 | Function Loss:  -1.8158\n",
      "Total loss:  -0.6767 | PDE Loss:  -1.2387 | Function Loss:  -1.8159\n",
      "Total loss:  -0.6768 | PDE Loss:  -1.2386 | Function Loss:  -1.816\n",
      "Total loss:  -0.6768 | PDE Loss:  -1.2385 | Function Loss:  -1.8161\n",
      "Total loss:  -0.6769 | PDE Loss:  -1.2384 | Function Loss:  -1.8163\n",
      "Total loss:  -0.677 | PDE Loss:  -1.2383 | Function Loss:  -1.8164\n",
      "Total loss:  -0.677 | PDE Loss:  -1.2382 | Function Loss:  -1.8165\n",
      "Total loss:  -0.677 | PDE Loss:  -1.2381 | Function Loss:  -1.8165\n",
      "Total loss:  -0.6771 | PDE Loss:  -1.2381 | Function Loss:  -1.8166\n",
      "Total loss:  -0.6771 | PDE Loss:  -1.238 | Function Loss:  -1.8167\n",
      "Total loss:  -0.6771 | PDE Loss:  -1.2378 | Function Loss:  -1.8167\n",
      "Total loss:  -0.6771 | PDE Loss:  -1.238 | Function Loss:  -1.8167\n",
      "Total loss:  -0.6772 | PDE Loss:  -1.2379 | Function Loss:  -1.8168\n",
      "Total loss:  -0.6772 | PDE Loss:  -1.2378 | Function Loss:  -1.8169\n",
      "Total loss:  -0.6772 | PDE Loss:  -1.2377 | Function Loss:  -1.817\n",
      "Total loss:  -0.6773 | PDE Loss:  -1.2373 | Function Loss:  -1.8172\n",
      "Total loss:  -0.6773 | PDE Loss:  -1.2373 | Function Loss:  -1.8172\n",
      "Total loss:  -0.6773 | PDE Loss:  -1.2372 | Function Loss:  -1.8173\n",
      "Total loss:  -0.6774 | PDE Loss:  -1.2372 | Function Loss:  -1.8174\n",
      "Total loss:  -0.6775 | PDE Loss:  -1.2374 | Function Loss:  -1.8174\n",
      "Total loss:  -0.6775 | PDE Loss:  -1.2375 | Function Loss:  -1.8174\n",
      "Total loss:  -0.6776 | PDE Loss:  -1.2378 | Function Loss:  -1.8174\n",
      "Total loss:  -0.6777 | PDE Loss:  -1.2382 | Function Loss:  -1.8174\n",
      "Total loss:  -0.6777 | PDE Loss:  -1.2389 | Function Loss:  -1.8172\n",
      "Total loss:  -0.6778 | PDE Loss:  -1.239 | Function Loss:  -1.8172\n",
      "Total loss:  -0.6778 | PDE Loss:  -1.2391 | Function Loss:  -1.8172\n",
      "Total loss:  -0.6778 | PDE Loss:  -1.2392 | Function Loss:  -1.8172\n",
      "Total loss:  -0.6779 | PDE Loss:  -1.2393 | Function Loss:  -1.8172\n",
      "Total loss:  -0.6779 | PDE Loss:  -1.2394 | Function Loss:  -1.8172\n",
      "Total loss:  -0.6779 | PDE Loss:  -1.2395 | Function Loss:  -1.8172\n",
      "Total loss:  -0.678 | PDE Loss:  -1.2396 | Function Loss:  -1.8173\n",
      "Total loss:  -0.678 | PDE Loss:  -1.2395 | Function Loss:  -1.8173\n",
      "Total loss:  -0.678 | PDE Loss:  -1.2395 | Function Loss:  -1.8174\n",
      "Total loss:  -0.6781 | PDE Loss:  -1.2394 | Function Loss:  -1.8175\n",
      "Total loss:  -0.6781 | PDE Loss:  -1.2392 | Function Loss:  -1.8176\n",
      "Total loss:  -0.6782 | PDE Loss:  -1.2391 | Function Loss:  -1.8178\n",
      "Total loss:  -0.6783 | PDE Loss:  -1.2389 | Function Loss:  -1.8179\n",
      "Total loss:  -0.6783 | PDE Loss:  -1.2388 | Function Loss:  -1.8181\n",
      "Total loss:  -0.6784 | PDE Loss:  -1.2388 | Function Loss:  -1.8182\n",
      "Total loss:  -0.6784 | PDE Loss:  -1.2383 | Function Loss:  -1.8184\n",
      "Total loss:  -0.6785 | PDE Loss:  -1.2387 | Function Loss:  -1.8183\n",
      "Total loss:  -0.6786 | PDE Loss:  -1.2388 | Function Loss:  -1.8184\n",
      "Total loss:  -0.6786 | PDE Loss:  -1.2392 | Function Loss:  -1.8183\n",
      "Total loss:  -0.6787 | PDE Loss:  -1.24 | Function Loss:  -1.8181\n",
      "Total loss:  -0.6788 | PDE Loss:  -1.2403 | Function Loss:  -1.8182\n",
      "Total loss:  -0.6788 | PDE Loss:  -1.2407 | Function Loss:  -1.8181\n",
      "Total loss:  -0.6789 | PDE Loss:  -1.2412 | Function Loss:  -1.8179\n",
      "Total loss:  -0.679 | PDE Loss:  -1.2418 | Function Loss:  -1.8178\n",
      "Total loss:  -0.679 | PDE Loss:  -1.2425 | Function Loss:  -1.8176\n",
      "Total loss:  -0.6791 | PDE Loss:  -1.2433 | Function Loss:  -1.8174\n",
      "Total loss:  -0.6792 | PDE Loss:  -1.2443 | Function Loss:  -1.8172\n",
      "Total loss:  -0.6793 | PDE Loss:  -1.2448 | Function Loss:  -1.8171\n",
      "Total loss:  -0.6793 | PDE Loss:  -1.2455 | Function Loss:  -1.8169\n",
      "Total loss:  -0.6794 | PDE Loss:  -1.2459 | Function Loss:  -1.8169\n",
      "Total loss:  -0.6795 | PDE Loss:  -1.2465 | Function Loss:  -1.8168\n",
      "Total loss:  -0.6796 | PDE Loss:  -1.2467 | Function Loss:  -1.8169\n",
      "Total loss:  -0.6797 | PDE Loss:  -1.247 | Function Loss:  -1.8169\n",
      "Total loss:  -0.6799 | PDE Loss:  -1.247 | Function Loss:  -1.8171\n",
      "Total loss:  -0.6799 | PDE Loss:  -1.2471 | Function Loss:  -1.8171\n",
      "Total loss:  -0.6801 | PDE Loss:  -1.2471 | Function Loss:  -1.8173\n",
      "Total loss:  -0.6801 | PDE Loss:  -1.2471 | Function Loss:  -1.8175\n",
      "Total loss:  -0.6802 | PDE Loss:  -1.247 | Function Loss:  -1.8176\n",
      "Total loss:  -0.6803 | PDE Loss:  -1.2469 | Function Loss:  -1.8178\n",
      "Total loss:  -0.6804 | PDE Loss:  -1.2468 | Function Loss:  -1.8179\n",
      "Total loss:  -0.6805 | PDE Loss:  -1.2467 | Function Loss:  -1.8181\n",
      "Total loss:  -0.6806 | PDE Loss:  -1.2465 | Function Loss:  -1.8182\n",
      "Total loss:  -0.6806 | PDE Loss:  -1.2461 | Function Loss:  -1.8185\n",
      "Total loss:  -0.6807 | PDE Loss:  -1.2461 | Function Loss:  -1.8186\n",
      "Total loss:  -0.6808 | PDE Loss:  -1.246 | Function Loss:  -1.8187\n",
      "Total loss:  -0.6808 | PDE Loss:  -1.246 | Function Loss:  -1.8188\n",
      "Total loss:  -0.6809 | PDE Loss:  -1.2462 | Function Loss:  -1.8188\n",
      "Total loss:  -0.681 | PDE Loss:  -1.2459 | Function Loss:  -1.8191\n",
      "Total loss:  -0.681 | PDE Loss:  -1.2461 | Function Loss:  -1.8191\n",
      "Total loss:  -0.6811 | PDE Loss:  -1.2463 | Function Loss:  -1.8191\n",
      "Total loss:  -0.6812 | PDE Loss:  -1.2464 | Function Loss:  -1.8191\n",
      "Total loss:  -0.6813 | PDE Loss:  -1.2465 | Function Loss:  -1.8192\n",
      "Total loss:  -0.6813 | PDE Loss:  -1.2465 | Function Loss:  -1.8193\n",
      "Total loss:  -0.6813 | PDE Loss:  -1.2463 | Function Loss:  -1.8194\n",
      "Total loss:  -0.6814 | PDE Loss:  -1.2463 | Function Loss:  -1.8194\n",
      "Total loss:  -0.6814 | PDE Loss:  -1.2462 | Function Loss:  -1.8195\n",
      "Total loss:  -0.6814 | PDE Loss:  -1.2462 | Function Loss:  -1.8195\n",
      "Total loss:  -0.6814 | PDE Loss:  -1.2462 | Function Loss:  -1.8196\n",
      "Total loss:  -0.6815 | PDE Loss:  -1.2457 | Function Loss:  -1.8198\n",
      "Total loss:  -0.6815 | PDE Loss:  -1.2456 | Function Loss:  -1.8199\n",
      "Total loss:  -0.6816 | PDE Loss:  -1.2454 | Function Loss:  -1.82\n",
      "Total loss:  -0.6816 | PDE Loss:  -1.2452 | Function Loss:  -1.8202\n",
      "Total loss:  -0.6817 | PDE Loss:  -1.2449 | Function Loss:  -1.8204\n",
      "Total loss:  -0.6818 | PDE Loss:  -1.2447 | Function Loss:  -1.8206\n",
      "Total loss:  -0.6818 | PDE Loss:  -1.2446 | Function Loss:  -1.8207\n",
      "Total loss:  -0.6818 | PDE Loss:  -1.2443 | Function Loss:  -1.8208\n",
      "Total loss:  -0.6819 | PDE Loss:  -1.2444 | Function Loss:  -1.8208\n",
      "Total loss:  -0.6819 | PDE Loss:  -1.2443 | Function Loss:  -1.8209\n",
      "Total loss:  -0.6819 | PDE Loss:  -1.2443 | Function Loss:  -1.8209\n",
      "Total loss:  -0.682 | PDE Loss:  -1.2442 | Function Loss:  -1.821\n",
      "Total loss:  -0.682 | PDE Loss:  -1.2443 | Function Loss:  -1.8211\n",
      "Total loss:  -0.6821 | PDE Loss:  -1.2441 | Function Loss:  -1.8212\n",
      "Total loss:  -0.6821 | PDE Loss:  -1.2442 | Function Loss:  -1.8212\n",
      "Total loss:  -0.6821 | PDE Loss:  -1.2438 | Function Loss:  -1.8214\n",
      "Total loss:  -0.6822 | PDE Loss:  -1.2439 | Function Loss:  -1.8214\n",
      "Total loss:  -0.6822 | PDE Loss:  -1.2441 | Function Loss:  -1.8214\n",
      "Total loss:  -0.6822 | PDE Loss:  -1.2443 | Function Loss:  -1.8213\n",
      "Total loss:  -0.6822 | PDE Loss:  -1.2445 | Function Loss:  -1.8213\n",
      "Total loss:  -0.6823 | PDE Loss:  -1.2447 | Function Loss:  -1.8213\n",
      "Total loss:  -0.6823 | PDE Loss:  -1.2445 | Function Loss:  -1.8214\n",
      "Total loss:  -0.6823 | PDE Loss:  -1.2447 | Function Loss:  -1.8213\n",
      "Total loss:  -0.6823 | PDE Loss:  -1.2448 | Function Loss:  -1.8213\n",
      "Total loss:  -0.6824 | PDE Loss:  -1.2449 | Function Loss:  -1.8213\n",
      "Total loss:  -0.6824 | PDE Loss:  -1.2449 | Function Loss:  -1.8213\n",
      "Total loss:  -0.6824 | PDE Loss:  -1.2448 | Function Loss:  -1.8214\n",
      "Total loss:  -0.6824 | PDE Loss:  -1.2449 | Function Loss:  -1.8214\n",
      "Total loss:  -0.6824 | PDE Loss:  -1.2447 | Function Loss:  -1.8215\n",
      "Total loss:  -0.6825 | PDE Loss:  -1.2446 | Function Loss:  -1.8216\n",
      "Total loss:  -0.6825 | PDE Loss:  -1.2445 | Function Loss:  -1.8216\n",
      "Total loss:  -0.6825 | PDE Loss:  -1.2445 | Function Loss:  -1.8217\n",
      "Total loss:  -0.6825 | PDE Loss:  -1.2444 | Function Loss:  -1.8217\n",
      "Total loss:  -0.6825 | PDE Loss:  -1.2445 | Function Loss:  -1.8217\n",
      "Total loss:  -0.6826 | PDE Loss:  -1.2446 | Function Loss:  -1.8217\n",
      "Total loss:  -0.6826 | PDE Loss:  -1.2448 | Function Loss:  -1.8217\n",
      "Total loss:  -0.6826 | PDE Loss:  -1.2453 | Function Loss:  -1.8215\n",
      "Total loss:  -0.6827 | PDE Loss:  -1.2455 | Function Loss:  -1.8215\n",
      "Total loss:  -0.6827 | PDE Loss:  -1.2457 | Function Loss:  -1.8215\n",
      "Total loss:  -0.6828 | PDE Loss:  -1.246 | Function Loss:  -1.8214\n",
      "Total loss:  -0.6828 | PDE Loss:  -1.2463 | Function Loss:  -1.8214\n",
      "Total loss:  -0.6829 | PDE Loss:  -1.2466 | Function Loss:  -1.8214\n",
      "Total loss:  -0.6829 | PDE Loss:  -1.2468 | Function Loss:  -1.8214\n",
      "Total loss:  -0.683 | PDE Loss:  -1.2469 | Function Loss:  -1.8214\n",
      "Total loss:  -0.6831 | PDE Loss:  -1.2469 | Function Loss:  -1.8215\n",
      "Total loss:  -0.6831 | PDE Loss:  -1.2468 | Function Loss:  -1.8216\n",
      "Total loss:  -0.6832 | PDE Loss:  -1.2467 | Function Loss:  -1.8217\n",
      "Total loss:  -0.6832 | PDE Loss:  -1.2465 | Function Loss:  -1.8219\n",
      "Total loss:  -0.6832 | PDE Loss:  -1.2463 | Function Loss:  -1.822\n",
      "Total loss:  -0.6832 | PDE Loss:  -1.2461 | Function Loss:  -1.8221\n",
      "Total loss:  -0.6832 | PDE Loss:  -1.246 | Function Loss:  -1.8221\n",
      "Total loss:  -0.6833 | PDE Loss:  -1.2459 | Function Loss:  -1.8222\n",
      "Total loss:  -0.6833 | PDE Loss:  -1.2458 | Function Loss:  -1.8222\n",
      "Total loss:  -0.6833 | PDE Loss:  -1.2457 | Function Loss:  -1.8223\n",
      "Total loss:  -0.6833 | PDE Loss:  -1.2463 | Function Loss:  -1.822\n",
      "Total loss:  -0.6833 | PDE Loss:  -1.246 | Function Loss:  -1.8223\n",
      "Total loss:  -0.6834 | PDE Loss:  -1.2457 | Function Loss:  -1.8224\n",
      "Total loss:  -0.6834 | PDE Loss:  -1.2459 | Function Loss:  -1.8224\n",
      "Total loss:  -0.6835 | PDE Loss:  -1.2462 | Function Loss:  -1.8224\n",
      "Total loss:  -0.6835 | PDE Loss:  -1.2464 | Function Loss:  -1.8223\n",
      "Total loss:  -0.6835 | PDE Loss:  -1.2467 | Function Loss:  -1.8222\n",
      "Total loss:  -0.6835 | PDE Loss:  -1.2469 | Function Loss:  -1.8222\n",
      "Total loss:  -0.6836 | PDE Loss:  -1.2471 | Function Loss:  -1.8222\n",
      "Total loss:  -0.6836 | PDE Loss:  -1.2473 | Function Loss:  -1.8221\n",
      "Total loss:  -0.6837 | PDE Loss:  -1.2476 | Function Loss:  -1.8221\n",
      "Total loss:  -0.6837 | PDE Loss:  -1.2477 | Function Loss:  -1.8221\n",
      "Total loss:  -0.6838 | PDE Loss:  -1.2478 | Function Loss:  -1.8222\n",
      "Total loss:  -0.6838 | PDE Loss:  -1.2478 | Function Loss:  -1.8222\n",
      "Total loss:  -0.6839 | PDE Loss:  -1.2477 | Function Loss:  -1.8223\n",
      "Total loss:  -0.6839 | PDE Loss:  -1.2478 | Function Loss:  -1.8223\n",
      "Total loss:  -0.6839 | PDE Loss:  -1.2478 | Function Loss:  -1.8224\n",
      "Total loss:  -0.6839 | PDE Loss:  -1.248 | Function Loss:  -1.8223\n",
      "Total loss:  -0.684 | PDE Loss:  -1.2481 | Function Loss:  -1.8223\n",
      "Total loss:  -0.684 | PDE Loss:  -1.2483 | Function Loss:  -1.8223\n",
      "Total loss:  -0.6841 | PDE Loss:  -1.2487 | Function Loss:  -1.8222\n",
      "Total loss:  -0.6841 | PDE Loss:  -1.249 | Function Loss:  -1.8222\n",
      "Total loss:  -0.6841 | PDE Loss:  -1.2494 | Function Loss:  -1.8221\n",
      "Total loss:  -0.6842 | PDE Loss:  -1.2497 | Function Loss:  -1.822\n",
      "Total loss:  -0.6842 | PDE Loss:  -1.2503 | Function Loss:  -1.8218\n",
      "Total loss:  -0.6843 | PDE Loss:  -1.2504 | Function Loss:  -1.8219\n",
      "Total loss:  -0.6843 | PDE Loss:  -1.2504 | Function Loss:  -1.8219\n",
      "Total loss:  -0.6843 | PDE Loss:  -1.2503 | Function Loss:  -1.822\n",
      "Total loss:  -0.6844 | PDE Loss:  -1.2503 | Function Loss:  -1.822\n",
      "Total loss:  -0.6844 | PDE Loss:  -1.2501 | Function Loss:  -1.8221\n",
      "Total loss:  -0.6844 | PDE Loss:  -1.25 | Function Loss:  -1.8222\n",
      "Total loss:  -0.6844 | PDE Loss:  -1.2495 | Function Loss:  -1.8224\n",
      "Total loss:  -0.6844 | PDE Loss:  -1.2498 | Function Loss:  -1.8223\n",
      "Total loss:  -0.6844 | PDE Loss:  -1.2498 | Function Loss:  -1.8223\n",
      "Total loss:  -0.6844 | PDE Loss:  -1.2497 | Function Loss:  -1.8224\n",
      "Total loss:  -0.6845 | PDE Loss:  -1.2497 | Function Loss:  -1.8224\n",
      "Total loss:  -0.6845 | PDE Loss:  -1.2496 | Function Loss:  -1.8225\n",
      "Total loss:  -0.6845 | PDE Loss:  -1.2495 | Function Loss:  -1.8225\n",
      "Total loss:  -0.6845 | PDE Loss:  -1.2495 | Function Loss:  -1.8226\n",
      "Total loss:  -0.6846 | PDE Loss:  -1.2494 | Function Loss:  -1.8227\n",
      "Total loss:  -0.6846 | PDE Loss:  -1.2495 | Function Loss:  -1.8227\n",
      "Total loss:  -0.6846 | PDE Loss:  -1.2488 | Function Loss:  -1.823\n",
      "Total loss:  -0.6847 | PDE Loss:  -1.249 | Function Loss:  -1.8229\n",
      "Total loss:  -0.6847 | PDE Loss:  -1.2493 | Function Loss:  -1.8229\n",
      "Total loss:  -0.6847 | PDE Loss:  -1.2495 | Function Loss:  -1.8229\n",
      "Total loss:  -0.6848 | PDE Loss:  -1.2498 | Function Loss:  -1.8228\n",
      "Total loss:  -0.6848 | PDE Loss:  -1.2499 | Function Loss:  -1.8228\n",
      "Total loss:  -0.6849 | PDE Loss:  -1.2501 | Function Loss:  -1.8228\n",
      "Total loss:  -0.6849 | PDE Loss:  -1.2503 | Function Loss:  -1.8228\n",
      "Total loss:  -0.685 | PDE Loss:  -1.2506 | Function Loss:  -1.8228\n",
      "Total loss:  -0.685 | PDE Loss:  -1.2507 | Function Loss:  -1.8228\n",
      "Total loss:  -0.685 | PDE Loss:  -1.2508 | Function Loss:  -1.8228\n",
      "Total loss:  -0.6851 | PDE Loss:  -1.2509 | Function Loss:  -1.8228\n",
      "Total loss:  -0.6851 | PDE Loss:  -1.251 | Function Loss:  -1.8228\n",
      "Total loss:  -0.6851 | PDE Loss:  -1.2511 | Function Loss:  -1.8228\n",
      "Total loss:  -0.6852 | PDE Loss:  -1.2512 | Function Loss:  -1.8228\n",
      "Total loss:  -0.6852 | PDE Loss:  -1.2514 | Function Loss:  -1.8228\n",
      "Total loss:  -0.6853 | PDE Loss:  -1.2516 | Function Loss:  -1.8228\n",
      "Total loss:  -0.6853 | PDE Loss:  -1.2518 | Function Loss:  -1.8228\n",
      "Total loss:  -0.6853 | PDE Loss:  -1.2518 | Function Loss:  -1.8228\n",
      "Total loss:  -0.6854 | PDE Loss:  -1.2519 | Function Loss:  -1.8228\n",
      "Total loss:  -0.6854 | PDE Loss:  -1.252 | Function Loss:  -1.8228\n",
      "Total loss:  -0.6854 | PDE Loss:  -1.252 | Function Loss:  -1.8229\n",
      "Total loss:  -0.6855 | PDE Loss:  -1.252 | Function Loss:  -1.8229\n",
      "Total loss:  -0.6855 | PDE Loss:  -1.252 | Function Loss:  -1.8229\n",
      "Total loss:  -0.6855 | PDE Loss:  -1.252 | Function Loss:  -1.823\n",
      "Total loss:  -0.6855 | PDE Loss:  -1.2521 | Function Loss:  -1.823\n",
      "Total loss:  -0.6856 | PDE Loss:  -1.2521 | Function Loss:  -1.823\n",
      "Total loss:  -0.6856 | PDE Loss:  -1.2526 | Function Loss:  -1.8228\n",
      "Total loss:  -0.6856 | PDE Loss:  -1.2526 | Function Loss:  -1.8229\n",
      "Total loss:  -0.6856 | PDE Loss:  -1.2525 | Function Loss:  -1.823\n",
      "Total loss:  -0.6857 | PDE Loss:  -1.2525 | Function Loss:  -1.823\n",
      "Total loss:  -0.6857 | PDE Loss:  -1.2525 | Function Loss:  -1.8231\n",
      "Total loss:  -0.6857 | PDE Loss:  -1.2525 | Function Loss:  -1.8231\n",
      "Total loss:  -0.6857 | PDE Loss:  -1.2525 | Function Loss:  -1.8231\n",
      "Total loss:  -0.6858 | PDE Loss:  -1.2525 | Function Loss:  -1.8231\n",
      "Total loss:  -0.6858 | PDE Loss:  -1.2526 | Function Loss:  -1.8232\n",
      "Total loss:  -0.6859 | PDE Loss:  -1.2527 | Function Loss:  -1.8232\n",
      "Total loss:  -0.6859 | PDE Loss:  -1.2528 | Function Loss:  -1.8232\n",
      "Total loss:  -0.686 | PDE Loss:  -1.2529 | Function Loss:  -1.8233\n",
      "Total loss:  -0.686 | PDE Loss:  -1.2529 | Function Loss:  -1.8234\n",
      "Total loss:  -0.6861 | PDE Loss:  -1.253 | Function Loss:  -1.8234\n",
      "Total loss:  -0.6861 | PDE Loss:  -1.2531 | Function Loss:  -1.8234\n",
      "Total loss:  -0.6862 | PDE Loss:  -1.2532 | Function Loss:  -1.8234\n",
      "Total loss:  -0.6861 | PDE Loss:  -1.2538 | Function Loss:  -1.8231\n",
      "Total loss:  -0.6862 | PDE Loss:  -1.2535 | Function Loss:  -1.8234\n",
      "Total loss:  -0.6862 | PDE Loss:  -1.2536 | Function Loss:  -1.8234\n",
      "Total loss:  -0.6863 | PDE Loss:  -1.2537 | Function Loss:  -1.8234\n",
      "Total loss:  -0.6863 | PDE Loss:  -1.2538 | Function Loss:  -1.8234\n",
      "Total loss:  -0.6864 | PDE Loss:  -1.254 | Function Loss:  -1.8234\n",
      "Total loss:  -0.6864 | PDE Loss:  -1.2541 | Function Loss:  -1.8235\n",
      "Total loss:  -0.6865 | PDE Loss:  -1.2542 | Function Loss:  -1.8235\n",
      "Total loss:  -0.6865 | PDE Loss:  -1.2542 | Function Loss:  -1.8236\n",
      "Total loss:  -0.6866 | PDE Loss:  -1.2541 | Function Loss:  -1.8237\n",
      "Total loss:  -0.6866 | PDE Loss:  -1.2539 | Function Loss:  -1.8238\n",
      "Total loss:  -0.6867 | PDE Loss:  -1.2537 | Function Loss:  -1.824\n",
      "Total loss:  -0.6867 | PDE Loss:  -1.253 | Function Loss:  -1.8243\n",
      "Total loss:  -0.6868 | PDE Loss:  -1.2529 | Function Loss:  -1.8244\n",
      "Total loss:  -0.6868 | PDE Loss:  -1.2528 | Function Loss:  -1.8245\n",
      "Total loss:  -0.6869 | PDE Loss:  -1.2527 | Function Loss:  -1.8246\n",
      "Total loss:  -0.6869 | PDE Loss:  -1.2526 | Function Loss:  -1.8246\n",
      "Total loss:  -0.6869 | PDE Loss:  -1.2525 | Function Loss:  -1.8247\n",
      "Total loss:  -0.687 | PDE Loss:  -1.2526 | Function Loss:  -1.8248\n",
      "Total loss:  -0.6868 | PDE Loss:  -1.2494 | Function Loss:  -1.8258\n",
      "Total loss:  -0.687 | PDE Loss:  -1.2518 | Function Loss:  -1.8251\n",
      "Total loss:  -0.6871 | PDE Loss:  -1.2519 | Function Loss:  -1.8251\n",
      "Total loss:  -0.6871 | PDE Loss:  -1.2521 | Function Loss:  -1.8251\n",
      "Total loss:  -0.6872 | PDE Loss:  -1.2523 | Function Loss:  -1.8251\n",
      "Total loss:  -0.6872 | PDE Loss:  -1.2524 | Function Loss:  -1.8251\n",
      "Total loss:  -0.6872 | PDE Loss:  -1.2526 | Function Loss:  -1.8251\n",
      "Total loss:  -0.6873 | PDE Loss:  -1.2529 | Function Loss:  -1.8251\n",
      "Total loss:  -0.6874 | PDE Loss:  -1.253 | Function Loss:  -1.8251\n",
      "Total loss:  -0.6874 | PDE Loss:  -1.253 | Function Loss:  -1.8252\n",
      "Total loss:  -0.6875 | PDE Loss:  -1.253 | Function Loss:  -1.8253\n",
      "Total loss:  -0.6875 | PDE Loss:  -1.2529 | Function Loss:  -1.8254\n",
      "Total loss:  -0.6876 | PDE Loss:  -1.2528 | Function Loss:  -1.8255\n",
      "Total loss:  -0.6876 | PDE Loss:  -1.2527 | Function Loss:  -1.8256\n",
      "Total loss:  -0.6877 | PDE Loss:  -1.2527 | Function Loss:  -1.8257\n",
      "Total loss:  -0.6877 | PDE Loss:  -1.2526 | Function Loss:  -1.8257\n",
      "Total loss:  -0.6877 | PDE Loss:  -1.2526 | Function Loss:  -1.8258\n",
      "Total loss:  -0.6877 | PDE Loss:  -1.2526 | Function Loss:  -1.8258\n",
      "Total loss:  -0.6878 | PDE Loss:  -1.2526 | Function Loss:  -1.8259\n",
      "Total loss:  -0.6878 | PDE Loss:  -1.2526 | Function Loss:  -1.8259\n",
      "Total loss:  -0.6878 | PDE Loss:  -1.2526 | Function Loss:  -1.8259\n",
      "Total loss:  -0.6879 | PDE Loss:  -1.2527 | Function Loss:  -1.8259\n",
      "Total loss:  -0.6879 | PDE Loss:  -1.2528 | Function Loss:  -1.826\n",
      "Total loss:  -0.6879 | PDE Loss:  -1.2527 | Function Loss:  -1.826\n",
      "Total loss:  -0.6879 | PDE Loss:  -1.2529 | Function Loss:  -1.826\n",
      "Total loss:  -0.688 | PDE Loss:  -1.253 | Function Loss:  -1.826\n",
      "Total loss:  -0.688 | PDE Loss:  -1.2531 | Function Loss:  -1.826\n",
      "Total loss:  -0.6881 | PDE Loss:  -1.2535 | Function Loss:  -1.8259\n",
      "Total loss:  -0.6881 | PDE Loss:  -1.2535 | Function Loss:  -1.826\n",
      "Total loss:  -0.6881 | PDE Loss:  -1.2535 | Function Loss:  -1.826\n",
      "Total loss:  -0.6882 | PDE Loss:  -1.2537 | Function Loss:  -1.826\n",
      "Total loss:  -0.6882 | PDE Loss:  -1.254 | Function Loss:  -1.826\n",
      "Total loss:  -0.6883 | PDE Loss:  -1.2542 | Function Loss:  -1.826\n",
      "Total loss:  -0.6883 | PDE Loss:  -1.2544 | Function Loss:  -1.8259\n",
      "Total loss:  -0.6884 | PDE Loss:  -1.2546 | Function Loss:  -1.8259\n",
      "Total loss:  -0.6884 | PDE Loss:  -1.2548 | Function Loss:  -1.8259\n",
      "Total loss:  -0.6884 | PDE Loss:  -1.2556 | Function Loss:  -1.8257\n",
      "Total loss:  -0.6885 | PDE Loss:  -1.2557 | Function Loss:  -1.8257\n",
      "Total loss:  -0.6886 | PDE Loss:  -1.2558 | Function Loss:  -1.8257\n",
      "Total loss:  -0.6886 | PDE Loss:  -1.2559 | Function Loss:  -1.8258\n",
      "Total loss:  -0.6886 | PDE Loss:  -1.256 | Function Loss:  -1.8258\n",
      "Total loss:  -0.6887 | PDE Loss:  -1.2561 | Function Loss:  -1.8258\n",
      "Total loss:  -0.6887 | PDE Loss:  -1.2562 | Function Loss:  -1.8258\n",
      "Total loss:  -0.6887 | PDE Loss:  -1.2563 | Function Loss:  -1.8258\n",
      "Total loss:  -0.6887 | PDE Loss:  -1.2563 | Function Loss:  -1.8257\n",
      "Total loss:  -0.6888 | PDE Loss:  -1.2563 | Function Loss:  -1.8258\n",
      "Total loss:  -0.6888 | PDE Loss:  -1.2563 | Function Loss:  -1.8259\n",
      "Total loss:  -0.6888 | PDE Loss:  -1.2563 | Function Loss:  -1.8259\n",
      "Total loss:  -0.6888 | PDE Loss:  -1.2562 | Function Loss:  -1.826\n",
      "Total loss:  -0.6889 | PDE Loss:  -1.2561 | Function Loss:  -1.8261\n",
      "Total loss:  -0.6889 | PDE Loss:  -1.2561 | Function Loss:  -1.8261\n",
      "Total loss:  -0.6889 | PDE Loss:  -1.256 | Function Loss:  -1.8262\n",
      "Total loss:  -0.6889 | PDE Loss:  -1.256 | Function Loss:  -1.8262\n",
      "Total loss:  -0.689 | PDE Loss:  -1.256 | Function Loss:  -1.8262\n",
      "Total loss:  -0.689 | PDE Loss:  -1.2558 | Function Loss:  -1.8263\n",
      "Total loss:  -0.689 | PDE Loss:  -1.2559 | Function Loss:  -1.8264\n",
      "Total loss:  -0.689 | PDE Loss:  -1.2559 | Function Loss:  -1.8264\n",
      "Total loss:  -0.6891 | PDE Loss:  -1.256 | Function Loss:  -1.8264\n",
      "Total loss:  -0.6891 | PDE Loss:  -1.2561 | Function Loss:  -1.8264\n",
      "Total loss:  -0.6892 | PDE Loss:  -1.2563 | Function Loss:  -1.8264\n",
      "Total loss:  -0.6892 | PDE Loss:  -1.2565 | Function Loss:  -1.8264\n",
      "Total loss:  -0.6892 | PDE Loss:  -1.2567 | Function Loss:  -1.8263\n",
      "Total loss:  -0.6893 | PDE Loss:  -1.2569 | Function Loss:  -1.8264\n",
      "Total loss:  -0.6893 | PDE Loss:  -1.2575 | Function Loss:  -1.8262\n",
      "Total loss:  -0.6894 | PDE Loss:  -1.2574 | Function Loss:  -1.8263\n",
      "Total loss:  -0.6894 | PDE Loss:  -1.2573 | Function Loss:  -1.8264\n",
      "Total loss:  -0.6895 | PDE Loss:  -1.2572 | Function Loss:  -1.8265\n",
      "Total loss:  -0.6895 | PDE Loss:  -1.2571 | Function Loss:  -1.8265\n",
      "Total loss:  -0.6895 | PDE Loss:  -1.2571 | Function Loss:  -1.8266\n",
      "Total loss:  -0.6895 | PDE Loss:  -1.2571 | Function Loss:  -1.8266\n",
      "Total loss:  -0.6896 | PDE Loss:  -1.257 | Function Loss:  -1.8267\n",
      "Total loss:  -0.6896 | PDE Loss:  -1.2571 | Function Loss:  -1.8267\n",
      "Total loss:  -0.6896 | PDE Loss:  -1.2573 | Function Loss:  -1.8267\n",
      "Total loss:  -0.6896 | PDE Loss:  -1.2575 | Function Loss:  -1.8266\n",
      "Total loss:  -0.6897 | PDE Loss:  -1.2578 | Function Loss:  -1.8265\n",
      "Total loss:  -0.6897 | PDE Loss:  -1.2581 | Function Loss:  -1.8265\n",
      "Total loss:  -0.6898 | PDE Loss:  -1.2586 | Function Loss:  -1.8264\n",
      "Total loss:  -0.6898 | PDE Loss:  -1.2587 | Function Loss:  -1.8264\n",
      "Total loss:  -0.6899 | PDE Loss:  -1.2593 | Function Loss:  -1.8263\n",
      "Total loss:  -0.69 | PDE Loss:  -1.2598 | Function Loss:  -1.8262\n",
      "Total loss:  -0.69 | PDE Loss:  -1.2602 | Function Loss:  -1.8261\n",
      "Total loss:  -0.6901 | PDE Loss:  -1.2605 | Function Loss:  -1.8261\n",
      "Total loss:  -0.6901 | PDE Loss:  -1.2607 | Function Loss:  -1.8261\n",
      "Total loss:  -0.6901 | PDE Loss:  -1.2609 | Function Loss:  -1.8261\n",
      "Total loss:  -0.6901 | PDE Loss:  -1.2601 | Function Loss:  -1.8263\n",
      "Total loss:  -0.6902 | PDE Loss:  -1.2607 | Function Loss:  -1.8261\n",
      "Total loss:  -0.6902 | PDE Loss:  -1.2609 | Function Loss:  -1.8261\n",
      "Total loss:  -0.6902 | PDE Loss:  -1.261 | Function Loss:  -1.8261\n",
      "Total loss:  -0.6903 | PDE Loss:  -1.2613 | Function Loss:  -1.8261\n",
      "Total loss:  -0.6902 | PDE Loss:  -1.2614 | Function Loss:  -1.826\n",
      "Total loss:  -0.6903 | PDE Loss:  -1.2614 | Function Loss:  -1.826\n",
      "Total loss:  -0.6903 | PDE Loss:  -1.2617 | Function Loss:  -1.826\n",
      "Total loss:  -0.6904 | PDE Loss:  -1.2619 | Function Loss:  -1.826\n",
      "Total loss:  -0.6905 | PDE Loss:  -1.2622 | Function Loss:  -1.826\n",
      "Total loss:  -0.6905 | PDE Loss:  -1.2624 | Function Loss:  -1.826\n",
      "Total loss:  -0.6906 | PDE Loss:  -1.2626 | Function Loss:  -1.826\n",
      "Total loss:  -0.6906 | PDE Loss:  -1.2626 | Function Loss:  -1.8261\n",
      "Total loss:  -0.6907 | PDE Loss:  -1.2629 | Function Loss:  -1.826\n",
      "Total loss:  -0.6907 | PDE Loss:  -1.263 | Function Loss:  -1.8261\n",
      "Total loss:  -0.6908 | PDE Loss:  -1.2631 | Function Loss:  -1.8261\n",
      "Total loss:  -0.6908 | PDE Loss:  -1.2633 | Function Loss:  -1.8261\n",
      "Total loss:  -0.6909 | PDE Loss:  -1.2636 | Function Loss:  -1.826\n",
      "Total loss:  -0.6909 | PDE Loss:  -1.2638 | Function Loss:  -1.826\n",
      "Total loss:  -0.691 | PDE Loss:  -1.2643 | Function Loss:  -1.8259\n",
      "Total loss:  -0.691 | PDE Loss:  -1.2644 | Function Loss:  -1.8259\n",
      "Total loss:  -0.691 | PDE Loss:  -1.2646 | Function Loss:  -1.8259\n",
      "Total loss:  -0.691 | PDE Loss:  -1.2647 | Function Loss:  -1.8259\n",
      "Total loss:  -0.6911 | PDE Loss:  -1.2648 | Function Loss:  -1.8258\n",
      "Total loss:  -0.6911 | PDE Loss:  -1.2649 | Function Loss:  -1.8259\n",
      "Total loss:  -0.6911 | PDE Loss:  -1.2649 | Function Loss:  -1.8259\n",
      "Total loss:  -0.6911 | PDE Loss:  -1.2649 | Function Loss:  -1.8259\n",
      "Total loss:  -0.6912 | PDE Loss:  -1.2647 | Function Loss:  -1.8261\n",
      "Total loss:  -0.6912 | PDE Loss:  -1.2647 | Function Loss:  -1.8261\n",
      "Total loss:  -0.6912 | PDE Loss:  -1.2645 | Function Loss:  -1.8262\n",
      "Total loss:  -0.6913 | PDE Loss:  -1.2642 | Function Loss:  -1.8264\n",
      "Total loss:  -0.6913 | PDE Loss:  -1.2641 | Function Loss:  -1.8265\n",
      "Total loss:  -0.6913 | PDE Loss:  -1.2639 | Function Loss:  -1.8266\n",
      "Total loss:  -0.6914 | PDE Loss:  -1.2638 | Function Loss:  -1.8267\n",
      "Total loss:  -0.6914 | PDE Loss:  -1.2638 | Function Loss:  -1.8267\n",
      "Total loss:  -0.6913 | PDE Loss:  -1.2627 | Function Loss:  -1.827\n",
      "Total loss:  -0.6914 | PDE Loss:  -1.2636 | Function Loss:  -1.8268\n",
      "Total loss:  -0.6914 | PDE Loss:  -1.2636 | Function Loss:  -1.8268\n",
      "Total loss:  -0.6915 | PDE Loss:  -1.2637 | Function Loss:  -1.8268\n",
      "Total loss:  -0.6915 | PDE Loss:  -1.2638 | Function Loss:  -1.8268\n",
      "Total loss:  -0.6915 | PDE Loss:  -1.2639 | Function Loss:  -1.8268\n",
      "Total loss:  -0.6915 | PDE Loss:  -1.264 | Function Loss:  -1.8268\n",
      "Total loss:  -0.6915 | PDE Loss:  -1.2641 | Function Loss:  -1.8268\n",
      "Total loss:  -0.6915 | PDE Loss:  -1.2641 | Function Loss:  -1.8268\n",
      "Total loss:  -0.6916 | PDE Loss:  -1.2643 | Function Loss:  -1.8267\n",
      "Total loss:  -0.6916 | PDE Loss:  -1.2643 | Function Loss:  -1.8268\n",
      "Total loss:  -0.6916 | PDE Loss:  -1.2643 | Function Loss:  -1.8268\n",
      "Total loss:  -0.6916 | PDE Loss:  -1.2643 | Function Loss:  -1.8268\n",
      "Total loss:  -0.6917 | PDE Loss:  -1.2644 | Function Loss:  -1.8268\n",
      "Total loss:  -0.6917 | PDE Loss:  -1.2645 | Function Loss:  -1.8268\n",
      "Total loss:  -0.6917 | PDE Loss:  -1.2647 | Function Loss:  -1.8268\n",
      "Total loss:  -0.6917 | PDE Loss:  -1.2649 | Function Loss:  -1.8267\n",
      "Total loss:  -0.6918 | PDE Loss:  -1.2653 | Function Loss:  -1.8266\n",
      "Total loss:  -0.6918 | PDE Loss:  -1.2659 | Function Loss:  -1.8265\n",
      "Total loss:  -0.6919 | PDE Loss:  -1.2665 | Function Loss:  -1.8264\n",
      "Total loss:  -0.6919 | PDE Loss:  -1.2669 | Function Loss:  -1.8263\n",
      "Total loss:  -0.692 | PDE Loss:  -1.2673 | Function Loss:  -1.8262\n",
      "Total loss:  -0.692 | PDE Loss:  -1.2676 | Function Loss:  -1.8262\n",
      "Total loss:  -0.6921 | PDE Loss:  -1.2679 | Function Loss:  -1.8261\n",
      "Total loss:  -0.6921 | PDE Loss:  -1.268 | Function Loss:  -1.8261\n",
      "Total loss:  -0.6921 | PDE Loss:  -1.2681 | Function Loss:  -1.8261\n",
      "Total loss:  -0.6922 | PDE Loss:  -1.2682 | Function Loss:  -1.8262\n",
      "Total loss:  -0.6922 | PDE Loss:  -1.2683 | Function Loss:  -1.8262\n",
      "Total loss:  -0.6923 | PDE Loss:  -1.2685 | Function Loss:  -1.8262\n",
      "Total loss:  -0.6923 | PDE Loss:  -1.2688 | Function Loss:  -1.8262\n",
      "Total loss:  -0.6924 | PDE Loss:  -1.2691 | Function Loss:  -1.8261\n",
      "Total loss:  -0.6924 | PDE Loss:  -1.2695 | Function Loss:  -1.826\n",
      "Total loss:  -0.6925 | PDE Loss:  -1.27 | Function Loss:  -1.8259\n",
      "Total loss:  -0.6923 | PDE Loss:  -1.271 | Function Loss:  -1.8253\n",
      "Total loss:  -0.6925 | PDE Loss:  -1.2704 | Function Loss:  -1.8258\n",
      "Total loss:  -0.6925 | PDE Loss:  -1.2709 | Function Loss:  -1.8257\n",
      "Total loss:  -0.6926 | PDE Loss:  -1.2713 | Function Loss:  -1.8256\n",
      "Total loss:  -0.6926 | PDE Loss:  -1.2716 | Function Loss:  -1.8255\n",
      "Total loss:  -0.6926 | PDE Loss:  -1.2718 | Function Loss:  -1.8255\n",
      "Total loss:  -0.6926 | PDE Loss:  -1.272 | Function Loss:  -1.8254\n",
      "Total loss:  -0.6927 | PDE Loss:  -1.272 | Function Loss:  -1.8254\n",
      "Total loss:  -0.6927 | PDE Loss:  -1.2721 | Function Loss:  -1.8255\n",
      "Total loss:  -0.6927 | PDE Loss:  -1.272 | Function Loss:  -1.8255\n",
      "Total loss:  -0.6928 | PDE Loss:  -1.2722 | Function Loss:  -1.8255\n",
      "Total loss:  -0.6928 | PDE Loss:  -1.272 | Function Loss:  -1.8257\n",
      "Total loss:  -0.6929 | PDE Loss:  -1.2718 | Function Loss:  -1.8258\n",
      "Total loss:  -0.6929 | PDE Loss:  -1.2716 | Function Loss:  -1.8259\n",
      "Total loss:  -0.693 | PDE Loss:  -1.2715 | Function Loss:  -1.8261\n",
      "Total loss:  -0.693 | PDE Loss:  -1.2714 | Function Loss:  -1.8261\n",
      "Total loss:  -0.693 | PDE Loss:  -1.2714 | Function Loss:  -1.8262\n",
      "Total loss:  -0.6931 | PDE Loss:  -1.2714 | Function Loss:  -1.8262\n",
      "Total loss:  -0.6927 | PDE Loss:  -1.2691 | Function Loss:  -1.8265\n",
      "Total loss:  -0.6931 | PDE Loss:  -1.2712 | Function Loss:  -1.8263\n",
      "Total loss:  -0.6931 | PDE Loss:  -1.2714 | Function Loss:  -1.8263\n",
      "Total loss:  -0.6932 | PDE Loss:  -1.2715 | Function Loss:  -1.8263\n",
      "Total loss:  -0.6932 | PDE Loss:  -1.2718 | Function Loss:  -1.8263\n",
      "Total loss:  -0.6933 | PDE Loss:  -1.2718 | Function Loss:  -1.8264\n",
      "Total loss:  -0.6933 | PDE Loss:  -1.2718 | Function Loss:  -1.8264\n",
      "Total loss:  -0.6934 | PDE Loss:  -1.2718 | Function Loss:  -1.8265\n",
      "Total loss:  -0.6934 | PDE Loss:  -1.2718 | Function Loss:  -1.8265\n",
      "Total loss:  -0.6934 | PDE Loss:  -1.2717 | Function Loss:  -1.8266\n",
      "Total loss:  -0.6935 | PDE Loss:  -1.2717 | Function Loss:  -1.8267\n",
      "Total loss:  -0.6936 | PDE Loss:  -1.2716 | Function Loss:  -1.8268\n",
      "Total loss:  -0.6936 | PDE Loss:  -1.2713 | Function Loss:  -1.827\n",
      "Total loss:  -0.6937 | PDE Loss:  -1.2712 | Function Loss:  -1.8271\n",
      "Total loss:  -0.6937 | PDE Loss:  -1.2711 | Function Loss:  -1.8272\n",
      "Total loss:  -0.6938 | PDE Loss:  -1.2712 | Function Loss:  -1.8273\n",
      "Total loss:  -0.6938 | PDE Loss:  -1.2709 | Function Loss:  -1.8274\n",
      "Total loss:  -0.6939 | PDE Loss:  -1.2711 | Function Loss:  -1.8274\n",
      "Total loss:  -0.6939 | PDE Loss:  -1.2714 | Function Loss:  -1.8273\n",
      "Total loss:  -0.6939 | PDE Loss:  -1.2717 | Function Loss:  -1.8273\n",
      "Total loss:  -0.694 | PDE Loss:  -1.2719 | Function Loss:  -1.8272\n",
      "Total loss:  -0.694 | PDE Loss:  -1.272 | Function Loss:  -1.8272\n",
      "Total loss:  -0.694 | PDE Loss:  -1.2721 | Function Loss:  -1.8273\n",
      "Total loss:  -0.6941 | PDE Loss:  -1.2722 | Function Loss:  -1.8273\n",
      "Total loss:  -0.6941 | PDE Loss:  -1.2715 | Function Loss:  -1.8275\n",
      "Total loss:  -0.6941 | PDE Loss:  -1.2717 | Function Loss:  -1.8275\n",
      "Total loss:  -0.6941 | PDE Loss:  -1.2718 | Function Loss:  -1.8275\n",
      "Total loss:  -0.6942 | PDE Loss:  -1.2719 | Function Loss:  -1.8275\n",
      "Total loss:  -0.6942 | PDE Loss:  -1.272 | Function Loss:  -1.8275\n",
      "Total loss:  -0.6942 | PDE Loss:  -1.2722 | Function Loss:  -1.8274\n",
      "Total loss:  -0.6942 | PDE Loss:  -1.2725 | Function Loss:  -1.8274\n",
      "Total loss:  -0.6943 | PDE Loss:  -1.2727 | Function Loss:  -1.8274\n",
      "Total loss:  -0.6944 | PDE Loss:  -1.2733 | Function Loss:  -1.8273\n",
      "Total loss:  -0.6944 | PDE Loss:  -1.2739 | Function Loss:  -1.8272\n",
      "Total loss:  -0.6943 | PDE Loss:  -1.275 | Function Loss:  -1.8266\n",
      "Total loss:  -0.6944 | PDE Loss:  -1.2743 | Function Loss:  -1.827\n",
      "Total loss:  -0.6945 | PDE Loss:  -1.2745 | Function Loss:  -1.827\n",
      "Total loss:  -0.6945 | PDE Loss:  -1.2748 | Function Loss:  -1.8269\n",
      "Total loss:  -0.6945 | PDE Loss:  -1.275 | Function Loss:  -1.8269\n",
      "Total loss:  -0.6945 | PDE Loss:  -1.2751 | Function Loss:  -1.8269\n",
      "Total loss:  -0.6946 | PDE Loss:  -1.2753 | Function Loss:  -1.8269\n",
      "Total loss:  -0.6946 | PDE Loss:  -1.2754 | Function Loss:  -1.8268\n",
      "Total loss:  -0.6946 | PDE Loss:  -1.2754 | Function Loss:  -1.8269\n",
      "Total loss:  -0.6947 | PDE Loss:  -1.2759 | Function Loss:  -1.8268\n",
      "Total loss:  -0.6947 | PDE Loss:  -1.2759 | Function Loss:  -1.8268\n",
      "Total loss:  -0.6948 | PDE Loss:  -1.2758 | Function Loss:  -1.8269\n",
      "Total loss:  -0.6948 | PDE Loss:  -1.2759 | Function Loss:  -1.827\n",
      "Total loss:  -0.6948 | PDE Loss:  -1.2759 | Function Loss:  -1.827\n",
      "Total loss:  -0.6949 | PDE Loss:  -1.276 | Function Loss:  -1.827\n",
      "Total loss:  -0.6949 | PDE Loss:  -1.2762 | Function Loss:  -1.827\n",
      "Total loss:  -0.695 | PDE Loss:  -1.2765 | Function Loss:  -1.827\n",
      "Total loss:  -0.695 | PDE Loss:  -1.2768 | Function Loss:  -1.827\n",
      "Total loss:  -0.6951 | PDE Loss:  -1.277 | Function Loss:  -1.827\n",
      "Total loss:  -0.6951 | PDE Loss:  -1.2771 | Function Loss:  -1.827\n",
      "Total loss:  -0.6952 | PDE Loss:  -1.2774 | Function Loss:  -1.8269\n",
      "Total loss:  -0.6952 | PDE Loss:  -1.2772 | Function Loss:  -1.827\n",
      "Total loss:  -0.6952 | PDE Loss:  -1.2774 | Function Loss:  -1.827\n",
      "Total loss:  -0.6953 | PDE Loss:  -1.2776 | Function Loss:  -1.827\n",
      "Total loss:  -0.6953 | PDE Loss:  -1.2778 | Function Loss:  -1.827\n",
      "Total loss:  -0.6953 | PDE Loss:  -1.278 | Function Loss:  -1.8269\n",
      "Total loss:  -0.6954 | PDE Loss:  -1.2781 | Function Loss:  -1.827\n",
      "Total loss:  -0.6954 | PDE Loss:  -1.2782 | Function Loss:  -1.827\n",
      "Total loss:  -0.6955 | PDE Loss:  -1.2783 | Function Loss:  -1.827\n",
      "Total loss:  -0.6955 | PDE Loss:  -1.2786 | Function Loss:  -1.827\n",
      "Total loss:  -0.6956 | PDE Loss:  -1.2788 | Function Loss:  -1.827\n",
      "Total loss:  -0.6956 | PDE Loss:  -1.2788 | Function Loss:  -1.827\n",
      "Total loss:  -0.6956 | PDE Loss:  -1.2789 | Function Loss:  -1.827\n",
      "Total loss:  -0.6957 | PDE Loss:  -1.279 | Function Loss:  -1.827\n",
      "Total loss:  -0.6957 | PDE Loss:  -1.2791 | Function Loss:  -1.8271\n",
      "Total loss:  -0.6958 | PDE Loss:  -1.2793 | Function Loss:  -1.8271\n",
      "Total loss:  -0.6958 | PDE Loss:  -1.2794 | Function Loss:  -1.8271\n",
      "Total loss:  -0.6959 | PDE Loss:  -1.2793 | Function Loss:  -1.8272\n",
      "Total loss:  -0.696 | PDE Loss:  -1.2796 | Function Loss:  -1.8272\n",
      "Total loss:  -0.696 | PDE Loss:  -1.2797 | Function Loss:  -1.8273\n",
      "Total loss:  -0.6961 | PDE Loss:  -1.2801 | Function Loss:  -1.8272\n",
      "Total loss:  -0.6961 | PDE Loss:  -1.2803 | Function Loss:  -1.8272\n",
      "Total loss:  -0.6962 | PDE Loss:  -1.2805 | Function Loss:  -1.8272\n",
      "Total loss:  -0.6962 | PDE Loss:  -1.2807 | Function Loss:  -1.8272\n",
      "Total loss:  -0.6963 | PDE Loss:  -1.281 | Function Loss:  -1.8272\n",
      "Total loss:  -0.6964 | PDE Loss:  -1.2813 | Function Loss:  -1.8272\n",
      "Total loss:  -0.6965 | PDE Loss:  -1.2818 | Function Loss:  -1.8272\n",
      "Total loss:  -0.6966 | PDE Loss:  -1.282 | Function Loss:  -1.8272\n",
      "Total loss:  -0.6967 | PDE Loss:  -1.2823 | Function Loss:  -1.8272\n",
      "Total loss:  -0.6968 | PDE Loss:  -1.2825 | Function Loss:  -1.8272\n",
      "Total loss:  -0.6968 | PDE Loss:  -1.2827 | Function Loss:  -1.8273\n",
      "Total loss:  -0.6969 | PDE Loss:  -1.2829 | Function Loss:  -1.8273\n",
      "Total loss:  -0.697 | PDE Loss:  -1.283 | Function Loss:  -1.8274\n",
      "Total loss:  -0.697 | PDE Loss:  -1.2822 | Function Loss:  -1.8278\n",
      "Total loss:  -0.6971 | PDE Loss:  -1.2826 | Function Loss:  -1.8277\n",
      "Total loss:  -0.6971 | PDE Loss:  -1.283 | Function Loss:  -1.8276\n",
      "Total loss:  -0.6972 | PDE Loss:  -1.2831 | Function Loss:  -1.8276\n",
      "Total loss:  -0.6972 | PDE Loss:  -1.2832 | Function Loss:  -1.8277\n",
      "Total loss:  -0.6972 | PDE Loss:  -1.2831 | Function Loss:  -1.8277\n",
      "Total loss:  -0.6973 | PDE Loss:  -1.2833 | Function Loss:  -1.8277\n",
      "Total loss:  -0.6973 | PDE Loss:  -1.2831 | Function Loss:  -1.8278\n",
      "Total loss:  -0.6973 | PDE Loss:  -1.2829 | Function Loss:  -1.8279\n",
      "Total loss:  -0.6973 | PDE Loss:  -1.2827 | Function Loss:  -1.828\n",
      "Total loss:  -0.6974 | PDE Loss:  -1.2824 | Function Loss:  -1.8281\n",
      "Total loss:  -0.6974 | PDE Loss:  -1.2823 | Function Loss:  -1.8282\n",
      "Total loss:  -0.6974 | PDE Loss:  -1.2822 | Function Loss:  -1.8282\n",
      "Total loss:  -0.6974 | PDE Loss:  -1.2822 | Function Loss:  -1.8283\n",
      "Total loss:  -0.6974 | PDE Loss:  -1.2821 | Function Loss:  -1.8283\n",
      "Total loss:  -0.6975 | PDE Loss:  -1.2821 | Function Loss:  -1.8283\n",
      "Total loss:  -0.6975 | PDE Loss:  -1.2823 | Function Loss:  -1.8283\n",
      "Total loss:  -0.6975 | PDE Loss:  -1.2826 | Function Loss:  -1.8283\n",
      "Total loss:  -0.6976 | PDE Loss:  -1.2829 | Function Loss:  -1.8282\n",
      "Total loss:  -0.6976 | PDE Loss:  -1.2828 | Function Loss:  -1.8283\n",
      "Total loss:  -0.6976 | PDE Loss:  -1.283 | Function Loss:  -1.8282\n",
      "Total loss:  -0.6976 | PDE Loss:  -1.2831 | Function Loss:  -1.8282\n",
      "Total loss:  -0.6976 | PDE Loss:  -1.2831 | Function Loss:  -1.8283\n",
      "Total loss:  -0.6977 | PDE Loss:  -1.2831 | Function Loss:  -1.8283\n",
      "Total loss:  -0.6977 | PDE Loss:  -1.2831 | Function Loss:  -1.8283\n",
      "Total loss:  -0.6977 | PDE Loss:  -1.283 | Function Loss:  -1.8284\n",
      "Total loss:  -0.6977 | PDE Loss:  -1.2829 | Function Loss:  -1.8285\n",
      "Total loss:  -0.6978 | PDE Loss:  -1.283 | Function Loss:  -1.8285\n",
      "Total loss:  -0.6978 | PDE Loss:  -1.2825 | Function Loss:  -1.8287\n",
      "Total loss:  -0.6978 | PDE Loss:  -1.2827 | Function Loss:  -1.8286\n",
      "Total loss:  -0.6979 | PDE Loss:  -1.2831 | Function Loss:  -1.8286\n",
      "Total loss:  -0.6979 | PDE Loss:  -1.2834 | Function Loss:  -1.8286\n",
      "Total loss:  -0.698 | PDE Loss:  -1.2835 | Function Loss:  -1.8285\n",
      "Total loss:  -0.698 | PDE Loss:  -1.2837 | Function Loss:  -1.8285\n",
      "Total loss:  -0.698 | PDE Loss:  -1.2838 | Function Loss:  -1.8285\n",
      "Total loss:  -0.698 | PDE Loss:  -1.284 | Function Loss:  -1.8285\n",
      "Total loss:  -0.6981 | PDE Loss:  -1.2841 | Function Loss:  -1.8285\n",
      "Total loss:  -0.6981 | PDE Loss:  -1.2842 | Function Loss:  -1.8285\n",
      "Total loss:  -0.6982 | PDE Loss:  -1.2842 | Function Loss:  -1.8286\n",
      "Total loss:  -0.6982 | PDE Loss:  -1.2838 | Function Loss:  -1.8287\n",
      "Total loss:  -0.6982 | PDE Loss:  -1.284 | Function Loss:  -1.8287\n",
      "Total loss:  -0.6982 | PDE Loss:  -1.284 | Function Loss:  -1.8287\n",
      "Total loss:  -0.6983 | PDE Loss:  -1.2839 | Function Loss:  -1.8288\n",
      "Total loss:  -0.6983 | PDE Loss:  -1.2839 | Function Loss:  -1.8289\n",
      "Total loss:  -0.6983 | PDE Loss:  -1.2838 | Function Loss:  -1.8289\n",
      "Total loss:  -0.6983 | PDE Loss:  -1.2837 | Function Loss:  -1.829\n",
      "Total loss:  -0.6984 | PDE Loss:  -1.2836 | Function Loss:  -1.8291\n",
      "Total loss:  -0.6984 | PDE Loss:  -1.2835 | Function Loss:  -1.8291\n",
      "Total loss:  -0.6984 | PDE Loss:  -1.2835 | Function Loss:  -1.8292\n",
      "Total loss:  -0.6985 | PDE Loss:  -1.2833 | Function Loss:  -1.8293\n",
      "Total loss:  -0.6981 | PDE Loss:  -1.2835 | Function Loss:  -1.8288\n",
      "Total loss:  -0.6985 | PDE Loss:  -1.2835 | Function Loss:  -1.8293\n",
      "Total loss:  -0.6985 | PDE Loss:  -1.2834 | Function Loss:  -1.8293\n",
      "Total loss:  -0.6986 | PDE Loss:  -1.2835 | Function Loss:  -1.8294\n",
      "Total loss:  -0.6986 | PDE Loss:  -1.2836 | Function Loss:  -1.8294\n",
      "Total loss:  -0.6986 | PDE Loss:  -1.2838 | Function Loss:  -1.8294\n",
      "Total loss:  -0.6987 | PDE Loss:  -1.2839 | Function Loss:  -1.8293\n",
      "Total loss:  -0.6987 | PDE Loss:  -1.2841 | Function Loss:  -1.8293\n",
      "Total loss:  -0.6987 | PDE Loss:  -1.2843 | Function Loss:  -1.8293\n",
      "Total loss:  -0.6988 | PDE Loss:  -1.2846 | Function Loss:  -1.8292\n",
      "Total loss:  -0.6988 | PDE Loss:  -1.285 | Function Loss:  -1.8292\n",
      "Total loss:  -0.6988 | PDE Loss:  -1.2853 | Function Loss:  -1.8291\n",
      "Total loss:  -0.6989 | PDE Loss:  -1.2855 | Function Loss:  -1.8291\n",
      "Total loss:  -0.6989 | PDE Loss:  -1.2854 | Function Loss:  -1.8292\n",
      "Total loss:  -0.699 | PDE Loss:  -1.2856 | Function Loss:  -1.8292\n",
      "Total loss:  -0.699 | PDE Loss:  -1.2857 | Function Loss:  -1.8292\n",
      "Total loss:  -0.699 | PDE Loss:  -1.2859 | Function Loss:  -1.8291\n",
      "Total loss:  -0.699 | PDE Loss:  -1.286 | Function Loss:  -1.8291\n",
      "Total loss:  -0.6991 | PDE Loss:  -1.2861 | Function Loss:  -1.8291\n",
      "Total loss:  -0.6991 | PDE Loss:  -1.2865 | Function Loss:  -1.829\n",
      "Total loss:  -0.6989 | PDE Loss:  -1.2859 | Function Loss:  -1.829\n",
      "Total loss:  -0.6991 | PDE Loss:  -1.2865 | Function Loss:  -1.8291\n",
      "Total loss:  -0.6991 | PDE Loss:  -1.2868 | Function Loss:  -1.829\n",
      "Total loss:  -0.6992 | PDE Loss:  -1.2872 | Function Loss:  -1.8289\n",
      "Total loss:  -0.6992 | PDE Loss:  -1.2879 | Function Loss:  -1.8287\n",
      "Total loss:  -0.6993 | PDE Loss:  -1.2886 | Function Loss:  -1.8286\n",
      "Total loss:  -0.6993 | PDE Loss:  -1.2887 | Function Loss:  -1.8285\n",
      "Total loss:  -0.6993 | PDE Loss:  -1.2889 | Function Loss:  -1.8285\n",
      "Total loss:  -0.6994 | PDE Loss:  -1.2891 | Function Loss:  -1.8285\n",
      "Total loss:  -0.6994 | PDE Loss:  -1.2892 | Function Loss:  -1.8285\n",
      "Total loss:  -0.6994 | PDE Loss:  -1.2893 | Function Loss:  -1.8285\n",
      "Total loss:  -0.6995 | PDE Loss:  -1.2895 | Function Loss:  -1.8285\n",
      "Total loss:  -0.6995 | PDE Loss:  -1.2897 | Function Loss:  -1.8285\n",
      "Total loss:  -0.6995 | PDE Loss:  -1.2898 | Function Loss:  -1.8285\n",
      "Total loss:  -0.6995 | PDE Loss:  -1.2905 | Function Loss:  -1.8282\n",
      "Total loss:  -0.6996 | PDE Loss:  -1.2902 | Function Loss:  -1.8284\n",
      "Total loss:  -0.6996 | PDE Loss:  -1.2902 | Function Loss:  -1.8284\n",
      "Total loss:  -0.6996 | PDE Loss:  -1.2902 | Function Loss:  -1.8285\n",
      "Total loss:  -0.6996 | PDE Loss:  -1.2902 | Function Loss:  -1.8285\n",
      "Total loss:  -0.6997 | PDE Loss:  -1.2903 | Function Loss:  -1.8285\n",
      "Total loss:  -0.6997 | PDE Loss:  -1.2904 | Function Loss:  -1.8285\n",
      "Total loss:  -0.6997 | PDE Loss:  -1.2906 | Function Loss:  -1.8285\n",
      "Total loss:  -0.6998 | PDE Loss:  -1.2905 | Function Loss:  -1.8286\n",
      "Total loss:  -0.6998 | PDE Loss:  -1.2907 | Function Loss:  -1.8285\n",
      "Total loss:  -0.6998 | PDE Loss:  -1.291 | Function Loss:  -1.8285\n",
      "Total loss:  -0.6999 | PDE Loss:  -1.2913 | Function Loss:  -1.8284\n",
      "Total loss:  -0.6999 | PDE Loss:  -1.2917 | Function Loss:  -1.8283\n",
      "Total loss:  -0.6999 | PDE Loss:  -1.292 | Function Loss:  -1.8283\n",
      "Total loss:  -0.7 | PDE Loss:  -1.2923 | Function Loss:  -1.8282\n",
      "Total loss:  -0.7 | PDE Loss:  -1.2925 | Function Loss:  -1.8282\n",
      "Total loss:  -0.7 | PDE Loss:  -1.2927 | Function Loss:  -1.8281\n",
      "Total loss:  -0.7001 | PDE Loss:  -1.293 | Function Loss:  -1.8281\n",
      "Total loss:  -0.7001 | PDE Loss:  -1.2932 | Function Loss:  -1.8281\n",
      "Total loss:  -0.7 | PDE Loss:  -1.2934 | Function Loss:  -1.8278\n",
      "Total loss:  -0.7001 | PDE Loss:  -1.2934 | Function Loss:  -1.8281\n",
      "Total loss:  -0.7002 | PDE Loss:  -1.2935 | Function Loss:  -1.8281\n",
      "Total loss:  -0.7002 | PDE Loss:  -1.2936 | Function Loss:  -1.8281\n",
      "Total loss:  -0.7002 | PDE Loss:  -1.2937 | Function Loss:  -1.8281\n",
      "Total loss:  -0.7003 | PDE Loss:  -1.2937 | Function Loss:  -1.8281\n",
      "Total loss:  -0.7003 | PDE Loss:  -1.2938 | Function Loss:  -1.8281\n",
      "Total loss:  -0.7003 | PDE Loss:  -1.2939 | Function Loss:  -1.8281\n",
      "Total loss:  -0.7003 | PDE Loss:  -1.294 | Function Loss:  -1.8281\n",
      "Total loss:  -0.7004 | PDE Loss:  -1.2941 | Function Loss:  -1.8281\n",
      "Total loss:  -0.7004 | PDE Loss:  -1.2943 | Function Loss:  -1.8281\n",
      "Total loss:  -0.7004 | PDE Loss:  -1.2947 | Function Loss:  -1.8279\n",
      "Total loss:  -0.7004 | PDE Loss:  -1.2945 | Function Loss:  -1.8281\n",
      "Total loss:  -0.7005 | PDE Loss:  -1.2946 | Function Loss:  -1.8281\n",
      "Total loss:  -0.7005 | PDE Loss:  -1.2946 | Function Loss:  -1.8281\n",
      "Total loss:  -0.7005 | PDE Loss:  -1.2946 | Function Loss:  -1.8281\n",
      "Total loss:  -0.7005 | PDE Loss:  -1.2946 | Function Loss:  -1.8282\n",
      "Total loss:  -0.7005 | PDE Loss:  -1.2946 | Function Loss:  -1.8282\n",
      "Total loss:  -0.7006 | PDE Loss:  -1.2946 | Function Loss:  -1.8282\n",
      "Total loss:  -0.7006 | PDE Loss:  -1.2947 | Function Loss:  -1.8282\n",
      "Total loss:  -0.7006 | PDE Loss:  -1.2949 | Function Loss:  -1.8281\n",
      "Total loss:  -0.7006 | PDE Loss:  -1.2952 | Function Loss:  -1.8281\n",
      "Total loss:  -0.7006 | PDE Loss:  -1.2957 | Function Loss:  -1.828\n",
      "Total loss:  -0.7007 | PDE Loss:  -1.2959 | Function Loss:  -1.8279\n",
      "Total loss:  -0.7007 | PDE Loss:  -1.2963 | Function Loss:  -1.8278\n",
      "Total loss:  -0.7007 | PDE Loss:  -1.2965 | Function Loss:  -1.8278\n",
      "Total loss:  -0.7008 | PDE Loss:  -1.2968 | Function Loss:  -1.8277\n",
      "Total loss:  -0.7008 | PDE Loss:  -1.2967 | Function Loss:  -1.8278\n",
      "Total loss:  -0.7009 | PDE Loss:  -1.2969 | Function Loss:  -1.8278\n",
      "Total loss:  -0.7009 | PDE Loss:  -1.297 | Function Loss:  -1.8278\n",
      "Total loss:  -0.7009 | PDE Loss:  -1.2968 | Function Loss:  -1.8279\n",
      "Total loss:  -0.7009 | PDE Loss:  -1.2965 | Function Loss:  -1.8281\n",
      "Total loss:  -0.701 | PDE Loss:  -1.2962 | Function Loss:  -1.8282\n",
      "Total loss:  -0.701 | PDE Loss:  -1.2959 | Function Loss:  -1.8283\n",
      "Total loss:  -0.701 | PDE Loss:  -1.2956 | Function Loss:  -1.8285\n",
      "Total loss:  -0.701 | PDE Loss:  -1.2954 | Function Loss:  -1.8286\n",
      "Total loss:  -0.7011 | PDE Loss:  -1.2952 | Function Loss:  -1.8287\n",
      "Total loss:  -0.7011 | PDE Loss:  -1.295 | Function Loss:  -1.8288\n",
      "Total loss:  -0.7011 | PDE Loss:  -1.295 | Function Loss:  -1.8288\n",
      "Total loss:  -0.7012 | PDE Loss:  -1.295 | Function Loss:  -1.8289\n",
      "Total loss:  -0.7012 | PDE Loss:  -1.2951 | Function Loss:  -1.8289\n",
      "Total loss:  -0.7012 | PDE Loss:  -1.2952 | Function Loss:  -1.8289\n",
      "Total loss:  -0.7013 | PDE Loss:  -1.2951 | Function Loss:  -1.8289\n",
      "Total loss:  -0.7013 | PDE Loss:  -1.2954 | Function Loss:  -1.8289\n",
      "Total loss:  -0.7013 | PDE Loss:  -1.2955 | Function Loss:  -1.8289\n",
      "Total loss:  -0.7014 | PDE Loss:  -1.2957 | Function Loss:  -1.8289\n",
      "Total loss:  -0.7014 | PDE Loss:  -1.2958 | Function Loss:  -1.8289\n",
      "Total loss:  -0.7014 | PDE Loss:  -1.296 | Function Loss:  -1.8289\n",
      "Total loss:  -0.7015 | PDE Loss:  -1.2962 | Function Loss:  -1.8289\n",
      "Total loss:  -0.7015 | PDE Loss:  -1.2965 | Function Loss:  -1.8289\n",
      "Total loss:  -0.7016 | PDE Loss:  -1.2967 | Function Loss:  -1.8289\n",
      "Total loss:  -0.7017 | PDE Loss:  -1.297 | Function Loss:  -1.8288\n",
      "Total loss:  -0.7017 | PDE Loss:  -1.2974 | Function Loss:  -1.8288\n",
      "Total loss:  -0.7018 | PDE Loss:  -1.2978 | Function Loss:  -1.8287\n",
      "Total loss:  -0.7016 | PDE Loss:  -1.2998 | Function Loss:  -1.8279\n",
      "Total loss:  -0.7018 | PDE Loss:  -1.2983 | Function Loss:  -1.8286\n",
      "Total loss:  -0.7018 | PDE Loss:  -1.2985 | Function Loss:  -1.8286\n",
      "Total loss:  -0.7018 | PDE Loss:  -1.2986 | Function Loss:  -1.8285\n",
      "Total loss:  -0.7019 | PDE Loss:  -1.2989 | Function Loss:  -1.8285\n",
      "Total loss:  -0.7019 | PDE Loss:  -1.2992 | Function Loss:  -1.8285\n",
      "Total loss:  -0.702 | PDE Loss:  -1.2994 | Function Loss:  -1.8284\n",
      "Total loss:  -0.702 | PDE Loss:  -1.2996 | Function Loss:  -1.8284\n",
      "Total loss:  -0.702 | PDE Loss:  -1.2998 | Function Loss:  -1.8284\n",
      "Total loss:  -0.7021 | PDE Loss:  -1.3007 | Function Loss:  -1.8281\n",
      "Total loss:  -0.7021 | PDE Loss:  -1.3006 | Function Loss:  -1.8282\n",
      "Total loss:  -0.7021 | PDE Loss:  -1.3005 | Function Loss:  -1.8283\n",
      "Total loss:  -0.7022 | PDE Loss:  -1.3005 | Function Loss:  -1.8284\n",
      "Total loss:  -0.7023 | PDE Loss:  -1.3005 | Function Loss:  -1.8285\n",
      "Total loss:  -0.7024 | PDE Loss:  -1.3003 | Function Loss:  -1.8287\n",
      "Total loss:  -0.7025 | PDE Loss:  -1.3005 | Function Loss:  -1.8288\n",
      "Total loss:  -0.7026 | PDE Loss:  -1.3008 | Function Loss:  -1.8288\n",
      "Total loss:  -0.7026 | PDE Loss:  -1.3011 | Function Loss:  -1.8288\n",
      "Total loss:  -0.7027 | PDE Loss:  -1.3014 | Function Loss:  -1.8288\n",
      "Total loss:  -0.7027 | PDE Loss:  -1.3016 | Function Loss:  -1.8288\n",
      "Total loss:  -0.7028 | PDE Loss:  -1.3017 | Function Loss:  -1.8288\n",
      "Total loss:  -0.7028 | PDE Loss:  -1.3016 | Function Loss:  -1.8289\n",
      "Total loss:  -0.7029 | PDE Loss:  -1.3018 | Function Loss:  -1.8289\n",
      "Total loss:  -0.7029 | PDE Loss:  -1.3018 | Function Loss:  -1.8289\n",
      "Total loss:  -0.703 | PDE Loss:  -1.3017 | Function Loss:  -1.829\n",
      "Total loss:  -0.703 | PDE Loss:  -1.3016 | Function Loss:  -1.8291\n",
      "Total loss:  -0.703 | PDE Loss:  -1.3014 | Function Loss:  -1.8292\n",
      "Total loss:  -0.7031 | PDE Loss:  -1.3012 | Function Loss:  -1.8293\n",
      "Total loss:  -0.7031 | PDE Loss:  -1.3012 | Function Loss:  -1.8294\n",
      "Total loss:  -0.7031 | PDE Loss:  -1.3013 | Function Loss:  -1.8294\n",
      "Total loss:  -0.7032 | PDE Loss:  -1.3014 | Function Loss:  -1.8294\n",
      "Total loss:  -0.7032 | PDE Loss:  -1.3016 | Function Loss:  -1.8294\n",
      "Total loss:  -0.7032 | PDE Loss:  -1.3016 | Function Loss:  -1.8294\n",
      "Total loss:  -0.7032 | PDE Loss:  -1.3026 | Function Loss:  -1.829\n",
      "Total loss:  -0.7033 | PDE Loss:  -1.3021 | Function Loss:  -1.8293\n",
      "Total loss:  -0.7033 | PDE Loss:  -1.3022 | Function Loss:  -1.8293\n",
      "Total loss:  -0.7034 | PDE Loss:  -1.3025 | Function Loss:  -1.8293\n",
      "Total loss:  -0.7034 | PDE Loss:  -1.3026 | Function Loss:  -1.8293\n",
      "Total loss:  -0.7034 | PDE Loss:  -1.3027 | Function Loss:  -1.8293\n",
      "Total loss:  -0.7034 | PDE Loss:  -1.3028 | Function Loss:  -1.8293\n",
      "Total loss:  -0.7035 | PDE Loss:  -1.303 | Function Loss:  -1.8292\n",
      "Total loss:  -0.7035 | PDE Loss:  -1.3031 | Function Loss:  -1.8292\n",
      "Total loss:  -0.7035 | PDE Loss:  -1.3033 | Function Loss:  -1.8292\n",
      "Total loss:  -0.7036 | PDE Loss:  -1.3035 | Function Loss:  -1.8292\n",
      "Total loss:  -0.7036 | PDE Loss:  -1.3036 | Function Loss:  -1.8292\n",
      "Total loss:  -0.7036 | PDE Loss:  -1.3038 | Function Loss:  -1.8292\n",
      "Total loss:  -0.7037 | PDE Loss:  -1.304 | Function Loss:  -1.8292\n",
      "Total loss:  -0.7038 | PDE Loss:  -1.3044 | Function Loss:  -1.8292\n",
      "Total loss:  -0.7037 | PDE Loss:  -1.3038 | Function Loss:  -1.8294\n",
      "Total loss:  -0.7038 | PDE Loss:  -1.3042 | Function Loss:  -1.8293\n",
      "Total loss:  -0.7039 | PDE Loss:  -1.305 | Function Loss:  -1.8291\n",
      "Total loss:  -0.7039 | PDE Loss:  -1.3052 | Function Loss:  -1.8291\n",
      "Total loss:  -0.704 | PDE Loss:  -1.3055 | Function Loss:  -1.8291\n",
      "Total loss:  -0.704 | PDE Loss:  -1.3059 | Function Loss:  -1.829\n",
      "Total loss:  -0.704 | PDE Loss:  -1.306 | Function Loss:  -1.829\n",
      "Total loss:  -0.7041 | PDE Loss:  -1.306 | Function Loss:  -1.829\n",
      "Total loss:  -0.7041 | PDE Loss:  -1.3061 | Function Loss:  -1.829\n",
      "Total loss:  -0.7041 | PDE Loss:  -1.3063 | Function Loss:  -1.829\n",
      "Total loss:  -0.7042 | PDE Loss:  -1.3065 | Function Loss:  -1.829\n",
      "Total loss:  -0.7042 | PDE Loss:  -1.3067 | Function Loss:  -1.829\n",
      "Total loss:  -0.7042 | PDE Loss:  -1.307 | Function Loss:  -1.8289\n",
      "Total loss:  -0.7043 | PDE Loss:  -1.3072 | Function Loss:  -1.8289\n",
      "Total loss:  -0.7043 | PDE Loss:  -1.3075 | Function Loss:  -1.8289\n",
      "Total loss:  -0.7043 | PDE Loss:  -1.3076 | Function Loss:  -1.8289\n",
      "Total loss:  -0.7044 | PDE Loss:  -1.3077 | Function Loss:  -1.8289\n",
      "Total loss:  -0.7044 | PDE Loss:  -1.3078 | Function Loss:  -1.8289\n",
      "Total loss:  -0.7044 | PDE Loss:  -1.3077 | Function Loss:  -1.829\n",
      "Total loss:  -0.7045 | PDE Loss:  -1.3077 | Function Loss:  -1.829\n",
      "Total loss:  -0.7045 | PDE Loss:  -1.3079 | Function Loss:  -1.829\n",
      "Total loss:  -0.7046 | PDE Loss:  -1.3083 | Function Loss:  -1.8289\n",
      "Total loss:  -0.7046 | PDE Loss:  -1.3079 | Function Loss:  -1.8291\n",
      "Total loss:  -0.7046 | PDE Loss:  -1.3082 | Function Loss:  -1.829\n",
      "Total loss:  -0.7047 | PDE Loss:  -1.3092 | Function Loss:  -1.8288\n",
      "Total loss:  -0.7047 | PDE Loss:  -1.3092 | Function Loss:  -1.8288\n",
      "Total loss:  -0.7048 | PDE Loss:  -1.3093 | Function Loss:  -1.8289\n",
      "Total loss:  -0.7048 | PDE Loss:  -1.3095 | Function Loss:  -1.8289\n",
      "Total loss:  -0.7049 | PDE Loss:  -1.3096 | Function Loss:  -1.8289\n",
      "Total loss:  -0.7049 | PDE Loss:  -1.3098 | Function Loss:  -1.8289\n",
      "Total loss:  -0.7049 | PDE Loss:  -1.3099 | Function Loss:  -1.8289\n",
      "Total loss:  -0.7049 | PDE Loss:  -1.3105 | Function Loss:  -1.8287\n",
      "Total loss:  -0.705 | PDE Loss:  -1.3103 | Function Loss:  -1.8289\n",
      "Total loss:  -0.7051 | PDE Loss:  -1.3105 | Function Loss:  -1.8289\n",
      "Total loss:  -0.7051 | PDE Loss:  -1.3107 | Function Loss:  -1.8289\n",
      "Total loss:  -0.7052 | PDE Loss:  -1.3109 | Function Loss:  -1.829\n",
      "Total loss:  -0.7053 | PDE Loss:  -1.3108 | Function Loss:  -1.8291\n",
      "Total loss:  -0.7053 | PDE Loss:  -1.3108 | Function Loss:  -1.8291\n",
      "Total loss:  -0.7053 | PDE Loss:  -1.3095 | Function Loss:  -1.8295\n",
      "Total loss:  -0.7053 | PDE Loss:  -1.3106 | Function Loss:  -1.8292\n",
      "Total loss:  -0.7054 | PDE Loss:  -1.3106 | Function Loss:  -1.8292\n",
      "Total loss:  -0.7054 | PDE Loss:  -1.3106 | Function Loss:  -1.8293\n",
      "Total loss:  -0.7054 | PDE Loss:  -1.3107 | Function Loss:  -1.8293\n",
      "Total loss:  -0.7055 | PDE Loss:  -1.3108 | Function Loss:  -1.8293\n",
      "Total loss:  -0.7055 | PDE Loss:  -1.311 | Function Loss:  -1.8293\n",
      "Total loss:  -0.7055 | PDE Loss:  -1.3113 | Function Loss:  -1.8293\n",
      "Total loss:  -0.7056 | PDE Loss:  -1.3112 | Function Loss:  -1.8293\n",
      "Total loss:  -0.7056 | PDE Loss:  -1.3112 | Function Loss:  -1.8293\n",
      "Total loss:  -0.7056 | PDE Loss:  -1.3112 | Function Loss:  -1.8294\n",
      "Total loss:  -0.7057 | PDE Loss:  -1.3114 | Function Loss:  -1.8294\n",
      "Total loss:  -0.7057 | PDE Loss:  -1.3109 | Function Loss:  -1.8296\n",
      "Total loss:  -0.7058 | PDE Loss:  -1.3115 | Function Loss:  -1.8295\n",
      "Total loss:  -0.7058 | PDE Loss:  -1.3119 | Function Loss:  -1.8294\n",
      "Total loss:  -0.7059 | PDE Loss:  -1.3124 | Function Loss:  -1.8293\n",
      "Total loss:  -0.7059 | PDE Loss:  -1.3127 | Function Loss:  -1.8293\n",
      "Total loss:  -0.706 | PDE Loss:  -1.3129 | Function Loss:  -1.8293\n",
      "Total loss:  -0.7061 | PDE Loss:  -1.3131 | Function Loss:  -1.8294\n",
      "Total loss:  -0.7061 | PDE Loss:  -1.3131 | Function Loss:  -1.8294\n",
      "Total loss:  -0.7062 | PDE Loss:  -1.3132 | Function Loss:  -1.8295\n",
      "Total loss:  -0.7062 | PDE Loss:  -1.3133 | Function Loss:  -1.8295\n",
      "Total loss:  -0.7063 | PDE Loss:  -1.3133 | Function Loss:  -1.8296\n",
      "Total loss:  -0.7063 | PDE Loss:  -1.3133 | Function Loss:  -1.8297\n",
      "Total loss:  -0.7064 | PDE Loss:  -1.3133 | Function Loss:  -1.8297\n",
      "Total loss:  -0.7064 | PDE Loss:  -1.3132 | Function Loss:  -1.8298\n",
      "Total loss:  -0.7065 | PDE Loss:  -1.3132 | Function Loss:  -1.8299\n",
      "Total loss:  -0.7065 | PDE Loss:  -1.3132 | Function Loss:  -1.8299\n",
      "Total loss:  -0.7066 | PDE Loss:  -1.3132 | Function Loss:  -1.83\n",
      "Total loss:  -0.7067 | PDE Loss:  -1.3131 | Function Loss:  -1.8301\n",
      "Total loss:  -0.7067 | PDE Loss:  -1.3128 | Function Loss:  -1.8303\n",
      "Total loss:  -0.7067 | PDE Loss:  -1.3129 | Function Loss:  -1.8303\n",
      "Total loss:  -0.7068 | PDE Loss:  -1.313 | Function Loss:  -1.8303\n",
      "Total loss:  -0.7068 | PDE Loss:  -1.3132 | Function Loss:  -1.8303\n",
      "Total loss:  -0.7069 | PDE Loss:  -1.3134 | Function Loss:  -1.8303\n",
      "Total loss:  -0.7069 | PDE Loss:  -1.3136 | Function Loss:  -1.8303\n",
      "Total loss:  -0.7069 | PDE Loss:  -1.3138 | Function Loss:  -1.8303\n",
      "Total loss:  -0.7063 | PDE Loss:  -1.3143 | Function Loss:  -1.8293\n",
      "Total loss:  -0.7069 | PDE Loss:  -1.314 | Function Loss:  -1.8302\n",
      "Total loss:  -0.707 | PDE Loss:  -1.3142 | Function Loss:  -1.8302\n",
      "Total loss:  -0.707 | PDE Loss:  -1.3143 | Function Loss:  -1.8302\n",
      "Total loss:  -0.707 | PDE Loss:  -1.3144 | Function Loss:  -1.8302\n",
      "Total loss:  -0.707 | PDE Loss:  -1.3144 | Function Loss:  -1.8302\n",
      "Total loss:  -0.7071 | PDE Loss:  -1.3145 | Function Loss:  -1.8302\n",
      "Total loss:  -0.7071 | PDE Loss:  -1.3145 | Function Loss:  -1.8303\n",
      "Total loss:  -0.7071 | PDE Loss:  -1.3144 | Function Loss:  -1.8303\n",
      "Total loss:  -0.7072 | PDE Loss:  -1.3143 | Function Loss:  -1.8304\n",
      "Total loss:  -0.7072 | PDE Loss:  -1.314 | Function Loss:  -1.8306\n",
      "Total loss:  -0.7072 | PDE Loss:  -1.3139 | Function Loss:  -1.8306\n",
      "Total loss:  -0.7073 | PDE Loss:  -1.3138 | Function Loss:  -1.8307\n",
      "Total loss:  -0.7073 | PDE Loss:  -1.3138 | Function Loss:  -1.8308\n",
      "Total loss:  -0.7073 | PDE Loss:  -1.3137 | Function Loss:  -1.8308\n",
      "Total loss:  -0.7074 | PDE Loss:  -1.3137 | Function Loss:  -1.8309\n",
      "Total loss:  -0.7074 | PDE Loss:  -1.3138 | Function Loss:  -1.8309\n",
      "Total loss:  -0.7075 | PDE Loss:  -1.3138 | Function Loss:  -1.831\n",
      "Total loss:  -0.7075 | PDE Loss:  -1.3139 | Function Loss:  -1.831\n",
      "Total loss:  -0.7076 | PDE Loss:  -1.3139 | Function Loss:  -1.8311\n",
      "Total loss:  -0.7076 | PDE Loss:  -1.314 | Function Loss:  -1.8311\n",
      "Total loss:  -0.7076 | PDE Loss:  -1.314 | Function Loss:  -1.8312\n",
      "Total loss:  -0.7077 | PDE Loss:  -1.314 | Function Loss:  -1.8312\n",
      "Total loss:  -0.7077 | PDE Loss:  -1.3139 | Function Loss:  -1.8313\n",
      "Total loss:  -0.7078 | PDE Loss:  -1.3138 | Function Loss:  -1.8314\n",
      "Total loss:  -0.7078 | PDE Loss:  -1.3135 | Function Loss:  -1.8316\n",
      "Total loss:  -0.7079 | PDE Loss:  -1.3135 | Function Loss:  -1.8316\n",
      "Total loss:  -0.7079 | PDE Loss:  -1.3135 | Function Loss:  -1.8317\n",
      "Total loss:  -0.7079 | PDE Loss:  -1.3135 | Function Loss:  -1.8317\n",
      "Total loss:  -0.708 | PDE Loss:  -1.3135 | Function Loss:  -1.8317\n",
      "Total loss:  -0.708 | PDE Loss:  -1.3135 | Function Loss:  -1.8318\n",
      "Total loss:  -0.708 | PDE Loss:  -1.3135 | Function Loss:  -1.8318\n",
      "Total loss:  -0.708 | PDE Loss:  -1.3136 | Function Loss:  -1.8318\n",
      "Total loss:  -0.708 | PDE Loss:  -1.3137 | Function Loss:  -1.8318\n",
      "Total loss:  -0.7081 | PDE Loss:  -1.3137 | Function Loss:  -1.8318\n",
      "Total loss:  -0.7081 | PDE Loss:  -1.3142 | Function Loss:  -1.8317\n",
      "Total loss:  -0.7081 | PDE Loss:  -1.3141 | Function Loss:  -1.8318\n",
      "Total loss:  -0.7082 | PDE Loss:  -1.3139 | Function Loss:  -1.8319\n",
      "Total loss:  -0.7082 | PDE Loss:  -1.3138 | Function Loss:  -1.832\n",
      "Total loss:  -0.7083 | PDE Loss:  -1.3137 | Function Loss:  -1.8321\n",
      "Total loss:  -0.7083 | PDE Loss:  -1.3135 | Function Loss:  -1.8322\n",
      "Total loss:  -0.7083 | PDE Loss:  -1.3133 | Function Loss:  -1.8323\n",
      "Total loss:  -0.7083 | PDE Loss:  -1.3128 | Function Loss:  -1.8325\n",
      "Total loss:  -0.7084 | PDE Loss:  -1.3127 | Function Loss:  -1.8325\n",
      "Total loss:  -0.7084 | PDE Loss:  -1.3128 | Function Loss:  -1.8326\n",
      "Total loss:  -0.7084 | PDE Loss:  -1.3129 | Function Loss:  -1.8325\n",
      "Total loss:  -0.7084 | PDE Loss:  -1.3132 | Function Loss:  -1.8325\n",
      "Total loss:  -0.7085 | PDE Loss:  -1.3136 | Function Loss:  -1.8324\n",
      "Total loss:  -0.7085 | PDE Loss:  -1.3142 | Function Loss:  -1.8322\n",
      "Total loss:  -0.7086 | PDE Loss:  -1.315 | Function Loss:  -1.832\n",
      "Total loss:  -0.7086 | PDE Loss:  -1.3156 | Function Loss:  -1.8319\n",
      "Total loss:  -0.7086 | PDE Loss:  -1.316 | Function Loss:  -1.8318\n",
      "Total loss:  -0.7087 | PDE Loss:  -1.3164 | Function Loss:  -1.8318\n",
      "Total loss:  -0.7088 | PDE Loss:  -1.3166 | Function Loss:  -1.8318\n",
      "Total loss:  -0.7088 | PDE Loss:  -1.3166 | Function Loss:  -1.8319\n",
      "Total loss:  -0.7089 | PDE Loss:  -1.3164 | Function Loss:  -1.832\n",
      "Total loss:  -0.7089 | PDE Loss:  -1.3161 | Function Loss:  -1.8322\n",
      "Total loss:  -0.7088 | PDE Loss:  -1.3153 | Function Loss:  -1.8323\n",
      "Total loss:  -0.7089 | PDE Loss:  -1.316 | Function Loss:  -1.8322\n",
      "Total loss:  -0.709 | PDE Loss:  -1.3155 | Function Loss:  -1.8324\n",
      "Total loss:  -0.709 | PDE Loss:  -1.3151 | Function Loss:  -1.8326\n",
      "Total loss:  -0.7091 | PDE Loss:  -1.3148 | Function Loss:  -1.8328\n",
      "Total loss:  -0.7091 | PDE Loss:  -1.3146 | Function Loss:  -1.8329\n",
      "Total loss:  -0.7091 | PDE Loss:  -1.3146 | Function Loss:  -1.833\n",
      "Total loss:  -0.7092 | PDE Loss:  -1.3147 | Function Loss:  -1.833\n",
      "Total loss:  -0.7092 | PDE Loss:  -1.3149 | Function Loss:  -1.8329\n",
      "Total loss:  -0.7092 | PDE Loss:  -1.3151 | Function Loss:  -1.8329\n",
      "Total loss:  -0.7092 | PDE Loss:  -1.3153 | Function Loss:  -1.8329\n",
      "Total loss:  -0.7093 | PDE Loss:  -1.3154 | Function Loss:  -1.8329\n",
      "Total loss:  -0.7093 | PDE Loss:  -1.3157 | Function Loss:  -1.8329\n",
      "Total loss:  -0.7094 | PDE Loss:  -1.3159 | Function Loss:  -1.8328\n",
      "Total loss:  -0.7094 | PDE Loss:  -1.316 | Function Loss:  -1.8329\n",
      "Total loss:  -0.7095 | PDE Loss:  -1.3161 | Function Loss:  -1.8329\n",
      "Total loss:  -0.7095 | PDE Loss:  -1.3161 | Function Loss:  -1.833\n",
      "Total loss:  -0.7096 | PDE Loss:  -1.3164 | Function Loss:  -1.8329\n",
      "Total loss:  -0.7096 | PDE Loss:  -1.3165 | Function Loss:  -1.8329\n",
      "Total loss:  -0.7096 | PDE Loss:  -1.3166 | Function Loss:  -1.833\n",
      "Total loss:  -0.7097 | PDE Loss:  -1.3168 | Function Loss:  -1.8329\n",
      "Total loss:  -0.7097 | PDE Loss:  -1.3171 | Function Loss:  -1.8329\n",
      "Total loss:  -0.7097 | PDE Loss:  -1.3174 | Function Loss:  -1.8328\n",
      "Total loss:  -0.7098 | PDE Loss:  -1.3178 | Function Loss:  -1.8327\n",
      "Total loss:  -0.7098 | PDE Loss:  -1.3182 | Function Loss:  -1.8326\n",
      "Total loss:  -0.7097 | PDE Loss:  -1.3183 | Function Loss:  -1.8325\n",
      "Total loss:  -0.7098 | PDE Loss:  -1.3183 | Function Loss:  -1.8326\n",
      "Total loss:  -0.7098 | PDE Loss:  -1.3186 | Function Loss:  -1.8326\n",
      "Total loss:  -0.7099 | PDE Loss:  -1.319 | Function Loss:  -1.8325\n",
      "Total loss:  -0.7099 | PDE Loss:  -1.3192 | Function Loss:  -1.8325\n",
      "Total loss:  -0.7099 | PDE Loss:  -1.3193 | Function Loss:  -1.8325\n",
      "Total loss:  -0.71 | PDE Loss:  -1.3194 | Function Loss:  -1.8325\n",
      "Total loss:  -0.71 | PDE Loss:  -1.3194 | Function Loss:  -1.8325\n",
      "Total loss:  -0.71 | PDE Loss:  -1.3194 | Function Loss:  -1.8325\n",
      "Total loss:  -0.7101 | PDE Loss:  -1.3194 | Function Loss:  -1.8326\n",
      "Total loss:  -0.7101 | PDE Loss:  -1.3192 | Function Loss:  -1.8327\n",
      "Total loss:  -0.7101 | PDE Loss:  -1.3192 | Function Loss:  -1.8328\n",
      "Total loss:  -0.7102 | PDE Loss:  -1.3193 | Function Loss:  -1.8328\n",
      "Total loss:  -0.7102 | PDE Loss:  -1.3193 | Function Loss:  -1.8328\n",
      "Total loss:  -0.7102 | PDE Loss:  -1.3194 | Function Loss:  -1.8328\n",
      "Total loss:  -0.7103 | PDE Loss:  -1.3195 | Function Loss:  -1.8328\n",
      "Total loss:  -0.7103 | PDE Loss:  -1.3197 | Function Loss:  -1.8328\n",
      "Total loss:  -0.7104 | PDE Loss:  -1.3198 | Function Loss:  -1.8329\n",
      "Total loss:  -0.7104 | PDE Loss:  -1.3201 | Function Loss:  -1.8329\n",
      "Total loss:  -0.7105 | PDE Loss:  -1.3203 | Function Loss:  -1.8328\n",
      "Total loss:  -0.7105 | PDE Loss:  -1.3205 | Function Loss:  -1.8328\n",
      "Total loss:  -0.7105 | PDE Loss:  -1.3206 | Function Loss:  -1.8328\n",
      "Total loss:  -0.7106 | PDE Loss:  -1.3209 | Function Loss:  -1.8328\n",
      "Total loss:  -0.7106 | PDE Loss:  -1.3211 | Function Loss:  -1.8328\n",
      "Total loss:  -0.7107 | PDE Loss:  -1.3213 | Function Loss:  -1.8328\n",
      "Total loss:  -0.7107 | PDE Loss:  -1.3215 | Function Loss:  -1.8328\n",
      "Total loss:  -0.7108 | PDE Loss:  -1.3221 | Function Loss:  -1.8326\n",
      "Total loss:  -0.7108 | PDE Loss:  -1.3222 | Function Loss:  -1.8327\n",
      "Total loss:  -0.7109 | PDE Loss:  -1.3222 | Function Loss:  -1.8327\n",
      "Total loss:  -0.7109 | PDE Loss:  -1.3221 | Function Loss:  -1.8328\n",
      "Total loss:  -0.7109 | PDE Loss:  -1.322 | Function Loss:  -1.8329\n",
      "Total loss:  -0.7109 | PDE Loss:  -1.3219 | Function Loss:  -1.8329\n",
      "Total loss:  -0.711 | PDE Loss:  -1.3217 | Function Loss:  -1.833\n",
      "Total loss:  -0.711 | PDE Loss:  -1.3216 | Function Loss:  -1.8331\n",
      "Total loss:  -0.711 | PDE Loss:  -1.3215 | Function Loss:  -1.8332\n",
      "Total loss:  -0.711 | PDE Loss:  -1.3214 | Function Loss:  -1.8332\n",
      "Total loss:  -0.7111 | PDE Loss:  -1.3213 | Function Loss:  -1.8333\n",
      "Total loss:  -0.7111 | PDE Loss:  -1.3211 | Function Loss:  -1.8334\n",
      "Total loss:  -0.7111 | PDE Loss:  -1.3211 | Function Loss:  -1.8334\n",
      "Total loss:  -0.7111 | PDE Loss:  -1.3211 | Function Loss:  -1.8335\n",
      "Total loss:  -0.7112 | PDE Loss:  -1.3213 | Function Loss:  -1.8335\n",
      "Total loss:  -0.7112 | PDE Loss:  -1.3216 | Function Loss:  -1.8335\n",
      "Total loss:  -0.7113 | PDE Loss:  -1.3217 | Function Loss:  -1.8335\n",
      "Total loss:  -0.7087 | PDE Loss:  -1.3217 | Function Loss:  -1.8301\n",
      "Total loss:  -0.7113 | PDE Loss:  -1.3221 | Function Loss:  -1.8334\n",
      "Total loss:  -0.7114 | PDE Loss:  -1.3222 | Function Loss:  -1.8334\n",
      "Total loss:  -0.7114 | PDE Loss:  -1.3223 | Function Loss:  -1.8334\n",
      "Total loss:  -0.7114 | PDE Loss:  -1.3224 | Function Loss:  -1.8335\n",
      "Total loss:  -0.7115 | PDE Loss:  -1.3225 | Function Loss:  -1.8335\n",
      "Total loss:  -0.7115 | PDE Loss:  -1.3227 | Function Loss:  -1.8335\n",
      "Total loss:  -0.7116 | PDE Loss:  -1.3229 | Function Loss:  -1.8335\n",
      "Total loss:  -0.7116 | PDE Loss:  -1.3231 | Function Loss:  -1.8334\n",
      "Total loss:  -0.7116 | PDE Loss:  -1.3234 | Function Loss:  -1.8334\n",
      "Total loss:  -0.7117 | PDE Loss:  -1.3236 | Function Loss:  -1.8333\n",
      "Total loss:  -0.7117 | PDE Loss:  -1.3239 | Function Loss:  -1.8333\n",
      "Total loss:  -0.7117 | PDE Loss:  -1.3241 | Function Loss:  -1.8333\n",
      "Total loss:  -0.7118 | PDE Loss:  -1.3243 | Function Loss:  -1.8333\n",
      "Total loss:  -0.7118 | PDE Loss:  -1.3244 | Function Loss:  -1.8333\n",
      "Total loss:  -0.7118 | PDE Loss:  -1.3244 | Function Loss:  -1.8334\n",
      "Total loss:  -0.7119 | PDE Loss:  -1.3243 | Function Loss:  -1.8334\n",
      "Total loss:  -0.7119 | PDE Loss:  -1.3241 | Function Loss:  -1.8336\n",
      "Total loss:  -0.712 | PDE Loss:  -1.324 | Function Loss:  -1.8337\n",
      "Total loss:  -0.7121 | PDE Loss:  -1.3239 | Function Loss:  -1.8338\n",
      "Total loss:  -0.7121 | PDE Loss:  -1.3236 | Function Loss:  -1.834\n",
      "Total loss:  -0.7122 | PDE Loss:  -1.3235 | Function Loss:  -1.8341\n",
      "Total loss:  -0.7123 | PDE Loss:  -1.3235 | Function Loss:  -1.8342\n",
      "Total loss:  -0.7123 | PDE Loss:  -1.3236 | Function Loss:  -1.8342\n",
      "Total loss:  -0.7123 | PDE Loss:  -1.3235 | Function Loss:  -1.8343\n",
      "Total loss:  -0.7124 | PDE Loss:  -1.3238 | Function Loss:  -1.8343\n",
      "Total loss:  -0.7124 | PDE Loss:  -1.3241 | Function Loss:  -1.8342\n",
      "Total loss:  -0.7125 | PDE Loss:  -1.3244 | Function Loss:  -1.8342\n",
      "Total loss:  -0.7125 | PDE Loss:  -1.3248 | Function Loss:  -1.8341\n",
      "Total loss:  -0.7126 | PDE Loss:  -1.3251 | Function Loss:  -1.8341\n",
      "Total loss:  -0.7127 | PDE Loss:  -1.3255 | Function Loss:  -1.8341\n",
      "Total loss:  -0.7128 | PDE Loss:  -1.3258 | Function Loss:  -1.8341\n",
      "Total loss:  -0.7128 | PDE Loss:  -1.3259 | Function Loss:  -1.8342\n",
      "Total loss:  -0.7129 | PDE Loss:  -1.3259 | Function Loss:  -1.8342\n",
      "Total loss:  -0.7129 | PDE Loss:  -1.3258 | Function Loss:  -1.8343\n",
      "Total loss:  -0.713 | PDE Loss:  -1.3258 | Function Loss:  -1.8344\n",
      "Total loss:  -0.713 | PDE Loss:  -1.3258 | Function Loss:  -1.8344\n",
      "Total loss:  -0.713 | PDE Loss:  -1.3258 | Function Loss:  -1.8344\n",
      "Total loss:  -0.713 | PDE Loss:  -1.326 | Function Loss:  -1.8344\n",
      "Total loss:  -0.7131 | PDE Loss:  -1.3261 | Function Loss:  -1.8344\n",
      "Total loss:  -0.7131 | PDE Loss:  -1.3263 | Function Loss:  -1.8344\n",
      "Total loss:  -0.7131 | PDE Loss:  -1.3266 | Function Loss:  -1.8343\n",
      "Total loss:  -0.7132 | PDE Loss:  -1.327 | Function Loss:  -1.8343\n",
      "Total loss:  -0.7132 | PDE Loss:  -1.3273 | Function Loss:  -1.8342\n",
      "Total loss:  -0.7132 | PDE Loss:  -1.3276 | Function Loss:  -1.8341\n",
      "Total loss:  -0.7133 | PDE Loss:  -1.3278 | Function Loss:  -1.8341\n",
      "Total loss:  -0.7133 | PDE Loss:  -1.3281 | Function Loss:  -1.8341\n",
      "Total loss:  -0.7133 | PDE Loss:  -1.3287 | Function Loss:  -1.8339\n",
      "Total loss:  -0.7134 | PDE Loss:  -1.3287 | Function Loss:  -1.834\n",
      "Total loss:  -0.7134 | PDE Loss:  -1.3286 | Function Loss:  -1.8341\n",
      "Total loss:  -0.7135 | PDE Loss:  -1.3285 | Function Loss:  -1.8342\n",
      "Total loss:  -0.7135 | PDE Loss:  -1.3283 | Function Loss:  -1.8343\n",
      "Total loss:  -0.7136 | PDE Loss:  -1.3283 | Function Loss:  -1.8343\n",
      "Total loss:  -0.7136 | PDE Loss:  -1.328 | Function Loss:  -1.8345\n",
      "Total loss:  -0.7136 | PDE Loss:  -1.3282 | Function Loss:  -1.8345\n",
      "Total loss:  -0.7136 | PDE Loss:  -1.3281 | Function Loss:  -1.8345\n",
      "Total loss:  -0.7137 | PDE Loss:  -1.3283 | Function Loss:  -1.8345\n",
      "Total loss:  -0.7137 | PDE Loss:  -1.3285 | Function Loss:  -1.8345\n",
      "Total loss:  -0.7137 | PDE Loss:  -1.329 | Function Loss:  -1.8344\n",
      "Total loss:  -0.7138 | PDE Loss:  -1.3291 | Function Loss:  -1.8344\n",
      "Total loss:  -0.7138 | PDE Loss:  -1.3294 | Function Loss:  -1.8343\n",
      "Total loss:  -0.7139 | PDE Loss:  -1.3295 | Function Loss:  -1.8344\n",
      "Total loss:  -0.7139 | PDE Loss:  -1.3294 | Function Loss:  -1.8345\n",
      "Total loss:  -0.714 | PDE Loss:  -1.3296 | Function Loss:  -1.8346\n",
      "Total loss:  -0.7141 | PDE Loss:  -1.3296 | Function Loss:  -1.8346\n",
      "Total loss:  -0.7142 | PDE Loss:  -1.3295 | Function Loss:  -1.8348\n",
      "Total loss:  -0.7142 | PDE Loss:  -1.3299 | Function Loss:  -1.8347\n",
      "Total loss:  -0.7143 | PDE Loss:  -1.3297 | Function Loss:  -1.8349\n",
      "Total loss:  -0.7143 | PDE Loss:  -1.3297 | Function Loss:  -1.8349\n",
      "Total loss:  -0.7144 | PDE Loss:  -1.3301 | Function Loss:  -1.8349\n",
      "Total loss:  -0.7145 | PDE Loss:  -1.3304 | Function Loss:  -1.8349\n",
      "Total loss:  -0.7145 | PDE Loss:  -1.3303 | Function Loss:  -1.835\n",
      "Total loss:  -0.7146 | PDE Loss:  -1.3313 | Function Loss:  -1.8348\n",
      "Total loss:  -0.7147 | PDE Loss:  -1.331 | Function Loss:  -1.8349\n",
      "Total loss:  -0.7147 | PDE Loss:  -1.331 | Function Loss:  -1.835\n",
      "Total loss:  -0.7148 | PDE Loss:  -1.3309 | Function Loss:  -1.8351\n",
      "Total loss:  -0.7148 | PDE Loss:  -1.3313 | Function Loss:  -1.8351\n",
      "Total loss:  -0.7149 | PDE Loss:  -1.3312 | Function Loss:  -1.8352\n",
      "Total loss:  -0.7149 | PDE Loss:  -1.3311 | Function Loss:  -1.8353\n",
      "Total loss:  -0.715 | PDE Loss:  -1.3311 | Function Loss:  -1.8353\n",
      "Total loss:  -0.715 | PDE Loss:  -1.331 | Function Loss:  -1.8354\n",
      "Total loss:  -0.715 | PDE Loss:  -1.3311 | Function Loss:  -1.8354\n",
      "Total loss:  -0.7151 | PDE Loss:  -1.331 | Function Loss:  -1.8355\n",
      "Total loss:  -0.7152 | PDE Loss:  -1.3306 | Function Loss:  -1.8358\n",
      "Total loss:  -0.7153 | PDE Loss:  -1.3303 | Function Loss:  -1.836\n",
      "Total loss:  -0.7153 | PDE Loss:  -1.33 | Function Loss:  -1.8362\n",
      "Total loss:  -0.7154 | PDE Loss:  -1.3298 | Function Loss:  -1.8363\n",
      "Total loss:  -0.7155 | PDE Loss:  -1.3289 | Function Loss:  -1.8368\n",
      "Total loss:  -0.7156 | PDE Loss:  -1.3291 | Function Loss:  -1.8368\n",
      "Total loss:  -0.7157 | PDE Loss:  -1.3295 | Function Loss:  -1.8368\n",
      "Total loss:  -0.7158 | PDE Loss:  -1.3298 | Function Loss:  -1.8368\n",
      "Total loss:  -0.7158 | PDE Loss:  -1.3301 | Function Loss:  -1.8368\n",
      "Total loss:  -0.7159 | PDE Loss:  -1.3304 | Function Loss:  -1.8367\n",
      "Total loss:  -0.7159 | PDE Loss:  -1.3306 | Function Loss:  -1.8368\n",
      "Total loss:  -0.716 | PDE Loss:  -1.3306 | Function Loss:  -1.8368\n",
      "Total loss:  -0.716 | PDE Loss:  -1.3307 | Function Loss:  -1.8368\n",
      "Total loss:  -0.7161 | PDE Loss:  -1.3307 | Function Loss:  -1.8369\n",
      "Total loss:  -0.7161 | PDE Loss:  -1.3309 | Function Loss:  -1.8369\n",
      "Total loss:  -0.7162 | PDE Loss:  -1.3309 | Function Loss:  -1.837\n",
      "Total loss:  -0.7162 | PDE Loss:  -1.3309 | Function Loss:  -1.837\n",
      "Total loss:  -0.7162 | PDE Loss:  -1.331 | Function Loss:  -1.837\n",
      "Total loss:  -0.7163 | PDE Loss:  -1.3312 | Function Loss:  -1.837\n",
      "Total loss:  -0.7163 | PDE Loss:  -1.3314 | Function Loss:  -1.8371\n",
      "Total loss:  -0.7164 | PDE Loss:  -1.3315 | Function Loss:  -1.8371\n",
      "Total loss:  -0.7164 | PDE Loss:  -1.3317 | Function Loss:  -1.8371\n",
      "Total loss:  -0.7165 | PDE Loss:  -1.3319 | Function Loss:  -1.8371\n",
      "Total loss:  -0.7165 | PDE Loss:  -1.3322 | Function Loss:  -1.837\n",
      "Total loss:  -0.7165 | PDE Loss:  -1.3324 | Function Loss:  -1.837\n",
      "Total loss:  -0.7166 | PDE Loss:  -1.3324 | Function Loss:  -1.837\n",
      "Total loss:  -0.7166 | PDE Loss:  -1.3325 | Function Loss:  -1.837\n",
      "Total loss:  -0.7166 | PDE Loss:  -1.3325 | Function Loss:  -1.837\n",
      "Total loss:  -0.7166 | PDE Loss:  -1.3326 | Function Loss:  -1.837\n",
      "Total loss:  -0.7167 | PDE Loss:  -1.3327 | Function Loss:  -1.837\n",
      "Total loss:  -0.7167 | PDE Loss:  -1.3327 | Function Loss:  -1.837\n",
      "Total loss:  -0.7167 | PDE Loss:  -1.3327 | Function Loss:  -1.8371\n",
      "Total loss:  -0.7167 | PDE Loss:  -1.3327 | Function Loss:  -1.8371\n",
      "Total loss:  -0.7167 | PDE Loss:  -1.3328 | Function Loss:  -1.8371\n",
      "Total loss:  -0.7168 | PDE Loss:  -1.3329 | Function Loss:  -1.8371\n",
      "Total loss:  -0.7168 | PDE Loss:  -1.333 | Function Loss:  -1.8371\n",
      "Total loss:  -0.7166 | PDE Loss:  -1.3337 | Function Loss:  -1.8366\n",
      "Total loss:  -0.7168 | PDE Loss:  -1.3332 | Function Loss:  -1.8371\n",
      "Total loss:  -0.7169 | PDE Loss:  -1.3333 | Function Loss:  -1.8371\n",
      "Total loss:  -0.7169 | PDE Loss:  -1.3335 | Function Loss:  -1.8371\n",
      "Total loss:  -0.7169 | PDE Loss:  -1.3336 | Function Loss:  -1.8371\n",
      "Total loss:  -0.7169 | PDE Loss:  -1.3338 | Function Loss:  -1.8371\n",
      "Total loss:  -0.717 | PDE Loss:  -1.334 | Function Loss:  -1.837\n",
      "Total loss:  -0.717 | PDE Loss:  -1.3341 | Function Loss:  -1.837\n",
      "Total loss:  -0.717 | PDE Loss:  -1.3342 | Function Loss:  -1.837\n",
      "Total loss:  -0.717 | PDE Loss:  -1.3343 | Function Loss:  -1.837\n",
      "Total loss:  -0.717 | PDE Loss:  -1.3344 | Function Loss:  -1.837\n",
      "Total loss:  -0.7171 | PDE Loss:  -1.3345 | Function Loss:  -1.837\n",
      "Total loss:  -0.7171 | PDE Loss:  -1.3347 | Function Loss:  -1.837\n",
      "Total loss:  -0.7171 | PDE Loss:  -1.3348 | Function Loss:  -1.837\n",
      "Total loss:  -0.7171 | PDE Loss:  -1.3349 | Function Loss:  -1.837\n",
      "Total loss:  -0.7172 | PDE Loss:  -1.3349 | Function Loss:  -1.837\n",
      "Total loss:  -0.7172 | PDE Loss:  -1.3349 | Function Loss:  -1.837\n",
      "Total loss:  -0.7172 | PDE Loss:  -1.3348 | Function Loss:  -1.8371\n",
      "Total loss:  -0.7172 | PDE Loss:  -1.3351 | Function Loss:  -1.8371\n",
      "Total loss:  -0.7173 | PDE Loss:  -1.3348 | Function Loss:  -1.8372\n",
      "Total loss:  -0.7173 | PDE Loss:  -1.335 | Function Loss:  -1.8371\n",
      "Total loss:  -0.7173 | PDE Loss:  -1.3348 | Function Loss:  -1.8373\n",
      "Total loss:  -0.7174 | PDE Loss:  -1.3346 | Function Loss:  -1.8374\n",
      "Total loss:  -0.7174 | PDE Loss:  -1.3345 | Function Loss:  -1.8375\n",
      "Total loss:  -0.7175 | PDE Loss:  -1.3344 | Function Loss:  -1.8376\n",
      "Total loss:  -0.7175 | PDE Loss:  -1.3344 | Function Loss:  -1.8376\n",
      "Total loss:  -0.7175 | PDE Loss:  -1.3345 | Function Loss:  -1.8376\n",
      "Total loss:  -0.7176 | PDE Loss:  -1.3346 | Function Loss:  -1.8377\n",
      "Total loss:  -0.7176 | PDE Loss:  -1.3347 | Function Loss:  -1.8377\n",
      "Total loss:  -0.7177 | PDE Loss:  -1.3348 | Function Loss:  -1.8377\n",
      "Total loss:  -0.7177 | PDE Loss:  -1.3348 | Function Loss:  -1.8378\n",
      "Total loss:  -0.7178 | PDE Loss:  -1.3348 | Function Loss:  -1.8378\n",
      "Total loss:  -0.7178 | PDE Loss:  -1.3348 | Function Loss:  -1.8379\n",
      "Total loss:  -0.7179 | PDE Loss:  -1.3345 | Function Loss:  -1.838\n",
      "Total loss:  -0.7179 | PDE Loss:  -1.3348 | Function Loss:  -1.8379\n",
      "Total loss:  -0.7179 | PDE Loss:  -1.3347 | Function Loss:  -1.838\n",
      "Total loss:  -0.7179 | PDE Loss:  -1.3344 | Function Loss:  -1.8381\n",
      "Total loss:  -0.718 | PDE Loss:  -1.3341 | Function Loss:  -1.8383\n",
      "Total loss:  -0.718 | PDE Loss:  -1.3337 | Function Loss:  -1.8385\n",
      "Total loss:  -0.7181 | PDE Loss:  -1.3332 | Function Loss:  -1.8388\n",
      "Total loss:  -0.7182 | PDE Loss:  -1.3328 | Function Loss:  -1.8391\n",
      "Total loss:  -0.7178 | PDE Loss:  -1.3287 | Function Loss:  -1.8398\n",
      "Total loss:  -0.7183 | PDE Loss:  -1.3321 | Function Loss:  -1.8393\n",
      "Total loss:  -0.7184 | PDE Loss:  -1.332 | Function Loss:  -1.8395\n",
      "Total loss:  -0.7185 | PDE Loss:  -1.3319 | Function Loss:  -1.8397\n",
      "Total loss:  -0.7187 | PDE Loss:  -1.3323 | Function Loss:  -1.8399\n",
      "Total loss:  -0.7189 | PDE Loss:  -1.3327 | Function Loss:  -1.8399\n",
      "Total loss:  -0.719 | PDE Loss:  -1.3334 | Function Loss:  -1.8399\n",
      "Total loss:  -0.719 | PDE Loss:  -1.3339 | Function Loss:  -1.8398\n",
      "Total loss:  -0.7191 | PDE Loss:  -1.3344 | Function Loss:  -1.8397\n",
      "Total loss:  -0.7191 | PDE Loss:  -1.3348 | Function Loss:  -1.8397\n",
      "Total loss:  -0.7192 | PDE Loss:  -1.335 | Function Loss:  -1.8396\n",
      "Total loss:  -0.7192 | PDE Loss:  -1.335 | Function Loss:  -1.8397\n",
      "Total loss:  -0.7193 | PDE Loss:  -1.335 | Function Loss:  -1.8398\n",
      "Total loss:  -0.7193 | PDE Loss:  -1.3349 | Function Loss:  -1.8399\n",
      "Total loss:  -0.7194 | PDE Loss:  -1.3346 | Function Loss:  -1.84\n",
      "Total loss:  -0.7194 | PDE Loss:  -1.3345 | Function Loss:  -1.8401\n",
      "Total loss:  -0.7194 | PDE Loss:  -1.3345 | Function Loss:  -1.8401\n",
      "Total loss:  -0.7195 | PDE Loss:  -1.3345 | Function Loss:  -1.8402\n",
      "Total loss:  -0.7195 | PDE Loss:  -1.3346 | Function Loss:  -1.8402\n",
      "Total loss:  -0.7196 | PDE Loss:  -1.3349 | Function Loss:  -1.8402\n",
      "Total loss:  -0.7196 | PDE Loss:  -1.3349 | Function Loss:  -1.8402\n",
      "Total loss:  -0.7196 | PDE Loss:  -1.3351 | Function Loss:  -1.8402\n",
      "Total loss:  -0.7197 | PDE Loss:  -1.3354 | Function Loss:  -1.8401\n",
      "Total loss:  -0.7197 | PDE Loss:  -1.3356 | Function Loss:  -1.8401\n",
      "Total loss:  -0.7197 | PDE Loss:  -1.3358 | Function Loss:  -1.8401\n",
      "Total loss:  -0.7197 | PDE Loss:  -1.3359 | Function Loss:  -1.8401\n",
      "Total loss:  -0.7197 | PDE Loss:  -1.336 | Function Loss:  -1.84\n",
      "Total loss:  -0.7198 | PDE Loss:  -1.3361 | Function Loss:  -1.84\n",
      "Total loss:  -0.7198 | PDE Loss:  -1.3362 | Function Loss:  -1.8401\n",
      "Total loss:  -0.7198 | PDE Loss:  -1.3363 | Function Loss:  -1.8401\n",
      "Total loss:  -0.7199 | PDE Loss:  -1.3364 | Function Loss:  -1.8401\n",
      "Total loss:  -0.7199 | PDE Loss:  -1.3364 | Function Loss:  -1.8402\n",
      "Total loss:  -0.72 | PDE Loss:  -1.3364 | Function Loss:  -1.8402\n",
      "Total loss:  -0.72 | PDE Loss:  -1.3364 | Function Loss:  -1.8402\n",
      "Total loss:  -0.72 | PDE Loss:  -1.3367 | Function Loss:  -1.8402\n",
      "Total loss:  -0.72 | PDE Loss:  -1.3366 | Function Loss:  -1.8402\n",
      "Total loss:  -0.7201 | PDE Loss:  -1.3366 | Function Loss:  -1.8403\n",
      "Total loss:  -0.7201 | PDE Loss:  -1.3366 | Function Loss:  -1.8403\n",
      "Total loss:  -0.7201 | PDE Loss:  -1.3367 | Function Loss:  -1.8404\n",
      "Total loss:  -0.7202 | PDE Loss:  -1.3368 | Function Loss:  -1.8404\n",
      "Total loss:  -0.7203 | PDE Loss:  -1.3368 | Function Loss:  -1.8405\n",
      "Total loss:  -0.7203 | PDE Loss:  -1.3369 | Function Loss:  -1.8405\n",
      "Total loss:  -0.7204 | PDE Loss:  -1.3369 | Function Loss:  -1.8406\n",
      "Total loss:  -0.7204 | PDE Loss:  -1.337 | Function Loss:  -1.8406\n",
      "Total loss:  -0.7205 | PDE Loss:  -1.337 | Function Loss:  -1.8407\n",
      "Total loss:  -0.7205 | PDE Loss:  -1.3372 | Function Loss:  -1.8407\n",
      "Total loss:  -0.7206 | PDE Loss:  -1.3374 | Function Loss:  -1.8407\n",
      "Total loss:  -0.7206 | PDE Loss:  -1.3377 | Function Loss:  -1.8407\n",
      "Total loss:  -0.7207 | PDE Loss:  -1.3381 | Function Loss:  -1.8406\n",
      "Total loss:  -0.7205 | PDE Loss:  -1.3383 | Function Loss:  -1.8404\n",
      "Total loss:  -0.7207 | PDE Loss:  -1.3382 | Function Loss:  -1.8406\n",
      "Total loss:  -0.7207 | PDE Loss:  -1.3387 | Function Loss:  -1.8405\n",
      "Total loss:  -0.7208 | PDE Loss:  -1.3394 | Function Loss:  -1.8404\n",
      "Total loss:  -0.7209 | PDE Loss:  -1.3401 | Function Loss:  -1.8402\n",
      "Total loss:  -0.7209 | PDE Loss:  -1.3408 | Function Loss:  -1.8401\n",
      "Total loss:  -0.721 | PDE Loss:  -1.3413 | Function Loss:  -1.84\n",
      "Total loss:  -0.721 | PDE Loss:  -1.3418 | Function Loss:  -1.8399\n",
      "Total loss:  -0.7211 | PDE Loss:  -1.3423 | Function Loss:  -1.8398\n",
      "Total loss:  -0.7211 | PDE Loss:  -1.3426 | Function Loss:  -1.8397\n",
      "Total loss:  -0.7211 | PDE Loss:  -1.343 | Function Loss:  -1.8397\n",
      "Total loss:  -0.7212 | PDE Loss:  -1.3433 | Function Loss:  -1.8397\n",
      "Total loss:  -0.7212 | PDE Loss:  -1.3435 | Function Loss:  -1.8396\n",
      "Total loss:  -0.7213 | PDE Loss:  -1.3436 | Function Loss:  -1.8397\n",
      "Total loss:  -0.7213 | PDE Loss:  -1.3441 | Function Loss:  -1.8396\n",
      "Total loss:  -0.7214 | PDE Loss:  -1.3439 | Function Loss:  -1.8397\n",
      "Total loss:  -0.7214 | PDE Loss:  -1.3441 | Function Loss:  -1.8397\n",
      "Total loss:  -0.7214 | PDE Loss:  -1.3444 | Function Loss:  -1.8396\n",
      "Total loss:  -0.7215 | PDE Loss:  -1.3446 | Function Loss:  -1.8396\n",
      "Total loss:  -0.7215 | PDE Loss:  -1.3449 | Function Loss:  -1.8396\n",
      "Total loss:  -0.7215 | PDE Loss:  -1.3449 | Function Loss:  -1.8396\n",
      "Total loss:  -0.7216 | PDE Loss:  -1.3453 | Function Loss:  -1.8395\n",
      "Total loss:  -0.7216 | PDE Loss:  -1.3453 | Function Loss:  -1.8395\n",
      "Total loss:  -0.7216 | PDE Loss:  -1.3455 | Function Loss:  -1.8395\n",
      "Total loss:  -0.7217 | PDE Loss:  -1.3451 | Function Loss:  -1.8397\n",
      "Total loss:  -0.7217 | PDE Loss:  -1.3453 | Function Loss:  -1.8397\n",
      "Total loss:  -0.7218 | PDE Loss:  -1.3455 | Function Loss:  -1.8397\n",
      "Total loss:  -0.7219 | PDE Loss:  -1.3457 | Function Loss:  -1.8398\n",
      "Total loss:  -0.722 | PDE Loss:  -1.3457 | Function Loss:  -1.8399\n",
      "Total loss:  -0.722 | PDE Loss:  -1.3458 | Function Loss:  -1.8399\n",
      "Total loss:  -0.7221 | PDE Loss:  -1.3459 | Function Loss:  -1.84\n",
      "Total loss:  -0.7222 | PDE Loss:  -1.346 | Function Loss:  -1.8401\n",
      "Total loss:  -0.7222 | PDE Loss:  -1.346 | Function Loss:  -1.8401\n",
      "Total loss:  -0.7223 | PDE Loss:  -1.3459 | Function Loss:  -1.8402\n",
      "Total loss:  -0.7223 | PDE Loss:  -1.346 | Function Loss:  -1.8403\n",
      "Total loss:  -0.7223 | PDE Loss:  -1.346 | Function Loss:  -1.8403\n",
      "Total loss:  -0.7224 | PDE Loss:  -1.346 | Function Loss:  -1.8403\n",
      "Total loss:  -0.7224 | PDE Loss:  -1.3461 | Function Loss:  -1.8403\n",
      "Total loss:  -0.7224 | PDE Loss:  -1.3461 | Function Loss:  -1.8404\n",
      "Total loss:  -0.7224 | PDE Loss:  -1.3461 | Function Loss:  -1.8404\n",
      "Total loss:  -0.7225 | PDE Loss:  -1.3462 | Function Loss:  -1.8404\n",
      "Total loss:  -0.7225 | PDE Loss:  -1.3462 | Function Loss:  -1.8404\n",
      "Total loss:  -0.7225 | PDE Loss:  -1.3461 | Function Loss:  -1.8405\n",
      "Total loss:  -0.7225 | PDE Loss:  -1.3461 | Function Loss:  -1.8405\n",
      "Total loss:  -0.7225 | PDE Loss:  -1.3461 | Function Loss:  -1.8405\n",
      "Total loss:  -0.7226 | PDE Loss:  -1.3461 | Function Loss:  -1.8406\n",
      "Total loss:  -0.7226 | PDE Loss:  -1.3461 | Function Loss:  -1.8406\n",
      "Total loss:  -0.7226 | PDE Loss:  -1.3461 | Function Loss:  -1.8407\n",
      "Total loss:  -0.7227 | PDE Loss:  -1.3461 | Function Loss:  -1.8407\n",
      "Total loss:  -0.7227 | PDE Loss:  -1.3461 | Function Loss:  -1.8407\n",
      "Total loss:  -0.7227 | PDE Loss:  -1.3461 | Function Loss:  -1.8408\n",
      "Total loss:  -0.7227 | PDE Loss:  -1.3462 | Function Loss:  -1.8408\n",
      "Total loss:  -0.7228 | PDE Loss:  -1.3462 | Function Loss:  -1.8408\n",
      "Total loss:  -0.7228 | PDE Loss:  -1.3463 | Function Loss:  -1.8408\n",
      "Total loss:  -0.7228 | PDE Loss:  -1.3463 | Function Loss:  -1.8408\n",
      "Total loss:  -0.7228 | PDE Loss:  -1.3464 | Function Loss:  -1.8408\n",
      "Total loss:  -0.7228 | PDE Loss:  -1.3465 | Function Loss:  -1.8408\n",
      "Total loss:  -0.7229 | PDE Loss:  -1.3466 | Function Loss:  -1.8408\n",
      "Total loss:  -0.7229 | PDE Loss:  -1.3467 | Function Loss:  -1.8408\n",
      "Total loss:  -0.7229 | PDE Loss:  -1.3468 | Function Loss:  -1.8408\n",
      "Total loss:  -0.723 | PDE Loss:  -1.3468 | Function Loss:  -1.8409\n",
      "Total loss:  -0.723 | PDE Loss:  -1.3469 | Function Loss:  -1.8409\n",
      "Total loss:  -0.723 | PDE Loss:  -1.3469 | Function Loss:  -1.8409\n",
      "Total loss:  -0.723 | PDE Loss:  -1.3468 | Function Loss:  -1.841\n",
      "Total loss:  -0.7231 | PDE Loss:  -1.3468 | Function Loss:  -1.841\n",
      "Total loss:  -0.7231 | PDE Loss:  -1.3468 | Function Loss:  -1.841\n",
      "Total loss:  -0.7231 | PDE Loss:  -1.3466 | Function Loss:  -1.8411\n",
      "Total loss:  -0.7231 | PDE Loss:  -1.3467 | Function Loss:  -1.8411\n",
      "Total loss:  -0.7232 | PDE Loss:  -1.3468 | Function Loss:  -1.8411\n",
      "Total loss:  -0.7232 | PDE Loss:  -1.3469 | Function Loss:  -1.8411\n",
      "Total loss:  -0.7232 | PDE Loss:  -1.3469 | Function Loss:  -1.8411\n",
      "Total loss:  -0.7232 | PDE Loss:  -1.347 | Function Loss:  -1.8412\n",
      "Total loss:  -0.7232 | PDE Loss:  -1.347 | Function Loss:  -1.8412\n",
      "Total loss:  -0.7232 | PDE Loss:  -1.3469 | Function Loss:  -1.8412\n",
      "Total loss:  -0.7233 | PDE Loss:  -1.3469 | Function Loss:  -1.8412\n",
      "Total loss:  -0.7233 | PDE Loss:  -1.3468 | Function Loss:  -1.8413\n",
      "Total loss:  -0.7233 | PDE Loss:  -1.3467 | Function Loss:  -1.8414\n",
      "Total loss:  -0.7233 | PDE Loss:  -1.3466 | Function Loss:  -1.8414\n",
      "Total loss:  -0.7234 | PDE Loss:  -1.3464 | Function Loss:  -1.8415\n",
      "Total loss:  -0.7234 | PDE Loss:  -1.3453 | Function Loss:  -1.8419\n",
      "Total loss:  -0.7235 | PDE Loss:  -1.3454 | Function Loss:  -1.842\n",
      "Total loss:  -0.7235 | PDE Loss:  -1.3455 | Function Loss:  -1.842\n",
      "Total loss:  -0.7236 | PDE Loss:  -1.3456 | Function Loss:  -1.8421\n",
      "Total loss:  -0.7236 | PDE Loss:  -1.3456 | Function Loss:  -1.8421\n",
      "Total loss:  -0.7236 | PDE Loss:  -1.3456 | Function Loss:  -1.8421\n",
      "Total loss:  -0.7236 | PDE Loss:  -1.3456 | Function Loss:  -1.8421\n",
      "Total loss:  -0.7237 | PDE Loss:  -1.3456 | Function Loss:  -1.8422\n",
      "Total loss:  -0.7237 | PDE Loss:  -1.3456 | Function Loss:  -1.8422\n",
      "Total loss:  -0.7237 | PDE Loss:  -1.3457 | Function Loss:  -1.8422\n",
      "Total loss:  -0.7237 | PDE Loss:  -1.3458 | Function Loss:  -1.8422\n",
      "Total loss:  -0.7238 | PDE Loss:  -1.3464 | Function Loss:  -1.842\n",
      "Total loss:  -0.7238 | PDE Loss:  -1.3465 | Function Loss:  -1.8421\n",
      "Total loss:  -0.7238 | PDE Loss:  -1.3465 | Function Loss:  -1.8421\n",
      "Total loss:  -0.7238 | PDE Loss:  -1.3467 | Function Loss:  -1.8421\n",
      "Total loss:  -0.7239 | PDE Loss:  -1.3469 | Function Loss:  -1.842\n",
      "Total loss:  -0.7239 | PDE Loss:  -1.3472 | Function Loss:  -1.842\n",
      "Total loss:  -0.7239 | PDE Loss:  -1.3475 | Function Loss:  -1.8419\n",
      "Total loss:  -0.724 | PDE Loss:  -1.3478 | Function Loss:  -1.8419\n",
      "Total loss:  -0.724 | PDE Loss:  -1.3487 | Function Loss:  -1.8416\n",
      "Total loss:  -0.724 | PDE Loss:  -1.3488 | Function Loss:  -1.8416\n",
      "Total loss:  -0.724 | PDE Loss:  -1.3487 | Function Loss:  -1.8417\n",
      "Total loss:  -0.7241 | PDE Loss:  -1.3486 | Function Loss:  -1.8418\n",
      "Total loss:  -0.7241 | PDE Loss:  -1.3484 | Function Loss:  -1.8419\n",
      "Total loss:  -0.7242 | PDE Loss:  -1.3481 | Function Loss:  -1.842\n",
      "Total loss:  -0.7242 | PDE Loss:  -1.3481 | Function Loss:  -1.8421\n",
      "Total loss:  -0.7242 | PDE Loss:  -1.3472 | Function Loss:  -1.8424\n",
      "Total loss:  -0.7242 | PDE Loss:  -1.3475 | Function Loss:  -1.8423\n",
      "Total loss:  -0.7243 | PDE Loss:  -1.3478 | Function Loss:  -1.8423\n",
      "Total loss:  -0.7243 | PDE Loss:  -1.3481 | Function Loss:  -1.8422\n",
      "Total loss:  -0.7244 | PDE Loss:  -1.3484 | Function Loss:  -1.8422\n",
      "Total loss:  -0.7244 | PDE Loss:  -1.3487 | Function Loss:  -1.8422\n",
      "Total loss:  -0.7244 | PDE Loss:  -1.3489 | Function Loss:  -1.8421\n",
      "Total loss:  -0.7244 | PDE Loss:  -1.3492 | Function Loss:  -1.8421\n",
      "Total loss:  -0.7245 | PDE Loss:  -1.3494 | Function Loss:  -1.842\n",
      "Total loss:  -0.7245 | PDE Loss:  -1.3497 | Function Loss:  -1.842\n",
      "Total loss:  -0.7245 | PDE Loss:  -1.3498 | Function Loss:  -1.842\n",
      "Total loss:  -0.7246 | PDE Loss:  -1.3501 | Function Loss:  -1.8419\n",
      "Total loss:  -0.7246 | PDE Loss:  -1.3499 | Function Loss:  -1.842\n",
      "Total loss:  -0.7246 | PDE Loss:  -1.3497 | Function Loss:  -1.8421\n",
      "Total loss:  -0.7247 | PDE Loss:  -1.3496 | Function Loss:  -1.8422\n",
      "Total loss:  -0.7247 | PDE Loss:  -1.3494 | Function Loss:  -1.8424\n",
      "Total loss:  -0.7247 | PDE Loss:  -1.3486 | Function Loss:  -1.8426\n",
      "Total loss:  -0.7247 | PDE Loss:  -1.3491 | Function Loss:  -1.8425\n",
      "Total loss:  -0.7248 | PDE Loss:  -1.3493 | Function Loss:  -1.8425\n",
      "Total loss:  -0.7248 | PDE Loss:  -1.3492 | Function Loss:  -1.8426\n",
      "Total loss:  -0.7249 | PDE Loss:  -1.3492 | Function Loss:  -1.8426\n",
      "Total loss:  -0.7249 | PDE Loss:  -1.3494 | Function Loss:  -1.8426\n",
      "Total loss:  -0.7249 | PDE Loss:  -1.3495 | Function Loss:  -1.8426\n",
      "Total loss:  -0.725 | PDE Loss:  -1.3496 | Function Loss:  -1.8426\n",
      "Total loss:  -0.725 | PDE Loss:  -1.3496 | Function Loss:  -1.8427\n",
      "Total loss:  -0.7248 | PDE Loss:  -1.3488 | Function Loss:  -1.8426\n",
      "Total loss:  -0.725 | PDE Loss:  -1.3496 | Function Loss:  -1.8427\n",
      "Total loss:  -0.7251 | PDE Loss:  -1.3497 | Function Loss:  -1.8428\n",
      "Total loss:  -0.7251 | PDE Loss:  -1.3497 | Function Loss:  -1.8428\n",
      "Total loss:  -0.7251 | PDE Loss:  -1.3497 | Function Loss:  -1.8428\n",
      "Total loss:  -0.7252 | PDE Loss:  -1.3497 | Function Loss:  -1.8429\n",
      "Total loss:  -0.7252 | PDE Loss:  -1.3497 | Function Loss:  -1.8429\n",
      "Total loss:  -0.7252 | PDE Loss:  -1.3496 | Function Loss:  -1.8429\n",
      "Total loss:  -0.7252 | PDE Loss:  -1.3496 | Function Loss:  -1.843\n",
      "Total loss:  -0.7253 | PDE Loss:  -1.3498 | Function Loss:  -1.843\n",
      "Total loss:  -0.7253 | PDE Loss:  -1.3502 | Function Loss:  -1.8429\n",
      "Total loss:  -0.7254 | PDE Loss:  -1.3502 | Function Loss:  -1.843\n",
      "Total loss:  -0.7254 | PDE Loss:  -1.3504 | Function Loss:  -1.843\n",
      "Total loss:  -0.7255 | PDE Loss:  -1.3507 | Function Loss:  -1.843\n",
      "Total loss:  -0.7255 | PDE Loss:  -1.3509 | Function Loss:  -1.8429\n",
      "Total loss:  -0.7255 | PDE Loss:  -1.3511 | Function Loss:  -1.8429\n",
      "Total loss:  -0.7256 | PDE Loss:  -1.3513 | Function Loss:  -1.8429\n",
      "Total loss:  -0.7256 | PDE Loss:  -1.3514 | Function Loss:  -1.8429\n",
      "Total loss:  -0.7257 | PDE Loss:  -1.3515 | Function Loss:  -1.843\n",
      "Total loss:  -0.7257 | PDE Loss:  -1.3515 | Function Loss:  -1.843\n",
      "Total loss:  -0.7258 | PDE Loss:  -1.3513 | Function Loss:  -1.8432\n",
      "Total loss:  -0.7258 | PDE Loss:  -1.3511 | Function Loss:  -1.8433\n",
      "Total loss:  -0.7259 | PDE Loss:  -1.3507 | Function Loss:  -1.8435\n",
      "Total loss:  -0.7259 | PDE Loss:  -1.3506 | Function Loss:  -1.8436\n",
      "Total loss:  -0.726 | PDE Loss:  -1.3503 | Function Loss:  -1.8437\n",
      "Total loss:  -0.726 | PDE Loss:  -1.3503 | Function Loss:  -1.8438\n",
      "Total loss:  -0.7261 | PDE Loss:  -1.3503 | Function Loss:  -1.8439\n",
      "Total loss:  -0.7261 | PDE Loss:  -1.3503 | Function Loss:  -1.8439\n",
      "Total loss:  -0.7261 | PDE Loss:  -1.3504 | Function Loss:  -1.8439\n",
      "Total loss:  -0.7261 | PDE Loss:  -1.3506 | Function Loss:  -1.8439\n",
      "Total loss:  -0.7262 | PDE Loss:  -1.3509 | Function Loss:  -1.8438\n",
      "Total loss:  -0.7262 | PDE Loss:  -1.3511 | Function Loss:  -1.8438\n",
      "Total loss:  -0.7262 | PDE Loss:  -1.3521 | Function Loss:  -1.8435\n",
      "Total loss:  -0.7263 | PDE Loss:  -1.3516 | Function Loss:  -1.8437\n",
      "Total loss:  -0.7263 | PDE Loss:  -1.3518 | Function Loss:  -1.8437\n",
      "Total loss:  -0.7263 | PDE Loss:  -1.3519 | Function Loss:  -1.8437\n",
      "Total loss:  -0.7264 | PDE Loss:  -1.3519 | Function Loss:  -1.8437\n",
      "Total loss:  -0.7264 | PDE Loss:  -1.3519 | Function Loss:  -1.8438\n",
      "Total loss:  -0.7264 | PDE Loss:  -1.3518 | Function Loss:  -1.8438\n",
      "Total loss:  -0.7264 | PDE Loss:  -1.3518 | Function Loss:  -1.8438\n",
      "Total loss:  -0.7264 | PDE Loss:  -1.3517 | Function Loss:  -1.8439\n",
      "Total loss:  -0.7265 | PDE Loss:  -1.3517 | Function Loss:  -1.844\n",
      "Total loss:  -0.7265 | PDE Loss:  -1.3516 | Function Loss:  -1.8441\n",
      "Total loss:  -0.7266 | PDE Loss:  -1.3517 | Function Loss:  -1.8441\n",
      "Total loss:  -0.7266 | PDE Loss:  -1.352 | Function Loss:  -1.8441\n",
      "Total loss:  -0.7267 | PDE Loss:  -1.3524 | Function Loss:  -1.844\n",
      "Total loss:  -0.7268 | PDE Loss:  -1.3528 | Function Loss:  -1.844\n",
      "Total loss:  -0.7268 | PDE Loss:  -1.3533 | Function Loss:  -1.8439\n",
      "Total loss:  -0.7268 | PDE Loss:  -1.3534 | Function Loss:  -1.8439\n",
      "Total loss:  -0.7269 | PDE Loss:  -1.3539 | Function Loss:  -1.8438\n",
      "Total loss:  -0.7269 | PDE Loss:  -1.3538 | Function Loss:  -1.8439\n",
      "Total loss:  -0.727 | PDE Loss:  -1.3539 | Function Loss:  -1.8439\n",
      "Total loss:  -0.727 | PDE Loss:  -1.3538 | Function Loss:  -1.844\n",
      "Total loss:  -0.7271 | PDE Loss:  -1.3538 | Function Loss:  -1.8441\n",
      "Total loss:  -0.7271 | PDE Loss:  -1.3537 | Function Loss:  -1.8442\n",
      "Total loss:  -0.7271 | PDE Loss:  -1.3537 | Function Loss:  -1.8442\n",
      "Total loss:  -0.7272 | PDE Loss:  -1.3538 | Function Loss:  -1.8442\n",
      "Total loss:  -0.7273 | PDE Loss:  -1.3541 | Function Loss:  -1.8442\n",
      "Total loss:  -0.7273 | PDE Loss:  -1.3545 | Function Loss:  -1.8442\n",
      "Total loss:  -0.7274 | PDE Loss:  -1.3549 | Function Loss:  -1.8442\n",
      "Total loss:  -0.7274 | PDE Loss:  -1.3552 | Function Loss:  -1.8441\n",
      "Total loss:  -0.7274 | PDE Loss:  -1.3558 | Function Loss:  -1.844\n",
      "Total loss:  -0.7275 | PDE Loss:  -1.3555 | Function Loss:  -1.8441\n",
      "Total loss:  -0.7275 | PDE Loss:  -1.356 | Function Loss:  -1.844\n",
      "Total loss:  -0.7275 | PDE Loss:  -1.3564 | Function Loss:  -1.8439\n",
      "Total loss:  -0.7276 | PDE Loss:  -1.3567 | Function Loss:  -1.8439\n",
      "Total loss:  -0.7276 | PDE Loss:  -1.3571 | Function Loss:  -1.8438\n",
      "Total loss:  -0.7277 | PDE Loss:  -1.3574 | Function Loss:  -1.8438\n",
      "Total loss:  -0.7277 | PDE Loss:  -1.3575 | Function Loss:  -1.8438\n",
      "Total loss:  -0.7278 | PDE Loss:  -1.3579 | Function Loss:  -1.8438\n",
      "Total loss:  -0.7279 | PDE Loss:  -1.3579 | Function Loss:  -1.8439\n",
      "Total loss:  -0.7279 | PDE Loss:  -1.3579 | Function Loss:  -1.844\n",
      "Total loss:  -0.728 | PDE Loss:  -1.3579 | Function Loss:  -1.844\n",
      "Total loss:  -0.7281 | PDE Loss:  -1.358 | Function Loss:  -1.8441\n",
      "Total loss:  -0.7281 | PDE Loss:  -1.3581 | Function Loss:  -1.8442\n",
      "Total loss:  -0.7282 | PDE Loss:  -1.3583 | Function Loss:  -1.8442\n",
      "Total loss:  -0.7282 | PDE Loss:  -1.3587 | Function Loss:  -1.8441\n",
      "Total loss:  -0.7283 | PDE Loss:  -1.359 | Function Loss:  -1.8441\n",
      "Total loss:  -0.7283 | PDE Loss:  -1.3593 | Function Loss:  -1.844\n",
      "Total loss:  -0.7284 | PDE Loss:  -1.3597 | Function Loss:  -1.844\n",
      "Total loss:  -0.7284 | PDE Loss:  -1.36 | Function Loss:  -1.8439\n",
      "Total loss:  -0.7284 | PDE Loss:  -1.3603 | Function Loss:  -1.8438\n",
      "Total loss:  -0.7284 | PDE Loss:  -1.3605 | Function Loss:  -1.8438\n",
      "Total loss:  -0.7285 | PDE Loss:  -1.3607 | Function Loss:  -1.8438\n",
      "Total loss:  -0.7285 | PDE Loss:  -1.3609 | Function Loss:  -1.8438\n",
      "Total loss:  -0.7285 | PDE Loss:  -1.361 | Function Loss:  -1.8438\n",
      "Total loss:  -0.7286 | PDE Loss:  -1.3611 | Function Loss:  -1.8438\n",
      "Total loss:  -0.7286 | PDE Loss:  -1.3611 | Function Loss:  -1.8439\n",
      "Total loss:  -0.7286 | PDE Loss:  -1.361 | Function Loss:  -1.8439\n",
      "Total loss:  -0.7287 | PDE Loss:  -1.3609 | Function Loss:  -1.844\n",
      "Total loss:  -0.7287 | PDE Loss:  -1.3608 | Function Loss:  -1.8441\n",
      "Total loss:  -0.7287 | PDE Loss:  -1.3606 | Function Loss:  -1.8441\n",
      "Total loss:  -0.7287 | PDE Loss:  -1.3606 | Function Loss:  -1.8442\n",
      "Total loss:  -0.7288 | PDE Loss:  -1.3603 | Function Loss:  -1.8443\n",
      "Total loss:  -0.7285 | PDE Loss:  -1.3608 | Function Loss:  -1.8438\n",
      "Total loss:  -0.7288 | PDE Loss:  -1.3604 | Function Loss:  -1.8443\n",
      "Total loss:  -0.7288 | PDE Loss:  -1.3605 | Function Loss:  -1.8443\n",
      "Total loss:  -0.7288 | PDE Loss:  -1.3607 | Function Loss:  -1.8443\n",
      "Total loss:  -0.7288 | PDE Loss:  -1.3609 | Function Loss:  -1.8442\n",
      "Total loss:  -0.7289 | PDE Loss:  -1.3611 | Function Loss:  -1.8442\n",
      "Total loss:  -0.7289 | PDE Loss:  -1.3612 | Function Loss:  -1.8442\n",
      "Total loss:  -0.7289 | PDE Loss:  -1.3613 | Function Loss:  -1.8442\n",
      "Total loss:  -0.7289 | PDE Loss:  -1.3609 | Function Loss:  -1.8443\n",
      "Total loss:  -0.729 | PDE Loss:  -1.3612 | Function Loss:  -1.8443\n",
      "Total loss:  -0.729 | PDE Loss:  -1.3612 | Function Loss:  -1.8443\n",
      "Total loss:  -0.729 | PDE Loss:  -1.361 | Function Loss:  -1.8444\n",
      "Total loss:  -0.729 | PDE Loss:  -1.3608 | Function Loss:  -1.8445\n",
      "Total loss:  -0.7291 | PDE Loss:  -1.3607 | Function Loss:  -1.8446\n",
      "Total loss:  -0.7291 | PDE Loss:  -1.3605 | Function Loss:  -1.8446\n",
      "Total loss:  -0.7291 | PDE Loss:  -1.3604 | Function Loss:  -1.8447\n",
      "Total loss:  -0.7291 | PDE Loss:  -1.3603 | Function Loss:  -1.8447\n",
      "Total loss:  -0.7291 | PDE Loss:  -1.3603 | Function Loss:  -1.8448\n",
      "Total loss:  -0.7292 | PDE Loss:  -1.3604 | Function Loss:  -1.8448\n",
      "Total loss:  -0.7292 | PDE Loss:  -1.3607 | Function Loss:  -1.8448\n",
      "Total loss:  -0.7291 | PDE Loss:  -1.3617 | Function Loss:  -1.8443\n",
      "Total loss:  -0.7293 | PDE Loss:  -1.3613 | Function Loss:  -1.8446\n",
      "Total loss:  -0.7293 | PDE Loss:  -1.3615 | Function Loss:  -1.8446\n",
      "Total loss:  -0.7293 | PDE Loss:  -1.3617 | Function Loss:  -1.8446\n",
      "Total loss:  -0.7293 | PDE Loss:  -1.3621 | Function Loss:  -1.8445\n",
      "Total loss:  -0.7294 | PDE Loss:  -1.3624 | Function Loss:  -1.8445\n",
      "Total loss:  -0.7294 | PDE Loss:  -1.3627 | Function Loss:  -1.8444\n",
      "Total loss:  -0.7294 | PDE Loss:  -1.363 | Function Loss:  -1.8443\n",
      "Total loss:  -0.7294 | PDE Loss:  -1.3635 | Function Loss:  -1.8442\n",
      "Total loss:  -0.7295 | PDE Loss:  -1.3637 | Function Loss:  -1.8442\n",
      "Total loss:  -0.7295 | PDE Loss:  -1.3638 | Function Loss:  -1.8442\n",
      "Total loss:  -0.7296 | PDE Loss:  -1.3639 | Function Loss:  -1.8442\n",
      "Total loss:  -0.7296 | PDE Loss:  -1.3644 | Function Loss:  -1.8441\n",
      "Total loss:  -0.7296 | PDE Loss:  -1.3641 | Function Loss:  -1.8442\n",
      "Total loss:  -0.7296 | PDE Loss:  -1.364 | Function Loss:  -1.8443\n",
      "Total loss:  -0.7297 | PDE Loss:  -1.3639 | Function Loss:  -1.8444\n",
      "Total loss:  -0.7297 | PDE Loss:  -1.364 | Function Loss:  -1.8444\n",
      "Total loss:  -0.7297 | PDE Loss:  -1.364 | Function Loss:  -1.8444\n",
      "Total loss:  -0.7298 | PDE Loss:  -1.3642 | Function Loss:  -1.8444\n",
      "Total loss:  -0.7298 | PDE Loss:  -1.3643 | Function Loss:  -1.8444\n",
      "Total loss:  -0.7298 | PDE Loss:  -1.3654 | Function Loss:  -1.8441\n",
      "Total loss:  -0.7298 | PDE Loss:  -1.3648 | Function Loss:  -1.8443\n",
      "Total loss:  -0.7298 | PDE Loss:  -1.365 | Function Loss:  -1.8443\n",
      "Total loss:  -0.7299 | PDE Loss:  -1.3653 | Function Loss:  -1.8443\n",
      "Total loss:  -0.7299 | PDE Loss:  -1.3656 | Function Loss:  -1.8442\n",
      "Total loss:  -0.73 | PDE Loss:  -1.3656 | Function Loss:  -1.8443\n",
      "Total loss:  -0.73 | PDE Loss:  -1.3663 | Function Loss:  -1.8441\n",
      "Total loss:  -0.73 | PDE Loss:  -1.3661 | Function Loss:  -1.8442\n",
      "Total loss:  -0.7301 | PDE Loss:  -1.366 | Function Loss:  -1.8443\n",
      "Total loss:  -0.7301 | PDE Loss:  -1.3658 | Function Loss:  -1.8444\n",
      "Total loss:  -0.7302 | PDE Loss:  -1.3655 | Function Loss:  -1.8446\n",
      "Total loss:  -0.7302 | PDE Loss:  -1.3651 | Function Loss:  -1.8447\n",
      "Total loss:  -0.7303 | PDE Loss:  -1.3648 | Function Loss:  -1.8449\n",
      "Total loss:  -0.7303 | PDE Loss:  -1.3646 | Function Loss:  -1.845\n",
      "Total loss:  -0.7303 | PDE Loss:  -1.3643 | Function Loss:  -1.8451\n",
      "Total loss:  -0.7303 | PDE Loss:  -1.3645 | Function Loss:  -1.8451\n",
      "Total loss:  -0.7304 | PDE Loss:  -1.3644 | Function Loss:  -1.8452\n",
      "Total loss:  -0.7304 | PDE Loss:  -1.3643 | Function Loss:  -1.8453\n",
      "Total loss:  -0.7305 | PDE Loss:  -1.3643 | Function Loss:  -1.8453\n",
      "Total loss:  -0.7305 | PDE Loss:  -1.3644 | Function Loss:  -1.8454\n",
      "Total loss:  -0.7306 | PDE Loss:  -1.3647 | Function Loss:  -1.8454\n",
      "Total loss:  -0.7306 | PDE Loss:  -1.3646 | Function Loss:  -1.8454\n",
      "Total loss:  -0.7307 | PDE Loss:  -1.3649 | Function Loss:  -1.8454\n",
      "Total loss:  -0.7308 | PDE Loss:  -1.3652 | Function Loss:  -1.8454\n",
      "Total loss:  -0.7308 | PDE Loss:  -1.3655 | Function Loss:  -1.8454\n",
      "Total loss:  -0.7309 | PDE Loss:  -1.3657 | Function Loss:  -1.8455\n",
      "Total loss:  -0.731 | PDE Loss:  -1.3657 | Function Loss:  -1.8456\n",
      "Total loss:  -0.7311 | PDE Loss:  -1.3656 | Function Loss:  -1.8457\n",
      "Total loss:  -0.7311 | PDE Loss:  -1.3654 | Function Loss:  -1.8459\n",
      "Total loss:  -0.7312 | PDE Loss:  -1.3651 | Function Loss:  -1.8461\n",
      "Total loss:  -0.7313 | PDE Loss:  -1.3648 | Function Loss:  -1.8463\n",
      "Total loss:  -0.7312 | PDE Loss:  -1.3621 | Function Loss:  -1.8469\n",
      "Total loss:  -0.7314 | PDE Loss:  -1.3641 | Function Loss:  -1.8466\n",
      "Total loss:  -0.7315 | PDE Loss:  -1.3642 | Function Loss:  -1.8466\n",
      "Total loss:  -0.7316 | PDE Loss:  -1.3644 | Function Loss:  -1.8467\n",
      "Total loss:  -0.7317 | PDE Loss:  -1.3646 | Function Loss:  -1.8468\n",
      "Total loss:  -0.7317 | PDE Loss:  -1.3649 | Function Loss:  -1.8468\n",
      "Total loss:  -0.7318 | PDE Loss:  -1.3651 | Function Loss:  -1.8468\n",
      "Total loss:  -0.7317 | PDE Loss:  -1.3651 | Function Loss:  -1.8466\n",
      "Total loss:  -0.7318 | PDE Loss:  -1.3652 | Function Loss:  -1.8467\n",
      "Total loss:  -0.7318 | PDE Loss:  -1.3653 | Function Loss:  -1.8467\n",
      "Total loss:  -0.7318 | PDE Loss:  -1.3654 | Function Loss:  -1.8467\n",
      "Total loss:  -0.7319 | PDE Loss:  -1.3655 | Function Loss:  -1.8468\n",
      "Total loss:  -0.7319 | PDE Loss:  -1.3655 | Function Loss:  -1.8468\n",
      "Total loss:  -0.7319 | PDE Loss:  -1.3655 | Function Loss:  -1.8469\n",
      "Total loss:  -0.732 | PDE Loss:  -1.3654 | Function Loss:  -1.8469\n",
      "Total loss:  -0.732 | PDE Loss:  -1.3653 | Function Loss:  -1.847\n",
      "Total loss:  -0.732 | PDE Loss:  -1.3653 | Function Loss:  -1.847\n",
      "Total loss:  -0.7321 | PDE Loss:  -1.3654 | Function Loss:  -1.8471\n",
      "Total loss:  -0.7321 | PDE Loss:  -1.3655 | Function Loss:  -1.8471\n",
      "Total loss:  -0.7322 | PDE Loss:  -1.366 | Function Loss:  -1.847\n",
      "Total loss:  -0.7323 | PDE Loss:  -1.3661 | Function Loss:  -1.8471\n",
      "Total loss:  -0.7323 | PDE Loss:  -1.3664 | Function Loss:  -1.8471\n",
      "Total loss:  -0.7323 | PDE Loss:  -1.3665 | Function Loss:  -1.8471\n",
      "Total loss:  -0.7324 | PDE Loss:  -1.3668 | Function Loss:  -1.8471\n",
      "Total loss:  -0.7324 | PDE Loss:  -1.367 | Function Loss:  -1.8471\n",
      "Total loss:  -0.7325 | PDE Loss:  -1.3672 | Function Loss:  -1.8471\n",
      "Total loss:  -0.7325 | PDE Loss:  -1.3674 | Function Loss:  -1.847\n",
      "Total loss:  -0.7325 | PDE Loss:  -1.3676 | Function Loss:  -1.847\n",
      "Total loss:  -0.7326 | PDE Loss:  -1.3678 | Function Loss:  -1.847\n",
      "Total loss:  -0.7326 | PDE Loss:  -1.3681 | Function Loss:  -1.847\n",
      "Total loss:  -0.7327 | PDE Loss:  -1.3681 | Function Loss:  -1.847\n",
      "Total loss:  -0.7327 | PDE Loss:  -1.3683 | Function Loss:  -1.8471\n",
      "Total loss:  -0.7328 | PDE Loss:  -1.3684 | Function Loss:  -1.8471\n",
      "Total loss:  -0.7328 | PDE Loss:  -1.3685 | Function Loss:  -1.8471\n",
      "Total loss:  -0.7329 | PDE Loss:  -1.3686 | Function Loss:  -1.8472\n",
      "Total loss:  -0.733 | PDE Loss:  -1.3685 | Function Loss:  -1.8473\n",
      "Total loss:  -0.733 | PDE Loss:  -1.3686 | Function Loss:  -1.8473\n",
      "Total loss:  -0.7331 | PDE Loss:  -1.3685 | Function Loss:  -1.8475\n",
      "Total loss:  -0.7332 | PDE Loss:  -1.3686 | Function Loss:  -1.8476\n",
      "Total loss:  -0.7333 | PDE Loss:  -1.3688 | Function Loss:  -1.8476\n",
      "Total loss:  -0.7334 | PDE Loss:  -1.3688 | Function Loss:  -1.8477\n",
      "Total loss:  -0.7334 | PDE Loss:  -1.3689 | Function Loss:  -1.8478\n",
      "Total loss:  -0.7335 | PDE Loss:  -1.369 | Function Loss:  -1.8478\n",
      "Total loss:  -0.7336 | PDE Loss:  -1.369 | Function Loss:  -1.8479\n",
      "Total loss:  -0.7336 | PDE Loss:  -1.3692 | Function Loss:  -1.848\n",
      "Total loss:  -0.7337 | PDE Loss:  -1.3692 | Function Loss:  -1.848\n",
      "Total loss:  -0.7337 | PDE Loss:  -1.3692 | Function Loss:  -1.8481\n",
      "Total loss:  -0.7339 | PDE Loss:  -1.3689 | Function Loss:  -1.8483\n",
      "Total loss:  -0.734 | PDE Loss:  -1.3687 | Function Loss:  -1.8486\n",
      "Total loss:  -0.7341 | PDE Loss:  -1.3683 | Function Loss:  -1.8488\n",
      "Total loss:  -0.7342 | PDE Loss:  -1.368 | Function Loss:  -1.8491\n",
      "Total loss:  -0.7343 | PDE Loss:  -1.3676 | Function Loss:  -1.8494\n",
      "Total loss:  -0.7344 | PDE Loss:  -1.3675 | Function Loss:  -1.8495\n",
      "Total loss:  -0.7345 | PDE Loss:  -1.3676 | Function Loss:  -1.8496\n",
      "Total loss:  -0.7346 | PDE Loss:  -1.3678 | Function Loss:  -1.8497\n",
      "Total loss:  -0.7347 | PDE Loss:  -1.368 | Function Loss:  -1.8498\n",
      "Total loss:  -0.7348 | PDE Loss:  -1.3682 | Function Loss:  -1.8498\n",
      "Total loss:  -0.7349 | PDE Loss:  -1.3683 | Function Loss:  -1.8499\n",
      "Total loss:  -0.735 | PDE Loss:  -1.3686 | Function Loss:  -1.8499\n",
      "Total loss:  -0.7351 | PDE Loss:  -1.3688 | Function Loss:  -1.85\n",
      "Total loss:  -0.7352 | PDE Loss:  -1.369 | Function Loss:  -1.85\n",
      "Total loss:  -0.7353 | PDE Loss:  -1.3692 | Function Loss:  -1.8501\n",
      "Total loss:  -0.7354 | PDE Loss:  -1.3694 | Function Loss:  -1.8502\n",
      "Total loss:  -0.7355 | PDE Loss:  -1.3694 | Function Loss:  -1.8503\n",
      "Total loss:  -0.7355 | PDE Loss:  -1.3694 | Function Loss:  -1.8504\n",
      "Total loss:  -0.7356 | PDE Loss:  -1.3694 | Function Loss:  -1.8505\n",
      "Total loss:  -0.7357 | PDE Loss:  -1.3695 | Function Loss:  -1.8506\n",
      "Total loss:  -0.7358 | PDE Loss:  -1.3696 | Function Loss:  -1.8507\n",
      "Total loss:  -0.736 | PDE Loss:  -1.3699 | Function Loss:  -1.8508\n",
      "Total loss:  -0.7361 | PDE Loss:  -1.3701 | Function Loss:  -1.8509\n",
      "Total loss:  -0.7361 | PDE Loss:  -1.3706 | Function Loss:  -1.8508\n",
      "Total loss:  -0.7362 | PDE Loss:  -1.3708 | Function Loss:  -1.8508\n",
      "Total loss:  -0.7363 | PDE Loss:  -1.3708 | Function Loss:  -1.8509\n",
      "Total loss:  -0.7363 | PDE Loss:  -1.3709 | Function Loss:  -1.8509\n",
      "Total loss:  -0.7363 | PDE Loss:  -1.3709 | Function Loss:  -1.851\n",
      "Total loss:  -0.7364 | PDE Loss:  -1.3708 | Function Loss:  -1.851\n",
      "Total loss:  -0.7364 | PDE Loss:  -1.3707 | Function Loss:  -1.8512\n",
      "Total loss:  -0.734 | PDE Loss:  -1.3659 | Function Loss:  -1.8495\n",
      "Total loss:  -0.7365 | PDE Loss:  -1.3708 | Function Loss:  -1.8512\n",
      "Total loss:  -0.7365 | PDE Loss:  -1.3706 | Function Loss:  -1.8513\n",
      "Total loss:  -0.7366 | PDE Loss:  -1.3704 | Function Loss:  -1.8515\n",
      "Total loss:  -0.7367 | PDE Loss:  -1.3703 | Function Loss:  -1.8516\n",
      "Total loss:  -0.7368 | PDE Loss:  -1.37 | Function Loss:  -1.8518\n",
      "Total loss:  -0.7368 | PDE Loss:  -1.3699 | Function Loss:  -1.8519\n",
      "Total loss:  -0.7369 | PDE Loss:  -1.3698 | Function Loss:  -1.852\n",
      "Total loss:  -0.7369 | PDE Loss:  -1.3698 | Function Loss:  -1.852\n",
      "Total loss:  -0.737 | PDE Loss:  -1.3698 | Function Loss:  -1.8521\n",
      "Total loss:  -0.737 | PDE Loss:  -1.3698 | Function Loss:  -1.8522\n",
      "Total loss:  -0.7371 | PDE Loss:  -1.3699 | Function Loss:  -1.8523\n",
      "Total loss:  -0.7369 | PDE Loss:  -1.3682 | Function Loss:  -1.8525\n",
      "Total loss:  -0.7371 | PDE Loss:  -1.3697 | Function Loss:  -1.8523\n",
      "Total loss:  -0.7372 | PDE Loss:  -1.3698 | Function Loss:  -1.8523\n",
      "Total loss:  -0.7372 | PDE Loss:  -1.37 | Function Loss:  -1.8524\n",
      "Total loss:  -0.7373 | PDE Loss:  -1.37 | Function Loss:  -1.8525\n",
      "Total loss:  -0.7373 | PDE Loss:  -1.37 | Function Loss:  -1.8525\n",
      "Total loss:  -0.7374 | PDE Loss:  -1.3699 | Function Loss:  -1.8526\n",
      "Total loss:  -0.7374 | PDE Loss:  -1.37 | Function Loss:  -1.8526\n",
      "Total loss:  -0.7374 | PDE Loss:  -1.3699 | Function Loss:  -1.8527\n",
      "Total loss:  -0.7375 | PDE Loss:  -1.3698 | Function Loss:  -1.8528\n",
      "Total loss:  -0.7375 | PDE Loss:  -1.3696 | Function Loss:  -1.8529\n",
      "Total loss:  -0.7372 | PDE Loss:  -1.369 | Function Loss:  -1.8527\n",
      "Total loss:  -0.7376 | PDE Loss:  -1.3696 | Function Loss:  -1.8529\n",
      "Total loss:  -0.7376 | PDE Loss:  -1.3694 | Function Loss:  -1.8531\n",
      "Total loss:  -0.7377 | PDE Loss:  -1.3692 | Function Loss:  -1.8532\n",
      "Total loss:  -0.7377 | PDE Loss:  -1.3691 | Function Loss:  -1.8533\n",
      "Total loss:  -0.7378 | PDE Loss:  -1.369 | Function Loss:  -1.8534\n",
      "Total loss:  -0.7378 | PDE Loss:  -1.369 | Function Loss:  -1.8534\n",
      "Total loss:  -0.7378 | PDE Loss:  -1.3691 | Function Loss:  -1.8535\n",
      "Total loss:  -0.7379 | PDE Loss:  -1.3692 | Function Loss:  -1.8535\n",
      "Total loss:  -0.7379 | PDE Loss:  -1.3694 | Function Loss:  -1.8535\n",
      "Total loss:  -0.738 | PDE Loss:  -1.3697 | Function Loss:  -1.8535\n",
      "Total loss:  -0.738 | PDE Loss:  -1.3699 | Function Loss:  -1.8534\n",
      "Total loss:  -0.7381 | PDE Loss:  -1.3702 | Function Loss:  -1.8534\n",
      "Total loss:  -0.7382 | PDE Loss:  -1.3704 | Function Loss:  -1.8535\n",
      "Total loss:  -0.7382 | PDE Loss:  -1.3706 | Function Loss:  -1.8535\n",
      "Total loss:  -0.7383 | PDE Loss:  -1.3707 | Function Loss:  -1.8536\n",
      "Total loss:  -0.7383 | PDE Loss:  -1.3708 | Function Loss:  -1.8536\n",
      "Total loss:  -0.7384 | PDE Loss:  -1.3709 | Function Loss:  -1.8537\n",
      "Total loss:  -0.7385 | PDE Loss:  -1.3707 | Function Loss:  -1.8538\n",
      "Total loss:  -0.7385 | PDE Loss:  -1.3709 | Function Loss:  -1.8538\n",
      "Total loss:  -0.7386 | PDE Loss:  -1.3707 | Function Loss:  -1.854\n",
      "Total loss:  -0.7387 | PDE Loss:  -1.3705 | Function Loss:  -1.8541\n",
      "Total loss:  -0.7388 | PDE Loss:  -1.3703 | Function Loss:  -1.8543\n",
      "Total loss:  -0.7389 | PDE Loss:  -1.3702 | Function Loss:  -1.8545\n",
      "Total loss:  -0.739 | PDE Loss:  -1.3701 | Function Loss:  -1.8546\n",
      "Total loss:  -0.7391 | PDE Loss:  -1.3702 | Function Loss:  -1.8547\n",
      "Total loss:  -0.7391 | PDE Loss:  -1.3703 | Function Loss:  -1.8548\n",
      "Total loss:  -0.7392 | PDE Loss:  -1.3707 | Function Loss:  -1.8547\n",
      "Total loss:  -0.7392 | PDE Loss:  -1.371 | Function Loss:  -1.8547\n",
      "Total loss:  -0.7393 | PDE Loss:  -1.3714 | Function Loss:  -1.8547\n",
      "Total loss:  -0.7393 | PDE Loss:  -1.3716 | Function Loss:  -1.8546\n",
      "Total loss:  -0.7394 | PDE Loss:  -1.372 | Function Loss:  -1.8546\n",
      "Total loss:  -0.7395 | PDE Loss:  -1.3719 | Function Loss:  -1.8548\n",
      "Total loss:  -0.7396 | PDE Loss:  -1.3725 | Function Loss:  -1.8547\n",
      "Total loss:  -0.7396 | PDE Loss:  -1.3723 | Function Loss:  -1.8548\n",
      "Total loss:  -0.7397 | PDE Loss:  -1.372 | Function Loss:  -1.855\n",
      "Total loss:  -0.7398 | PDE Loss:  -1.3717 | Function Loss:  -1.8552\n",
      "Total loss:  -0.7398 | PDE Loss:  -1.3714 | Function Loss:  -1.8554\n",
      "Total loss:  -0.7399 | PDE Loss:  -1.3706 | Function Loss:  -1.8557\n",
      "Total loss:  -0.74 | PDE Loss:  -1.3704 | Function Loss:  -1.8558\n",
      "Total loss:  -0.7401 | PDE Loss:  -1.3702 | Function Loss:  -1.856\n",
      "Total loss:  -0.7401 | PDE Loss:  -1.3698 | Function Loss:  -1.8563\n",
      "Total loss:  -0.7402 | PDE Loss:  -1.3698 | Function Loss:  -1.8564\n",
      "Total loss:  -0.7403 | PDE Loss:  -1.3699 | Function Loss:  -1.8564\n",
      "Total loss:  -0.7403 | PDE Loss:  -1.3699 | Function Loss:  -1.8565\n",
      "Total loss:  -0.7404 | PDE Loss:  -1.3699 | Function Loss:  -1.8565\n",
      "Total loss:  -0.7404 | PDE Loss:  -1.3698 | Function Loss:  -1.8566\n",
      "Total loss:  -0.7405 | PDE Loss:  -1.3695 | Function Loss:  -1.8568\n",
      "Total loss:  -0.7405 | PDE Loss:  -1.3694 | Function Loss:  -1.8568\n",
      "Total loss:  -0.7405 | PDE Loss:  -1.3692 | Function Loss:  -1.857\n",
      "Total loss:  -0.7406 | PDE Loss:  -1.3691 | Function Loss:  -1.857\n",
      "Total loss:  -0.7406 | PDE Loss:  -1.3689 | Function Loss:  -1.8572\n",
      "Total loss:  -0.7406 | PDE Loss:  -1.3689 | Function Loss:  -1.8572\n",
      "Total loss:  -0.7407 | PDE Loss:  -1.3689 | Function Loss:  -1.8572\n",
      "Total loss:  -0.7407 | PDE Loss:  -1.3692 | Function Loss:  -1.8572\n",
      "Total loss:  -0.7408 | PDE Loss:  -1.3695 | Function Loss:  -1.8572\n",
      "Total loss:  -0.7409 | PDE Loss:  -1.37 | Function Loss:  -1.8571\n",
      "Total loss:  -0.7409 | PDE Loss:  -1.3703 | Function Loss:  -1.8571\n",
      "Total loss:  -0.741 | PDE Loss:  -1.3707 | Function Loss:  -1.8571\n",
      "Total loss:  -0.7411 | PDE Loss:  -1.371 | Function Loss:  -1.8572\n",
      "Total loss:  -0.7412 | PDE Loss:  -1.3713 | Function Loss:  -1.8572\n",
      "Total loss:  -0.7413 | PDE Loss:  -1.3713 | Function Loss:  -1.8573\n",
      "Total loss:  -0.7414 | PDE Loss:  -1.3711 | Function Loss:  -1.8575\n",
      "Total loss:  -0.7404 | PDE Loss:  -1.3685 | Function Loss:  -1.857\n",
      "Total loss:  -0.7414 | PDE Loss:  -1.371 | Function Loss:  -1.8576\n",
      "Total loss:  -0.7415 | PDE Loss:  -1.3706 | Function Loss:  -1.8578\n",
      "Total loss:  -0.7416 | PDE Loss:  -1.3702 | Function Loss:  -1.8581\n",
      "Total loss:  -0.7417 | PDE Loss:  -1.3697 | Function Loss:  -1.8583\n",
      "Total loss:  -0.7418 | PDE Loss:  -1.3691 | Function Loss:  -1.8586\n",
      "Total loss:  -0.7419 | PDE Loss:  -1.3688 | Function Loss:  -1.8588\n",
      "Total loss:  -0.7419 | PDE Loss:  -1.3687 | Function Loss:  -1.8589\n",
      "Total loss:  -0.742 | PDE Loss:  -1.369 | Function Loss:  -1.859\n",
      "Total loss:  -0.7421 | PDE Loss:  -1.3692 | Function Loss:  -1.859\n",
      "Total loss:  -0.7422 | PDE Loss:  -1.3696 | Function Loss:  -1.859\n",
      "Total loss:  -0.7423 | PDE Loss:  -1.3698 | Function Loss:  -1.8591\n",
      "Total loss:  -0.7424 | PDE Loss:  -1.37 | Function Loss:  -1.8591\n",
      "Total loss:  -0.7424 | PDE Loss:  -1.3701 | Function Loss:  -1.8592\n",
      "Total loss:  -0.7425 | PDE Loss:  -1.3703 | Function Loss:  -1.8592\n",
      "Total loss:  -0.7425 | PDE Loss:  -1.3699 | Function Loss:  -1.8593\n",
      "Total loss:  -0.7425 | PDE Loss:  -1.3702 | Function Loss:  -1.8593\n",
      "Total loss:  -0.7426 | PDE Loss:  -1.3702 | Function Loss:  -1.8593\n",
      "Total loss:  -0.7426 | PDE Loss:  -1.3702 | Function Loss:  -1.8594\n",
      "Total loss:  -0.7426 | PDE Loss:  -1.3701 | Function Loss:  -1.8594\n",
      "Total loss:  -0.7427 | PDE Loss:  -1.3701 | Function Loss:  -1.8595\n",
      "Total loss:  -0.7427 | PDE Loss:  -1.3701 | Function Loss:  -1.8595\n",
      "Total loss:  -0.7427 | PDE Loss:  -1.3701 | Function Loss:  -1.8595\n",
      "Total loss:  -0.7428 | PDE Loss:  -1.3701 | Function Loss:  -1.8596\n",
      "Total loss:  -0.7428 | PDE Loss:  -1.3701 | Function Loss:  -1.8596\n",
      "Total loss:  -0.7428 | PDE Loss:  -1.3702 | Function Loss:  -1.8596\n",
      "Total loss:  -0.7429 | PDE Loss:  -1.3704 | Function Loss:  -1.8597\n",
      "Total loss:  -0.7429 | PDE Loss:  -1.3705 | Function Loss:  -1.8597\n",
      "Total loss:  -0.743 | PDE Loss:  -1.3707 | Function Loss:  -1.8597\n",
      "Total loss:  -0.743 | PDE Loss:  -1.3709 | Function Loss:  -1.8597\n",
      "Total loss:  -0.743 | PDE Loss:  -1.371 | Function Loss:  -1.8597\n",
      "Total loss:  -0.7431 | PDE Loss:  -1.3711 | Function Loss:  -1.8597\n",
      "Total loss:  -0.7432 | PDE Loss:  -1.3713 | Function Loss:  -1.8598\n",
      "Total loss:  -0.7431 | PDE Loss:  -1.3707 | Function Loss:  -1.8598\n",
      "Total loss:  -0.7432 | PDE Loss:  -1.3712 | Function Loss:  -1.8598\n",
      "Total loss:  -0.7433 | PDE Loss:  -1.3713 | Function Loss:  -1.8599\n",
      "Total loss:  -0.7434 | PDE Loss:  -1.3715 | Function Loss:  -1.86\n",
      "Total loss:  -0.7434 | PDE Loss:  -1.3715 | Function Loss:  -1.86\n",
      "Total loss:  -0.7435 | PDE Loss:  -1.3716 | Function Loss:  -1.8601\n",
      "Total loss:  -0.7435 | PDE Loss:  -1.3718 | Function Loss:  -1.8601\n",
      "Total loss:  -0.7436 | PDE Loss:  -1.3719 | Function Loss:  -1.8601\n",
      "Total loss:  -0.7437 | PDE Loss:  -1.3721 | Function Loss:  -1.8601\n",
      "Total loss:  -0.7437 | PDE Loss:  -1.3726 | Function Loss:  -1.8601\n",
      "Total loss:  -0.7438 | PDE Loss:  -1.3729 | Function Loss:  -1.86\n",
      "Total loss:  -0.7438 | PDE Loss:  -1.3736 | Function Loss:  -1.8599\n",
      "Total loss:  -0.7439 | PDE Loss:  -1.3739 | Function Loss:  -1.8599\n",
      "Total loss:  -0.7439 | PDE Loss:  -1.3742 | Function Loss:  -1.8598\n",
      "Total loss:  -0.7439 | PDE Loss:  -1.3744 | Function Loss:  -1.8598\n",
      "Total loss:  -0.744 | PDE Loss:  -1.3745 | Function Loss:  -1.8598\n",
      "Total loss:  -0.744 | PDE Loss:  -1.3745 | Function Loss:  -1.8599\n",
      "Total loss:  -0.744 | PDE Loss:  -1.3745 | Function Loss:  -1.8599\n",
      "Total loss:  -0.7441 | PDE Loss:  -1.3744 | Function Loss:  -1.86\n",
      "Total loss:  -0.7441 | PDE Loss:  -1.3742 | Function Loss:  -1.8601\n",
      "Total loss:  -0.7442 | PDE Loss:  -1.3739 | Function Loss:  -1.8603\n",
      "Total loss:  -0.7443 | PDE Loss:  -1.3737 | Function Loss:  -1.8605\n",
      "Total loss:  -0.7443 | PDE Loss:  -1.3733 | Function Loss:  -1.8606\n",
      "Total loss:  -0.744 | PDE Loss:  -1.3725 | Function Loss:  -1.8605\n",
      "Total loss:  -0.7443 | PDE Loss:  -1.3733 | Function Loss:  -1.8606\n",
      "Total loss:  -0.7444 | PDE Loss:  -1.373 | Function Loss:  -1.8608\n",
      "Total loss:  -0.7445 | PDE Loss:  -1.3726 | Function Loss:  -1.861\n",
      "Total loss:  -0.7445 | PDE Loss:  -1.3723 | Function Loss:  -1.8612\n",
      "Total loss:  -0.7446 | PDE Loss:  -1.3722 | Function Loss:  -1.8613\n",
      "Total loss:  -0.7446 | PDE Loss:  -1.3721 | Function Loss:  -1.8614\n",
      "Total loss:  -0.7446 | PDE Loss:  -1.3721 | Function Loss:  -1.8614\n",
      "Total loss:  -0.7447 | PDE Loss:  -1.372 | Function Loss:  -1.8615\n",
      "Total loss:  -0.7448 | PDE Loss:  -1.3721 | Function Loss:  -1.8616\n",
      "Total loss:  -0.7446 | PDE Loss:  -1.3705 | Function Loss:  -1.8619\n",
      "Total loss:  -0.7448 | PDE Loss:  -1.3717 | Function Loss:  -1.8618\n",
      "Total loss:  -0.7449 | PDE Loss:  -1.3718 | Function Loss:  -1.8618\n",
      "Total loss:  -0.745 | PDE Loss:  -1.3721 | Function Loss:  -1.8619\n",
      "Total loss:  -0.745 | PDE Loss:  -1.3722 | Function Loss:  -1.8619\n",
      "Total loss:  -0.7451 | PDE Loss:  -1.3723 | Function Loss:  -1.8619\n",
      "Total loss:  -0.7451 | PDE Loss:  -1.3724 | Function Loss:  -1.862\n",
      "Total loss:  -0.7451 | PDE Loss:  -1.3725 | Function Loss:  -1.862\n",
      "Total loss:  -0.7446 | PDE Loss:  -1.3721 | Function Loss:  -1.8614\n",
      "Total loss:  -0.7452 | PDE Loss:  -1.3726 | Function Loss:  -1.862\n",
      "Total loss:  -0.7452 | PDE Loss:  -1.3727 | Function Loss:  -1.862\n",
      "Total loss:  -0.7453 | PDE Loss:  -1.3727 | Function Loss:  -1.862\n",
      "Total loss:  -0.7453 | PDE Loss:  -1.3728 | Function Loss:  -1.8621\n",
      "Total loss:  -0.7453 | PDE Loss:  -1.3729 | Function Loss:  -1.8621\n",
      "Total loss:  -0.7454 | PDE Loss:  -1.373 | Function Loss:  -1.8621\n",
      "Total loss:  -0.7454 | PDE Loss:  -1.3731 | Function Loss:  -1.8622\n",
      "Total loss:  -0.7455 | PDE Loss:  -1.3734 | Function Loss:  -1.8621\n",
      "Total loss:  -0.7455 | PDE Loss:  -1.3733 | Function Loss:  -1.8622\n",
      "Total loss:  -0.7456 | PDE Loss:  -1.3734 | Function Loss:  -1.8623\n",
      "Total loss:  -0.7457 | PDE Loss:  -1.3735 | Function Loss:  -1.8623\n",
      "Total loss:  -0.7439 | PDE Loss:  -1.3672 | Function Loss:  -1.8619\n",
      "Total loss:  -0.7457 | PDE Loss:  -1.3732 | Function Loss:  -1.8624\n",
      "Total loss:  -0.7457 | PDE Loss:  -1.3733 | Function Loss:  -1.8625\n",
      "Total loss:  -0.7458 | PDE Loss:  -1.3733 | Function Loss:  -1.8626\n",
      "Total loss:  -0.7458 | PDE Loss:  -1.3733 | Function Loss:  -1.8626\n",
      "Total loss:  -0.7459 | PDE Loss:  -1.3733 | Function Loss:  -1.8627\n",
      "Total loss:  -0.746 | PDE Loss:  -1.3733 | Function Loss:  -1.8628\n",
      "Total loss:  -0.746 | PDE Loss:  -1.3734 | Function Loss:  -1.8628\n",
      "Total loss:  -0.7461 | PDE Loss:  -1.373 | Function Loss:  -1.863\n",
      "Total loss:  -0.7461 | PDE Loss:  -1.3733 | Function Loss:  -1.863\n",
      "Total loss:  -0.7462 | PDE Loss:  -1.3735 | Function Loss:  -1.863\n",
      "Total loss:  -0.7462 | PDE Loss:  -1.3738 | Function Loss:  -1.863\n",
      "Total loss:  -0.7463 | PDE Loss:  -1.3742 | Function Loss:  -1.8629\n",
      "Total loss:  -0.7463 | PDE Loss:  -1.3739 | Function Loss:  -1.8631\n",
      "Total loss:  -0.7464 | PDE Loss:  -1.3741 | Function Loss:  -1.8631\n",
      "Total loss:  -0.7464 | PDE Loss:  -1.3743 | Function Loss:  -1.8631\n",
      "Total loss:  -0.7465 | PDE Loss:  -1.3745 | Function Loss:  -1.8631\n",
      "Total loss:  -0.7465 | PDE Loss:  -1.3745 | Function Loss:  -1.8632\n",
      "Total loss:  -0.7466 | PDE Loss:  -1.3747 | Function Loss:  -1.8632\n",
      "Total loss:  -0.7467 | PDE Loss:  -1.3749 | Function Loss:  -1.8632\n",
      "Total loss:  -0.7468 | PDE Loss:  -1.3756 | Function Loss:  -1.8631\n",
      "Total loss:  -0.7468 | PDE Loss:  -1.3759 | Function Loss:  -1.863\n",
      "Total loss:  -0.7468 | PDE Loss:  -1.3759 | Function Loss:  -1.8631\n",
      "Total loss:  -0.7469 | PDE Loss:  -1.3762 | Function Loss:  -1.8631\n",
      "Total loss:  -0.7469 | PDE Loss:  -1.3767 | Function Loss:  -1.863\n",
      "Total loss:  -0.747 | PDE Loss:  -1.3771 | Function Loss:  -1.8629\n",
      "Total loss:  -0.747 | PDE Loss:  -1.3776 | Function Loss:  -1.8629\n",
      "Total loss:  -0.7471 | PDE Loss:  -1.3781 | Function Loss:  -1.8628\n",
      "Total loss:  -0.7471 | PDE Loss:  -1.3787 | Function Loss:  -1.8626\n",
      "Total loss:  -0.7472 | PDE Loss:  -1.379 | Function Loss:  -1.8626\n",
      "Total loss:  -0.7472 | PDE Loss:  -1.3792 | Function Loss:  -1.8626\n",
      "Total loss:  -0.7473 | PDE Loss:  -1.3793 | Function Loss:  -1.8627\n",
      "Total loss:  -0.7473 | PDE Loss:  -1.3794 | Function Loss:  -1.8627\n",
      "Total loss:  -0.7474 | PDE Loss:  -1.3794 | Function Loss:  -1.8628\n",
      "Total loss:  -0.7474 | PDE Loss:  -1.3794 | Function Loss:  -1.8628\n",
      "Total loss:  -0.7475 | PDE Loss:  -1.3797 | Function Loss:  -1.8628\n",
      "Total loss:  -0.7476 | PDE Loss:  -1.3798 | Function Loss:  -1.8629\n",
      "Total loss:  -0.7476 | PDE Loss:  -1.3797 | Function Loss:  -1.863\n",
      "Total loss:  -0.7477 | PDE Loss:  -1.38 | Function Loss:  -1.863\n",
      "Total loss:  -0.7477 | PDE Loss:  -1.38 | Function Loss:  -1.8631\n",
      "Total loss:  -0.7478 | PDE Loss:  -1.3802 | Function Loss:  -1.8631\n",
      "Total loss:  -0.7478 | PDE Loss:  -1.3802 | Function Loss:  -1.8631\n",
      "Total loss:  -0.7479 | PDE Loss:  -1.3803 | Function Loss:  -1.8632\n",
      "Total loss:  -0.748 | PDE Loss:  -1.38 | Function Loss:  -1.8634\n",
      "Total loss:  -0.7481 | PDE Loss:  -1.3799 | Function Loss:  -1.8635\n",
      "Total loss:  -0.7482 | PDE Loss:  -1.3793 | Function Loss:  -1.8638\n",
      "Total loss:  -0.7482 | PDE Loss:  -1.3794 | Function Loss:  -1.8639\n",
      "Total loss:  -0.7483 | PDE Loss:  -1.3795 | Function Loss:  -1.864\n",
      "Total loss:  -0.7484 | PDE Loss:  -1.3795 | Function Loss:  -1.8641\n",
      "Total loss:  -0.7485 | PDE Loss:  -1.3796 | Function Loss:  -1.8641\n",
      "Total loss:  -0.7485 | PDE Loss:  -1.3798 | Function Loss:  -1.8642\n",
      "Total loss:  -0.7486 | PDE Loss:  -1.3798 | Function Loss:  -1.8643\n",
      "Total loss:  -0.7487 | PDE Loss:  -1.3799 | Function Loss:  -1.8643\n",
      "Total loss:  -0.7487 | PDE Loss:  -1.38 | Function Loss:  -1.8643\n",
      "Total loss:  -0.7487 | PDE Loss:  -1.38 | Function Loss:  -1.8644\n",
      "Total loss:  -0.7488 | PDE Loss:  -1.3802 | Function Loss:  -1.8644\n",
      "Total loss:  -0.7488 | PDE Loss:  -1.3803 | Function Loss:  -1.8644\n",
      "Total loss:  -0.7489 | PDE Loss:  -1.3804 | Function Loss:  -1.8644\n",
      "Total loss:  -0.7484 | PDE Loss:  -1.3775 | Function Loss:  -1.8647\n",
      "Total loss:  -0.7489 | PDE Loss:  -1.3802 | Function Loss:  -1.8645\n",
      "Total loss:  -0.7489 | PDE Loss:  -1.3803 | Function Loss:  -1.8645\n",
      "Total loss:  -0.7489 | PDE Loss:  -1.3803 | Function Loss:  -1.8645\n",
      "Total loss:  -0.749 | PDE Loss:  -1.3802 | Function Loss:  -1.8646\n",
      "Total loss:  -0.749 | PDE Loss:  -1.3801 | Function Loss:  -1.8647\n",
      "Total loss:  -0.749 | PDE Loss:  -1.38 | Function Loss:  -1.8647\n",
      "Total loss:  -0.7491 | PDE Loss:  -1.3799 | Function Loss:  -1.8648\n",
      "Total loss:  -0.7491 | PDE Loss:  -1.3793 | Function Loss:  -1.8651\n",
      "Total loss:  -0.7491 | PDE Loss:  -1.3794 | Function Loss:  -1.8651\n",
      "Total loss:  -0.7492 | PDE Loss:  -1.3795 | Function Loss:  -1.8651\n",
      "Total loss:  -0.7492 | PDE Loss:  -1.3796 | Function Loss:  -1.8651\n",
      "Total loss:  -0.7493 | PDE Loss:  -1.3798 | Function Loss:  -1.8651\n",
      "Total loss:  -0.7493 | PDE Loss:  -1.38 | Function Loss:  -1.8651\n",
      "Total loss:  -0.7494 | PDE Loss:  -1.3803 | Function Loss:  -1.8651\n",
      "Total loss:  -0.7495 | PDE Loss:  -1.3807 | Function Loss:  -1.8651\n",
      "Total loss:  -0.7496 | PDE Loss:  -1.381 | Function Loss:  -1.8652\n",
      "Total loss:  -0.7495 | PDE Loss:  -1.3812 | Function Loss:  -1.865\n",
      "Total loss:  -0.7496 | PDE Loss:  -1.3811 | Function Loss:  -1.8652\n",
      "Total loss:  -0.7497 | PDE Loss:  -1.3815 | Function Loss:  -1.8652\n",
      "Total loss:  -0.7498 | PDE Loss:  -1.3817 | Function Loss:  -1.8652\n",
      "Total loss:  -0.7498 | PDE Loss:  -1.382 | Function Loss:  -1.8652\n",
      "Total loss:  -0.7499 | PDE Loss:  -1.3821 | Function Loss:  -1.8652\n",
      "Total loss:  -0.75 | PDE Loss:  -1.3822 | Function Loss:  -1.8653\n",
      "Total loss:  -0.75 | PDE Loss:  -1.3822 | Function Loss:  -1.8654\n",
      "Total loss:  -0.7501 | PDE Loss:  -1.3823 | Function Loss:  -1.8654\n",
      "Total loss:  -0.7502 | PDE Loss:  -1.3823 | Function Loss:  -1.8656\n",
      "Total loss:  -0.7503 | PDE Loss:  -1.3825 | Function Loss:  -1.8656\n",
      "Total loss:  -0.7504 | PDE Loss:  -1.3823 | Function Loss:  -1.8658\n",
      "Total loss:  -0.7504 | PDE Loss:  -1.3819 | Function Loss:  -1.866\n",
      "Total loss:  -0.7505 | PDE Loss:  -1.3819 | Function Loss:  -1.8661\n",
      "Total loss:  -0.7506 | PDE Loss:  -1.3816 | Function Loss:  -1.8662\n",
      "Total loss:  -0.7507 | PDE Loss:  -1.3815 | Function Loss:  -1.8664\n",
      "Total loss:  -0.7507 | PDE Loss:  -1.3816 | Function Loss:  -1.8664\n",
      "Total loss:  -0.7508 | PDE Loss:  -1.3817 | Function Loss:  -1.8665\n",
      "Total loss:  -0.7509 | PDE Loss:  -1.3818 | Function Loss:  -1.8666\n",
      "Total loss:  -0.751 | PDE Loss:  -1.3818 | Function Loss:  -1.8668\n",
      "Total loss:  -0.7511 | PDE Loss:  -1.3817 | Function Loss:  -1.8669\n",
      "Total loss:  -0.7512 | PDE Loss:  -1.3814 | Function Loss:  -1.8671\n",
      "Total loss:  -0.7511 | PDE Loss:  -1.3808 | Function Loss:  -1.8671\n",
      "Total loss:  -0.7512 | PDE Loss:  -1.3814 | Function Loss:  -1.8672\n",
      "Total loss:  -0.7513 | PDE Loss:  -1.3813 | Function Loss:  -1.8673\n",
      "Total loss:  -0.7514 | PDE Loss:  -1.3811 | Function Loss:  -1.8675\n",
      "Total loss:  -0.7515 | PDE Loss:  -1.3809 | Function Loss:  -1.8677\n",
      "Total loss:  -0.7515 | PDE Loss:  -1.3808 | Function Loss:  -1.8678\n",
      "Total loss:  -0.7516 | PDE Loss:  -1.3808 | Function Loss:  -1.8679\n",
      "Total loss:  -0.7517 | PDE Loss:  -1.3807 | Function Loss:  -1.868\n",
      "Total loss:  -0.7515 | PDE Loss:  -1.381 | Function Loss:  -1.8677\n",
      "Total loss:  -0.7517 | PDE Loss:  -1.3811 | Function Loss:  -1.8679\n",
      "Total loss:  -0.7518 | PDE Loss:  -1.3812 | Function Loss:  -1.868\n",
      "Total loss:  -0.7519 | PDE Loss:  -1.3815 | Function Loss:  -1.868\n",
      "Total loss:  -0.752 | PDE Loss:  -1.3818 | Function Loss:  -1.868\n",
      "Total loss:  -0.7521 | PDE Loss:  -1.3824 | Function Loss:  -1.868\n",
      "Total loss:  -0.7521 | PDE Loss:  -1.3828 | Function Loss:  -1.8679\n",
      "Total loss:  -0.7522 | PDE Loss:  -1.3833 | Function Loss:  -1.8679\n",
      "Total loss:  -0.7523 | PDE Loss:  -1.3839 | Function Loss:  -1.8678\n",
      "Total loss:  -0.7524 | PDE Loss:  -1.3845 | Function Loss:  -1.8678\n",
      "Total loss:  -0.7524 | PDE Loss:  -1.3843 | Function Loss:  -1.8678\n",
      "Total loss:  -0.7525 | PDE Loss:  -1.3847 | Function Loss:  -1.8679\n",
      "Total loss:  -0.7526 | PDE Loss:  -1.3849 | Function Loss:  -1.8679\n",
      "Total loss:  -0.7527 | PDE Loss:  -1.3851 | Function Loss:  -1.868\n",
      "Total loss:  -0.7528 | PDE Loss:  -1.3852 | Function Loss:  -1.8681\n",
      "Total loss:  -0.753 | PDE Loss:  -1.3851 | Function Loss:  -1.8683\n",
      "Total loss:  -0.7531 | PDE Loss:  -1.3852 | Function Loss:  -1.8684\n",
      "Total loss:  -0.7532 | PDE Loss:  -1.3851 | Function Loss:  -1.8686\n",
      "Total loss:  -0.7532 | PDE Loss:  -1.3849 | Function Loss:  -1.8687\n",
      "Total loss:  -0.7533 | PDE Loss:  -1.385 | Function Loss:  -1.8688\n",
      "Total loss:  -0.7534 | PDE Loss:  -1.3851 | Function Loss:  -1.8689\n",
      "Total loss:  -0.7535 | PDE Loss:  -1.3853 | Function Loss:  -1.869\n",
      "Total loss:  -0.7535 | PDE Loss:  -1.3854 | Function Loss:  -1.869\n",
      "Total loss:  -0.7536 | PDE Loss:  -1.3851 | Function Loss:  -1.8691\n",
      "Total loss:  -0.7536 | PDE Loss:  -1.3861 | Function Loss:  -1.8689\n",
      "Total loss:  -0.7537 | PDE Loss:  -1.3859 | Function Loss:  -1.869\n",
      "Total loss:  -0.7537 | PDE Loss:  -1.3859 | Function Loss:  -1.8691\n",
      "Total loss:  -0.7538 | PDE Loss:  -1.3858 | Function Loss:  -1.8692\n",
      "Total loss:  -0.7538 | PDE Loss:  -1.3858 | Function Loss:  -1.8692\n",
      "Total loss:  -0.7539 | PDE Loss:  -1.3857 | Function Loss:  -1.8693\n",
      "Total loss:  -0.7534 | PDE Loss:  -1.3858 | Function Loss:  -1.8687\n",
      "Total loss:  -0.7539 | PDE Loss:  -1.3859 | Function Loss:  -1.8693\n",
      "Total loss:  -0.7539 | PDE Loss:  -1.3858 | Function Loss:  -1.8693\n",
      "Total loss:  -0.754 | PDE Loss:  -1.3858 | Function Loss:  -1.8694\n",
      "Total loss:  -0.754 | PDE Loss:  -1.3859 | Function Loss:  -1.8694\n",
      "Total loss:  -0.754 | PDE Loss:  -1.3859 | Function Loss:  -1.8695\n",
      "Total loss:  -0.754 | PDE Loss:  -1.386 | Function Loss:  -1.8695\n",
      "Total loss:  -0.7541 | PDE Loss:  -1.386 | Function Loss:  -1.8695\n",
      "Total loss:  -0.7541 | PDE Loss:  -1.3861 | Function Loss:  -1.8695\n",
      "Total loss:  -0.7542 | PDE Loss:  -1.3862 | Function Loss:  -1.8695\n",
      "Total loss:  -0.7541 | PDE Loss:  -1.3855 | Function Loss:  -1.8696\n",
      "Total loss:  -0.7542 | PDE Loss:  -1.3861 | Function Loss:  -1.8696\n",
      "Total loss:  -0.7543 | PDE Loss:  -1.3863 | Function Loss:  -1.8696\n",
      "Total loss:  -0.7543 | PDE Loss:  -1.3866 | Function Loss:  -1.8697\n",
      "Total loss:  -0.7544 | PDE Loss:  -1.3869 | Function Loss:  -1.8697\n",
      "Total loss:  -0.754 | PDE Loss:  -1.3874 | Function Loss:  -1.869\n",
      "Total loss:  -0.7545 | PDE Loss:  -1.3872 | Function Loss:  -1.8696\n",
      "Total loss:  -0.7545 | PDE Loss:  -1.3874 | Function Loss:  -1.8696\n",
      "Total loss:  -0.7546 | PDE Loss:  -1.3877 | Function Loss:  -1.8697\n",
      "Total loss:  -0.7547 | PDE Loss:  -1.3874 | Function Loss:  -1.8699\n",
      "Total loss:  -0.7548 | PDE Loss:  -1.3875 | Function Loss:  -1.87\n",
      "Total loss:  -0.7549 | PDE Loss:  -1.3878 | Function Loss:  -1.87\n",
      "Total loss:  -0.755 | PDE Loss:  -1.3878 | Function Loss:  -1.8701\n",
      "Total loss:  -0.755 | PDE Loss:  -1.3878 | Function Loss:  -1.8702\n",
      "Total loss:  -0.7551 | PDE Loss:  -1.3877 | Function Loss:  -1.8703\n",
      "Total loss:  -0.7551 | PDE Loss:  -1.3875 | Function Loss:  -1.8704\n",
      "Total loss:  -0.7552 | PDE Loss:  -1.3871 | Function Loss:  -1.8706\n",
      "Total loss:  -0.7552 | PDE Loss:  -1.387 | Function Loss:  -1.8707\n",
      "Total loss:  -0.7553 | PDE Loss:  -1.3869 | Function Loss:  -1.8709\n",
      "Total loss:  -0.7554 | PDE Loss:  -1.3871 | Function Loss:  -1.871\n",
      "Total loss:  -0.7556 | PDE Loss:  -1.3871 | Function Loss:  -1.8711\n",
      "Total loss:  -0.7557 | PDE Loss:  -1.3878 | Function Loss:  -1.8711\n",
      "Total loss:  -0.7557 | PDE Loss:  -1.3881 | Function Loss:  -1.871\n",
      "Total loss:  -0.7559 | PDE Loss:  -1.3889 | Function Loss:  -1.871\n",
      "Total loss:  -0.756 | PDE Loss:  -1.3899 | Function Loss:  -1.8708\n",
      "Total loss:  -0.7561 | PDE Loss:  -1.3906 | Function Loss:  -1.8707\n",
      "Total loss:  -0.7561 | PDE Loss:  -1.3911 | Function Loss:  -1.8706\n",
      "Total loss:  -0.7562 | PDE Loss:  -1.3914 | Function Loss:  -1.8706\n",
      "Total loss:  -0.7563 | PDE Loss:  -1.3917 | Function Loss:  -1.8706\n",
      "Total loss:  -0.7563 | PDE Loss:  -1.3917 | Function Loss:  -1.8707\n",
      "Total loss:  -0.7563 | PDE Loss:  -1.3918 | Function Loss:  -1.8707\n",
      "Total loss:  -0.7564 | PDE Loss:  -1.3916 | Function Loss:  -1.8708\n",
      "Total loss:  -0.7564 | PDE Loss:  -1.3916 | Function Loss:  -1.8709\n",
      "Total loss:  -0.7565 | PDE Loss:  -1.3915 | Function Loss:  -1.871\n",
      "Total loss:  -0.7566 | PDE Loss:  -1.3919 | Function Loss:  -1.8711\n",
      "Total loss:  -0.7568 | PDE Loss:  -1.3919 | Function Loss:  -1.8712\n",
      "Total loss:  -0.7569 | PDE Loss:  -1.3926 | Function Loss:  -1.8711\n",
      "Total loss:  -0.7569 | PDE Loss:  -1.3933 | Function Loss:  -1.871\n",
      "Total loss:  -0.757 | PDE Loss:  -1.3937 | Function Loss:  -1.871\n",
      "Total loss:  -0.7571 | PDE Loss:  -1.3943 | Function Loss:  -1.8709\n",
      "Total loss:  -0.7571 | PDE Loss:  -1.3947 | Function Loss:  -1.8708\n",
      "Total loss:  -0.7571 | PDE Loss:  -1.3951 | Function Loss:  -1.8707\n",
      "Total loss:  -0.7572 | PDE Loss:  -1.3954 | Function Loss:  -1.8707\n",
      "Total loss:  -0.7572 | PDE Loss:  -1.3957 | Function Loss:  -1.8707\n",
      "Total loss:  -0.7572 | PDE Loss:  -1.3958 | Function Loss:  -1.8707\n",
      "Total loss:  -0.7573 | PDE Loss:  -1.3959 | Function Loss:  -1.8707\n",
      "Total loss:  -0.7573 | PDE Loss:  -1.396 | Function Loss:  -1.8707\n",
      "Total loss:  -0.7574 | PDE Loss:  -1.396 | Function Loss:  -1.8708\n",
      "Total loss:  -0.7575 | PDE Loss:  -1.3958 | Function Loss:  -1.871\n",
      "Total loss:  -0.7576 | PDE Loss:  -1.3955 | Function Loss:  -1.8712\n",
      "Total loss:  -0.7576 | PDE Loss:  -1.3953 | Function Loss:  -1.8714\n",
      "Total loss:  -0.7577 | PDE Loss:  -1.395 | Function Loss:  -1.8715\n",
      "Total loss:  -0.7578 | PDE Loss:  -1.3951 | Function Loss:  -1.8716\n",
      "Total loss:  -0.7579 | PDE Loss:  -1.3951 | Function Loss:  -1.8717\n",
      "Total loss:  -0.7579 | PDE Loss:  -1.3951 | Function Loss:  -1.8717\n",
      "Total loss:  -0.758 | PDE Loss:  -1.3956 | Function Loss:  -1.8717\n",
      "Total loss:  -0.7581 | PDE Loss:  -1.3961 | Function Loss:  -1.8717\n",
      "Total loss:  -0.7582 | PDE Loss:  -1.3971 | Function Loss:  -1.8715\n",
      "Total loss:  -0.7583 | PDE Loss:  -1.3978 | Function Loss:  -1.8714\n",
      "Total loss:  -0.7584 | PDE Loss:  -1.399 | Function Loss:  -1.8712\n",
      "Total loss:  -0.7585 | PDE Loss:  -1.3996 | Function Loss:  -1.8711\n",
      "Total loss:  -0.7585 | PDE Loss:  -1.4003 | Function Loss:  -1.871\n",
      "Total loss:  -0.7587 | PDE Loss:  -1.4008 | Function Loss:  -1.871\n",
      "Total loss:  -0.7588 | PDE Loss:  -1.4017 | Function Loss:  -1.8709\n",
      "Total loss:  -0.7589 | PDE Loss:  -1.402 | Function Loss:  -1.8709\n",
      "Total loss:  -0.7589 | PDE Loss:  -1.4022 | Function Loss:  -1.8709\n",
      "Total loss:  -0.759 | PDE Loss:  -1.4026 | Function Loss:  -1.871\n",
      "Total loss:  -0.7591 | PDE Loss:  -1.403 | Function Loss:  -1.871\n",
      "Total loss:  -0.7592 | PDE Loss:  -1.4033 | Function Loss:  -1.871\n",
      "Total loss:  -0.7592 | PDE Loss:  -1.4036 | Function Loss:  -1.871\n",
      "Total loss:  -0.7593 | PDE Loss:  -1.4039 | Function Loss:  -1.8709\n",
      "Total loss:  -0.7594 | PDE Loss:  -1.4042 | Function Loss:  -1.8709\n",
      "Total loss:  -0.7594 | PDE Loss:  -1.4045 | Function Loss:  -1.8709\n",
      "Total loss:  -0.7595 | PDE Loss:  -1.4043 | Function Loss:  -1.871\n",
      "Total loss:  -0.7595 | PDE Loss:  -1.4048 | Function Loss:  -1.871\n",
      "Total loss:  -0.7596 | PDE Loss:  -1.4046 | Function Loss:  -1.8711\n",
      "Total loss:  -0.7597 | PDE Loss:  -1.4043 | Function Loss:  -1.8713\n",
      "Total loss:  -0.7597 | PDE Loss:  -1.4041 | Function Loss:  -1.8714\n",
      "Total loss:  -0.7598 | PDE Loss:  -1.404 | Function Loss:  -1.8716\n",
      "Total loss:  -0.7599 | PDE Loss:  -1.4039 | Function Loss:  -1.8717\n",
      "Total loss:  -0.7599 | PDE Loss:  -1.404 | Function Loss:  -1.8717\n",
      "Total loss:  -0.76 | PDE Loss:  -1.4038 | Function Loss:  -1.8718\n",
      "Total loss:  -0.76 | PDE Loss:  -1.404 | Function Loss:  -1.8719\n",
      "Total loss:  -0.7601 | PDE Loss:  -1.4042 | Function Loss:  -1.8719\n",
      "Total loss:  -0.7602 | PDE Loss:  -1.4044 | Function Loss:  -1.8719\n",
      "Total loss:  -0.7602 | PDE Loss:  -1.4046 | Function Loss:  -1.872\n",
      "Total loss:  -0.7603 | PDE Loss:  -1.4045 | Function Loss:  -1.8721\n",
      "Total loss:  -0.7604 | PDE Loss:  -1.4044 | Function Loss:  -1.8722\n",
      "Total loss:  -0.7605 | PDE Loss:  -1.4041 | Function Loss:  -1.8724\n",
      "Total loss:  -0.7606 | PDE Loss:  -1.4039 | Function Loss:  -1.8726\n",
      "Total loss:  -0.7601 | PDE Loss:  -1.4018 | Function Loss:  -1.8726\n",
      "Total loss:  -0.7606 | PDE Loss:  -1.4037 | Function Loss:  -1.8727\n",
      "Total loss:  -0.7607 | PDE Loss:  -1.4034 | Function Loss:  -1.8729\n",
      "Total loss:  -0.7608 | PDE Loss:  -1.4032 | Function Loss:  -1.8731\n",
      "Total loss:  -0.7609 | PDE Loss:  -1.4031 | Function Loss:  -1.8732\n",
      "Total loss:  -0.7609 | PDE Loss:  -1.4031 | Function Loss:  -1.8732\n",
      "Total loss:  -0.7609 | PDE Loss:  -1.4033 | Function Loss:  -1.8733\n",
      "Total loss:  -0.761 | PDE Loss:  -1.4035 | Function Loss:  -1.8732\n",
      "Total loss:  -0.761 | PDE Loss:  -1.4037 | Function Loss:  -1.8732\n",
      "Total loss:  -0.7611 | PDE Loss:  -1.4041 | Function Loss:  -1.8732\n",
      "Total loss:  -0.7611 | PDE Loss:  -1.4045 | Function Loss:  -1.8731\n",
      "Total loss:  -0.7612 | PDE Loss:  -1.405 | Function Loss:  -1.8731\n",
      "Total loss:  -0.7614 | PDE Loss:  -1.4054 | Function Loss:  -1.8732\n",
      "Total loss:  -0.7615 | PDE Loss:  -1.4066 | Function Loss:  -1.873\n",
      "Total loss:  -0.7616 | PDE Loss:  -1.4064 | Function Loss:  -1.8732\n",
      "Total loss:  -0.7618 | PDE Loss:  -1.4061 | Function Loss:  -1.8735\n",
      "Total loss:  -0.7618 | PDE Loss:  -1.4056 | Function Loss:  -1.8737\n",
      "Total loss:  -0.7619 | PDE Loss:  -1.4053 | Function Loss:  -1.8739\n",
      "Total loss:  -0.7619 | PDE Loss:  -1.405 | Function Loss:  -1.874\n",
      "Total loss:  -0.762 | PDE Loss:  -1.4047 | Function Loss:  -1.8742\n",
      "Total loss:  -0.7621 | PDE Loss:  -1.4045 | Function Loss:  -1.8744\n",
      "Total loss:  -0.7622 | PDE Loss:  -1.4045 | Function Loss:  -1.8745\n",
      "Total loss:  -0.7622 | PDE Loss:  -1.4045 | Function Loss:  -1.8746\n",
      "Total loss:  -0.7623 | PDE Loss:  -1.4046 | Function Loss:  -1.8746\n",
      "Total loss:  -0.7624 | PDE Loss:  -1.4048 | Function Loss:  -1.8746\n",
      "Total loss:  -0.7624 | PDE Loss:  -1.4053 | Function Loss:  -1.8746\n",
      "Total loss:  -0.7625 | PDE Loss:  -1.4047 | Function Loss:  -1.8748\n",
      "Total loss:  -0.7625 | PDE Loss:  -1.4054 | Function Loss:  -1.8747\n",
      "Total loss:  -0.7626 | PDE Loss:  -1.406 | Function Loss:  -1.8746\n",
      "Total loss:  -0.7627 | PDE Loss:  -1.4065 | Function Loss:  -1.8745\n",
      "Total loss:  -0.7627 | PDE Loss:  -1.4068 | Function Loss:  -1.8745\n",
      "Total loss:  -0.7628 | PDE Loss:  -1.4069 | Function Loss:  -1.8745\n",
      "Total loss:  -0.7628 | PDE Loss:  -1.407 | Function Loss:  -1.8745\n",
      "Total loss:  -0.7628 | PDE Loss:  -1.407 | Function Loss:  -1.8746\n",
      "Total loss:  -0.7629 | PDE Loss:  -1.407 | Function Loss:  -1.8747\n",
      "Total loss:  -0.7629 | PDE Loss:  -1.407 | Function Loss:  -1.8747\n",
      "Total loss:  -0.763 | PDE Loss:  -1.407 | Function Loss:  -1.8748\n",
      "Total loss:  -0.763 | PDE Loss:  -1.4071 | Function Loss:  -1.8748\n",
      "Total loss:  -0.7631 | PDE Loss:  -1.4072 | Function Loss:  -1.8748\n",
      "Total loss:  -0.7631 | PDE Loss:  -1.4075 | Function Loss:  -1.8748\n",
      "Total loss:  -0.7631 | PDE Loss:  -1.4077 | Function Loss:  -1.8748\n",
      "Total loss:  -0.7632 | PDE Loss:  -1.4081 | Function Loss:  -1.8747\n",
      "Total loss:  -0.7632 | PDE Loss:  -1.4085 | Function Loss:  -1.8747\n",
      "Total loss:  -0.7633 | PDE Loss:  -1.4092 | Function Loss:  -1.8746\n",
      "Total loss:  -0.7632 | PDE Loss:  -1.4084 | Function Loss:  -1.8747\n",
      "Total loss:  -0.7634 | PDE Loss:  -1.4091 | Function Loss:  -1.8747\n",
      "Total loss:  -0.7635 | PDE Loss:  -1.4098 | Function Loss:  -1.8746\n",
      "Total loss:  -0.7636 | PDE Loss:  -1.4106 | Function Loss:  -1.8745\n",
      "Total loss:  -0.7637 | PDE Loss:  -1.4112 | Function Loss:  -1.8745\n",
      "Total loss:  -0.7638 | PDE Loss:  -1.4115 | Function Loss:  -1.8745\n",
      "Total loss:  -0.7638 | PDE Loss:  -1.4117 | Function Loss:  -1.8745\n",
      "Total loss:  -0.7639 | PDE Loss:  -1.4116 | Function Loss:  -1.8746\n",
      "Total loss:  -0.7639 | PDE Loss:  -1.4115 | Function Loss:  -1.8747\n",
      "Total loss:  -0.7639 | PDE Loss:  -1.4115 | Function Loss:  -1.8747\n",
      "Total loss:  -0.764 | PDE Loss:  -1.4116 | Function Loss:  -1.8747\n",
      "Total loss:  -0.7641 | PDE Loss:  -1.4118 | Function Loss:  -1.8748\n",
      "Total loss:  -0.7641 | PDE Loss:  -1.4119 | Function Loss:  -1.8748\n",
      "Total loss:  -0.7642 | PDE Loss:  -1.4119 | Function Loss:  -1.8749\n",
      "Total loss:  -0.7642 | PDE Loss:  -1.4126 | Function Loss:  -1.8747\n",
      "Total loss:  -0.7642 | PDE Loss:  -1.4126 | Function Loss:  -1.8748\n",
      "Total loss:  -0.7643 | PDE Loss:  -1.4126 | Function Loss:  -1.8748\n",
      "Total loss:  -0.7643 | PDE Loss:  -1.4128 | Function Loss:  -1.8748\n",
      "Total loss:  -0.7643 | PDE Loss:  -1.4129 | Function Loss:  -1.8748\n",
      "Total loss:  -0.7644 | PDE Loss:  -1.413 | Function Loss:  -1.8749\n",
      "Total loss:  -0.7644 | PDE Loss:  -1.413 | Function Loss:  -1.8749\n",
      "Total loss:  -0.7645 | PDE Loss:  -1.413 | Function Loss:  -1.875\n",
      "Total loss:  -0.7641 | PDE Loss:  -1.4114 | Function Loss:  -1.875\n",
      "Total loss:  -0.7645 | PDE Loss:  -1.4129 | Function Loss:  -1.875\n",
      "Total loss:  -0.7646 | PDE Loss:  -1.4128 | Function Loss:  -1.8752\n",
      "Total loss:  -0.7647 | PDE Loss:  -1.4126 | Function Loss:  -1.8754\n",
      "Total loss:  -0.7648 | PDE Loss:  -1.4123 | Function Loss:  -1.8756\n",
      "Total loss:  -0.7648 | PDE Loss:  -1.4119 | Function Loss:  -1.8758\n",
      "Total loss:  -0.7649 | PDE Loss:  -1.4115 | Function Loss:  -1.876\n",
      "Total loss:  -0.765 | PDE Loss:  -1.4112 | Function Loss:  -1.8761\n",
      "Total loss:  -0.7651 | PDE Loss:  -1.4105 | Function Loss:  -1.8764\n",
      "Total loss:  -0.7651 | PDE Loss:  -1.41 | Function Loss:  -1.8767\n",
      "Total loss:  -0.7652 | PDE Loss:  -1.4099 | Function Loss:  -1.8767\n",
      "Total loss:  -0.7652 | PDE Loss:  -1.41 | Function Loss:  -1.8768\n",
      "Total loss:  -0.7653 | PDE Loss:  -1.4101 | Function Loss:  -1.8769\n",
      "Total loss:  -0.7653 | PDE Loss:  -1.4101 | Function Loss:  -1.8769\n",
      "Total loss:  -0.7654 | PDE Loss:  -1.4102 | Function Loss:  -1.877\n",
      "Total loss:  -0.7654 | PDE Loss:  -1.4102 | Function Loss:  -1.877\n",
      "Total loss:  -0.7654 | PDE Loss:  -1.4097 | Function Loss:  -1.8771\n",
      "Total loss:  -0.7654 | PDE Loss:  -1.41 | Function Loss:  -1.8771\n",
      "Total loss:  -0.7655 | PDE Loss:  -1.41 | Function Loss:  -1.8771\n",
      "Total loss:  -0.7655 | PDE Loss:  -1.41 | Function Loss:  -1.8771\n",
      "Total loss:  -0.7655 | PDE Loss:  -1.41 | Function Loss:  -1.8772\n",
      "Total loss:  -0.7655 | PDE Loss:  -1.4099 | Function Loss:  -1.8772\n",
      "Total loss:  -0.7656 | PDE Loss:  -1.4099 | Function Loss:  -1.8773\n",
      "Total loss:  -0.7656 | PDE Loss:  -1.4098 | Function Loss:  -1.8773\n",
      "Total loss:  -0.7656 | PDE Loss:  -1.4098 | Function Loss:  -1.8774\n",
      "Total loss:  -0.7657 | PDE Loss:  -1.4098 | Function Loss:  -1.8774\n",
      "Total loss:  -0.7657 | PDE Loss:  -1.4095 | Function Loss:  -1.8775\n",
      "Total loss:  -0.7657 | PDE Loss:  -1.4097 | Function Loss:  -1.8775\n",
      "Total loss:  -0.7658 | PDE Loss:  -1.4097 | Function Loss:  -1.8776\n",
      "Total loss:  -0.7658 | PDE Loss:  -1.4097 | Function Loss:  -1.8776\n",
      "Total loss:  -0.7658 | PDE Loss:  -1.4096 | Function Loss:  -1.8777\n",
      "Total loss:  -0.7659 | PDE Loss:  -1.4095 | Function Loss:  -1.8778\n",
      "Total loss:  -0.7659 | PDE Loss:  -1.4093 | Function Loss:  -1.8779\n",
      "Total loss:  -0.766 | PDE Loss:  -1.409 | Function Loss:  -1.8781\n",
      "Total loss:  -0.766 | PDE Loss:  -1.4089 | Function Loss:  -1.8782\n",
      "Total loss:  -0.7661 | PDE Loss:  -1.4085 | Function Loss:  -1.8784\n",
      "Total loss:  -0.7662 | PDE Loss:  -1.4083 | Function Loss:  -1.8785\n",
      "Total loss:  -0.7662 | PDE Loss:  -1.4082 | Function Loss:  -1.8787\n",
      "Total loss:  -0.7663 | PDE Loss:  -1.4079 | Function Loss:  -1.8789\n",
      "Total loss:  -0.7664 | PDE Loss:  -1.4078 | Function Loss:  -1.879\n",
      "Total loss:  -0.7664 | PDE Loss:  -1.407 | Function Loss:  -1.8792\n",
      "Total loss:  -0.7664 | PDE Loss:  -1.4075 | Function Loss:  -1.8791\n",
      "Total loss:  -0.7665 | PDE Loss:  -1.4076 | Function Loss:  -1.8791\n",
      "Total loss:  -0.7665 | PDE Loss:  -1.4077 | Function Loss:  -1.8792\n",
      "Total loss:  -0.7666 | PDE Loss:  -1.4078 | Function Loss:  -1.8792\n",
      "Total loss:  -0.7666 | PDE Loss:  -1.4079 | Function Loss:  -1.8792\n",
      "Total loss:  -0.7666 | PDE Loss:  -1.4079 | Function Loss:  -1.8792\n",
      "Total loss:  -0.7667 | PDE Loss:  -1.4079 | Function Loss:  -1.8793\n",
      "Total loss:  -0.7667 | PDE Loss:  -1.408 | Function Loss:  -1.8793\n",
      "Total loss:  -0.7668 | PDE Loss:  -1.408 | Function Loss:  -1.8794\n",
      "Total loss:  -0.7668 | PDE Loss:  -1.4079 | Function Loss:  -1.8795\n",
      "Total loss:  -0.7669 | PDE Loss:  -1.4079 | Function Loss:  -1.8796\n",
      "Total loss:  -0.7669 | PDE Loss:  -1.408 | Function Loss:  -1.8796\n",
      "Total loss:  -0.767 | PDE Loss:  -1.408 | Function Loss:  -1.8797\n",
      "Total loss:  -0.7671 | PDE Loss:  -1.4081 | Function Loss:  -1.8797\n",
      "Total loss:  -0.7671 | PDE Loss:  -1.4081 | Function Loss:  -1.8798\n",
      "Total loss:  -0.7672 | PDE Loss:  -1.4082 | Function Loss:  -1.8798\n",
      "Total loss:  -0.7672 | PDE Loss:  -1.4082 | Function Loss:  -1.8799\n",
      "Total loss:  -0.7673 | PDE Loss:  -1.4082 | Function Loss:  -1.88\n",
      "Total loss:  -0.7673 | PDE Loss:  -1.4082 | Function Loss:  -1.8801\n",
      "Total loss:  -0.7674 | PDE Loss:  -1.4081 | Function Loss:  -1.8802\n",
      "Total loss:  -0.7675 | PDE Loss:  -1.4079 | Function Loss:  -1.8804\n",
      "Total loss:  -0.7675 | PDE Loss:  -1.4078 | Function Loss:  -1.8805\n",
      "Total loss:  -0.7676 | PDE Loss:  -1.4077 | Function Loss:  -1.8805\n",
      "Total loss:  -0.7676 | PDE Loss:  -1.4077 | Function Loss:  -1.8806\n",
      "Total loss:  -0.7677 | PDE Loss:  -1.4077 | Function Loss:  -1.8807\n",
      "Total loss:  -0.7677 | PDE Loss:  -1.4078 | Function Loss:  -1.8807\n",
      "Total loss:  -0.7678 | PDE Loss:  -1.4079 | Function Loss:  -1.8808\n",
      "Total loss:  -0.7679 | PDE Loss:  -1.4084 | Function Loss:  -1.8808\n",
      "Total loss:  -0.768 | PDE Loss:  -1.4087 | Function Loss:  -1.8808\n",
      "Total loss:  -0.7681 | PDE Loss:  -1.4089 | Function Loss:  -1.8808\n",
      "Total loss:  -0.7681 | PDE Loss:  -1.4098 | Function Loss:  -1.8806\n",
      "Total loss:  -0.7682 | PDE Loss:  -1.4097 | Function Loss:  -1.8807\n",
      "Total loss:  -0.7682 | PDE Loss:  -1.4096 | Function Loss:  -1.8808\n",
      "Total loss:  -0.7682 | PDE Loss:  -1.4096 | Function Loss:  -1.8808\n",
      "Total loss:  -0.7683 | PDE Loss:  -1.4096 | Function Loss:  -1.8809\n",
      "Total loss:  -0.7683 | PDE Loss:  -1.4097 | Function Loss:  -1.8809\n",
      "Total loss:  -0.7684 | PDE Loss:  -1.4097 | Function Loss:  -1.8809\n",
      "Total loss:  -0.7683 | PDE Loss:  -1.411 | Function Loss:  -1.8805\n",
      "Total loss:  -0.7684 | PDE Loss:  -1.4104 | Function Loss:  -1.8808\n",
      "Total loss:  -0.7684 | PDE Loss:  -1.4104 | Function Loss:  -1.8808\n",
      "Total loss:  -0.7685 | PDE Loss:  -1.4106 | Function Loss:  -1.8809\n",
      "Total loss:  -0.7686 | PDE Loss:  -1.4109 | Function Loss:  -1.8809\n",
      "Total loss:  -0.7687 | PDE Loss:  -1.4113 | Function Loss:  -1.8809\n",
      "Total loss:  -0.7687 | PDE Loss:  -1.4114 | Function Loss:  -1.8809\n",
      "Total loss:  -0.7688 | PDE Loss:  -1.4118 | Function Loss:  -1.8809\n",
      "Total loss:  -0.7688 | PDE Loss:  -1.4123 | Function Loss:  -1.8808\n",
      "Total loss:  -0.7689 | PDE Loss:  -1.4127 | Function Loss:  -1.8807\n",
      "Total loss:  -0.769 | PDE Loss:  -1.4132 | Function Loss:  -1.8807\n",
      "Total loss:  -0.7691 | PDE Loss:  -1.4135 | Function Loss:  -1.8808\n",
      "Total loss:  -0.7692 | PDE Loss:  -1.4139 | Function Loss:  -1.8808\n",
      "Total loss:  -0.7692 | PDE Loss:  -1.4139 | Function Loss:  -1.8808\n",
      "Total loss:  -0.7693 | PDE Loss:  -1.414 | Function Loss:  -1.8809\n",
      "Total loss:  -0.7685 | PDE Loss:  -1.4127 | Function Loss:  -1.8803\n",
      "Total loss:  -0.7693 | PDE Loss:  -1.414 | Function Loss:  -1.8809\n",
      "Total loss:  -0.7694 | PDE Loss:  -1.414 | Function Loss:  -1.881\n",
      "Total loss:  -0.7694 | PDE Loss:  -1.4141 | Function Loss:  -1.8811\n",
      "Total loss:  -0.7695 | PDE Loss:  -1.4142 | Function Loss:  -1.8811\n",
      "Total loss:  -0.7696 | PDE Loss:  -1.4144 | Function Loss:  -1.8812\n",
      "Total loss:  -0.7698 | PDE Loss:  -1.4146 | Function Loss:  -1.8813\n",
      "Total loss:  -0.7699 | PDE Loss:  -1.4151 | Function Loss:  -1.8813\n",
      "Total loss:  -0.77 | PDE Loss:  -1.415 | Function Loss:  -1.8815\n",
      "Total loss:  -0.7701 | PDE Loss:  -1.4155 | Function Loss:  -1.8815\n",
      "Total loss:  -0.7702 | PDE Loss:  -1.4159 | Function Loss:  -1.8815\n",
      "Total loss:  -0.7703 | PDE Loss:  -1.4164 | Function Loss:  -1.8815\n",
      "Total loss:  -0.7705 | PDE Loss:  -1.4166 | Function Loss:  -1.8816\n",
      "Total loss:  -0.7706 | PDE Loss:  -1.417 | Function Loss:  -1.8817\n",
      "Total loss:  -0.7706 | PDE Loss:  -1.4167 | Function Loss:  -1.8818\n",
      "Total loss:  -0.7707 | PDE Loss:  -1.4169 | Function Loss:  -1.8818\n",
      "Total loss:  -0.7707 | PDE Loss:  -1.417 | Function Loss:  -1.8819\n",
      "Total loss:  -0.7708 | PDE Loss:  -1.417 | Function Loss:  -1.8819\n",
      "Total loss:  -0.7708 | PDE Loss:  -1.4169 | Function Loss:  -1.882\n",
      "Total loss:  -0.7708 | PDE Loss:  -1.4168 | Function Loss:  -1.8821\n",
      "Total loss:  -0.7709 | PDE Loss:  -1.4167 | Function Loss:  -1.8822\n",
      "Total loss:  -0.7709 | PDE Loss:  -1.4163 | Function Loss:  -1.8823\n",
      "Total loss:  -0.771 | PDE Loss:  -1.4162 | Function Loss:  -1.8824\n",
      "Total loss:  -0.771 | PDE Loss:  -1.4161 | Function Loss:  -1.8825\n",
      "Total loss:  -0.7711 | PDE Loss:  -1.4161 | Function Loss:  -1.8826\n",
      "Total loss:  -0.7712 | PDE Loss:  -1.4164 | Function Loss:  -1.8826\n",
      "Total loss:  -0.7713 | PDE Loss:  -1.4168 | Function Loss:  -1.8827\n",
      "Total loss:  -0.7714 | PDE Loss:  -1.4175 | Function Loss:  -1.8826\n",
      "Total loss:  -0.7715 | PDE Loss:  -1.4179 | Function Loss:  -1.8825\n",
      "Total loss:  -0.7715 | PDE Loss:  -1.4185 | Function Loss:  -1.8825\n",
      "Total loss:  -0.7716 | PDE Loss:  -1.419 | Function Loss:  -1.8824\n",
      "Total loss:  -0.7717 | PDE Loss:  -1.4196 | Function Loss:  -1.8824\n",
      "Total loss:  -0.7717 | PDE Loss:  -1.42 | Function Loss:  -1.8823\n",
      "Total loss:  -0.7718 | PDE Loss:  -1.4201 | Function Loss:  -1.8824\n",
      "Total loss:  -0.7719 | PDE Loss:  -1.4204 | Function Loss:  -1.8824\n",
      "Total loss:  -0.772 | PDE Loss:  -1.4205 | Function Loss:  -1.8824\n",
      "Total loss:  -0.772 | PDE Loss:  -1.4205 | Function Loss:  -1.8825\n",
      "Total loss:  -0.7721 | PDE Loss:  -1.4205 | Function Loss:  -1.8826\n",
      "Total loss:  -0.7721 | PDE Loss:  -1.4205 | Function Loss:  -1.8827\n",
      "Total loss:  -0.7722 | PDE Loss:  -1.4205 | Function Loss:  -1.8828\n",
      "Total loss:  -0.7723 | PDE Loss:  -1.4207 | Function Loss:  -1.8828\n",
      "Total loss:  -0.7724 | PDE Loss:  -1.4209 | Function Loss:  -1.8829\n",
      "Total loss:  -0.7725 | PDE Loss:  -1.4215 | Function Loss:  -1.8829\n",
      "Total loss:  -0.7726 | PDE Loss:  -1.4215 | Function Loss:  -1.8829\n",
      "Total loss:  -0.7726 | PDE Loss:  -1.4219 | Function Loss:  -1.8829\n",
      "Total loss:  -0.7727 | PDE Loss:  -1.4223 | Function Loss:  -1.8829\n",
      "Total loss:  -0.7727 | PDE Loss:  -1.4226 | Function Loss:  -1.8828\n",
      "Total loss:  -0.7727 | PDE Loss:  -1.4228 | Function Loss:  -1.8828\n",
      "Total loss:  -0.7728 | PDE Loss:  -1.423 | Function Loss:  -1.8828\n",
      "Total loss:  -0.7728 | PDE Loss:  -1.4232 | Function Loss:  -1.8828\n",
      "Total loss:  -0.7706 | PDE Loss:  -1.4235 | Function Loss:  -1.8798\n",
      "Total loss:  -0.7728 | PDE Loss:  -1.4236 | Function Loss:  -1.8827\n",
      "Total loss:  -0.7729 | PDE Loss:  -1.4237 | Function Loss:  -1.8827\n",
      "Total loss:  -0.7729 | PDE Loss:  -1.424 | Function Loss:  -1.8827\n",
      "Total loss:  -0.773 | PDE Loss:  -1.4242 | Function Loss:  -1.8827\n",
      "Total loss:  -0.773 | PDE Loss:  -1.4244 | Function Loss:  -1.8827\n",
      "Total loss:  -0.7731 | PDE Loss:  -1.4246 | Function Loss:  -1.8827\n",
      "Total loss:  -0.7731 | PDE Loss:  -1.4248 | Function Loss:  -1.8827\n",
      "Total loss:  -0.773 | PDE Loss:  -1.426 | Function Loss:  -1.8822\n",
      "Total loss:  -0.7731 | PDE Loss:  -1.4251 | Function Loss:  -1.8826\n",
      "Total loss:  -0.7731 | PDE Loss:  -1.4253 | Function Loss:  -1.8826\n",
      "Total loss:  -0.7732 | PDE Loss:  -1.4254 | Function Loss:  -1.8826\n",
      "Total loss:  -0.7732 | PDE Loss:  -1.4255 | Function Loss:  -1.8826\n",
      "Total loss:  -0.7732 | PDE Loss:  -1.4256 | Function Loss:  -1.8826\n",
      "Total loss:  -0.7733 | PDE Loss:  -1.4257 | Function Loss:  -1.8826\n",
      "Total loss:  -0.7733 | PDE Loss:  -1.4257 | Function Loss:  -1.8827\n",
      "Total loss:  -0.7733 | PDE Loss:  -1.4256 | Function Loss:  -1.8827\n",
      "Total loss:  -0.7734 | PDE Loss:  -1.4254 | Function Loss:  -1.8829\n",
      "Total loss:  -0.7734 | PDE Loss:  -1.4251 | Function Loss:  -1.883\n",
      "Total loss:  -0.7734 | PDE Loss:  -1.4252 | Function Loss:  -1.883\n",
      "Total loss:  -0.7735 | PDE Loss:  -1.4251 | Function Loss:  -1.8831\n",
      "Total loss:  -0.7736 | PDE Loss:  -1.4252 | Function Loss:  -1.8832\n",
      "Total loss:  -0.7736 | PDE Loss:  -1.4252 | Function Loss:  -1.8832\n",
      "Total loss:  -0.7736 | PDE Loss:  -1.4252 | Function Loss:  -1.8832\n",
      "Total loss:  -0.7737 | PDE Loss:  -1.4254 | Function Loss:  -1.8833\n",
      "Total loss:  -0.7738 | PDE Loss:  -1.4258 | Function Loss:  -1.8832\n",
      "Total loss:  -0.7737 | PDE Loss:  -1.4262 | Function Loss:  -1.8831\n",
      "Total loss:  -0.7738 | PDE Loss:  -1.426 | Function Loss:  -1.8832\n",
      "Total loss:  -0.7739 | PDE Loss:  -1.4267 | Function Loss:  -1.8831\n",
      "Total loss:  -0.7739 | PDE Loss:  -1.4272 | Function Loss:  -1.8831\n",
      "Total loss:  -0.774 | PDE Loss:  -1.4275 | Function Loss:  -1.8831\n",
      "Total loss:  -0.7741 | PDE Loss:  -1.4278 | Function Loss:  -1.8831\n",
      "Total loss:  -0.7741 | PDE Loss:  -1.4279 | Function Loss:  -1.8831\n",
      "Total loss:  -0.7741 | PDE Loss:  -1.4282 | Function Loss:  -1.883\n",
      "Total loss:  -0.7741 | PDE Loss:  -1.4282 | Function Loss:  -1.883\n",
      "Total loss:  -0.7742 | PDE Loss:  -1.4282 | Function Loss:  -1.8831\n",
      "Total loss:  -0.7742 | PDE Loss:  -1.428 | Function Loss:  -1.8832\n",
      "Total loss:  -0.7743 | PDE Loss:  -1.4278 | Function Loss:  -1.8833\n",
      "Total loss:  -0.7743 | PDE Loss:  -1.4275 | Function Loss:  -1.8834\n",
      "Total loss:  -0.7743 | PDE Loss:  -1.4277 | Function Loss:  -1.8834\n",
      "Total loss:  -0.7743 | PDE Loss:  -1.4275 | Function Loss:  -1.8835\n",
      "Total loss:  -0.7744 | PDE Loss:  -1.4271 | Function Loss:  -1.8837\n",
      "Total loss:  -0.7744 | PDE Loss:  -1.4268 | Function Loss:  -1.8838\n",
      "Total loss:  -0.7745 | PDE Loss:  -1.4265 | Function Loss:  -1.884\n",
      "Total loss:  -0.7745 | PDE Loss:  -1.4264 | Function Loss:  -1.8841\n",
      "Total loss:  -0.7746 | PDE Loss:  -1.4263 | Function Loss:  -1.8841\n",
      "Total loss:  -0.7746 | PDE Loss:  -1.4263 | Function Loss:  -1.8842\n",
      "Total loss:  -0.7747 | PDE Loss:  -1.4264 | Function Loss:  -1.8842\n",
      "Total loss:  -0.7747 | PDE Loss:  -1.4264 | Function Loss:  -1.8843\n",
      "Total loss:  -0.7748 | PDE Loss:  -1.4265 | Function Loss:  -1.8844\n",
      "Total loss:  -0.7749 | PDE Loss:  -1.4267 | Function Loss:  -1.8844\n",
      "Total loss:  -0.7749 | PDE Loss:  -1.4269 | Function Loss:  -1.8844\n",
      "Total loss:  -0.775 | PDE Loss:  -1.4272 | Function Loss:  -1.8844\n",
      "Total loss:  -0.775 | PDE Loss:  -1.4273 | Function Loss:  -1.8844\n",
      "Total loss:  -0.775 | PDE Loss:  -1.4274 | Function Loss:  -1.8844\n",
      "Total loss:  -0.775 | PDE Loss:  -1.4274 | Function Loss:  -1.8844\n",
      "Total loss:  -0.775 | PDE Loss:  -1.4275 | Function Loss:  -1.8844\n",
      "Total loss:  -0.7751 | PDE Loss:  -1.4275 | Function Loss:  -1.8844\n",
      "Total loss:  -0.7751 | PDE Loss:  -1.4275 | Function Loss:  -1.8845\n",
      "Total loss:  -0.7751 | PDE Loss:  -1.4275 | Function Loss:  -1.8845\n",
      "Total loss:  -0.7752 | PDE Loss:  -1.4275 | Function Loss:  -1.8845\n",
      "Total loss:  -0.7752 | PDE Loss:  -1.4275 | Function Loss:  -1.8846\n",
      "Total loss:  -0.7752 | PDE Loss:  -1.4276 | Function Loss:  -1.8846\n",
      "Total loss:  -0.7753 | PDE Loss:  -1.4277 | Function Loss:  -1.8846\n",
      "Total loss:  -0.7753 | PDE Loss:  -1.4278 | Function Loss:  -1.8846\n",
      "Total loss:  -0.7753 | PDE Loss:  -1.4279 | Function Loss:  -1.8846\n",
      "Total loss:  -0.7753 | PDE Loss:  -1.428 | Function Loss:  -1.8847\n",
      "Total loss:  -0.7754 | PDE Loss:  -1.428 | Function Loss:  -1.8847\n",
      "Total loss:  -0.7754 | PDE Loss:  -1.428 | Function Loss:  -1.8847\n",
      "Total loss:  -0.7755 | PDE Loss:  -1.4281 | Function Loss:  -1.8848\n",
      "Total loss:  -0.7755 | PDE Loss:  -1.4277 | Function Loss:  -1.885\n",
      "Total loss:  -0.7756 | PDE Loss:  -1.4277 | Function Loss:  -1.885\n",
      "Total loss:  -0.7756 | PDE Loss:  -1.4277 | Function Loss:  -1.8851\n",
      "Total loss:  -0.7757 | PDE Loss:  -1.4276 | Function Loss:  -1.8852\n",
      "Total loss:  -0.7757 | PDE Loss:  -1.4275 | Function Loss:  -1.8853\n",
      "Total loss:  -0.7758 | PDE Loss:  -1.4273 | Function Loss:  -1.8854\n",
      "Total loss:  -0.7758 | PDE Loss:  -1.4271 | Function Loss:  -1.8855\n",
      "Total loss:  -0.7759 | PDE Loss:  -1.4267 | Function Loss:  -1.8857\n",
      "Total loss:  -0.7759 | PDE Loss:  -1.4263 | Function Loss:  -1.8858\n",
      "Total loss:  -0.7759 | PDE Loss:  -1.4262 | Function Loss:  -1.8859\n",
      "Total loss:  -0.776 | PDE Loss:  -1.426 | Function Loss:  -1.886\n",
      "Total loss:  -0.776 | PDE Loss:  -1.4258 | Function Loss:  -1.8861\n",
      "Total loss:  -0.776 | PDE Loss:  -1.4258 | Function Loss:  -1.8861\n",
      "Total loss:  -0.776 | PDE Loss:  -1.4258 | Function Loss:  -1.8862\n",
      "Total loss:  -0.7761 | PDE Loss:  -1.426 | Function Loss:  -1.8862\n",
      "Total loss:  -0.7761 | PDE Loss:  -1.4262 | Function Loss:  -1.8862\n",
      "Total loss:  -0.7762 | PDE Loss:  -1.4263 | Function Loss:  -1.8862\n",
      "Total loss:  -0.7763 | PDE Loss:  -1.4266 | Function Loss:  -1.8862\n",
      "Total loss:  -0.7763 | PDE Loss:  -1.4267 | Function Loss:  -1.8862\n",
      "Total loss:  -0.7763 | PDE Loss:  -1.4268 | Function Loss:  -1.8863\n",
      "Total loss:  -0.7764 | PDE Loss:  -1.4266 | Function Loss:  -1.8864\n",
      "Total loss:  -0.7764 | PDE Loss:  -1.4266 | Function Loss:  -1.8865\n",
      "Total loss:  -0.7765 | PDE Loss:  -1.4266 | Function Loss:  -1.8865\n",
      "Total loss:  -0.7766 | PDE Loss:  -1.4263 | Function Loss:  -1.8867\n",
      "Total loss:  -0.7766 | PDE Loss:  -1.4264 | Function Loss:  -1.8868\n",
      "Total loss:  -0.7767 | PDE Loss:  -1.4262 | Function Loss:  -1.8869\n",
      "Total loss:  -0.7767 | PDE Loss:  -1.4263 | Function Loss:  -1.8869\n",
      "Total loss:  -0.7768 | PDE Loss:  -1.426 | Function Loss:  -1.8871\n",
      "Total loss:  -0.7768 | PDE Loss:  -1.4259 | Function Loss:  -1.8872\n",
      "Total loss:  -0.7769 | PDE Loss:  -1.4257 | Function Loss:  -1.8873\n",
      "Total loss:  -0.7769 | PDE Loss:  -1.4256 | Function Loss:  -1.8874\n",
      "Total loss:  -0.777 | PDE Loss:  -1.4255 | Function Loss:  -1.8875\n",
      "Total loss:  -0.777 | PDE Loss:  -1.4254 | Function Loss:  -1.8875\n",
      "Total loss:  -0.777 | PDE Loss:  -1.4253 | Function Loss:  -1.8876\n",
      "Total loss:  -0.7771 | PDE Loss:  -1.4253 | Function Loss:  -1.8877\n",
      "Total loss:  -0.7771 | PDE Loss:  -1.4252 | Function Loss:  -1.8878\n",
      "Total loss:  -0.7772 | PDE Loss:  -1.4252 | Function Loss:  -1.8878\n",
      "Total loss:  -0.7772 | PDE Loss:  -1.4255 | Function Loss:  -1.8877\n",
      "Total loss:  -0.7772 | PDE Loss:  -1.4254 | Function Loss:  -1.8878\n",
      "Total loss:  -0.7772 | PDE Loss:  -1.4254 | Function Loss:  -1.8878\n",
      "Total loss:  -0.7772 | PDE Loss:  -1.4255 | Function Loss:  -1.8878\n",
      "Total loss:  -0.7772 | PDE Loss:  -1.4256 | Function Loss:  -1.8878\n",
      "Total loss:  -0.7773 | PDE Loss:  -1.4257 | Function Loss:  -1.8878\n",
      "Total loss:  -0.7773 | PDE Loss:  -1.4258 | Function Loss:  -1.8878\n",
      "Total loss:  -0.7773 | PDE Loss:  -1.426 | Function Loss:  -1.8878\n",
      "Total loss:  -0.7774 | PDE Loss:  -1.4262 | Function Loss:  -1.8878\n",
      "Total loss:  -0.7774 | PDE Loss:  -1.4262 | Function Loss:  -1.8878\n",
      "Total loss:  -0.7775 | PDE Loss:  -1.4263 | Function Loss:  -1.8878\n",
      "Total loss:  -0.7775 | PDE Loss:  -1.4263 | Function Loss:  -1.8879\n",
      "Total loss:  -0.7775 | PDE Loss:  -1.4263 | Function Loss:  -1.8879\n",
      "Total loss:  -0.7776 | PDE Loss:  -1.4263 | Function Loss:  -1.888\n",
      "Total loss:  -0.7776 | PDE Loss:  -1.4263 | Function Loss:  -1.888\n",
      "Total loss:  -0.7776 | PDE Loss:  -1.4263 | Function Loss:  -1.8881\n",
      "Total loss:  -0.7777 | PDE Loss:  -1.4264 | Function Loss:  -1.8881\n",
      "Total loss:  -0.7777 | PDE Loss:  -1.4265 | Function Loss:  -1.8881\n",
      "Total loss:  -0.7778 | PDE Loss:  -1.4268 | Function Loss:  -1.8881\n",
      "Total loss:  -0.7778 | PDE Loss:  -1.4269 | Function Loss:  -1.8882\n",
      "Total loss:  -0.7779 | PDE Loss:  -1.427 | Function Loss:  -1.8882\n",
      "Total loss:  -0.778 | PDE Loss:  -1.4272 | Function Loss:  -1.8883\n",
      "Total loss:  -0.7781 | PDE Loss:  -1.4273 | Function Loss:  -1.8884\n",
      "Total loss:  -0.7781 | PDE Loss:  -1.4273 | Function Loss:  -1.8884\n",
      "Total loss:  -0.7782 | PDE Loss:  -1.4273 | Function Loss:  -1.8885\n",
      "Total loss:  -0.7783 | PDE Loss:  -1.4272 | Function Loss:  -1.8886\n",
      "Total loss:  -0.7783 | PDE Loss:  -1.4272 | Function Loss:  -1.8887\n",
      "Total loss:  -0.7784 | PDE Loss:  -1.4271 | Function Loss:  -1.8888\n",
      "Total loss:  -0.7784 | PDE Loss:  -1.427 | Function Loss:  -1.8889\n",
      "Total loss:  -0.7784 | PDE Loss:  -1.427 | Function Loss:  -1.8889\n",
      "Total loss:  -0.7785 | PDE Loss:  -1.427 | Function Loss:  -1.889\n",
      "Total loss:  -0.7785 | PDE Loss:  -1.427 | Function Loss:  -1.889\n",
      "Total loss:  -0.7786 | PDE Loss:  -1.4271 | Function Loss:  -1.8891\n",
      "Total loss:  -0.7786 | PDE Loss:  -1.4273 | Function Loss:  -1.8891\n",
      "Total loss:  -0.7787 | PDE Loss:  -1.4276 | Function Loss:  -1.8891\n",
      "Total loss:  -0.7787 | PDE Loss:  -1.4278 | Function Loss:  -1.889\n",
      "Total loss:  -0.7787 | PDE Loss:  -1.428 | Function Loss:  -1.889\n",
      "Total loss:  -0.7787 | PDE Loss:  -1.4282 | Function Loss:  -1.889\n",
      "Total loss:  -0.7788 | PDE Loss:  -1.4285 | Function Loss:  -1.8889\n",
      "Total loss:  -0.7788 | PDE Loss:  -1.4287 | Function Loss:  -1.8889\n",
      "Total loss:  -0.7788 | PDE Loss:  -1.429 | Function Loss:  -1.8888\n",
      "Total loss:  -0.7789 | PDE Loss:  -1.4293 | Function Loss:  -1.8888\n",
      "Total loss:  -0.7789 | PDE Loss:  -1.4297 | Function Loss:  -1.8888\n",
      "Total loss:  -0.7789 | PDE Loss:  -1.4302 | Function Loss:  -1.8886\n",
      "Total loss:  -0.779 | PDE Loss:  -1.4302 | Function Loss:  -1.8887\n",
      "Total loss:  -0.779 | PDE Loss:  -1.4301 | Function Loss:  -1.8888\n",
      "Total loss:  -0.7791 | PDE Loss:  -1.43 | Function Loss:  -1.8889\n",
      "Total loss:  -0.7791 | PDE Loss:  -1.4298 | Function Loss:  -1.889\n",
      "Total loss:  -0.7791 | PDE Loss:  -1.4297 | Function Loss:  -1.889\n",
      "Total loss:  -0.7791 | PDE Loss:  -1.4295 | Function Loss:  -1.8891\n",
      "Total loss:  -0.7792 | PDE Loss:  -1.4295 | Function Loss:  -1.8891\n",
      "Total loss:  -0.7792 | PDE Loss:  -1.4294 | Function Loss:  -1.8892\n",
      "Total loss:  -0.7792 | PDE Loss:  -1.4295 | Function Loss:  -1.8892\n",
      "Total loss:  -0.7793 | PDE Loss:  -1.4297 | Function Loss:  -1.8893\n",
      "Total loss:  -0.7793 | PDE Loss:  -1.4299 | Function Loss:  -1.8892\n",
      "Total loss:  -0.7794 | PDE Loss:  -1.4301 | Function Loss:  -1.8892\n",
      "Total loss:  -0.7794 | PDE Loss:  -1.4303 | Function Loss:  -1.8892\n",
      "Total loss:  -0.7794 | PDE Loss:  -1.4305 | Function Loss:  -1.8891\n",
      "Total loss:  -0.7794 | PDE Loss:  -1.4304 | Function Loss:  -1.8892\n",
      "Total loss:  -0.7794 | PDE Loss:  -1.4305 | Function Loss:  -1.8892\n",
      "Total loss:  -0.7794 | PDE Loss:  -1.4306 | Function Loss:  -1.8891\n",
      "Total loss:  -0.7794 | PDE Loss:  -1.4307 | Function Loss:  -1.8891\n",
      "Total loss:  -0.7794 | PDE Loss:  -1.4307 | Function Loss:  -1.8891\n",
      "Total loss:  -0.7794 | PDE Loss:  -1.4307 | Function Loss:  -1.8892\n",
      "Total loss:  -0.7795 | PDE Loss:  -1.4307 | Function Loss:  -1.8892\n",
      "Total loss:  -0.7795 | PDE Loss:  -1.4307 | Function Loss:  -1.8892\n",
      "Total loss:  -0.7795 | PDE Loss:  -1.4307 | Function Loss:  -1.8892\n",
      "Total loss:  -0.7795 | PDE Loss:  -1.4308 | Function Loss:  -1.8892\n",
      "Total loss:  -0.7796 | PDE Loss:  -1.4309 | Function Loss:  -1.8892\n",
      "Total loss:  -0.7796 | PDE Loss:  -1.4311 | Function Loss:  -1.8892\n",
      "Total loss:  -0.7796 | PDE Loss:  -1.4312 | Function Loss:  -1.8892\n",
      "Total loss:  -0.7797 | PDE Loss:  -1.4314 | Function Loss:  -1.8892\n",
      "Total loss:  -0.7797 | PDE Loss:  -1.4316 | Function Loss:  -1.8893\n",
      "Total loss:  -0.7798 | PDE Loss:  -1.4317 | Function Loss:  -1.8893\n",
      "Total loss:  -0.7798 | PDE Loss:  -1.4317 | Function Loss:  -1.8893\n",
      "Total loss:  -0.7798 | PDE Loss:  -1.4318 | Function Loss:  -1.8893\n",
      "Total loss:  -0.7799 | PDE Loss:  -1.4315 | Function Loss:  -1.8895\n",
      "Total loss:  -0.7799 | PDE Loss:  -1.4316 | Function Loss:  -1.8895\n",
      "Total loss:  -0.78 | PDE Loss:  -1.4317 | Function Loss:  -1.8896\n",
      "Total loss:  -0.7801 | PDE Loss:  -1.4317 | Function Loss:  -1.8897\n",
      "Total loss:  -0.7801 | PDE Loss:  -1.4317 | Function Loss:  -1.8897\n",
      "Total loss:  -0.7802 | PDE Loss:  -1.4316 | Function Loss:  -1.8898\n",
      "Total loss:  -0.7796 | PDE Loss:  -1.4316 | Function Loss:  -1.889\n",
      "Total loss:  -0.7802 | PDE Loss:  -1.4318 | Function Loss:  -1.8898\n",
      "Total loss:  -0.7802 | PDE Loss:  -1.4318 | Function Loss:  -1.8898\n",
      "Total loss:  -0.7803 | PDE Loss:  -1.4317 | Function Loss:  -1.8899\n",
      "Total loss:  -0.7803 | PDE Loss:  -1.4319 | Function Loss:  -1.89\n",
      "Total loss:  -0.7804 | PDE Loss:  -1.432 | Function Loss:  -1.89\n",
      "Total loss:  -0.7805 | PDE Loss:  -1.432 | Function Loss:  -1.8901\n",
      "Total loss:  -0.7805 | PDE Loss:  -1.432 | Function Loss:  -1.8901\n",
      "Total loss:  -0.7805 | PDE Loss:  -1.432 | Function Loss:  -1.8902\n",
      "Total loss:  -0.7805 | PDE Loss:  -1.4319 | Function Loss:  -1.8902\n",
      "Total loss:  -0.7805 | PDE Loss:  -1.4318 | Function Loss:  -1.8902\n",
      "Total loss:  -0.7806 | PDE Loss:  -1.4317 | Function Loss:  -1.8903\n",
      "Total loss:  -0.7806 | PDE Loss:  -1.4316 | Function Loss:  -1.8904\n",
      "Total loss:  -0.7807 | PDE Loss:  -1.4314 | Function Loss:  -1.8905\n",
      "Total loss:  -0.7807 | PDE Loss:  -1.4313 | Function Loss:  -1.8906\n",
      "Total loss:  -0.7808 | PDE Loss:  -1.4312 | Function Loss:  -1.8907\n",
      "Total loss:  -0.7808 | PDE Loss:  -1.4311 | Function Loss:  -1.8908\n",
      "Total loss:  -0.7808 | PDE Loss:  -1.431 | Function Loss:  -1.8908\n",
      "Total loss:  -0.7809 | PDE Loss:  -1.431 | Function Loss:  -1.8909\n",
      "Total loss:  -0.7809 | PDE Loss:  -1.431 | Function Loss:  -1.8909\n",
      "Total loss:  -0.7809 | PDE Loss:  -1.431 | Function Loss:  -1.891\n",
      "Total loss:  -0.7809 | PDE Loss:  -1.4309 | Function Loss:  -1.891\n",
      "Total loss:  -0.781 | PDE Loss:  -1.4308 | Function Loss:  -1.8911\n",
      "Total loss:  -0.781 | PDE Loss:  -1.4303 | Function Loss:  -1.8912\n",
      "Total loss:  -0.781 | PDE Loss:  -1.4306 | Function Loss:  -1.8912\n",
      "Total loss:  -0.7811 | PDE Loss:  -1.4306 | Function Loss:  -1.8912\n",
      "Total loss:  -0.7811 | PDE Loss:  -1.4305 | Function Loss:  -1.8913\n",
      "Total loss:  -0.7812 | PDE Loss:  -1.4306 | Function Loss:  -1.8914\n",
      "Total loss:  -0.7812 | PDE Loss:  -1.4306 | Function Loss:  -1.8914\n",
      "Total loss:  -0.7812 | PDE Loss:  -1.4307 | Function Loss:  -1.8915\n",
      "Total loss:  -0.7813 | PDE Loss:  -1.4307 | Function Loss:  -1.8915\n",
      "Total loss:  -0.7813 | PDE Loss:  -1.4307 | Function Loss:  -1.8916\n",
      "Total loss:  -0.7813 | PDE Loss:  -1.4306 | Function Loss:  -1.8916\n",
      "Total loss:  -0.7814 | PDE Loss:  -1.4306 | Function Loss:  -1.8917\n",
      "Total loss:  -0.7814 | PDE Loss:  -1.4306 | Function Loss:  -1.8917\n",
      "Total loss:  -0.7815 | PDE Loss:  -1.4306 | Function Loss:  -1.8918\n",
      "Total loss:  -0.7815 | PDE Loss:  -1.4306 | Function Loss:  -1.8918\n",
      "Total loss:  -0.7816 | PDE Loss:  -1.4304 | Function Loss:  -1.892\n",
      "Total loss:  -0.7816 | PDE Loss:  -1.4305 | Function Loss:  -1.892\n",
      "Total loss:  -0.7816 | PDE Loss:  -1.4304 | Function Loss:  -1.8921\n",
      "Total loss:  -0.7817 | PDE Loss:  -1.4309 | Function Loss:  -1.892\n",
      "Total loss:  -0.7817 | PDE Loss:  -1.4308 | Function Loss:  -1.8921\n",
      "Total loss:  -0.7818 | PDE Loss:  -1.4308 | Function Loss:  -1.8921\n",
      "Total loss:  -0.7818 | PDE Loss:  -1.4308 | Function Loss:  -1.8922\n",
      "Total loss:  -0.7819 | PDE Loss:  -1.4309 | Function Loss:  -1.8922\n",
      "Total loss:  -0.7819 | PDE Loss:  -1.431 | Function Loss:  -1.8923\n",
      "Total loss:  -0.782 | PDE Loss:  -1.4314 | Function Loss:  -1.8922\n",
      "Total loss:  -0.782 | PDE Loss:  -1.4317 | Function Loss:  -1.8922\n",
      "Total loss:  -0.7821 | PDE Loss:  -1.4318 | Function Loss:  -1.8922\n",
      "Total loss:  -0.7821 | PDE Loss:  -1.432 | Function Loss:  -1.8922\n",
      "Total loss:  -0.7822 | PDE Loss:  -1.432 | Function Loss:  -1.8923\n",
      "Total loss:  -0.7822 | PDE Loss:  -1.432 | Function Loss:  -1.8924\n",
      "Total loss:  -0.7823 | PDE Loss:  -1.432 | Function Loss:  -1.8924\n",
      "Total loss:  -0.7823 | PDE Loss:  -1.4321 | Function Loss:  -1.8925\n",
      "Total loss:  -0.7824 | PDE Loss:  -1.4321 | Function Loss:  -1.8925\n",
      "Total loss:  -0.7824 | PDE Loss:  -1.4322 | Function Loss:  -1.8926\n",
      "Total loss:  -0.7825 | PDE Loss:  -1.4323 | Function Loss:  -1.8926\n",
      "Total loss:  -0.7825 | PDE Loss:  -1.4323 | Function Loss:  -1.8926\n",
      "Total loss:  -0.7825 | PDE Loss:  -1.4324 | Function Loss:  -1.8927\n",
      "Total loss:  -0.7826 | PDE Loss:  -1.4325 | Function Loss:  -1.8927\n",
      "Total loss:  -0.7826 | PDE Loss:  -1.4325 | Function Loss:  -1.8928\n",
      "Total loss:  -0.7827 | PDE Loss:  -1.4326 | Function Loss:  -1.8928\n",
      "Total loss:  -0.7827 | PDE Loss:  -1.4327 | Function Loss:  -1.8928\n",
      "Total loss:  -0.7828 | PDE Loss:  -1.4328 | Function Loss:  -1.8928\n",
      "Total loss:  -0.7828 | PDE Loss:  -1.4328 | Function Loss:  -1.8929\n",
      "Total loss:  -0.7828 | PDE Loss:  -1.4329 | Function Loss:  -1.8929\n",
      "Total loss:  -0.7829 | PDE Loss:  -1.4329 | Function Loss:  -1.8929\n",
      "Total loss:  -0.7829 | PDE Loss:  -1.433 | Function Loss:  -1.8929\n",
      "Total loss:  -0.7829 | PDE Loss:  -1.433 | Function Loss:  -1.893\n",
      "Total loss:  -0.783 | PDE Loss:  -1.4331 | Function Loss:  -1.893\n",
      "Total loss:  -0.783 | PDE Loss:  -1.4332 | Function Loss:  -1.893\n",
      "Total loss:  -0.783 | PDE Loss:  -1.4333 | Function Loss:  -1.893\n",
      "Total loss:  -0.7831 | PDE Loss:  -1.4334 | Function Loss:  -1.893\n",
      "Total loss:  -0.7831 | PDE Loss:  -1.4335 | Function Loss:  -1.8931\n",
      "Total loss:  -0.7831 | PDE Loss:  -1.4332 | Function Loss:  -1.8932\n",
      "Total loss:  -0.7832 | PDE Loss:  -1.4335 | Function Loss:  -1.8932\n",
      "Total loss:  -0.7832 | PDE Loss:  -1.4337 | Function Loss:  -1.8932\n",
      "Total loss:  -0.7833 | PDE Loss:  -1.4339 | Function Loss:  -1.8932\n",
      "Total loss:  -0.7833 | PDE Loss:  -1.4341 | Function Loss:  -1.8932\n",
      "Total loss:  -0.7834 | PDE Loss:  -1.4341 | Function Loss:  -1.8932\n",
      "Total loss:  -0.7834 | PDE Loss:  -1.4342 | Function Loss:  -1.8932\n",
      "Total loss:  -0.7834 | PDE Loss:  -1.4337 | Function Loss:  -1.8933\n",
      "Total loss:  -0.7834 | PDE Loss:  -1.4341 | Function Loss:  -1.8933\n",
      "Total loss:  -0.7835 | PDE Loss:  -1.4341 | Function Loss:  -1.8934\n",
      "Total loss:  -0.7835 | PDE Loss:  -1.434 | Function Loss:  -1.8935\n",
      "Total loss:  -0.7836 | PDE Loss:  -1.4338 | Function Loss:  -1.8936\n",
      "Total loss:  -0.7836 | PDE Loss:  -1.4337 | Function Loss:  -1.8937\n",
      "Total loss:  -0.7837 | PDE Loss:  -1.4335 | Function Loss:  -1.8938\n",
      "Total loss:  -0.7837 | PDE Loss:  -1.4333 | Function Loss:  -1.8939\n",
      "Total loss:  -0.7838 | PDE Loss:  -1.4331 | Function Loss:  -1.894\n",
      "Total loss:  -0.7838 | PDE Loss:  -1.4331 | Function Loss:  -1.8941\n",
      "Total loss:  -0.7838 | PDE Loss:  -1.4331 | Function Loss:  -1.8941\n",
      "Total loss:  -0.7839 | PDE Loss:  -1.4331 | Function Loss:  -1.8942\n",
      "Total loss:  -0.7839 | PDE Loss:  -1.4332 | Function Loss:  -1.8942\n",
      "Total loss:  -0.7839 | PDE Loss:  -1.4333 | Function Loss:  -1.8942\n",
      "Total loss:  -0.7839 | PDE Loss:  -1.4335 | Function Loss:  -1.8941\n",
      "Total loss:  -0.784 | PDE Loss:  -1.4334 | Function Loss:  -1.8942\n",
      "Total loss:  -0.784 | PDE Loss:  -1.4335 | Function Loss:  -1.8942\n",
      "Total loss:  -0.7841 | PDE Loss:  -1.4336 | Function Loss:  -1.8943\n",
      "Total loss:  -0.7841 | PDE Loss:  -1.4337 | Function Loss:  -1.8943\n",
      "Total loss:  -0.7842 | PDE Loss:  -1.4337 | Function Loss:  -1.8944\n",
      "Total loss:  -0.7843 | PDE Loss:  -1.4336 | Function Loss:  -1.8945\n",
      "Total loss:  -0.7843 | PDE Loss:  -1.4335 | Function Loss:  -1.8946\n",
      "Total loss:  -0.7843 | PDE Loss:  -1.4331 | Function Loss:  -1.8948\n",
      "Total loss:  -0.7844 | PDE Loss:  -1.433 | Function Loss:  -1.8949\n",
      "Total loss:  -0.7844 | PDE Loss:  -1.433 | Function Loss:  -1.8949\n",
      "Total loss:  -0.7845 | PDE Loss:  -1.433 | Function Loss:  -1.895\n",
      "Total loss:  -0.7845 | PDE Loss:  -1.433 | Function Loss:  -1.895\n",
      "Total loss:  -0.7846 | PDE Loss:  -1.433 | Function Loss:  -1.8951\n",
      "Total loss:  -0.7846 | PDE Loss:  -1.4329 | Function Loss:  -1.8951\n",
      "Total loss:  -0.7846 | PDE Loss:  -1.4328 | Function Loss:  -1.8952\n",
      "Total loss:  -0.7847 | PDE Loss:  -1.4327 | Function Loss:  -1.8953\n",
      "Total loss:  -0.7847 | PDE Loss:  -1.4323 | Function Loss:  -1.8954\n",
      "Total loss:  -0.7847 | PDE Loss:  -1.4322 | Function Loss:  -1.8955\n",
      "Total loss:  -0.7848 | PDE Loss:  -1.4322 | Function Loss:  -1.8956\n",
      "Total loss:  -0.7849 | PDE Loss:  -1.4322 | Function Loss:  -1.8957\n",
      "Total loss:  -0.7849 | PDE Loss:  -1.4323 | Function Loss:  -1.8957\n",
      "Total loss:  -0.7849 | PDE Loss:  -1.4323 | Function Loss:  -1.8958\n",
      "Total loss:  -0.785 | PDE Loss:  -1.4323 | Function Loss:  -1.8958\n",
      "Total loss:  -0.7849 | PDE Loss:  -1.4327 | Function Loss:  -1.8956\n",
      "Total loss:  -0.785 | PDE Loss:  -1.4325 | Function Loss:  -1.8958\n",
      "Total loss:  -0.785 | PDE Loss:  -1.4325 | Function Loss:  -1.8958\n",
      "Total loss:  -0.785 | PDE Loss:  -1.4325 | Function Loss:  -1.8959\n",
      "Total loss:  -0.7851 | PDE Loss:  -1.4324 | Function Loss:  -1.8959\n",
      "Total loss:  -0.7851 | PDE Loss:  -1.4324 | Function Loss:  -1.8959\n",
      "Total loss:  -0.7851 | PDE Loss:  -1.4324 | Function Loss:  -1.896\n",
      "Total loss:  -0.7851 | PDE Loss:  -1.4323 | Function Loss:  -1.896\n",
      "Total loss:  -0.7851 | PDE Loss:  -1.4323 | Function Loss:  -1.896\n",
      "Total loss:  -0.7852 | PDE Loss:  -1.4322 | Function Loss:  -1.8961\n",
      "Total loss:  -0.7852 | PDE Loss:  -1.432 | Function Loss:  -1.8962\n",
      "Total loss:  -0.7852 | PDE Loss:  -1.432 | Function Loss:  -1.8962\n",
      "Total loss:  -0.7853 | PDE Loss:  -1.432 | Function Loss:  -1.8963\n",
      "Total loss:  -0.7853 | PDE Loss:  -1.432 | Function Loss:  -1.8963\n",
      "Total loss:  -0.7853 | PDE Loss:  -1.4319 | Function Loss:  -1.8964\n",
      "Total loss:  -0.7853 | PDE Loss:  -1.4319 | Function Loss:  -1.8964\n",
      "Total loss:  -0.7853 | PDE Loss:  -1.4314 | Function Loss:  -1.8965\n",
      "Total loss:  -0.7854 | PDE Loss:  -1.4318 | Function Loss:  -1.8965\n",
      "Total loss:  -0.7854 | PDE Loss:  -1.4317 | Function Loss:  -1.8965\n",
      "Total loss:  -0.7854 | PDE Loss:  -1.4316 | Function Loss:  -1.8966\n",
      "Total loss:  -0.7855 | PDE Loss:  -1.4315 | Function Loss:  -1.8967\n",
      "Total loss:  -0.7855 | PDE Loss:  -1.4314 | Function Loss:  -1.8968\n",
      "Total loss:  -0.7855 | PDE Loss:  -1.4312 | Function Loss:  -1.8969\n",
      "Total loss:  -0.7856 | PDE Loss:  -1.431 | Function Loss:  -1.897\n",
      "Total loss:  -0.7856 | PDE Loss:  -1.4307 | Function Loss:  -1.8971\n",
      "Total loss:  -0.7857 | PDE Loss:  -1.4306 | Function Loss:  -1.8972\n",
      "Total loss:  -0.7857 | PDE Loss:  -1.4303 | Function Loss:  -1.8973\n",
      "Total loss:  -0.7857 | PDE Loss:  -1.4302 | Function Loss:  -1.8974\n",
      "Total loss:  -0.7857 | PDE Loss:  -1.4302 | Function Loss:  -1.8974\n",
      "Total loss:  -0.7858 | PDE Loss:  -1.4303 | Function Loss:  -1.8975\n",
      "Total loss:  -0.7858 | PDE Loss:  -1.4303 | Function Loss:  -1.8975\n",
      "Total loss:  -0.7858 | PDE Loss:  -1.4304 | Function Loss:  -1.8975\n",
      "Total loss:  -0.7859 | PDE Loss:  -1.4306 | Function Loss:  -1.8975\n",
      "Total loss:  -0.7859 | PDE Loss:  -1.4305 | Function Loss:  -1.8976\n",
      "Total loss:  -0.786 | PDE Loss:  -1.4306 | Function Loss:  -1.8976\n",
      "Total loss:  -0.786 | PDE Loss:  -1.4308 | Function Loss:  -1.8976\n",
      "Total loss:  -0.786 | PDE Loss:  -1.4309 | Function Loss:  -1.8976\n",
      "Total loss:  -0.7861 | PDE Loss:  -1.4308 | Function Loss:  -1.8977\n",
      "Total loss:  -0.7861 | PDE Loss:  -1.4307 | Function Loss:  -1.8977\n",
      "Total loss:  -0.7861 | PDE Loss:  -1.4305 | Function Loss:  -1.8978\n",
      "Total loss:  -0.786 | PDE Loss:  -1.4297 | Function Loss:  -1.898\n",
      "Total loss:  -0.7862 | PDE Loss:  -1.4303 | Function Loss:  -1.8979\n",
      "Total loss:  -0.7862 | PDE Loss:  -1.4302 | Function Loss:  -1.898\n",
      "Total loss:  -0.7862 | PDE Loss:  -1.43 | Function Loss:  -1.8981\n",
      "Total loss:  -0.7862 | PDE Loss:  -1.4298 | Function Loss:  -1.8982\n",
      "Total loss:  -0.7863 | PDE Loss:  -1.4297 | Function Loss:  -1.8982\n",
      "Total loss:  -0.7863 | PDE Loss:  -1.4296 | Function Loss:  -1.8983\n",
      "Total loss:  -0.7863 | PDE Loss:  -1.4297 | Function Loss:  -1.8983\n",
      "Total loss:  -0.7861 | PDE Loss:  -1.4279 | Function Loss:  -1.8985\n",
      "Total loss:  -0.7863 | PDE Loss:  -1.4295 | Function Loss:  -1.8984\n",
      "Total loss:  -0.7864 | PDE Loss:  -1.4296 | Function Loss:  -1.8984\n",
      "Total loss:  -0.7864 | PDE Loss:  -1.4299 | Function Loss:  -1.8984\n",
      "Total loss:  -0.7865 | PDE Loss:  -1.4302 | Function Loss:  -1.8984\n",
      "Total loss:  -0.7865 | PDE Loss:  -1.4304 | Function Loss:  -1.8984\n",
      "Total loss:  -0.7866 | PDE Loss:  -1.4304 | Function Loss:  -1.8984\n",
      "Total loss:  -0.7866 | PDE Loss:  -1.4306 | Function Loss:  -1.8984\n",
      "Total loss:  -0.7866 | PDE Loss:  -1.4306 | Function Loss:  -1.8984\n",
      "Total loss:  -0.7867 | PDE Loss:  -1.4307 | Function Loss:  -1.8984\n",
      "Total loss:  -0.7867 | PDE Loss:  -1.4308 | Function Loss:  -1.8985\n",
      "Total loss:  -0.7867 | PDE Loss:  -1.4308 | Function Loss:  -1.8985\n",
      "Total loss:  -0.7868 | PDE Loss:  -1.4309 | Function Loss:  -1.8985\n",
      "Total loss:  -0.7868 | PDE Loss:  -1.431 | Function Loss:  -1.8986\n",
      "Total loss:  -0.7868 | PDE Loss:  -1.4311 | Function Loss:  -1.8986\n",
      "Total loss:  -0.7869 | PDE Loss:  -1.4312 | Function Loss:  -1.8986\n",
      "Total loss:  -0.7869 | PDE Loss:  -1.4312 | Function Loss:  -1.8986\n",
      "Total loss:  -0.7869 | PDE Loss:  -1.4313 | Function Loss:  -1.8986\n",
      "Total loss:  -0.787 | PDE Loss:  -1.4314 | Function Loss:  -1.8987\n",
      "Total loss:  -0.787 | PDE Loss:  -1.4316 | Function Loss:  -1.8987\n",
      "Total loss:  -0.7871 | PDE Loss:  -1.4313 | Function Loss:  -1.8988\n",
      "Total loss:  -0.7871 | PDE Loss:  -1.4316 | Function Loss:  -1.8988\n",
      "Total loss:  -0.7872 | PDE Loss:  -1.4319 | Function Loss:  -1.8988\n",
      "Total loss:  -0.7872 | PDE Loss:  -1.4324 | Function Loss:  -1.8987\n",
      "Total loss:  -0.7873 | PDE Loss:  -1.4329 | Function Loss:  -1.8986\n",
      "Total loss:  -0.7874 | PDE Loss:  -1.4333 | Function Loss:  -1.8986\n",
      "Total loss:  -0.7874 | PDE Loss:  -1.4336 | Function Loss:  -1.8986\n",
      "Total loss:  -0.7875 | PDE Loss:  -1.4339 | Function Loss:  -1.8986\n",
      "Total loss:  -0.7876 | PDE Loss:  -1.434 | Function Loss:  -1.8987\n",
      "Total loss:  -0.7877 | PDE Loss:  -1.4341 | Function Loss:  -1.8988\n",
      "Total loss:  -0.7878 | PDE Loss:  -1.4341 | Function Loss:  -1.8989\n",
      "Total loss:  -0.7879 | PDE Loss:  -1.4341 | Function Loss:  -1.899\n",
      "Total loss:  -0.7879 | PDE Loss:  -1.434 | Function Loss:  -1.8991\n",
      "Total loss:  -0.788 | PDE Loss:  -1.4339 | Function Loss:  -1.8993\n",
      "Total loss:  -0.7881 | PDE Loss:  -1.4338 | Function Loss:  -1.8994\n",
      "Total loss:  -0.7881 | PDE Loss:  -1.4337 | Function Loss:  -1.8995\n",
      "Total loss:  -0.7882 | PDE Loss:  -1.4336 | Function Loss:  -1.8995\n",
      "Total loss:  -0.7882 | PDE Loss:  -1.4336 | Function Loss:  -1.8996\n",
      "Total loss:  -0.7882 | PDE Loss:  -1.4336 | Function Loss:  -1.8997\n",
      "Total loss:  -0.7883 | PDE Loss:  -1.4337 | Function Loss:  -1.8997\n",
      "Total loss:  -0.7883 | PDE Loss:  -1.4338 | Function Loss:  -1.8997\n",
      "Total loss:  -0.7884 | PDE Loss:  -1.434 | Function Loss:  -1.8998\n",
      "Total loss:  -0.7885 | PDE Loss:  -1.4343 | Function Loss:  -1.8998\n",
      "Total loss:  -0.7886 | PDE Loss:  -1.4344 | Function Loss:  -1.8998\n",
      "Total loss:  -0.7886 | PDE Loss:  -1.4345 | Function Loss:  -1.8999\n",
      "Total loss:  -0.7886 | PDE Loss:  -1.4346 | Function Loss:  -1.8999\n",
      "Total loss:  -0.7887 | PDE Loss:  -1.4345 | Function Loss:  -1.9\n",
      "Total loss:  -0.7887 | PDE Loss:  -1.4345 | Function Loss:  -1.9\n",
      "Total loss:  -0.7888 | PDE Loss:  -1.4343 | Function Loss:  -1.9001\n",
      "Total loss:  -0.7888 | PDE Loss:  -1.4341 | Function Loss:  -1.9003\n",
      "Total loss:  -0.7889 | PDE Loss:  -1.4338 | Function Loss:  -1.9004\n",
      "Total loss:  -0.7889 | PDE Loss:  -1.4333 | Function Loss:  -1.9006\n",
      "Total loss:  -0.7889 | PDE Loss:  -1.4332 | Function Loss:  -1.9007\n",
      "Total loss:  -0.789 | PDE Loss:  -1.4328 | Function Loss:  -1.9009\n",
      "Total loss:  -0.7892 | PDE Loss:  -1.4319 | Function Loss:  -1.9014\n",
      "Total loss:  -0.7893 | PDE Loss:  -1.4317 | Function Loss:  -1.9015\n",
      "Total loss:  -0.7894 | PDE Loss:  -1.4316 | Function Loss:  -1.9017\n",
      "Total loss:  -0.7895 | PDE Loss:  -1.4315 | Function Loss:  -1.9018\n",
      "Total loss:  -0.7896 | PDE Loss:  -1.4316 | Function Loss:  -1.9019\n",
      "Total loss:  -0.7896 | PDE Loss:  -1.4314 | Function Loss:  -1.9021\n",
      "Total loss:  -0.7897 | PDE Loss:  -1.4314 | Function Loss:  -1.9022\n",
      "Total loss:  -0.7898 | PDE Loss:  -1.4311 | Function Loss:  -1.9024\n",
      "Total loss:  -0.7899 | PDE Loss:  -1.4309 | Function Loss:  -1.9026\n",
      "Total loss:  -0.79 | PDE Loss:  -1.4305 | Function Loss:  -1.9028\n",
      "Total loss:  -0.7901 | PDE Loss:  -1.4302 | Function Loss:  -1.903\n",
      "Total loss:  -0.7901 | PDE Loss:  -1.4301 | Function Loss:  -1.9031\n",
      "Total loss:  -0.7901 | PDE Loss:  -1.4297 | Function Loss:  -1.9033\n",
      "Total loss:  -0.7902 | PDE Loss:  -1.4295 | Function Loss:  -1.9034\n",
      "Total loss:  -0.7902 | PDE Loss:  -1.4293 | Function Loss:  -1.9035\n",
      "Total loss:  -0.7903 | PDE Loss:  -1.4291 | Function Loss:  -1.9036\n",
      "Total loss:  -0.7903 | PDE Loss:  -1.4289 | Function Loss:  -1.9037\n",
      "Total loss:  -0.7904 | PDE Loss:  -1.4291 | Function Loss:  -1.9038\n",
      "Total loss:  -0.7904 | PDE Loss:  -1.4293 | Function Loss:  -1.9038\n",
      "Total loss:  -0.7905 | PDE Loss:  -1.4295 | Function Loss:  -1.9037\n",
      "Total loss:  -0.7905 | PDE Loss:  -1.4298 | Function Loss:  -1.9037\n",
      "Total loss:  -0.7905 | PDE Loss:  -1.4302 | Function Loss:  -1.9036\n",
      "Total loss:  -0.7906 | PDE Loss:  -1.4306 | Function Loss:  -1.9036\n",
      "Total loss:  -0.7907 | PDE Loss:  -1.4312 | Function Loss:  -1.9035\n",
      "Total loss:  -0.7908 | PDE Loss:  -1.4314 | Function Loss:  -1.9036\n",
      "Total loss:  -0.7909 | PDE Loss:  -1.4316 | Function Loss:  -1.9036\n",
      "Total loss:  -0.7909 | PDE Loss:  -1.4317 | Function Loss:  -1.9037\n",
      "Total loss:  -0.791 | PDE Loss:  -1.4317 | Function Loss:  -1.9038\n",
      "Total loss:  -0.791 | PDE Loss:  -1.4318 | Function Loss:  -1.9038\n",
      "Total loss:  -0.7911 | PDE Loss:  -1.4319 | Function Loss:  -1.9038\n",
      "Total loss:  -0.7911 | PDE Loss:  -1.4321 | Function Loss:  -1.9038\n",
      "Total loss:  -0.7912 | PDE Loss:  -1.4325 | Function Loss:  -1.9038\n",
      "Total loss:  -0.7912 | PDE Loss:  -1.4329 | Function Loss:  -1.9037\n",
      "Total loss:  -0.7913 | PDE Loss:  -1.4335 | Function Loss:  -1.9036\n",
      "Total loss:  -0.7913 | PDE Loss:  -1.4338 | Function Loss:  -1.9035\n",
      "Total loss:  -0.7913 | PDE Loss:  -1.4337 | Function Loss:  -1.9036\n",
      "Total loss:  -0.7914 | PDE Loss:  -1.434 | Function Loss:  -1.9036\n",
      "Total loss:  -0.7914 | PDE Loss:  -1.4343 | Function Loss:  -1.9035\n",
      "Total loss:  -0.7914 | PDE Loss:  -1.4345 | Function Loss:  -1.9035\n",
      "Total loss:  -0.7915 | PDE Loss:  -1.4345 | Function Loss:  -1.9035\n",
      "Total loss:  -0.7915 | PDE Loss:  -1.4346 | Function Loss:  -1.9036\n",
      "Total loss:  -0.7915 | PDE Loss:  -1.4345 | Function Loss:  -1.9036\n",
      "Total loss:  -0.7916 | PDE Loss:  -1.4345 | Function Loss:  -1.9037\n",
      "Total loss:  -0.7916 | PDE Loss:  -1.4344 | Function Loss:  -1.9038\n",
      "Total loss:  -0.7917 | PDE Loss:  -1.4342 | Function Loss:  -1.904\n",
      "Total loss:  -0.7917 | PDE Loss:  -1.4342 | Function Loss:  -1.904\n",
      "Total loss:  -0.7918 | PDE Loss:  -1.4342 | Function Loss:  -1.9041\n",
      "Total loss:  -0.7918 | PDE Loss:  -1.4344 | Function Loss:  -1.9041\n",
      "Total loss:  -0.7919 | PDE Loss:  -1.4345 | Function Loss:  -1.9041\n",
      "Total loss:  -0.7919 | PDE Loss:  -1.4347 | Function Loss:  -1.904\n",
      "Total loss:  -0.7919 | PDE Loss:  -1.435 | Function Loss:  -1.904\n",
      "Total loss:  -0.792 | PDE Loss:  -1.4352 | Function Loss:  -1.904\n",
      "Total loss:  -0.7915 | PDE Loss:  -1.4356 | Function Loss:  -1.9033\n",
      "Total loss:  -0.792 | PDE Loss:  -1.4354 | Function Loss:  -1.904\n",
      "Total loss:  -0.792 | PDE Loss:  -1.4356 | Function Loss:  -1.9039\n",
      "Total loss:  -0.792 | PDE Loss:  -1.4359 | Function Loss:  -1.9039\n",
      "Total loss:  -0.7921 | PDE Loss:  -1.436 | Function Loss:  -1.9039\n",
      "Total loss:  -0.7921 | PDE Loss:  -1.436 | Function Loss:  -1.9039\n",
      "Total loss:  -0.7921 | PDE Loss:  -1.4359 | Function Loss:  -1.904\n",
      "Total loss:  -0.792 | PDE Loss:  -1.4356 | Function Loss:  -1.904\n",
      "Total loss:  -0.7921 | PDE Loss:  -1.4359 | Function Loss:  -1.904\n",
      "Total loss:  -0.7922 | PDE Loss:  -1.4358 | Function Loss:  -1.9041\n",
      "Total loss:  -0.7922 | PDE Loss:  -1.4355 | Function Loss:  -1.9042\n",
      "Total loss:  -0.7923 | PDE Loss:  -1.4353 | Function Loss:  -1.9044\n",
      "Total loss:  -0.7923 | PDE Loss:  -1.4351 | Function Loss:  -1.9045\n",
      "Total loss:  -0.7924 | PDE Loss:  -1.435 | Function Loss:  -1.9047\n",
      "Total loss:  -0.7925 | PDE Loss:  -1.4349 | Function Loss:  -1.9048\n",
      "Total loss:  -0.7926 | PDE Loss:  -1.4349 | Function Loss:  -1.9049\n",
      "Total loss:  -0.7926 | PDE Loss:  -1.4349 | Function Loss:  -1.9049\n",
      "Total loss:  -0.7927 | PDE Loss:  -1.435 | Function Loss:  -1.905\n",
      "Total loss:  -0.7927 | PDE Loss:  -1.4347 | Function Loss:  -1.9051\n",
      "Total loss:  -0.7928 | PDE Loss:  -1.4349 | Function Loss:  -1.9051\n",
      "Total loss:  -0.7927 | PDE Loss:  -1.4342 | Function Loss:  -1.9052\n",
      "Total loss:  -0.7928 | PDE Loss:  -1.4349 | Function Loss:  -1.9052\n",
      "Total loss:  -0.7929 | PDE Loss:  -1.4349 | Function Loss:  -1.9053\n",
      "Total loss:  -0.793 | PDE Loss:  -1.4352 | Function Loss:  -1.9053\n",
      "Total loss:  -0.793 | PDE Loss:  -1.4352 | Function Loss:  -1.9054\n",
      "Total loss:  -0.7931 | PDE Loss:  -1.4355 | Function Loss:  -1.9054\n",
      "Total loss:  -0.7932 | PDE Loss:  -1.4356 | Function Loss:  -1.9055\n",
      "Total loss:  -0.7933 | PDE Loss:  -1.436 | Function Loss:  -1.9055\n",
      "Total loss:  -0.7935 | PDE Loss:  -1.4361 | Function Loss:  -1.9057\n",
      "Total loss:  -0.7936 | PDE Loss:  -1.4366 | Function Loss:  -1.9057\n",
      "Total loss:  -0.7937 | PDE Loss:  -1.437 | Function Loss:  -1.9057\n",
      "Total loss:  -0.7938 | PDE Loss:  -1.4377 | Function Loss:  -1.9056\n",
      "Total loss:  -0.7939 | PDE Loss:  -1.4387 | Function Loss:  -1.9055\n",
      "Total loss:  -0.794 | PDE Loss:  -1.4395 | Function Loss:  -1.9053\n",
      "Total loss:  -0.7941 | PDE Loss:  -1.4399 | Function Loss:  -1.9053\n",
      "Total loss:  -0.7942 | PDE Loss:  -1.4405 | Function Loss:  -1.9053\n",
      "Total loss:  -0.7943 | PDE Loss:  -1.4409 | Function Loss:  -1.9053\n",
      "Total loss:  -0.7944 | PDE Loss:  -1.4411 | Function Loss:  -1.9054\n",
      "Total loss:  -0.7945 | PDE Loss:  -1.4413 | Function Loss:  -1.9054\n",
      "Total loss:  -0.7945 | PDE Loss:  -1.4412 | Function Loss:  -1.9055\n",
      "Total loss:  -0.7946 | PDE Loss:  -1.441 | Function Loss:  -1.9057\n",
      "Total loss:  -0.7946 | PDE Loss:  -1.4407 | Function Loss:  -1.9058\n",
      "Total loss:  -0.7947 | PDE Loss:  -1.4403 | Function Loss:  -1.9061\n",
      "Total loss:  -0.7948 | PDE Loss:  -1.44 | Function Loss:  -1.9062\n",
      "Total loss:  -0.7949 | PDE Loss:  -1.4396 | Function Loss:  -1.9064\n",
      "Total loss:  -0.7949 | PDE Loss:  -1.4393 | Function Loss:  -1.9066\n",
      "Total loss:  -0.795 | PDE Loss:  -1.4394 | Function Loss:  -1.9067\n",
      "Total loss:  -0.795 | PDE Loss:  -1.4395 | Function Loss:  -1.9067\n",
      "Total loss:  -0.7951 | PDE Loss:  -1.4399 | Function Loss:  -1.9066\n",
      "Total loss:  -0.7951 | PDE Loss:  -1.4402 | Function Loss:  -1.9066\n",
      "Total loss:  -0.7952 | PDE Loss:  -1.4406 | Function Loss:  -1.9065\n",
      "Total loss:  -0.7952 | PDE Loss:  -1.4409 | Function Loss:  -1.9065\n",
      "Total loss:  -0.7953 | PDE Loss:  -1.4411 | Function Loss:  -1.9065\n",
      "Total loss:  -0.7953 | PDE Loss:  -1.4412 | Function Loss:  -1.9066\n",
      "Total loss:  -0.7954 | PDE Loss:  -1.4413 | Function Loss:  -1.9067\n",
      "Total loss:  -0.7955 | PDE Loss:  -1.4413 | Function Loss:  -1.9068\n",
      "Total loss:  -0.7955 | PDE Loss:  -1.4407 | Function Loss:  -1.907\n",
      "Total loss:  -0.7956 | PDE Loss:  -1.4408 | Function Loss:  -1.907\n",
      "Total loss:  -0.7956 | PDE Loss:  -1.4408 | Function Loss:  -1.9071\n",
      "Total loss:  -0.7957 | PDE Loss:  -1.4409 | Function Loss:  -1.9071\n",
      "Total loss:  -0.7957 | PDE Loss:  -1.441 | Function Loss:  -1.9072\n",
      "Total loss:  -0.7951 | PDE Loss:  -1.44 | Function Loss:  -1.9067\n",
      "Total loss:  -0.7957 | PDE Loss:  -1.4411 | Function Loss:  -1.9072\n",
      "Total loss:  -0.7958 | PDE Loss:  -1.4412 | Function Loss:  -1.9072\n",
      "Total loss:  -0.7959 | PDE Loss:  -1.4415 | Function Loss:  -1.9072\n",
      "Total loss:  -0.796 | PDE Loss:  -1.4418 | Function Loss:  -1.9073\n",
      "Total loss:  -0.7961 | PDE Loss:  -1.4425 | Function Loss:  -1.9072\n",
      "Total loss:  -0.7962 | PDE Loss:  -1.4427 | Function Loss:  -1.9072\n",
      "Total loss:  -0.7962 | PDE Loss:  -1.4429 | Function Loss:  -1.9072\n",
      "Total loss:  -0.7962 | PDE Loss:  -1.4432 | Function Loss:  -1.9072\n",
      "Total loss:  -0.7963 | PDE Loss:  -1.4434 | Function Loss:  -1.9072\n",
      "Total loss:  -0.7963 | PDE Loss:  -1.4433 | Function Loss:  -1.9073\n",
      "Total loss:  -0.7963 | PDE Loss:  -1.4444 | Function Loss:  -1.9069\n",
      "Total loss:  -0.7964 | PDE Loss:  -1.444 | Function Loss:  -1.9071\n",
      "Total loss:  -0.7964 | PDE Loss:  -1.4439 | Function Loss:  -1.9072\n",
      "Total loss:  -0.7964 | PDE Loss:  -1.4438 | Function Loss:  -1.9073\n",
      "Total loss:  -0.7965 | PDE Loss:  -1.4438 | Function Loss:  -1.9074\n",
      "Total loss:  -0.7966 | PDE Loss:  -1.4438 | Function Loss:  -1.9074\n",
      "Total loss:  -0.7966 | PDE Loss:  -1.444 | Function Loss:  -1.9074\n",
      "Total loss:  -0.7966 | PDE Loss:  -1.4442 | Function Loss:  -1.9074\n",
      "Total loss:  -0.7967 | PDE Loss:  -1.4448 | Function Loss:  -1.9073\n",
      "Total loss:  -0.7968 | PDE Loss:  -1.4454 | Function Loss:  -1.9072\n",
      "Total loss:  -0.7968 | PDE Loss:  -1.4458 | Function Loss:  -1.9072\n",
      "Total loss:  -0.7969 | PDE Loss:  -1.4463 | Function Loss:  -1.9071\n",
      "Total loss:  -0.7969 | PDE Loss:  -1.4467 | Function Loss:  -1.907\n",
      "Total loss:  -0.7969 | PDE Loss:  -1.447 | Function Loss:  -1.907\n",
      "Total loss:  -0.797 | PDE Loss:  -1.4472 | Function Loss:  -1.907\n",
      "Total loss:  -0.797 | PDE Loss:  -1.4472 | Function Loss:  -1.9071\n",
      "Total loss:  -0.7971 | PDE Loss:  -1.447 | Function Loss:  -1.9072\n",
      "Total loss:  -0.7972 | PDE Loss:  -1.4467 | Function Loss:  -1.9074\n",
      "Total loss:  -0.7973 | PDE Loss:  -1.4462 | Function Loss:  -1.9076\n",
      "Total loss:  -0.7973 | PDE Loss:  -1.4457 | Function Loss:  -1.9079\n",
      "Total loss:  -0.7975 | PDE Loss:  -1.445 | Function Loss:  -1.9083\n",
      "Total loss:  -0.7976 | PDE Loss:  -1.4444 | Function Loss:  -1.9086\n",
      "Total loss:  -0.7977 | PDE Loss:  -1.4438 | Function Loss:  -1.9089\n",
      "Total loss:  -0.7977 | PDE Loss:  -1.4435 | Function Loss:  -1.909\n",
      "Total loss:  -0.7978 | PDE Loss:  -1.4433 | Function Loss:  -1.9092\n",
      "Total loss:  -0.7979 | PDE Loss:  -1.4432 | Function Loss:  -1.9093\n",
      "Total loss:  -0.798 | PDE Loss:  -1.4431 | Function Loss:  -1.9095\n",
      "Total loss:  -0.7981 | PDE Loss:  -1.4432 | Function Loss:  -1.9095\n",
      "Total loss:  -0.7981 | PDE Loss:  -1.4433 | Function Loss:  -1.9096\n",
      "Total loss:  -0.7981 | PDE Loss:  -1.4435 | Function Loss:  -1.9096\n",
      "Total loss:  -0.7982 | PDE Loss:  -1.4434 | Function Loss:  -1.9097\n",
      "Total loss:  -0.7982 | PDE Loss:  -1.4438 | Function Loss:  -1.9096\n",
      "Total loss:  -0.7983 | PDE Loss:  -1.4436 | Function Loss:  -1.9098\n",
      "Total loss:  -0.7984 | PDE Loss:  -1.4436 | Function Loss:  -1.9098\n",
      "Total loss:  -0.7985 | PDE Loss:  -1.4434 | Function Loss:  -1.91\n",
      "Total loss:  -0.7986 | PDE Loss:  -1.4433 | Function Loss:  -1.9102\n",
      "Total loss:  -0.7987 | PDE Loss:  -1.4426 | Function Loss:  -1.9106\n",
      "Total loss:  -0.7988 | PDE Loss:  -1.4424 | Function Loss:  -1.9107\n",
      "Total loss:  -0.7989 | PDE Loss:  -1.442 | Function Loss:  -1.911\n",
      "Total loss:  -0.799 | PDE Loss:  -1.4419 | Function Loss:  -1.9112\n",
      "Total loss:  -0.7991 | PDE Loss:  -1.442 | Function Loss:  -1.9113\n",
      "Total loss:  -0.7992 | PDE Loss:  -1.4422 | Function Loss:  -1.9113\n",
      "Total loss:  -0.7993 | PDE Loss:  -1.4427 | Function Loss:  -1.9113\n",
      "Total loss:  -0.7994 | PDE Loss:  -1.4429 | Function Loss:  -1.9114\n",
      "Total loss:  -0.7995 | PDE Loss:  -1.4434 | Function Loss:  -1.9113\n",
      "Total loss:  -0.7996 | PDE Loss:  -1.4435 | Function Loss:  -1.9114\n",
      "Total loss:  -0.7996 | PDE Loss:  -1.4437 | Function Loss:  -1.9114\n",
      "Total loss:  -0.7997 | PDE Loss:  -1.4438 | Function Loss:  -1.9115\n",
      "Total loss:  -0.7998 | PDE Loss:  -1.4438 | Function Loss:  -1.9117\n",
      "Total loss:  -0.7997 | PDE Loss:  -1.4425 | Function Loss:  -1.9119\n",
      "Total loss:  -0.7999 | PDE Loss:  -1.4436 | Function Loss:  -1.9118\n",
      "Total loss:  -0.8 | PDE Loss:  -1.4434 | Function Loss:  -1.912\n",
      "Total loss:  -0.8001 | PDE Loss:  -1.4431 | Function Loss:  -1.9122\n",
      "Total loss:  -0.8002 | PDE Loss:  -1.443 | Function Loss:  -1.9123\n",
      "Total loss:  -0.8002 | PDE Loss:  -1.443 | Function Loss:  -1.9124\n",
      "Total loss:  -0.8003 | PDE Loss:  -1.4427 | Function Loss:  -1.9126\n",
      "Total loss:  -0.8003 | PDE Loss:  -1.4426 | Function Loss:  -1.9126\n",
      "Total loss:  -0.8004 | PDE Loss:  -1.4424 | Function Loss:  -1.9128\n",
      "Total loss:  -0.8004 | PDE Loss:  -1.4424 | Function Loss:  -1.9128\n",
      "Total loss:  -0.8005 | PDE Loss:  -1.4421 | Function Loss:  -1.913\n",
      "Total loss:  -0.8005 | PDE Loss:  -1.4421 | Function Loss:  -1.9131\n",
      "Total loss:  -0.8006 | PDE Loss:  -1.442 | Function Loss:  -1.9132\n",
      "Total loss:  -0.8007 | PDE Loss:  -1.442 | Function Loss:  -1.9133\n",
      "Total loss:  -0.8008 | PDE Loss:  -1.4419 | Function Loss:  -1.9134\n",
      "Total loss:  -0.8008 | PDE Loss:  -1.4418 | Function Loss:  -1.9135\n",
      "Total loss:  -0.8009 | PDE Loss:  -1.4417 | Function Loss:  -1.9136\n",
      "Total loss:  -0.8009 | PDE Loss:  -1.4415 | Function Loss:  -1.9137\n",
      "Total loss:  -0.801 | PDE Loss:  -1.4413 | Function Loss:  -1.9139\n",
      "Total loss:  -0.801 | PDE Loss:  -1.4409 | Function Loss:  -1.914\n",
      "Total loss:  -0.8011 | PDE Loss:  -1.4406 | Function Loss:  -1.9142\n",
      "Total loss:  -0.8012 | PDE Loss:  -1.4403 | Function Loss:  -1.9144\n",
      "Total loss:  -0.8012 | PDE Loss:  -1.4401 | Function Loss:  -1.9145\n",
      "Total loss:  -0.8013 | PDE Loss:  -1.4399 | Function Loss:  -1.9147\n",
      "Total loss:  -0.8014 | PDE Loss:  -1.4395 | Function Loss:  -1.9149\n",
      "Total loss:  -0.8014 | PDE Loss:  -1.4392 | Function Loss:  -1.9151\n",
      "Total loss:  -0.8015 | PDE Loss:  -1.4388 | Function Loss:  -1.9153\n",
      "Total loss:  -0.8016 | PDE Loss:  -1.4384 | Function Loss:  -1.9155\n",
      "Total loss:  -0.8016 | PDE Loss:  -1.4379 | Function Loss:  -1.9157\n",
      "Total loss:  -0.8017 | PDE Loss:  -1.4375 | Function Loss:  -1.916\n",
      "Total loss:  -0.8018 | PDE Loss:  -1.4369 | Function Loss:  -1.9163\n",
      "Total loss:  -0.8019 | PDE Loss:  -1.4361 | Function Loss:  -1.9166\n",
      "Total loss:  -0.8019 | PDE Loss:  -1.4352 | Function Loss:  -1.917\n",
      "Total loss:  -0.802 | PDE Loss:  -1.4355 | Function Loss:  -1.9169\n",
      "Total loss:  -0.802 | PDE Loss:  -1.4356 | Function Loss:  -1.9169\n",
      "Total loss:  -0.8021 | PDE Loss:  -1.4359 | Function Loss:  -1.9169\n",
      "Total loss:  -0.8021 | PDE Loss:  -1.4359 | Function Loss:  -1.917\n",
      "Total loss:  -0.8022 | PDE Loss:  -1.4359 | Function Loss:  -1.9171\n",
      "Total loss:  -0.8023 | PDE Loss:  -1.4359 | Function Loss:  -1.9172\n",
      "Total loss:  -0.8023 | PDE Loss:  -1.4356 | Function Loss:  -1.9174\n",
      "Total loss:  -0.8024 | PDE Loss:  -1.4356 | Function Loss:  -1.9174\n",
      "Total loss:  -0.8025 | PDE Loss:  -1.4355 | Function Loss:  -1.9175\n",
      "Total loss:  -0.8025 | PDE Loss:  -1.4356 | Function Loss:  -1.9176\n",
      "Total loss:  -0.801 | PDE Loss:  -1.4311 | Function Loss:  -1.917\n",
      "Total loss:  -0.8026 | PDE Loss:  -1.4358 | Function Loss:  -1.9176\n",
      "Total loss:  -0.8026 | PDE Loss:  -1.4359 | Function Loss:  -1.9177\n",
      "Total loss:  -0.8027 | PDE Loss:  -1.436 | Function Loss:  -1.9177\n",
      "Total loss:  -0.8029 | PDE Loss:  -1.4371 | Function Loss:  -1.9177\n",
      "Total loss:  -0.8028 | PDE Loss:  -1.4369 | Function Loss:  -1.9176\n",
      "Total loss:  -0.803 | PDE Loss:  -1.4372 | Function Loss:  -1.9177\n",
      "Total loss:  -0.8031 | PDE Loss:  -1.4375 | Function Loss:  -1.9178\n",
      "Total loss:  -0.8032 | PDE Loss:  -1.4379 | Function Loss:  -1.9177\n",
      "Total loss:  -0.8033 | PDE Loss:  -1.4382 | Function Loss:  -1.9178\n",
      "Total loss:  -0.8033 | PDE Loss:  -1.4384 | Function Loss:  -1.9178\n",
      "Total loss:  -0.8034 | PDE Loss:  -1.4387 | Function Loss:  -1.9179\n",
      "Total loss:  -0.8036 | PDE Loss:  -1.4389 | Function Loss:  -1.9179\n",
      "Total loss:  -0.8037 | PDE Loss:  -1.4393 | Function Loss:  -1.918\n",
      "Total loss:  -0.8038 | PDE Loss:  -1.4394 | Function Loss:  -1.9181\n",
      "Total loss:  -0.8039 | PDE Loss:  -1.4398 | Function Loss:  -1.9181\n",
      "Total loss:  -0.8039 | PDE Loss:  -1.4398 | Function Loss:  -1.9182\n",
      "Total loss:  -0.804 | PDE Loss:  -1.4399 | Function Loss:  -1.9182\n",
      "Total loss:  -0.8041 | PDE Loss:  -1.44 | Function Loss:  -1.9183\n",
      "Total loss:  -0.8043 | PDE Loss:  -1.4403 | Function Loss:  -1.9184\n",
      "Total loss:  -0.8044 | PDE Loss:  -1.4402 | Function Loss:  -1.9187\n",
      "Total loss:  -0.8046 | PDE Loss:  -1.4406 | Function Loss:  -1.9187\n",
      "Total loss:  -0.8047 | PDE Loss:  -1.4407 | Function Loss:  -1.9188\n",
      "Total loss:  -0.8048 | PDE Loss:  -1.4408 | Function Loss:  -1.9189\n",
      "Total loss:  -0.8048 | PDE Loss:  -1.4409 | Function Loss:  -1.919\n",
      "Total loss:  -0.8049 | PDE Loss:  -1.4408 | Function Loss:  -1.9191\n",
      "Total loss:  -0.805 | PDE Loss:  -1.4407 | Function Loss:  -1.9192\n",
      "Total loss:  -0.805 | PDE Loss:  -1.4404 | Function Loss:  -1.9194\n",
      "Total loss:  -0.8051 | PDE Loss:  -1.4403 | Function Loss:  -1.9196\n",
      "Total loss:  -0.8052 | PDE Loss:  -1.44 | Function Loss:  -1.9197\n",
      "Total loss:  -0.8053 | PDE Loss:  -1.4395 | Function Loss:  -1.92\n",
      "Total loss:  -0.8054 | PDE Loss:  -1.4397 | Function Loss:  -1.9201\n",
      "Total loss:  -0.8054 | PDE Loss:  -1.4396 | Function Loss:  -1.9202\n",
      "Total loss:  -0.8055 | PDE Loss:  -1.44 | Function Loss:  -1.9202\n",
      "Total loss:  -0.8056 | PDE Loss:  -1.4404 | Function Loss:  -1.9202\n",
      "Total loss:  -0.8057 | PDE Loss:  -1.4407 | Function Loss:  -1.9201\n",
      "Total loss:  -0.8057 | PDE Loss:  -1.4413 | Function Loss:  -1.9201\n",
      "Total loss:  -0.8058 | PDE Loss:  -1.4415 | Function Loss:  -1.9201\n",
      "Total loss:  -0.8058 | PDE Loss:  -1.4417 | Function Loss:  -1.9201\n",
      "Total loss:  -0.8059 | PDE Loss:  -1.4417 | Function Loss:  -1.9201\n",
      "Total loss:  -0.8059 | PDE Loss:  -1.4416 | Function Loss:  -1.9202\n",
      "Total loss:  -0.806 | PDE Loss:  -1.4414 | Function Loss:  -1.9204\n",
      "Total loss:  -0.8061 | PDE Loss:  -1.441 | Function Loss:  -1.9206\n",
      "Total loss:  -0.8062 | PDE Loss:  -1.4406 | Function Loss:  -1.9208\n",
      "Total loss:  -0.8063 | PDE Loss:  -1.4399 | Function Loss:  -1.9212\n",
      "Total loss:  -0.8064 | PDE Loss:  -1.4394 | Function Loss:  -1.9215\n",
      "Total loss:  -0.8065 | PDE Loss:  -1.4392 | Function Loss:  -1.9217\n",
      "Total loss:  -0.8066 | PDE Loss:  -1.4389 | Function Loss:  -1.9219\n",
      "Total loss:  -0.8067 | PDE Loss:  -1.4389 | Function Loss:  -1.922\n",
      "Total loss:  -0.8067 | PDE Loss:  -1.4389 | Function Loss:  -1.922\n",
      "Total loss:  -0.8068 | PDE Loss:  -1.4391 | Function Loss:  -1.9221\n",
      "Total loss:  -0.8068 | PDE Loss:  -1.4391 | Function Loss:  -1.9222\n",
      "Total loss:  -0.806 | PDE Loss:  -1.4406 | Function Loss:  -1.9206\n",
      "Total loss:  -0.8069 | PDE Loss:  -1.4398 | Function Loss:  -1.922\n",
      "Total loss:  -0.8069 | PDE Loss:  -1.44 | Function Loss:  -1.922\n",
      "Total loss:  -0.807 | PDE Loss:  -1.4404 | Function Loss:  -1.922\n",
      "Total loss:  -0.8071 | PDE Loss:  -1.4409 | Function Loss:  -1.9219\n",
      "Total loss:  -0.8071 | PDE Loss:  -1.4413 | Function Loss:  -1.9219\n",
      "Total loss:  -0.8072 | PDE Loss:  -1.4416 | Function Loss:  -1.9218\n",
      "Total loss:  -0.8072 | PDE Loss:  -1.4419 | Function Loss:  -1.9218\n",
      "Total loss:  -0.8072 | PDE Loss:  -1.4422 | Function Loss:  -1.9217\n",
      "Total loss:  -0.8071 | PDE Loss:  -1.4429 | Function Loss:  -1.9214\n",
      "Total loss:  -0.8072 | PDE Loss:  -1.4425 | Function Loss:  -1.9217\n",
      "Total loss:  -0.8073 | PDE Loss:  -1.4426 | Function Loss:  -1.9217\n",
      "Total loss:  -0.8073 | PDE Loss:  -1.4427 | Function Loss:  -1.9217\n",
      "Total loss:  -0.8074 | PDE Loss:  -1.4428 | Function Loss:  -1.9218\n",
      "Total loss:  -0.8075 | PDE Loss:  -1.4429 | Function Loss:  -1.9219\n",
      "Total loss:  -0.8076 | PDE Loss:  -1.4428 | Function Loss:  -1.922\n",
      "Total loss:  -0.8077 | PDE Loss:  -1.4428 | Function Loss:  -1.9221\n",
      "Total loss:  -0.8078 | PDE Loss:  -1.4424 | Function Loss:  -1.9224\n",
      "Total loss:  -0.8078 | PDE Loss:  -1.4424 | Function Loss:  -1.9225\n",
      "Total loss:  -0.8079 | PDE Loss:  -1.4422 | Function Loss:  -1.9226\n",
      "Total loss:  -0.808 | PDE Loss:  -1.442 | Function Loss:  -1.9228\n",
      "Total loss:  -0.8081 | PDE Loss:  -1.4418 | Function Loss:  -1.923\n",
      "Total loss:  -0.8082 | PDE Loss:  -1.4417 | Function Loss:  -1.9231\n",
      "Total loss:  -0.8082 | PDE Loss:  -1.4416 | Function Loss:  -1.9232\n",
      "Total loss:  -0.8083 | PDE Loss:  -1.4417 | Function Loss:  -1.9233\n",
      "Total loss:  -0.8084 | PDE Loss:  -1.4418 | Function Loss:  -1.9233\n",
      "Total loss:  -0.8084 | PDE Loss:  -1.4419 | Function Loss:  -1.9234\n",
      "Total loss:  -0.8085 | PDE Loss:  -1.4421 | Function Loss:  -1.9234\n",
      "Total loss:  -0.8085 | PDE Loss:  -1.4421 | Function Loss:  -1.9235\n",
      "Total loss:  -0.8086 | PDE Loss:  -1.4422 | Function Loss:  -1.9235\n",
      "Total loss:  -0.8087 | PDE Loss:  -1.4422 | Function Loss:  -1.9236\n",
      "Total loss:  -0.8088 | PDE Loss:  -1.4424 | Function Loss:  -1.9237\n",
      "Total loss:  -0.8088 | PDE Loss:  -1.4422 | Function Loss:  -1.9238\n",
      "Total loss:  -0.8089 | PDE Loss:  -1.4425 | Function Loss:  -1.9238\n",
      "Total loss:  -0.809 | PDE Loss:  -1.4424 | Function Loss:  -1.924\n",
      "Total loss:  -0.8091 | PDE Loss:  -1.4426 | Function Loss:  -1.924\n",
      "Total loss:  -0.809 | PDE Loss:  -1.4416 | Function Loss:  -1.9242\n",
      "Total loss:  -0.8091 | PDE Loss:  -1.4424 | Function Loss:  -1.9241\n",
      "Total loss:  -0.8092 | PDE Loss:  -1.4426 | Function Loss:  -1.9242\n",
      "Total loss:  -0.8093 | PDE Loss:  -1.4429 | Function Loss:  -1.9242\n",
      "Total loss:  -0.8094 | PDE Loss:  -1.4432 | Function Loss:  -1.9243\n",
      "Total loss:  -0.8095 | PDE Loss:  -1.4435 | Function Loss:  -1.9243\n",
      "Total loss:  -0.8096 | PDE Loss:  -1.4436 | Function Loss:  -1.9244\n",
      "Total loss:  -0.8096 | PDE Loss:  -1.4437 | Function Loss:  -1.9244\n",
      "Total loss:  -0.8097 | PDE Loss:  -1.4438 | Function Loss:  -1.9245\n",
      "Total loss:  -0.8097 | PDE Loss:  -1.4439 | Function Loss:  -1.9245\n",
      "Total loss:  -0.8098 | PDE Loss:  -1.4439 | Function Loss:  -1.9246\n",
      "Total loss:  -0.8099 | PDE Loss:  -1.444 | Function Loss:  -1.9246\n",
      "Total loss:  -0.8099 | PDE Loss:  -1.4441 | Function Loss:  -1.9246\n",
      "Total loss:  -0.8099 | PDE Loss:  -1.4442 | Function Loss:  -1.9247\n",
      "Total loss:  -0.81 | PDE Loss:  -1.4441 | Function Loss:  -1.9247\n",
      "Total loss:  -0.8101 | PDE Loss:  -1.4442 | Function Loss:  -1.9248\n",
      "Total loss:  -0.8101 | PDE Loss:  -1.4441 | Function Loss:  -1.9249\n",
      "Total loss:  -0.8102 | PDE Loss:  -1.4441 | Function Loss:  -1.925\n",
      "Total loss:  -0.8103 | PDE Loss:  -1.4439 | Function Loss:  -1.9252\n",
      "Total loss:  -0.8103 | PDE Loss:  -1.444 | Function Loss:  -1.9252\n",
      "Total loss:  -0.8104 | PDE Loss:  -1.4442 | Function Loss:  -1.9253\n",
      "Total loss:  -0.8103 | PDE Loss:  -1.4438 | Function Loss:  -1.9252\n",
      "Total loss:  -0.8105 | PDE Loss:  -1.4443 | Function Loss:  -1.9253\n",
      "Total loss:  -0.8106 | PDE Loss:  -1.4444 | Function Loss:  -1.9254\n",
      "Total loss:  -0.8107 | PDE Loss:  -1.4448 | Function Loss:  -1.9254\n",
      "Total loss:  -0.8108 | PDE Loss:  -1.445 | Function Loss:  -1.9255\n",
      "Total loss:  -0.8109 | PDE Loss:  -1.4455 | Function Loss:  -1.9255\n",
      "Total loss:  -0.8109 | PDE Loss:  -1.4456 | Function Loss:  -1.9255\n",
      "Total loss:  -0.811 | PDE Loss:  -1.4459 | Function Loss:  -1.9255\n",
      "Total loss:  -0.8111 | PDE Loss:  -1.4458 | Function Loss:  -1.9256\n",
      "Total loss:  -0.811 | PDE Loss:  -1.446 | Function Loss:  -1.9255\n",
      "Total loss:  -0.8111 | PDE Loss:  -1.446 | Function Loss:  -1.9256\n",
      "Total loss:  -0.8111 | PDE Loss:  -1.446 | Function Loss:  -1.9257\n",
      "Total loss:  -0.8112 | PDE Loss:  -1.4459 | Function Loss:  -1.9258\n",
      "Total loss:  -0.8113 | PDE Loss:  -1.4458 | Function Loss:  -1.9259\n",
      "Total loss:  -0.8113 | PDE Loss:  -1.4455 | Function Loss:  -1.926\n",
      "Total loss:  -0.8114 | PDE Loss:  -1.4453 | Function Loss:  -1.9262\n",
      "Total loss:  -0.8114 | PDE Loss:  -1.4451 | Function Loss:  -1.9263\n",
      "Total loss:  -0.8115 | PDE Loss:  -1.4449 | Function Loss:  -1.9265\n",
      "Total loss:  -0.8116 | PDE Loss:  -1.4446 | Function Loss:  -1.9266\n",
      "Total loss:  -0.8116 | PDE Loss:  -1.4445 | Function Loss:  -1.9267\n",
      "Total loss:  -0.8116 | PDE Loss:  -1.4445 | Function Loss:  -1.9268\n",
      "Total loss:  -0.8117 | PDE Loss:  -1.4446 | Function Loss:  -1.9268\n",
      "Total loss:  -0.8117 | PDE Loss:  -1.4447 | Function Loss:  -1.9268\n",
      "Total loss:  -0.8118 | PDE Loss:  -1.445 | Function Loss:  -1.9268\n",
      "Total loss:  -0.8118 | PDE Loss:  -1.4452 | Function Loss:  -1.9268\n",
      "Total loss:  -0.8119 | PDE Loss:  -1.4452 | Function Loss:  -1.9269\n",
      "Total loss:  -0.8119 | PDE Loss:  -1.4457 | Function Loss:  -1.9268\n",
      "Total loss:  -0.812 | PDE Loss:  -1.446 | Function Loss:  -1.9268\n",
      "Total loss:  -0.8121 | PDE Loss:  -1.4462 | Function Loss:  -1.9268\n",
      "Total loss:  -0.8121 | PDE Loss:  -1.4463 | Function Loss:  -1.9268\n",
      "Total loss:  -0.8121 | PDE Loss:  -1.4464 | Function Loss:  -1.9268\n",
      "Total loss:  -0.8122 | PDE Loss:  -1.4466 | Function Loss:  -1.9268\n",
      "Total loss:  -0.8122 | PDE Loss:  -1.4466 | Function Loss:  -1.9269\n",
      "Total loss:  -0.8122 | PDE Loss:  -1.4468 | Function Loss:  -1.9269\n",
      "Total loss:  -0.8123 | PDE Loss:  -1.4471 | Function Loss:  -1.9268\n",
      "Total loss:  -0.8123 | PDE Loss:  -1.4471 | Function Loss:  -1.9269\n",
      "Total loss:  -0.8124 | PDE Loss:  -1.4475 | Function Loss:  -1.9268\n",
      "Total loss:  -0.8124 | PDE Loss:  -1.4479 | Function Loss:  -1.9268\n",
      "Total loss:  -0.8125 | PDE Loss:  -1.4485 | Function Loss:  -1.9267\n",
      "Total loss:  -0.8126 | PDE Loss:  -1.449 | Function Loss:  -1.9266\n",
      "Total loss:  -0.8126 | PDE Loss:  -1.4493 | Function Loss:  -1.9266\n",
      "Total loss:  -0.8127 | PDE Loss:  -1.4494 | Function Loss:  -1.9266\n",
      "Total loss:  -0.8127 | PDE Loss:  -1.4496 | Function Loss:  -1.9266\n",
      "Total loss:  -0.8128 | PDE Loss:  -1.4495 | Function Loss:  -1.9267\n",
      "Total loss:  -0.8128 | PDE Loss:  -1.4494 | Function Loss:  -1.9269\n",
      "Total loss:  -0.8129 | PDE Loss:  -1.4491 | Function Loss:  -1.9271\n",
      "Total loss:  -0.813 | PDE Loss:  -1.4488 | Function Loss:  -1.9272\n",
      "Total loss:  -0.813 | PDE Loss:  -1.4486 | Function Loss:  -1.9274\n",
      "Total loss:  -0.8131 | PDE Loss:  -1.4484 | Function Loss:  -1.9275\n",
      "Total loss:  -0.8132 | PDE Loss:  -1.4483 | Function Loss:  -1.9276\n",
      "Total loss:  -0.8132 | PDE Loss:  -1.4483 | Function Loss:  -1.9277\n",
      "Total loss:  -0.8133 | PDE Loss:  -1.4484 | Function Loss:  -1.9278\n",
      "Total loss:  -0.8134 | PDE Loss:  -1.4485 | Function Loss:  -1.9278\n",
      "Total loss:  -0.8134 | PDE Loss:  -1.4487 | Function Loss:  -1.9278\n",
      "Total loss:  -0.8135 | PDE Loss:  -1.449 | Function Loss:  -1.9278\n",
      "Total loss:  -0.8135 | PDE Loss:  -1.4491 | Function Loss:  -1.9279\n",
      "Total loss:  -0.8136 | PDE Loss:  -1.4493 | Function Loss:  -1.9279\n",
      "Total loss:  -0.8137 | PDE Loss:  -1.4493 | Function Loss:  -1.928\n",
      "Total loss:  -0.8137 | PDE Loss:  -1.4494 | Function Loss:  -1.928\n",
      "Total loss:  -0.8138 | PDE Loss:  -1.4495 | Function Loss:  -1.9281\n",
      "Total loss:  -0.8139 | PDE Loss:  -1.4496 | Function Loss:  -1.9282\n",
      "Total loss:  -0.814 | PDE Loss:  -1.4498 | Function Loss:  -1.9283\n",
      "Total loss:  -0.8141 | PDE Loss:  -1.4499 | Function Loss:  -1.9284\n",
      "Total loss:  -0.8142 | PDE Loss:  -1.4498 | Function Loss:  -1.9285\n",
      "Total loss:  -0.8143 | PDE Loss:  -1.4497 | Function Loss:  -1.9286\n",
      "Total loss:  -0.8143 | PDE Loss:  -1.4495 | Function Loss:  -1.9287\n",
      "Total loss:  -0.8144 | PDE Loss:  -1.4495 | Function Loss:  -1.9288\n",
      "Total loss:  -0.8145 | PDE Loss:  -1.4492 | Function Loss:  -1.929\n",
      "Total loss:  -0.8145 | PDE Loss:  -1.4496 | Function Loss:  -1.929\n",
      "Total loss:  -0.8146 | PDE Loss:  -1.4494 | Function Loss:  -1.9291\n",
      "Total loss:  -0.8147 | PDE Loss:  -1.4493 | Function Loss:  -1.9293\n",
      "Total loss:  -0.8147 | PDE Loss:  -1.4494 | Function Loss:  -1.9293\n",
      "Total loss:  -0.8148 | PDE Loss:  -1.4494 | Function Loss:  -1.9294\n",
      "Total loss:  -0.8148 | PDE Loss:  -1.4497 | Function Loss:  -1.9294\n",
      "Total loss:  -0.8149 | PDE Loss:  -1.45 | Function Loss:  -1.9293\n",
      "Total loss:  -0.815 | PDE Loss:  -1.4507 | Function Loss:  -1.9292\n",
      "Total loss:  -0.815 | PDE Loss:  -1.4515 | Function Loss:  -1.9291\n",
      "Total loss:  -0.8151 | PDE Loss:  -1.4522 | Function Loss:  -1.929\n",
      "Total loss:  -0.8152 | PDE Loss:  -1.453 | Function Loss:  -1.9288\n",
      "Total loss:  -0.8153 | PDE Loss:  -1.4538 | Function Loss:  -1.9287\n",
      "Total loss:  -0.8153 | PDE Loss:  -1.4545 | Function Loss:  -1.9286\n",
      "Total loss:  -0.8154 | PDE Loss:  -1.4549 | Function Loss:  -1.9286\n",
      "Total loss:  -0.8155 | PDE Loss:  -1.4553 | Function Loss:  -1.9286\n",
      "Total loss:  -0.8156 | PDE Loss:  -1.4552 | Function Loss:  -1.9287\n",
      "Total loss:  -0.8156 | PDE Loss:  -1.455 | Function Loss:  -1.9288\n",
      "Total loss:  -0.8157 | PDE Loss:  -1.4547 | Function Loss:  -1.929\n",
      "Total loss:  -0.8157 | PDE Loss:  -1.4544 | Function Loss:  -1.9291\n",
      "Total loss:  -0.8158 | PDE Loss:  -1.4541 | Function Loss:  -1.9293\n",
      "Total loss:  -0.8158 | PDE Loss:  -1.4539 | Function Loss:  -1.9294\n",
      "Total loss:  -0.8159 | PDE Loss:  -1.4537 | Function Loss:  -1.9295\n",
      "Total loss:  -0.8159 | PDE Loss:  -1.4535 | Function Loss:  -1.9296\n",
      "Total loss:  -0.816 | PDE Loss:  -1.4534 | Function Loss:  -1.9297\n",
      "Total loss:  -0.816 | PDE Loss:  -1.4534 | Function Loss:  -1.9298\n",
      "Total loss:  -0.8161 | PDE Loss:  -1.4535 | Function Loss:  -1.9299\n",
      "Total loss:  -0.8161 | PDE Loss:  -1.4537 | Function Loss:  -1.9299\n",
      "Total loss:  -0.8162 | PDE Loss:  -1.4542 | Function Loss:  -1.9298\n",
      "Total loss:  -0.8163 | PDE Loss:  -1.4546 | Function Loss:  -1.9298\n",
      "Total loss:  -0.8164 | PDE Loss:  -1.4554 | Function Loss:  -1.9296\n",
      "Total loss:  -0.8164 | PDE Loss:  -1.4557 | Function Loss:  -1.9296\n",
      "Total loss:  -0.8164 | PDE Loss:  -1.456 | Function Loss:  -1.9296\n",
      "Total loss:  -0.8165 | PDE Loss:  -1.4565 | Function Loss:  -1.9295\n",
      "Total loss:  -0.8165 | PDE Loss:  -1.4565 | Function Loss:  -1.9295\n",
      "Total loss:  -0.8165 | PDE Loss:  -1.4564 | Function Loss:  -1.9296\n",
      "Total loss:  -0.8166 | PDE Loss:  -1.4563 | Function Loss:  -1.9296\n",
      "Total loss:  -0.8166 | PDE Loss:  -1.4561 | Function Loss:  -1.9297\n",
      "Total loss:  -0.8166 | PDE Loss:  -1.4559 | Function Loss:  -1.9298\n",
      "Total loss:  -0.8167 | PDE Loss:  -1.4557 | Function Loss:  -1.9299\n",
      "Total loss:  -0.8167 | PDE Loss:  -1.4553 | Function Loss:  -1.9301\n",
      "Total loss:  -0.8166 | PDE Loss:  -1.4542 | Function Loss:  -1.9304\n",
      "Total loss:  -0.8167 | PDE Loss:  -1.455 | Function Loss:  -1.9302\n",
      "Total loss:  -0.8168 | PDE Loss:  -1.4551 | Function Loss:  -1.9303\n",
      "Total loss:  -0.8168 | PDE Loss:  -1.4554 | Function Loss:  -1.9303\n",
      "Total loss:  -0.8169 | PDE Loss:  -1.4555 | Function Loss:  -1.9303\n",
      "Total loss:  -0.8169 | PDE Loss:  -1.4557 | Function Loss:  -1.9303\n",
      "Total loss:  -0.8169 | PDE Loss:  -1.4559 | Function Loss:  -1.9303\n",
      "Total loss:  -0.817 | PDE Loss:  -1.4561 | Function Loss:  -1.9303\n",
      "Total loss:  -0.8171 | PDE Loss:  -1.4564 | Function Loss:  -1.9303\n",
      "Total loss:  -0.8172 | PDE Loss:  -1.4566 | Function Loss:  -1.9303\n",
      "Total loss:  -0.8173 | PDE Loss:  -1.4568 | Function Loss:  -1.9304\n",
      "Total loss:  -0.8173 | PDE Loss:  -1.457 | Function Loss:  -1.9304\n",
      "Total loss:  -0.8174 | PDE Loss:  -1.4572 | Function Loss:  -1.9305\n",
      "Total loss:  -0.8175 | PDE Loss:  -1.4573 | Function Loss:  -1.9305\n",
      "Total loss:  -0.8175 | PDE Loss:  -1.4572 | Function Loss:  -1.9306\n",
      "Total loss:  -0.8175 | PDE Loss:  -1.4572 | Function Loss:  -1.9306\n",
      "Total loss:  -0.8176 | PDE Loss:  -1.4572 | Function Loss:  -1.9307\n",
      "Total loss:  -0.8177 | PDE Loss:  -1.4571 | Function Loss:  -1.9308\n",
      "Total loss:  -0.8177 | PDE Loss:  -1.4569 | Function Loss:  -1.9309\n",
      "Total loss:  -0.8178 | PDE Loss:  -1.4568 | Function Loss:  -1.931\n",
      "Total loss:  -0.8178 | PDE Loss:  -1.4567 | Function Loss:  -1.9311\n",
      "Total loss:  -0.8178 | PDE Loss:  -1.4565 | Function Loss:  -1.9312\n",
      "Total loss:  -0.8179 | PDE Loss:  -1.4562 | Function Loss:  -1.9314\n",
      "Total loss:  -0.8179 | PDE Loss:  -1.4559 | Function Loss:  -1.9315\n",
      "Total loss:  -0.818 | PDE Loss:  -1.4557 | Function Loss:  -1.9316\n",
      "Total loss:  -0.818 | PDE Loss:  -1.4555 | Function Loss:  -1.9317\n",
      "Total loss:  -0.818 | PDE Loss:  -1.4554 | Function Loss:  -1.9318\n",
      "Total loss:  -0.8181 | PDE Loss:  -1.4553 | Function Loss:  -1.9319\n",
      "Total loss:  -0.8181 | PDE Loss:  -1.4552 | Function Loss:  -1.932\n",
      "Total loss:  -0.8181 | PDE Loss:  -1.4552 | Function Loss:  -1.932\n",
      "Total loss:  -0.8182 | PDE Loss:  -1.4553 | Function Loss:  -1.932\n",
      "Total loss:  -0.8182 | PDE Loss:  -1.4554 | Function Loss:  -1.9321\n",
      "Total loss:  -0.8183 | PDE Loss:  -1.4555 | Function Loss:  -1.9321\n",
      "Total loss:  -0.8179 | PDE Loss:  -1.4544 | Function Loss:  -1.932\n",
      "Total loss:  -0.8183 | PDE Loss:  -1.4555 | Function Loss:  -1.9322\n",
      "Total loss:  -0.8184 | PDE Loss:  -1.4556 | Function Loss:  -1.9322\n",
      "Total loss:  -0.8184 | PDE Loss:  -1.4558 | Function Loss:  -1.9322\n",
      "Total loss:  -0.8184 | PDE Loss:  -1.4559 | Function Loss:  -1.9322\n",
      "Total loss:  -0.8185 | PDE Loss:  -1.456 | Function Loss:  -1.9322\n",
      "Total loss:  -0.8185 | PDE Loss:  -1.456 | Function Loss:  -1.9322\n",
      "Total loss:  -0.8185 | PDE Loss:  -1.4561 | Function Loss:  -1.9323\n",
      "Total loss:  -0.8186 | PDE Loss:  -1.456 | Function Loss:  -1.9324\n",
      "Total loss:  -0.8187 | PDE Loss:  -1.4557 | Function Loss:  -1.9325\n",
      "Total loss:  -0.8187 | PDE Loss:  -1.4557 | Function Loss:  -1.9326\n",
      "Total loss:  -0.8188 | PDE Loss:  -1.4554 | Function Loss:  -1.9328\n",
      "Total loss:  -0.8188 | PDE Loss:  -1.4553 | Function Loss:  -1.9329\n",
      "Total loss:  -0.8188 | PDE Loss:  -1.455 | Function Loss:  -1.933\n",
      "Total loss:  -0.8189 | PDE Loss:  -1.4549 | Function Loss:  -1.933\n",
      "Total loss:  -0.8189 | PDE Loss:  -1.4547 | Function Loss:  -1.9331\n",
      "Total loss:  -0.8189 | PDE Loss:  -1.4545 | Function Loss:  -1.9332\n",
      "Total loss:  -0.819 | PDE Loss:  -1.4543 | Function Loss:  -1.9333\n",
      "Total loss:  -0.819 | PDE Loss:  -1.4541 | Function Loss:  -1.9335\n",
      "Total loss:  -0.8191 | PDE Loss:  -1.4538 | Function Loss:  -1.9337\n",
      "Total loss:  -0.8192 | PDE Loss:  -1.4537 | Function Loss:  -1.9338\n",
      "Total loss:  -0.8193 | PDE Loss:  -1.454 | Function Loss:  -1.9338\n",
      "Total loss:  -0.8193 | PDE Loss:  -1.4538 | Function Loss:  -1.934\n",
      "Total loss:  -0.8194 | PDE Loss:  -1.4544 | Function Loss:  -1.9339\n",
      "Total loss:  -0.8195 | PDE Loss:  -1.4551 | Function Loss:  -1.9338\n",
      "Total loss:  -0.8195 | PDE Loss:  -1.4559 | Function Loss:  -1.9336\n",
      "Total loss:  -0.8196 | PDE Loss:  -1.4561 | Function Loss:  -1.9336\n",
      "Total loss:  -0.8196 | PDE Loss:  -1.4566 | Function Loss:  -1.9336\n",
      "Total loss:  -0.8197 | PDE Loss:  -1.4569 | Function Loss:  -1.9335\n",
      "Total loss:  -0.8198 | PDE Loss:  -1.457 | Function Loss:  -1.9336\n",
      "Total loss:  -0.8198 | PDE Loss:  -1.4569 | Function Loss:  -1.9337\n",
      "Total loss:  -0.8199 | PDE Loss:  -1.4567 | Function Loss:  -1.9338\n",
      "Total loss:  -0.8199 | PDE Loss:  -1.4562 | Function Loss:  -1.934\n",
      "Total loss:  -0.82 | PDE Loss:  -1.456 | Function Loss:  -1.9341\n",
      "Total loss:  -0.82 | PDE Loss:  -1.4558 | Function Loss:  -1.9342\n",
      "Total loss:  -0.82 | PDE Loss:  -1.4557 | Function Loss:  -1.9343\n",
      "Total loss:  -0.8201 | PDE Loss:  -1.4556 | Function Loss:  -1.9344\n",
      "Total loss:  -0.8201 | PDE Loss:  -1.4556 | Function Loss:  -1.9345\n",
      "Total loss:  -0.8202 | PDE Loss:  -1.4557 | Function Loss:  -1.9345\n",
      "Total loss:  -0.8202 | PDE Loss:  -1.4557 | Function Loss:  -1.9345\n",
      "Total loss:  -0.8203 | PDE Loss:  -1.4558 | Function Loss:  -1.9346\n",
      "Total loss:  -0.8204 | PDE Loss:  -1.4559 | Function Loss:  -1.9347\n",
      "Total loss:  -0.8205 | PDE Loss:  -1.4559 | Function Loss:  -1.9348\n",
      "Total loss:  -0.8206 | PDE Loss:  -1.4559 | Function Loss:  -1.9349\n",
      "Total loss:  -0.8206 | PDE Loss:  -1.4558 | Function Loss:  -1.9351\n",
      "Total loss:  -0.8207 | PDE Loss:  -1.4559 | Function Loss:  -1.9352\n",
      "Total loss:  -0.8208 | PDE Loss:  -1.4558 | Function Loss:  -1.9353\n",
      "Total loss:  -0.8209 | PDE Loss:  -1.456 | Function Loss:  -1.9353\n",
      "Total loss:  -0.8209 | PDE Loss:  -1.4558 | Function Loss:  -1.9355\n",
      "Total loss:  -0.821 | PDE Loss:  -1.4558 | Function Loss:  -1.9356\n",
      "Total loss:  -0.8211 | PDE Loss:  -1.4556 | Function Loss:  -1.9357\n",
      "Total loss:  -0.8211 | PDE Loss:  -1.4553 | Function Loss:  -1.9359\n",
      "Total loss:  -0.8212 | PDE Loss:  -1.455 | Function Loss:  -1.9361\n",
      "Total loss:  -0.8213 | PDE Loss:  -1.4546 | Function Loss:  -1.9363\n",
      "Total loss:  -0.8213 | PDE Loss:  -1.4544 | Function Loss:  -1.9364\n",
      "Total loss:  -0.8214 | PDE Loss:  -1.4543 | Function Loss:  -1.9365\n",
      "Total loss:  -0.8214 | PDE Loss:  -1.4543 | Function Loss:  -1.9366\n",
      "Total loss:  -0.8215 | PDE Loss:  -1.4544 | Function Loss:  -1.9366\n",
      "Total loss:  -0.8216 | PDE Loss:  -1.4545 | Function Loss:  -1.9367\n",
      "Total loss:  -0.8216 | PDE Loss:  -1.4546 | Function Loss:  -1.9367\n",
      "Total loss:  -0.8215 | PDE Loss:  -1.4542 | Function Loss:  -1.9367\n",
      "Total loss:  -0.8216 | PDE Loss:  -1.4546 | Function Loss:  -1.9367\n",
      "Total loss:  -0.8216 | PDE Loss:  -1.4547 | Function Loss:  -1.9367\n",
      "Total loss:  -0.8217 | PDE Loss:  -1.4547 | Function Loss:  -1.9368\n",
      "Total loss:  -0.8217 | PDE Loss:  -1.4548 | Function Loss:  -1.9368\n",
      "Total loss:  -0.8218 | PDE Loss:  -1.4548 | Function Loss:  -1.9369\n",
      "Total loss:  -0.8218 | PDE Loss:  -1.4548 | Function Loss:  -1.9369\n",
      "Total loss:  -0.8219 | PDE Loss:  -1.4548 | Function Loss:  -1.937\n",
      "Total loss:  -0.8219 | PDE Loss:  -1.4548 | Function Loss:  -1.937\n",
      "Total loss:  -0.822 | PDE Loss:  -1.4548 | Function Loss:  -1.9371\n",
      "Total loss:  -0.8216 | PDE Loss:  -1.4555 | Function Loss:  -1.9365\n",
      "Total loss:  -0.822 | PDE Loss:  -1.4551 | Function Loss:  -1.937\n",
      "Total loss:  -0.8221 | PDE Loss:  -1.455 | Function Loss:  -1.9372\n",
      "Total loss:  -0.8221 | PDE Loss:  -1.4549 | Function Loss:  -1.9373\n",
      "Total loss:  -0.8222 | PDE Loss:  -1.4547 | Function Loss:  -1.9375\n",
      "Total loss:  -0.8223 | PDE Loss:  -1.4547 | Function Loss:  -1.9375\n",
      "Total loss:  -0.8223 | PDE Loss:  -1.4546 | Function Loss:  -1.9376\n",
      "Total loss:  -0.8223 | PDE Loss:  -1.4546 | Function Loss:  -1.9376\n",
      "Total loss:  -0.8224 | PDE Loss:  -1.4545 | Function Loss:  -1.9377\n",
      "Total loss:  -0.8224 | PDE Loss:  -1.4544 | Function Loss:  -1.9378\n",
      "Total loss:  -0.8225 | PDE Loss:  -1.4541 | Function Loss:  -1.938\n",
      "Total loss:  -0.8225 | PDE Loss:  -1.4539 | Function Loss:  -1.9381\n",
      "Total loss:  -0.8225 | PDE Loss:  -1.4529 | Function Loss:  -1.9384\n",
      "Total loss:  -0.8226 | PDE Loss:  -1.453 | Function Loss:  -1.9384\n",
      "Total loss:  -0.8226 | PDE Loss:  -1.4532 | Function Loss:  -1.9384\n",
      "Total loss:  -0.8227 | PDE Loss:  -1.4534 | Function Loss:  -1.9385\n",
      "Total loss:  -0.8227 | PDE Loss:  -1.4536 | Function Loss:  -1.9385\n",
      "Total loss:  -0.8228 | PDE Loss:  -1.4538 | Function Loss:  -1.9385\n",
      "Total loss:  -0.8229 | PDE Loss:  -1.4542 | Function Loss:  -1.9385\n",
      "Total loss:  -0.823 | PDE Loss:  -1.4543 | Function Loss:  -1.9386\n",
      "Total loss:  -0.8231 | PDE Loss:  -1.4546 | Function Loss:  -1.9386\n",
      "Total loss:  -0.8231 | PDE Loss:  -1.4547 | Function Loss:  -1.9387\n",
      "Total loss:  -0.8232 | PDE Loss:  -1.4548 | Function Loss:  -1.9387\n",
      "Total loss:  -0.8233 | PDE Loss:  -1.455 | Function Loss:  -1.9387\n",
      "Total loss:  -0.8233 | PDE Loss:  -1.4552 | Function Loss:  -1.9388\n",
      "Total loss:  -0.8234 | PDE Loss:  -1.4555 | Function Loss:  -1.9388\n",
      "Total loss:  -0.8236 | PDE Loss:  -1.4557 | Function Loss:  -1.9389\n",
      "Total loss:  -0.8236 | PDE Loss:  -1.4561 | Function Loss:  -1.9389\n",
      "Total loss:  -0.8237 | PDE Loss:  -1.4563 | Function Loss:  -1.9389\n",
      "Total loss:  -0.8238 | PDE Loss:  -1.4565 | Function Loss:  -1.939\n",
      "Total loss:  -0.8237 | PDE Loss:  -1.4561 | Function Loss:  -1.939\n",
      "Total loss:  -0.8238 | PDE Loss:  -1.4564 | Function Loss:  -1.939\n",
      "Total loss:  -0.8238 | PDE Loss:  -1.4565 | Function Loss:  -1.939\n",
      "Total loss:  -0.8239 | PDE Loss:  -1.4565 | Function Loss:  -1.9391\n",
      "Total loss:  -0.8239 | PDE Loss:  -1.4565 | Function Loss:  -1.9392\n",
      "Total loss:  -0.824 | PDE Loss:  -1.4563 | Function Loss:  -1.9393\n",
      "Total loss:  -0.8241 | PDE Loss:  -1.4563 | Function Loss:  -1.9394\n",
      "Total loss:  -0.8242 | PDE Loss:  -1.4562 | Function Loss:  -1.9396\n",
      "Total loss:  -0.8243 | PDE Loss:  -1.4563 | Function Loss:  -1.9397\n",
      "Total loss:  -0.8244 | PDE Loss:  -1.4562 | Function Loss:  -1.9399\n",
      "Total loss:  -0.8246 | PDE Loss:  -1.4564 | Function Loss:  -1.94\n",
      "Total loss:  -0.8247 | PDE Loss:  -1.4568 | Function Loss:  -1.9401\n",
      "Total loss:  -0.8249 | PDE Loss:  -1.4572 | Function Loss:  -1.9403\n",
      "Total loss:  -0.8251 | PDE Loss:  -1.458 | Function Loss:  -1.9402\n",
      "Total loss:  -0.8253 | PDE Loss:  -1.4583 | Function Loss:  -1.9403\n",
      "Total loss:  -0.8253 | PDE Loss:  -1.4586 | Function Loss:  -1.9404\n",
      "Total loss:  -0.8254 | PDE Loss:  -1.4589 | Function Loss:  -1.9404\n",
      "Total loss:  -0.8255 | PDE Loss:  -1.4586 | Function Loss:  -1.9405\n",
      "Total loss:  -0.8255 | PDE Loss:  -1.4587 | Function Loss:  -1.9406\n",
      "Total loss:  -0.8256 | PDE Loss:  -1.4588 | Function Loss:  -1.9406\n",
      "Total loss:  -0.8256 | PDE Loss:  -1.4585 | Function Loss:  -1.9408\n",
      "Total loss:  -0.8257 | PDE Loss:  -1.4583 | Function Loss:  -1.941\n",
      "Total loss:  -0.8258 | PDE Loss:  -1.4579 | Function Loss:  -1.9412\n",
      "Total loss:  -0.826 | PDE Loss:  -1.4577 | Function Loss:  -1.9414\n",
      "Total loss:  -0.8261 | PDE Loss:  -1.4574 | Function Loss:  -1.9417\n",
      "Total loss:  -0.8262 | PDE Loss:  -1.4574 | Function Loss:  -1.9418\n",
      "Total loss:  -0.8262 | PDE Loss:  -1.457 | Function Loss:  -1.942\n",
      "Total loss:  -0.8263 | PDE Loss:  -1.4572 | Function Loss:  -1.9421\n",
      "Total loss:  -0.8264 | PDE Loss:  -1.4576 | Function Loss:  -1.942\n",
      "Total loss:  -0.8264 | PDE Loss:  -1.4581 | Function Loss:  -1.9419\n",
      "Total loss:  -0.8265 | PDE Loss:  -1.4586 | Function Loss:  -1.9419\n",
      "Total loss:  -0.8266 | PDE Loss:  -1.4592 | Function Loss:  -1.9418\n",
      "Total loss:  -0.8267 | PDE Loss:  -1.4599 | Function Loss:  -1.9417\n",
      "Total loss:  -0.8268 | PDE Loss:  -1.4605 | Function Loss:  -1.9417\n",
      "Total loss:  -0.8269 | PDE Loss:  -1.4611 | Function Loss:  -1.9416\n",
      "Total loss:  -0.827 | PDE Loss:  -1.4612 | Function Loss:  -1.9417\n",
      "Total loss:  -0.8271 | PDE Loss:  -1.461 | Function Loss:  -1.9419\n",
      "Total loss:  -0.8272 | PDE Loss:  -1.4606 | Function Loss:  -1.9421\n",
      "Total loss:  -0.8273 | PDE Loss:  -1.4602 | Function Loss:  -1.9424\n",
      "Total loss:  -0.8273 | PDE Loss:  -1.4597 | Function Loss:  -1.9426\n",
      "Total loss:  -0.8274 | PDE Loss:  -1.4593 | Function Loss:  -1.9428\n",
      "Total loss:  -0.8275 | PDE Loss:  -1.4588 | Function Loss:  -1.943\n",
      "Total loss:  -0.8273 | PDE Loss:  -1.4574 | Function Loss:  -1.9433\n",
      "Total loss:  -0.8275 | PDE Loss:  -1.4586 | Function Loss:  -1.9432\n",
      "Total loss:  -0.8275 | PDE Loss:  -1.4584 | Function Loss:  -1.9433\n",
      "Total loss:  -0.8276 | PDE Loss:  -1.4583 | Function Loss:  -1.9434\n",
      "Total loss:  -0.8277 | PDE Loss:  -1.4583 | Function Loss:  -1.9435\n",
      "Total loss:  -0.8278 | PDE Loss:  -1.4586 | Function Loss:  -1.9435\n",
      "Total loss:  -0.8278 | PDE Loss:  -1.4589 | Function Loss:  -1.9435\n",
      "Total loss:  -0.8279 | PDE Loss:  -1.4593 | Function Loss:  -1.9435\n",
      "Total loss:  -0.8279 | PDE Loss:  -1.4598 | Function Loss:  -1.9434\n",
      "Total loss:  -0.8279 | PDE Loss:  -1.46 | Function Loss:  -1.9433\n",
      "Total loss:  -0.828 | PDE Loss:  -1.46 | Function Loss:  -1.9434\n",
      "Total loss:  -0.828 | PDE Loss:  -1.4604 | Function Loss:  -1.9433\n",
      "Total loss:  -0.8281 | PDE Loss:  -1.4607 | Function Loss:  -1.9433\n",
      "Total loss:  -0.8282 | PDE Loss:  -1.461 | Function Loss:  -1.9433\n",
      "Total loss:  -0.8283 | PDE Loss:  -1.4612 | Function Loss:  -1.9434\n",
      "Total loss:  -0.8283 | PDE Loss:  -1.4612 | Function Loss:  -1.9435\n",
      "Total loss:  -0.8284 | PDE Loss:  -1.4611 | Function Loss:  -1.9436\n",
      "Total loss:  -0.8284 | PDE Loss:  -1.461 | Function Loss:  -1.9437\n",
      "Total loss:  -0.8285 | PDE Loss:  -1.4609 | Function Loss:  -1.9437\n",
      "Total loss:  -0.8284 | PDE Loss:  -1.4604 | Function Loss:  -1.9439\n",
      "Total loss:  -0.8285 | PDE Loss:  -1.4608 | Function Loss:  -1.9438\n",
      "Total loss:  -0.8286 | PDE Loss:  -1.4609 | Function Loss:  -1.9439\n",
      "Total loss:  -0.8287 | PDE Loss:  -1.4613 | Function Loss:  -1.9439\n",
      "Total loss:  -0.8288 | PDE Loss:  -1.4619 | Function Loss:  -1.9438\n",
      "Total loss:  -0.8289 | PDE Loss:  -1.4625 | Function Loss:  -1.9438\n",
      "Total loss:  -0.829 | PDE Loss:  -1.4628 | Function Loss:  -1.9438\n",
      "Total loss:  -0.829 | PDE Loss:  -1.4631 | Function Loss:  -1.9438\n",
      "Total loss:  -0.8291 | PDE Loss:  -1.4632 | Function Loss:  -1.9439\n",
      "Total loss:  -0.8291 | PDE Loss:  -1.4632 | Function Loss:  -1.9439\n",
      "Total loss:  -0.8292 | PDE Loss:  -1.4631 | Function Loss:  -1.944\n",
      "Total loss:  -0.8293 | PDE Loss:  -1.4628 | Function Loss:  -1.9442\n",
      "Total loss:  -0.8293 | PDE Loss:  -1.4626 | Function Loss:  -1.9443\n",
      "Total loss:  -0.8294 | PDE Loss:  -1.4623 | Function Loss:  -1.9446\n",
      "Total loss:  -0.8296 | PDE Loss:  -1.4619 | Function Loss:  -1.9449\n",
      "Total loss:  -0.8297 | PDE Loss:  -1.4617 | Function Loss:  -1.9451\n",
      "Total loss:  -0.8298 | PDE Loss:  -1.4615 | Function Loss:  -1.9452\n",
      "Total loss:  -0.8299 | PDE Loss:  -1.4615 | Function Loss:  -1.9454\n",
      "Total loss:  -0.8299 | PDE Loss:  -1.4617 | Function Loss:  -1.9454\n",
      "Total loss:  -0.83 | PDE Loss:  -1.4618 | Function Loss:  -1.9455\n",
      "Total loss:  -0.8301 | PDE Loss:  -1.462 | Function Loss:  -1.9455\n",
      "Total loss:  -0.8301 | PDE Loss:  -1.4622 | Function Loss:  -1.9455\n",
      "Total loss:  -0.8302 | PDE Loss:  -1.4622 | Function Loss:  -1.9455\n",
      "Total loss:  -0.8302 | PDE Loss:  -1.462 | Function Loss:  -1.9457\n",
      "Total loss:  -0.8303 | PDE Loss:  -1.4618 | Function Loss:  -1.9459\n",
      "Total loss:  -0.8304 | PDE Loss:  -1.4615 | Function Loss:  -1.946\n",
      "Total loss:  -0.8304 | PDE Loss:  -1.4613 | Function Loss:  -1.9462\n",
      "Total loss:  -0.8305 | PDE Loss:  -1.4608 | Function Loss:  -1.9464\n",
      "Total loss:  -0.8305 | PDE Loss:  -1.4607 | Function Loss:  -1.9465\n",
      "Total loss:  -0.8306 | PDE Loss:  -1.4607 | Function Loss:  -1.9465\n",
      "Total loss:  -0.8306 | PDE Loss:  -1.4607 | Function Loss:  -1.9466\n",
      "Total loss:  -0.8307 | PDE Loss:  -1.4608 | Function Loss:  -1.9466\n",
      "Total loss:  -0.8307 | PDE Loss:  -1.461 | Function Loss:  -1.9466\n",
      "Total loss:  -0.8307 | PDE Loss:  -1.4612 | Function Loss:  -1.9466\n",
      "Total loss:  -0.8308 | PDE Loss:  -1.4613 | Function Loss:  -1.9466\n",
      "Total loss:  -0.8308 | PDE Loss:  -1.4613 | Function Loss:  -1.9467\n",
      "Total loss:  -0.8307 | PDE Loss:  -1.4593 | Function Loss:  -1.9472\n",
      "Total loss:  -0.8308 | PDE Loss:  -1.4607 | Function Loss:  -1.9469\n",
      "Total loss:  -0.8309 | PDE Loss:  -1.4608 | Function Loss:  -1.947\n",
      "Total loss:  -0.831 | PDE Loss:  -1.4607 | Function Loss:  -1.9471\n",
      "Total loss:  -0.8311 | PDE Loss:  -1.4606 | Function Loss:  -1.9473\n",
      "Total loss:  -0.8312 | PDE Loss:  -1.4603 | Function Loss:  -1.9474\n",
      "Total loss:  -0.8312 | PDE Loss:  -1.4601 | Function Loss:  -1.9476\n",
      "Total loss:  -0.8313 | PDE Loss:  -1.4598 | Function Loss:  -1.9478\n",
      "Total loss:  -0.8314 | PDE Loss:  -1.4596 | Function Loss:  -1.9479\n",
      "Total loss:  -0.8314 | PDE Loss:  -1.4596 | Function Loss:  -1.948\n",
      "Total loss:  -0.8315 | PDE Loss:  -1.4594 | Function Loss:  -1.9482\n",
      "Total loss:  -0.8316 | PDE Loss:  -1.4595 | Function Loss:  -1.9482\n",
      "Total loss:  -0.8317 | PDE Loss:  -1.46 | Function Loss:  -1.9482\n",
      "Total loss:  -0.8318 | PDE Loss:  -1.4605 | Function Loss:  -1.9482\n",
      "Total loss:  -0.8319 | PDE Loss:  -1.461 | Function Loss:  -1.9482\n",
      "Total loss:  -0.832 | PDE Loss:  -1.461 | Function Loss:  -1.9483\n",
      "Total loss:  -0.8322 | PDE Loss:  -1.4611 | Function Loss:  -1.9485\n",
      "Total loss:  -0.8323 | PDE Loss:  -1.4611 | Function Loss:  -1.9486\n",
      "Total loss:  -0.8324 | PDE Loss:  -1.4609 | Function Loss:  -1.9488\n",
      "Total loss:  -0.8324 | PDE Loss:  -1.4608 | Function Loss:  -1.949\n",
      "Total loss:  -0.8325 | PDE Loss:  -1.4605 | Function Loss:  -1.9492\n",
      "Total loss:  -0.8326 | PDE Loss:  -1.4603 | Function Loss:  -1.9493\n",
      "Total loss:  -0.8327 | PDE Loss:  -1.46 | Function Loss:  -1.9495\n",
      "Total loss:  -0.8327 | PDE Loss:  -1.4598 | Function Loss:  -1.9496\n",
      "Total loss:  -0.8328 | PDE Loss:  -1.46 | Function Loss:  -1.9497\n",
      "Total loss:  -0.8329 | PDE Loss:  -1.4597 | Function Loss:  -1.9499\n",
      "Total loss:  -0.833 | PDE Loss:  -1.4598 | Function Loss:  -1.95\n",
      "Total loss:  -0.8331 | PDE Loss:  -1.4602 | Function Loss:  -1.95\n",
      "Total loss:  -0.8331 | PDE Loss:  -1.4603 | Function Loss:  -1.95\n",
      "Total loss:  -0.8332 | PDE Loss:  -1.4606 | Function Loss:  -1.95\n",
      "Total loss:  -0.8332 | PDE Loss:  -1.4608 | Function Loss:  -1.95\n",
      "Total loss:  -0.8333 | PDE Loss:  -1.4608 | Function Loss:  -1.95\n",
      "Total loss:  -0.8333 | PDE Loss:  -1.461 | Function Loss:  -1.95\n",
      "Total loss:  -0.8334 | PDE Loss:  -1.4608 | Function Loss:  -1.9502\n",
      "Total loss:  -0.8335 | PDE Loss:  -1.4607 | Function Loss:  -1.9503\n",
      "Total loss:  -0.8335 | PDE Loss:  -1.4605 | Function Loss:  -1.9505\n",
      "Total loss:  -0.8336 | PDE Loss:  -1.4603 | Function Loss:  -1.9506\n",
      "Total loss:  -0.8337 | PDE Loss:  -1.4601 | Function Loss:  -1.9509\n",
      "Total loss:  -0.8338 | PDE Loss:  -1.4599 | Function Loss:  -1.9511\n",
      "Total loss:  -0.8339 | PDE Loss:  -1.4598 | Function Loss:  -1.9512\n",
      "Total loss:  -0.834 | PDE Loss:  -1.4598 | Function Loss:  -1.9513\n",
      "Total loss:  -0.8341 | PDE Loss:  -1.4599 | Function Loss:  -1.9513\n",
      "Total loss:  -0.8341 | PDE Loss:  -1.4602 | Function Loss:  -1.9514\n",
      "Total loss:  -0.8342 | PDE Loss:  -1.4602 | Function Loss:  -1.9515\n",
      "Total loss:  -0.8343 | PDE Loss:  -1.461 | Function Loss:  -1.9513\n",
      "Total loss:  -0.8344 | PDE Loss:  -1.4605 | Function Loss:  -1.9516\n",
      "Total loss:  -0.8344 | PDE Loss:  -1.4608 | Function Loss:  -1.9515\n",
      "Total loss:  -0.8345 | PDE Loss:  -1.4613 | Function Loss:  -1.9515\n",
      "Total loss:  -0.8345 | PDE Loss:  -1.4615 | Function Loss:  -1.9515\n",
      "Total loss:  -0.8346 | PDE Loss:  -1.4616 | Function Loss:  -1.9515\n",
      "Total loss:  -0.8346 | PDE Loss:  -1.4617 | Function Loss:  -1.9516\n",
      "Total loss:  -0.8347 | PDE Loss:  -1.4616 | Function Loss:  -1.9516\n",
      "Total loss:  -0.8347 | PDE Loss:  -1.4616 | Function Loss:  -1.9517\n",
      "Total loss:  -0.8348 | PDE Loss:  -1.4615 | Function Loss:  -1.9518\n",
      "Total loss:  -0.8348 | PDE Loss:  -1.4615 | Function Loss:  -1.9519\n",
      "Total loss:  -0.8349 | PDE Loss:  -1.4614 | Function Loss:  -1.952\n",
      "Total loss:  -0.8349 | PDE Loss:  -1.4614 | Function Loss:  -1.952\n",
      "Total loss:  -0.835 | PDE Loss:  -1.4614 | Function Loss:  -1.9521\n",
      "Total loss:  -0.835 | PDE Loss:  -1.4614 | Function Loss:  -1.9521\n",
      "Total loss:  -0.8351 | PDE Loss:  -1.4615 | Function Loss:  -1.9522\n",
      "Total loss:  -0.8351 | PDE Loss:  -1.4619 | Function Loss:  -1.9521\n",
      "Total loss:  -0.8352 | PDE Loss:  -1.4619 | Function Loss:  -1.9522\n",
      "Total loss:  -0.8352 | PDE Loss:  -1.462 | Function Loss:  -1.9522\n",
      "Total loss:  -0.8353 | PDE Loss:  -1.4622 | Function Loss:  -1.9523\n",
      "Total loss:  -0.8353 | PDE Loss:  -1.4623 | Function Loss:  -1.9523\n",
      "Total loss:  -0.8354 | PDE Loss:  -1.4624 | Function Loss:  -1.9523\n",
      "Total loss:  -0.8355 | PDE Loss:  -1.4626 | Function Loss:  -1.9524\n",
      "Total loss:  -0.8356 | PDE Loss:  -1.4628 | Function Loss:  -1.9525\n",
      "Total loss:  -0.8357 | PDE Loss:  -1.4627 | Function Loss:  -1.9526\n",
      "Total loss:  -0.8358 | PDE Loss:  -1.4628 | Function Loss:  -1.9527\n",
      "Total loss:  -0.8358 | PDE Loss:  -1.4628 | Function Loss:  -1.9528\n",
      "Total loss:  -0.8359 | PDE Loss:  -1.4627 | Function Loss:  -1.9529\n",
      "Total loss:  -0.836 | PDE Loss:  -1.4624 | Function Loss:  -1.9532\n",
      "Total loss:  -0.8361 | PDE Loss:  -1.4621 | Function Loss:  -1.9534\n",
      "Total loss:  -0.8362 | PDE Loss:  -1.4618 | Function Loss:  -1.9536\n",
      "Total loss:  -0.8363 | PDE Loss:  -1.4615 | Function Loss:  -1.9538\n",
      "Total loss:  -0.8364 | PDE Loss:  -1.4614 | Function Loss:  -1.9539\n",
      "Total loss:  -0.8364 | PDE Loss:  -1.461 | Function Loss:  -1.9541\n",
      "Total loss:  -0.8365 | PDE Loss:  -1.461 | Function Loss:  -1.9542\n",
      "Total loss:  -0.8365 | PDE Loss:  -1.461 | Function Loss:  -1.9542\n",
      "Total loss:  -0.8366 | PDE Loss:  -1.4611 | Function Loss:  -1.9543\n",
      "Total loss:  -0.8366 | PDE Loss:  -1.4612 | Function Loss:  -1.9543\n",
      "Total loss:  -0.8367 | PDE Loss:  -1.4615 | Function Loss:  -1.9543\n",
      "Total loss:  -0.8368 | PDE Loss:  -1.4615 | Function Loss:  -1.9544\n",
      "Total loss:  -0.8369 | PDE Loss:  -1.4618 | Function Loss:  -1.9544\n",
      "Total loss:  -0.8369 | PDE Loss:  -1.462 | Function Loss:  -1.9545\n",
      "Total loss:  -0.837 | PDE Loss:  -1.4623 | Function Loss:  -1.9545\n",
      "Total loss:  -0.8371 | PDE Loss:  -1.4625 | Function Loss:  -1.9545\n",
      "Total loss:  -0.8371 | PDE Loss:  -1.4627 | Function Loss:  -1.9545\n",
      "Total loss:  -0.8372 | PDE Loss:  -1.4628 | Function Loss:  -1.9545\n",
      "Total loss:  -0.8372 | PDE Loss:  -1.463 | Function Loss:  -1.9546\n",
      "Total loss:  -0.8373 | PDE Loss:  -1.4632 | Function Loss:  -1.9546\n",
      "Total loss:  -0.8373 | PDE Loss:  -1.4635 | Function Loss:  -1.9545\n",
      "Total loss:  -0.8374 | PDE Loss:  -1.4636 | Function Loss:  -1.9546\n",
      "Total loss:  -0.8374 | PDE Loss:  -1.4637 | Function Loss:  -1.9546\n",
      "Total loss:  -0.8375 | PDE Loss:  -1.4639 | Function Loss:  -1.9546\n",
      "Total loss:  -0.8375 | PDE Loss:  -1.464 | Function Loss:  -1.9546\n",
      "Total loss:  -0.8376 | PDE Loss:  -1.4642 | Function Loss:  -1.9547\n",
      "Total loss:  -0.8377 | PDE Loss:  -1.4646 | Function Loss:  -1.9546\n",
      "Total loss:  -0.8377 | PDE Loss:  -1.4648 | Function Loss:  -1.9547\n",
      "Total loss:  -0.8378 | PDE Loss:  -1.4649 | Function Loss:  -1.9547\n",
      "Total loss:  -0.8378 | PDE Loss:  -1.4644 | Function Loss:  -1.9548\n",
      "Total loss:  -0.8378 | PDE Loss:  -1.4647 | Function Loss:  -1.9548\n",
      "Total loss:  -0.8379 | PDE Loss:  -1.465 | Function Loss:  -1.9548\n",
      "Total loss:  -0.8379 | PDE Loss:  -1.465 | Function Loss:  -1.9548\n",
      "Total loss:  -0.8381 | PDE Loss:  -1.4648 | Function Loss:  -1.9551\n",
      "Total loss:  -0.8381 | PDE Loss:  -1.4647 | Function Loss:  -1.9552\n",
      "Total loss:  -0.8382 | PDE Loss:  -1.4644 | Function Loss:  -1.9554\n",
      "Total loss:  -0.8383 | PDE Loss:  -1.4642 | Function Loss:  -1.9556\n",
      "Total loss:  -0.8383 | PDE Loss:  -1.4638 | Function Loss:  -1.9557\n",
      "Total loss:  -0.8384 | PDE Loss:  -1.463 | Function Loss:  -1.956\n",
      "Total loss:  -0.8385 | PDE Loss:  -1.4616 | Function Loss:  -1.9566\n",
      "Total loss:  -0.8385 | PDE Loss:  -1.4621 | Function Loss:  -1.9565\n",
      "Total loss:  -0.8386 | PDE Loss:  -1.4625 | Function Loss:  -1.9565\n",
      "Total loss:  -0.8387 | PDE Loss:  -1.4626 | Function Loss:  -1.9566\n",
      "Total loss:  -0.8387 | PDE Loss:  -1.4624 | Function Loss:  -1.9567\n",
      "Total loss:  -0.8388 | PDE Loss:  -1.4621 | Function Loss:  -1.9569\n",
      "Total loss:  -0.8389 | PDE Loss:  -1.4618 | Function Loss:  -1.9571\n",
      "Total loss:  -0.8379 | PDE Loss:  -1.455 | Function Loss:  -1.9579\n",
      "Total loss:  -0.839 | PDE Loss:  -1.461 | Function Loss:  -1.9575\n",
      "Total loss:  -0.8391 | PDE Loss:  -1.4608 | Function Loss:  -1.9576\n",
      "Total loss:  -0.8392 | PDE Loss:  -1.4605 | Function Loss:  -1.9579\n",
      "Total loss:  -0.8393 | PDE Loss:  -1.4604 | Function Loss:  -1.9581\n",
      "Total loss:  -0.8394 | PDE Loss:  -1.4604 | Function Loss:  -1.9581\n",
      "Total loss:  -0.8394 | PDE Loss:  -1.4605 | Function Loss:  -1.9582\n",
      "Total loss:  -0.8395 | PDE Loss:  -1.4607 | Function Loss:  -1.9582\n",
      "Total loss:  -0.8396 | PDE Loss:  -1.4612 | Function Loss:  -1.9582\n",
      "Total loss:  -0.8396 | PDE Loss:  -1.4615 | Function Loss:  -1.9582\n",
      "Total loss:  -0.8397 | PDE Loss:  -1.4618 | Function Loss:  -1.9581\n",
      "Total loss:  -0.8397 | PDE Loss:  -1.4623 | Function Loss:  -1.9581\n",
      "Total loss:  -0.8398 | PDE Loss:  -1.4628 | Function Loss:  -1.958\n",
      "Total loss:  -0.8399 | PDE Loss:  -1.4633 | Function Loss:  -1.958\n",
      "Total loss:  -0.84 | PDE Loss:  -1.4638 | Function Loss:  -1.9579\n",
      "Total loss:  -0.8401 | PDE Loss:  -1.4643 | Function Loss:  -1.9579\n",
      "Total loss:  -0.8402 | PDE Loss:  -1.4647 | Function Loss:  -1.9579\n",
      "Total loss:  -0.8402 | PDE Loss:  -1.4649 | Function Loss:  -1.9579\n",
      "Total loss:  -0.8403 | PDE Loss:  -1.465 | Function Loss:  -1.958\n",
      "Total loss:  -0.8404 | PDE Loss:  -1.4649 | Function Loss:  -1.9581\n",
      "Total loss:  -0.8404 | PDE Loss:  -1.465 | Function Loss:  -1.9581\n",
      "Total loss:  -0.8405 | PDE Loss:  -1.4649 | Function Loss:  -1.9582\n",
      "Total loss:  -0.8405 | PDE Loss:  -1.4648 | Function Loss:  -1.9583\n",
      "Total loss:  -0.8405 | PDE Loss:  -1.4646 | Function Loss:  -1.9584\n",
      "Total loss:  -0.8406 | PDE Loss:  -1.4644 | Function Loss:  -1.9585\n",
      "Total loss:  -0.8407 | PDE Loss:  -1.464 | Function Loss:  -1.9587\n",
      "Total loss:  -0.8407 | PDE Loss:  -1.4637 | Function Loss:  -1.959\n",
      "Total loss:  -0.8408 | PDE Loss:  -1.4633 | Function Loss:  -1.9592\n",
      "Total loss:  -0.8409 | PDE Loss:  -1.463 | Function Loss:  -1.9594\n",
      "Total loss:  -0.841 | PDE Loss:  -1.4627 | Function Loss:  -1.9595\n",
      "Total loss:  -0.841 | PDE Loss:  -1.4631 | Function Loss:  -1.9595\n",
      "Total loss:  -0.8411 | PDE Loss:  -1.463 | Function Loss:  -1.9596\n",
      "Total loss:  -0.8411 | PDE Loss:  -1.4632 | Function Loss:  -1.9596\n",
      "Total loss:  -0.8412 | PDE Loss:  -1.4634 | Function Loss:  -1.9596\n",
      "Total loss:  -0.8412 | PDE Loss:  -1.4637 | Function Loss:  -1.9596\n",
      "Total loss:  -0.8413 | PDE Loss:  -1.4641 | Function Loss:  -1.9595\n",
      "Total loss:  -0.8413 | PDE Loss:  -1.4649 | Function Loss:  -1.9593\n",
      "Total loss:  -0.8414 | PDE Loss:  -1.4649 | Function Loss:  -1.9594\n",
      "Total loss:  -0.8414 | PDE Loss:  -1.4653 | Function Loss:  -1.9593\n",
      "Total loss:  -0.8415 | PDE Loss:  -1.4654 | Function Loss:  -1.9594\n",
      "Total loss:  -0.8416 | PDE Loss:  -1.4656 | Function Loss:  -1.9594\n",
      "Total loss:  -0.8417 | PDE Loss:  -1.466 | Function Loss:  -1.9595\n",
      "Total loss:  -0.8418 | PDE Loss:  -1.4663 | Function Loss:  -1.9595\n",
      "Total loss:  -0.8413 | PDE Loss:  -1.4644 | Function Loss:  -1.9594\n",
      "Total loss:  -0.8419 | PDE Loss:  -1.4664 | Function Loss:  -1.9596\n",
      "Total loss:  -0.842 | PDE Loss:  -1.4666 | Function Loss:  -1.9596\n",
      "Total loss:  -0.8421 | PDE Loss:  -1.4668 | Function Loss:  -1.9597\n",
      "Total loss:  -0.8422 | PDE Loss:  -1.4671 | Function Loss:  -1.9597\n",
      "Total loss:  -0.8422 | PDE Loss:  -1.4673 | Function Loss:  -1.9597\n",
      "Total loss:  -0.8423 | PDE Loss:  -1.4675 | Function Loss:  -1.9597\n",
      "Total loss:  -0.8423 | PDE Loss:  -1.4677 | Function Loss:  -1.9598\n",
      "Total loss:  -0.8425 | PDE Loss:  -1.468 | Function Loss:  -1.9598\n",
      "Total loss:  -0.8426 | PDE Loss:  -1.4688 | Function Loss:  -1.9598\n",
      "Total loss:  -0.8427 | PDE Loss:  -1.4689 | Function Loss:  -1.9599\n",
      "Total loss:  -0.8428 | PDE Loss:  -1.4691 | Function Loss:  -1.96\n",
      "Total loss:  -0.8429 | PDE Loss:  -1.4691 | Function Loss:  -1.9601\n",
      "Total loss:  -0.843 | PDE Loss:  -1.4692 | Function Loss:  -1.9602\n",
      "Total loss:  -0.8431 | PDE Loss:  -1.4692 | Function Loss:  -1.9603\n",
      "Total loss:  -0.8432 | PDE Loss:  -1.4693 | Function Loss:  -1.9604\n",
      "Total loss:  -0.8432 | PDE Loss:  -1.4696 | Function Loss:  -1.9604\n",
      "Total loss:  -0.8433 | PDE Loss:  -1.4696 | Function Loss:  -1.9604\n",
      "Total loss:  -0.8434 | PDE Loss:  -1.4698 | Function Loss:  -1.9605\n",
      "Total loss:  -0.8435 | PDE Loss:  -1.4702 | Function Loss:  -1.9605\n",
      "Total loss:  -0.8436 | PDE Loss:  -1.4705 | Function Loss:  -1.9605\n",
      "Total loss:  -0.8437 | PDE Loss:  -1.4709 | Function Loss:  -1.9605\n",
      "Total loss:  -0.8437 | PDE Loss:  -1.4712 | Function Loss:  -1.9605\n",
      "Total loss:  -0.8438 | PDE Loss:  -1.4721 | Function Loss:  -1.9604\n",
      "Total loss:  -0.8439 | PDE Loss:  -1.4721 | Function Loss:  -1.9605\n",
      "Total loss:  -0.844 | PDE Loss:  -1.4722 | Function Loss:  -1.9606\n",
      "Total loss:  -0.8441 | PDE Loss:  -1.4722 | Function Loss:  -1.9607\n",
      "Total loss:  -0.8441 | PDE Loss:  -1.4723 | Function Loss:  -1.9607\n",
      "Total loss:  -0.8442 | PDE Loss:  -1.4723 | Function Loss:  -1.9608\n",
      "Total loss:  -0.8443 | PDE Loss:  -1.4723 | Function Loss:  -1.9609\n",
      "Total loss:  -0.8443 | PDE Loss:  -1.4724 | Function Loss:  -1.9609\n",
      "Total loss:  -0.8445 | PDE Loss:  -1.4726 | Function Loss:  -1.961\n",
      "Total loss:  -0.8446 | PDE Loss:  -1.4726 | Function Loss:  -1.9612\n",
      "Total loss:  -0.8447 | PDE Loss:  -1.4728 | Function Loss:  -1.9613\n",
      "Total loss:  -0.8448 | PDE Loss:  -1.4731 | Function Loss:  -1.9614\n",
      "Total loss:  -0.845 | PDE Loss:  -1.4734 | Function Loss:  -1.9615\n",
      "Total loss:  -0.8451 | PDE Loss:  -1.4736 | Function Loss:  -1.9615\n",
      "Total loss:  -0.8451 | PDE Loss:  -1.4737 | Function Loss:  -1.9616\n",
      "Total loss:  -0.8452 | PDE Loss:  -1.4737 | Function Loss:  -1.9616\n",
      "Total loss:  -0.8452 | PDE Loss:  -1.4737 | Function Loss:  -1.9617\n",
      "Total loss:  -0.8453 | PDE Loss:  -1.4732 | Function Loss:  -1.962\n",
      "Total loss:  -0.8454 | PDE Loss:  -1.4729 | Function Loss:  -1.9622\n",
      "Total loss:  -0.8455 | PDE Loss:  -1.4726 | Function Loss:  -1.9625\n",
      "Total loss:  -0.8457 | PDE Loss:  -1.4719 | Function Loss:  -1.9628\n",
      "Total loss:  -0.8458 | PDE Loss:  -1.4714 | Function Loss:  -1.9632\n",
      "Total loss:  -0.846 | PDE Loss:  -1.4707 | Function Loss:  -1.9636\n",
      "Total loss:  -0.8461 | PDE Loss:  -1.4703 | Function Loss:  -1.9639\n",
      "Total loss:  -0.8462 | PDE Loss:  -1.4697 | Function Loss:  -1.9643\n",
      "Total loss:  -0.8463 | PDE Loss:  -1.4695 | Function Loss:  -1.9645\n",
      "Total loss:  -0.8464 | PDE Loss:  -1.4692 | Function Loss:  -1.9646\n",
      "Total loss:  -0.8465 | PDE Loss:  -1.4695 | Function Loss:  -1.9647\n",
      "Total loss:  -0.8466 | PDE Loss:  -1.4695 | Function Loss:  -1.9648\n",
      "Total loss:  -0.8467 | PDE Loss:  -1.4695 | Function Loss:  -1.965\n",
      "Total loss:  -0.8468 | PDE Loss:  -1.4697 | Function Loss:  -1.9651\n",
      "Total loss:  -0.847 | PDE Loss:  -1.4695 | Function Loss:  -1.9653\n",
      "Total loss:  -0.847 | PDE Loss:  -1.4693 | Function Loss:  -1.9654\n",
      "Total loss:  -0.8471 | PDE Loss:  -1.4691 | Function Loss:  -1.9656\n",
      "Total loss:  -0.8472 | PDE Loss:  -1.469 | Function Loss:  -1.9658\n",
      "Total loss:  -0.8473 | PDE Loss:  -1.4688 | Function Loss:  -1.9659\n",
      "Total loss:  -0.8474 | PDE Loss:  -1.4689 | Function Loss:  -1.966\n",
      "Total loss:  -0.8476 | PDE Loss:  -1.469 | Function Loss:  -1.9663\n",
      "Total loss:  -0.8478 | PDE Loss:  -1.4692 | Function Loss:  -1.9665\n",
      "Total loss:  -0.848 | PDE Loss:  -1.4691 | Function Loss:  -1.9667\n",
      "Total loss:  -0.8481 | PDE Loss:  -1.4696 | Function Loss:  -1.9668\n",
      "Total loss:  -0.8482 | PDE Loss:  -1.4699 | Function Loss:  -1.9668\n",
      "Total loss:  -0.8483 | PDE Loss:  -1.4701 | Function Loss:  -1.9668\n",
      "Total loss:  -0.8483 | PDE Loss:  -1.4702 | Function Loss:  -1.9669\n",
      "Total loss:  -0.8484 | PDE Loss:  -1.4703 | Function Loss:  -1.9669\n",
      "Total loss:  -0.8484 | PDE Loss:  -1.4702 | Function Loss:  -1.967\n",
      "Total loss:  -0.8485 | PDE Loss:  -1.4699 | Function Loss:  -1.9672\n",
      "Total loss:  -0.8486 | PDE Loss:  -1.4697 | Function Loss:  -1.9674\n",
      "Total loss:  -0.8487 | PDE Loss:  -1.469 | Function Loss:  -1.9677\n",
      "Total loss:  -0.8487 | PDE Loss:  -1.4687 | Function Loss:  -1.9678\n",
      "Total loss:  -0.8488 | PDE Loss:  -1.4683 | Function Loss:  -1.968\n",
      "Total loss:  -0.8488 | PDE Loss:  -1.4681 | Function Loss:  -1.9682\n",
      "Total loss:  -0.8489 | PDE Loss:  -1.468 | Function Loss:  -1.9683\n",
      "Total loss:  -0.849 | PDE Loss:  -1.4681 | Function Loss:  -1.9685\n",
      "Total loss:  -0.8492 | PDE Loss:  -1.4683 | Function Loss:  -1.9685\n",
      "Total loss:  -0.8493 | PDE Loss:  -1.4679 | Function Loss:  -1.9689\n",
      "Total loss:  -0.8494 | PDE Loss:  -1.468 | Function Loss:  -1.9689\n",
      "Total loss:  -0.8495 | PDE Loss:  -1.4684 | Function Loss:  -1.9689\n",
      "Total loss:  -0.8496 | PDE Loss:  -1.4688 | Function Loss:  -1.9689\n",
      "Total loss:  -0.8496 | PDE Loss:  -1.4691 | Function Loss:  -1.9689\n",
      "Total loss:  -0.8497 | PDE Loss:  -1.4693 | Function Loss:  -1.969\n",
      "Total loss:  -0.8499 | PDE Loss:  -1.4697 | Function Loss:  -1.969\n",
      "Total loss:  -0.85 | PDE Loss:  -1.47 | Function Loss:  -1.9692\n",
      "Total loss:  -0.85 | PDE Loss:  -1.4712 | Function Loss:  -1.9687\n",
      "Total loss:  -0.8501 | PDE Loss:  -1.4708 | Function Loss:  -1.969\n",
      "Total loss:  -0.8502 | PDE Loss:  -1.4706 | Function Loss:  -1.9692\n",
      "Total loss:  -0.8504 | PDE Loss:  -1.4706 | Function Loss:  -1.9694\n",
      "Total loss:  -0.8505 | PDE Loss:  -1.4706 | Function Loss:  -1.9695\n",
      "Total loss:  -0.8506 | PDE Loss:  -1.4702 | Function Loss:  -1.9698\n",
      "Total loss:  -0.8506 | PDE Loss:  -1.4704 | Function Loss:  -1.9698\n",
      "Total loss:  -0.8507 | PDE Loss:  -1.4707 | Function Loss:  -1.9698\n",
      "Total loss:  -0.8508 | PDE Loss:  -1.4711 | Function Loss:  -1.9699\n",
      "Total loss:  -0.8509 | PDE Loss:  -1.4718 | Function Loss:  -1.9698\n",
      "Total loss:  -0.851 | PDE Loss:  -1.4722 | Function Loss:  -1.9698\n",
      "Total loss:  -0.8511 | PDE Loss:  -1.4727 | Function Loss:  -1.9697\n",
      "Total loss:  -0.8512 | PDE Loss:  -1.4732 | Function Loss:  -1.9697\n",
      "Total loss:  -0.8513 | PDE Loss:  -1.4736 | Function Loss:  -1.9697\n",
      "Total loss:  -0.8514 | PDE Loss:  -1.4742 | Function Loss:  -1.9696\n",
      "Total loss:  -0.8515 | PDE Loss:  -1.4742 | Function Loss:  -1.9697\n",
      "Total loss:  -0.8516 | PDE Loss:  -1.4742 | Function Loss:  -1.9698\n",
      "Total loss:  -0.8517 | PDE Loss:  -1.4742 | Function Loss:  -1.97\n",
      "Total loss:  -0.8518 | PDE Loss:  -1.4742 | Function Loss:  -1.9702\n",
      "Total loss:  -0.8519 | PDE Loss:  -1.4739 | Function Loss:  -1.9703\n",
      "Total loss:  -0.852 | PDE Loss:  -1.4743 | Function Loss:  -1.9704\n",
      "Total loss:  -0.8521 | PDE Loss:  -1.4746 | Function Loss:  -1.9704\n",
      "Total loss:  -0.8522 | PDE Loss:  -1.475 | Function Loss:  -1.9704\n",
      "Total loss:  -0.8523 | PDE Loss:  -1.4753 | Function Loss:  -1.9704\n",
      "Total loss:  -0.8524 | PDE Loss:  -1.4756 | Function Loss:  -1.9705\n",
      "Total loss:  -0.8525 | PDE Loss:  -1.4758 | Function Loss:  -1.9705\n",
      "Total loss:  -0.8525 | PDE Loss:  -1.4759 | Function Loss:  -1.9706\n",
      "Total loss:  -0.8526 | PDE Loss:  -1.476 | Function Loss:  -1.9707\n",
      "Total loss:  -0.8527 | PDE Loss:  -1.4759 | Function Loss:  -1.9708\n",
      "Total loss:  -0.8527 | PDE Loss:  -1.4759 | Function Loss:  -1.9709\n",
      "Total loss:  -0.8528 | PDE Loss:  -1.4757 | Function Loss:  -1.971\n",
      "Total loss:  -0.8529 | PDE Loss:  -1.4757 | Function Loss:  -1.9711\n",
      "Total loss:  -0.8529 | PDE Loss:  -1.4757 | Function Loss:  -1.9712\n",
      "Total loss:  -0.853 | PDE Loss:  -1.4759 | Function Loss:  -1.9713\n",
      "Total loss:  -0.8531 | PDE Loss:  -1.4766 | Function Loss:  -1.9712\n",
      "Total loss:  -0.8533 | PDE Loss:  -1.4762 | Function Loss:  -1.9715\n",
      "Total loss:  -0.8534 | PDE Loss:  -1.4762 | Function Loss:  -1.9716\n",
      "Total loss:  -0.8535 | PDE Loss:  -1.4766 | Function Loss:  -1.9716\n",
      "Total loss:  -0.8536 | PDE Loss:  -1.477 | Function Loss:  -1.9716\n",
      "Total loss:  -0.8536 | PDE Loss:  -1.4774 | Function Loss:  -1.9716\n",
      "Total loss:  -0.8537 | PDE Loss:  -1.4779 | Function Loss:  -1.9716\n",
      "Total loss:  -0.8539 | PDE Loss:  -1.4783 | Function Loss:  -1.9716\n",
      "Total loss:  -0.8541 | PDE Loss:  -1.4792 | Function Loss:  -1.9715\n",
      "Total loss:  -0.8542 | PDE Loss:  -1.4793 | Function Loss:  -1.9717\n",
      "Total loss:  -0.8539 | PDE Loss:  -1.48 | Function Loss:  -1.9711\n",
      "Total loss:  -0.8543 | PDE Loss:  -1.4798 | Function Loss:  -1.9717\n",
      "Total loss:  -0.8544 | PDE Loss:  -1.4801 | Function Loss:  -1.9717\n",
      "Total loss:  -0.8546 | PDE Loss:  -1.4806 | Function Loss:  -1.9718\n",
      "Total loss:  -0.8547 | PDE Loss:  -1.4809 | Function Loss:  -1.9719\n",
      "Total loss:  -0.8548 | PDE Loss:  -1.4813 | Function Loss:  -1.9719\n",
      "Total loss:  -0.855 | PDE Loss:  -1.4815 | Function Loss:  -1.9721\n",
      "Total loss:  -0.8551 | PDE Loss:  -1.4819 | Function Loss:  -1.9721\n",
      "Total loss:  -0.8552 | PDE Loss:  -1.482 | Function Loss:  -1.9722\n",
      "Total loss:  -0.8553 | PDE Loss:  -1.4822 | Function Loss:  -1.9722\n",
      "Total loss:  -0.8554 | PDE Loss:  -1.4826 | Function Loss:  -1.9723\n",
      "Total loss:  -0.8555 | PDE Loss:  -1.483 | Function Loss:  -1.9723\n",
      "Total loss:  -0.8556 | PDE Loss:  -1.4834 | Function Loss:  -1.9723\n",
      "Total loss:  -0.8557 | PDE Loss:  -1.4842 | Function Loss:  -1.9722\n",
      "Total loss:  -0.8558 | PDE Loss:  -1.4847 | Function Loss:  -1.9722\n",
      "Total loss:  -0.8559 | PDE Loss:  -1.4846 | Function Loss:  -1.9723\n",
      "Total loss:  -0.856 | PDE Loss:  -1.4849 | Function Loss:  -1.9724\n",
      "Total loss:  -0.8561 | PDE Loss:  -1.4848 | Function Loss:  -1.9725\n",
      "Total loss:  -0.8562 | PDE Loss:  -1.4851 | Function Loss:  -1.9725\n",
      "Total loss:  -0.8563 | PDE Loss:  -1.4854 | Function Loss:  -1.9725\n",
      "Total loss:  -0.8563 | PDE Loss:  -1.4859 | Function Loss:  -1.9725\n",
      "Total loss:  -0.8564 | PDE Loss:  -1.4863 | Function Loss:  -1.9724\n",
      "Total loss:  -0.8565 | PDE Loss:  -1.487 | Function Loss:  -1.9724\n",
      "Total loss:  -0.8567 | PDE Loss:  -1.4875 | Function Loss:  -1.9724\n",
      "Total loss:  -0.8568 | PDE Loss:  -1.488 | Function Loss:  -1.9724\n",
      "Total loss:  -0.8569 | PDE Loss:  -1.4883 | Function Loss:  -1.9724\n",
      "Total loss:  -0.8569 | PDE Loss:  -1.4887 | Function Loss:  -1.9724\n",
      "Total loss:  -0.857 | PDE Loss:  -1.4886 | Function Loss:  -1.9725\n",
      "Total loss:  -0.8571 | PDE Loss:  -1.4888 | Function Loss:  -1.9726\n",
      "Total loss:  -0.8573 | PDE Loss:  -1.4889 | Function Loss:  -1.9728\n",
      "Total loss:  -0.8574 | PDE Loss:  -1.489 | Function Loss:  -1.9729\n",
      "Total loss:  -0.8575 | PDE Loss:  -1.4892 | Function Loss:  -1.973\n",
      "Total loss:  -0.8576 | PDE Loss:  -1.4892 | Function Loss:  -1.9731\n",
      "Total loss:  -0.8577 | PDE Loss:  -1.4893 | Function Loss:  -1.9732\n",
      "Total loss:  -0.8578 | PDE Loss:  -1.4893 | Function Loss:  -1.9734\n",
      "Total loss:  -0.8579 | PDE Loss:  -1.4895 | Function Loss:  -1.9735\n",
      "Total loss:  -0.858 | PDE Loss:  -1.4895 | Function Loss:  -1.9736\n",
      "Total loss:  -0.8581 | PDE Loss:  -1.4896 | Function Loss:  -1.9737\n",
      "Total loss:  -0.8582 | PDE Loss:  -1.4897 | Function Loss:  -1.9738\n",
      "Total loss:  -0.8582 | PDE Loss:  -1.4898 | Function Loss:  -1.9738\n",
      "Total loss:  -0.8583 | PDE Loss:  -1.4899 | Function Loss:  -1.9738\n",
      "Total loss:  -0.8583 | PDE Loss:  -1.49 | Function Loss:  -1.9738\n",
      "Total loss:  -0.8584 | PDE Loss:  -1.4902 | Function Loss:  -1.9738\n",
      "Total loss:  -0.8584 | PDE Loss:  -1.4904 | Function Loss:  -1.9738\n",
      "Total loss:  -0.8585 | PDE Loss:  -1.4901 | Function Loss:  -1.974\n",
      "Total loss:  -0.8586 | PDE Loss:  -1.4905 | Function Loss:  -1.974\n",
      "Total loss:  -0.8587 | PDE Loss:  -1.4908 | Function Loss:  -1.9741\n",
      "Total loss:  -0.8588 | PDE Loss:  -1.4912 | Function Loss:  -1.9741\n",
      "Total loss:  -0.859 | PDE Loss:  -1.4915 | Function Loss:  -1.9742\n",
      "Total loss:  -0.8591 | PDE Loss:  -1.4919 | Function Loss:  -1.9742\n",
      "Total loss:  -0.8591 | PDE Loss:  -1.4922 | Function Loss:  -1.9742\n",
      "Total loss:  -0.8592 | PDE Loss:  -1.4923 | Function Loss:  -1.9743\n",
      "Total loss:  -0.8593 | PDE Loss:  -1.4927 | Function Loss:  -1.9743\n",
      "Total loss:  -0.8594 | PDE Loss:  -1.4929 | Function Loss:  -1.9744\n",
      "Total loss:  -0.8595 | PDE Loss:  -1.493 | Function Loss:  -1.9745\n",
      "Total loss:  -0.8596 | PDE Loss:  -1.4931 | Function Loss:  -1.9745\n",
      "Total loss:  -0.8597 | PDE Loss:  -1.4932 | Function Loss:  -1.9746\n",
      "Total loss:  -0.8597 | PDE Loss:  -1.4932 | Function Loss:  -1.9747\n",
      "Total loss:  -0.8598 | PDE Loss:  -1.4932 | Function Loss:  -1.9748\n",
      "Total loss:  -0.8599 | PDE Loss:  -1.4932 | Function Loss:  -1.9749\n",
      "Total loss:  -0.86 | PDE Loss:  -1.4932 | Function Loss:  -1.9751\n",
      "Total loss:  -0.8602 | PDE Loss:  -1.4933 | Function Loss:  -1.9752\n",
      "Total loss:  -0.8599 | PDE Loss:  -1.4908 | Function Loss:  -1.9757\n",
      "Total loss:  -0.8603 | PDE Loss:  -1.4929 | Function Loss:  -1.9755\n",
      "Total loss:  -0.8604 | PDE Loss:  -1.4931 | Function Loss:  -1.9756\n",
      "Total loss:  -0.8606 | PDE Loss:  -1.4934 | Function Loss:  -1.9757\n",
      "Total loss:  -0.8607 | PDE Loss:  -1.4935 | Function Loss:  -1.9759\n",
      "Total loss:  -0.8608 | PDE Loss:  -1.4936 | Function Loss:  -1.9759\n",
      "Total loss:  -0.8609 | PDE Loss:  -1.4935 | Function Loss:  -1.9761\n",
      "Total loss:  -0.861 | PDE Loss:  -1.4935 | Function Loss:  -1.9762\n",
      "Total loss:  -0.861 | PDE Loss:  -1.4933 | Function Loss:  -1.9764\n",
      "Total loss:  -0.8612 | PDE Loss:  -1.4932 | Function Loss:  -1.9766\n",
      "Total loss:  -0.8613 | PDE Loss:  -1.4929 | Function Loss:  -1.9768\n",
      "Total loss:  -0.8615 | PDE Loss:  -1.4923 | Function Loss:  -1.9772\n",
      "Total loss:  -0.8616 | PDE Loss:  -1.4921 | Function Loss:  -1.9775\n",
      "Total loss:  -0.8617 | PDE Loss:  -1.4917 | Function Loss:  -1.9777\n",
      "Total loss:  -0.8618 | PDE Loss:  -1.492 | Function Loss:  -1.9778\n",
      "Total loss:  -0.8619 | PDE Loss:  -1.4921 | Function Loss:  -1.9779\n",
      "Total loss:  -0.862 | PDE Loss:  -1.4922 | Function Loss:  -1.9779\n",
      "Total loss:  -0.8621 | PDE Loss:  -1.4925 | Function Loss:  -1.978\n",
      "Total loss:  -0.8622 | PDE Loss:  -1.4927 | Function Loss:  -1.978\n",
      "Total loss:  -0.8622 | PDE Loss:  -1.4927 | Function Loss:  -1.9781\n",
      "Total loss:  -0.8623 | PDE Loss:  -1.4932 | Function Loss:  -1.9781\n",
      "Total loss:  -0.8624 | PDE Loss:  -1.4932 | Function Loss:  -1.9781\n",
      "Total loss:  -0.8625 | PDE Loss:  -1.4933 | Function Loss:  -1.9782\n",
      "Total loss:  -0.8625 | PDE Loss:  -1.4935 | Function Loss:  -1.9782\n",
      "Total loss:  -0.8626 | PDE Loss:  -1.4936 | Function Loss:  -1.9783\n",
      "Total loss:  -0.8627 | PDE Loss:  -1.4936 | Function Loss:  -1.9784\n",
      "Total loss:  -0.8628 | PDE Loss:  -1.4935 | Function Loss:  -1.9786\n",
      "Total loss:  -0.8629 | PDE Loss:  -1.4937 | Function Loss:  -1.9786\n",
      "Total loss:  -0.863 | PDE Loss:  -1.4936 | Function Loss:  -1.9788\n",
      "Total loss:  -0.8631 | PDE Loss:  -1.4933 | Function Loss:  -1.979\n",
      "Total loss:  -0.8632 | PDE Loss:  -1.4928 | Function Loss:  -1.9794\n",
      "Total loss:  -0.8633 | PDE Loss:  -1.4925 | Function Loss:  -1.9796\n",
      "Total loss:  -0.8634 | PDE Loss:  -1.492 | Function Loss:  -1.9799\n",
      "Total loss:  -0.8635 | PDE Loss:  -1.4918 | Function Loss:  -1.9801\n",
      "Total loss:  -0.8636 | PDE Loss:  -1.4918 | Function Loss:  -1.9802\n",
      "Total loss:  -0.8638 | PDE Loss:  -1.4909 | Function Loss:  -1.9806\n",
      "Total loss:  -0.8638 | PDE Loss:  -1.4911 | Function Loss:  -1.9807\n",
      "Total loss:  -0.864 | PDE Loss:  -1.4915 | Function Loss:  -1.9807\n",
      "Total loss:  -0.8641 | PDE Loss:  -1.4916 | Function Loss:  -1.9808\n",
      "Total loss:  -0.8641 | PDE Loss:  -1.4916 | Function Loss:  -1.9809\n",
      "Total loss:  -0.8642 | PDE Loss:  -1.4917 | Function Loss:  -1.981\n",
      "Total loss:  -0.8643 | PDE Loss:  -1.4916 | Function Loss:  -1.9811\n",
      "Total loss:  -0.8643 | PDE Loss:  -1.4915 | Function Loss:  -1.9812\n",
      "Total loss:  -0.8644 | PDE Loss:  -1.4915 | Function Loss:  -1.9813\n",
      "Total loss:  -0.8645 | PDE Loss:  -1.4915 | Function Loss:  -1.9814\n",
      "Total loss:  -0.8646 | PDE Loss:  -1.4915 | Function Loss:  -1.9815\n",
      "Total loss:  -0.8647 | PDE Loss:  -1.4913 | Function Loss:  -1.9818\n",
      "Total loss:  -0.8648 | PDE Loss:  -1.4912 | Function Loss:  -1.982\n",
      "Total loss:  -0.865 | PDE Loss:  -1.4909 | Function Loss:  -1.9823\n",
      "Total loss:  -0.8652 | PDE Loss:  -1.4909 | Function Loss:  -1.9825\n",
      "Total loss:  -0.8653 | PDE Loss:  -1.4909 | Function Loss:  -1.9827\n",
      "Total loss:  -0.8655 | PDE Loss:  -1.4911 | Function Loss:  -1.9828\n",
      "Total loss:  -0.8656 | PDE Loss:  -1.4913 | Function Loss:  -1.9829\n",
      "Total loss:  -0.8657 | PDE Loss:  -1.4917 | Function Loss:  -1.9829\n",
      "Total loss:  -0.8658 | PDE Loss:  -1.4919 | Function Loss:  -1.983\n",
      "Total loss:  -0.8659 | PDE Loss:  -1.4923 | Function Loss:  -1.983\n",
      "Total loss:  -0.866 | PDE Loss:  -1.4926 | Function Loss:  -1.983\n",
      "Total loss:  -0.8661 | PDE Loss:  -1.4931 | Function Loss:  -1.983\n",
      "Total loss:  -0.8662 | PDE Loss:  -1.4931 | Function Loss:  -1.9831\n",
      "Total loss:  -0.8663 | PDE Loss:  -1.4931 | Function Loss:  -1.9833\n",
      "Total loss:  -0.8664 | PDE Loss:  -1.493 | Function Loss:  -1.9835\n",
      "Total loss:  -0.8666 | PDE Loss:  -1.4927 | Function Loss:  -1.9839\n",
      "Total loss:  -0.8668 | PDE Loss:  -1.4927 | Function Loss:  -1.9841\n",
      "Total loss:  -0.8671 | PDE Loss:  -1.4924 | Function Loss:  -1.9845\n",
      "Total loss:  -0.8672 | PDE Loss:  -1.4927 | Function Loss:  -1.9846\n",
      "Total loss:  -0.8673 | PDE Loss:  -1.4931 | Function Loss:  -1.9846\n",
      "Total loss:  -0.8674 | PDE Loss:  -1.4934 | Function Loss:  -1.9847\n",
      "Total loss:  -0.8675 | PDE Loss:  -1.494 | Function Loss:  -1.9846\n",
      "Total loss:  -0.8676 | PDE Loss:  -1.4948 | Function Loss:  -1.9844\n",
      "Total loss:  -0.8676 | PDE Loss:  -1.4956 | Function Loss:  -1.9843\n",
      "Total loss:  -0.8677 | PDE Loss:  -1.4963 | Function Loss:  -1.9841\n",
      "Total loss:  -0.8677 | PDE Loss:  -1.4971 | Function Loss:  -1.984\n",
      "Total loss:  -0.8678 | PDE Loss:  -1.4976 | Function Loss:  -1.9839\n",
      "Total loss:  -0.8679 | PDE Loss:  -1.498 | Function Loss:  -1.9838\n",
      "Total loss:  -0.8679 | PDE Loss:  -1.4982 | Function Loss:  -1.9838\n",
      "Total loss:  -0.868 | PDE Loss:  -1.4983 | Function Loss:  -1.9839\n",
      "Total loss:  -0.8681 | PDE Loss:  -1.4983 | Function Loss:  -1.984\n",
      "Total loss:  -0.8681 | PDE Loss:  -1.4982 | Function Loss:  -1.9841\n",
      "Total loss:  -0.8682 | PDE Loss:  -1.4981 | Function Loss:  -1.9843\n",
      "Total loss:  -0.8683 | PDE Loss:  -1.4981 | Function Loss:  -1.9844\n",
      "Total loss:  -0.8684 | PDE Loss:  -1.4982 | Function Loss:  -1.9845\n",
      "Total loss:  -0.8685 | PDE Loss:  -1.4985 | Function Loss:  -1.9845\n",
      "Total loss:  -0.8686 | PDE Loss:  -1.4987 | Function Loss:  -1.9845\n",
      "Total loss:  -0.8687 | PDE Loss:  -1.4989 | Function Loss:  -1.9846\n",
      "Total loss:  -0.8687 | PDE Loss:  -1.4992 | Function Loss:  -1.9846\n",
      "Total loss:  -0.8688 | PDE Loss:  -1.4995 | Function Loss:  -1.9846\n",
      "Total loss:  -0.8689 | PDE Loss:  -1.4997 | Function Loss:  -1.9846\n",
      "Total loss:  -0.869 | PDE Loss:  -1.4999 | Function Loss:  -1.9847\n",
      "Total loss:  -0.8691 | PDE Loss:  -1.5 | Function Loss:  -1.9848\n",
      "Total loss:  -0.8692 | PDE Loss:  -1.5003 | Function Loss:  -1.9849\n",
      "Total loss:  -0.8693 | PDE Loss:  -1.5003 | Function Loss:  -1.985\n",
      "Total loss:  -0.8695 | PDE Loss:  -1.5002 | Function Loss:  -1.9853\n",
      "Total loss:  -0.8696 | PDE Loss:  -1.4999 | Function Loss:  -1.9855\n",
      "Total loss:  -0.8697 | PDE Loss:  -1.4996 | Function Loss:  -1.9858\n",
      "Total loss:  -0.8699 | PDE Loss:  -1.4991 | Function Loss:  -1.9861\n",
      "Total loss:  -0.87 | PDE Loss:  -1.4986 | Function Loss:  -1.9864\n",
      "Total loss:  -0.8701 | PDE Loss:  -1.4982 | Function Loss:  -1.9867\n",
      "Total loss:  -0.8702 | PDE Loss:  -1.4981 | Function Loss:  -1.9869\n",
      "Total loss:  -0.8703 | PDE Loss:  -1.4979 | Function Loss:  -1.987\n",
      "Total loss:  -0.8704 | PDE Loss:  -1.4978 | Function Loss:  -1.9872\n",
      "Total loss:  -0.8704 | PDE Loss:  -1.4979 | Function Loss:  -1.9872\n",
      "Total loss:  -0.8705 | PDE Loss:  -1.4981 | Function Loss:  -1.9873\n",
      "Total loss:  -0.8706 | PDE Loss:  -1.4982 | Function Loss:  -1.9873\n",
      "Total loss:  -0.8706 | PDE Loss:  -1.4984 | Function Loss:  -1.9873\n",
      "Total loss:  -0.8707 | PDE Loss:  -1.4986 | Function Loss:  -1.9874\n",
      "Total loss:  -0.8709 | PDE Loss:  -1.4987 | Function Loss:  -1.9875\n",
      "Total loss:  -0.871 | PDE Loss:  -1.4988 | Function Loss:  -1.9877\n",
      "Total loss:  -0.8711 | PDE Loss:  -1.4987 | Function Loss:  -1.9879\n",
      "Total loss:  -0.8712 | PDE Loss:  -1.4985 | Function Loss:  -1.988\n",
      "Total loss:  -0.8712 | PDE Loss:  -1.4983 | Function Loss:  -1.9882\n",
      "Total loss:  -0.8713 | PDE Loss:  -1.4979 | Function Loss:  -1.9883\n",
      "Total loss:  -0.8713 | PDE Loss:  -1.4979 | Function Loss:  -1.9884\n",
      "Total loss:  -0.8714 | PDE Loss:  -1.498 | Function Loss:  -1.9884\n",
      "Total loss:  -0.8715 | PDE Loss:  -1.4982 | Function Loss:  -1.9885\n",
      "Total loss:  -0.8715 | PDE Loss:  -1.4985 | Function Loss:  -1.9885\n",
      "Total loss:  -0.8716 | PDE Loss:  -1.4988 | Function Loss:  -1.9885\n",
      "Total loss:  -0.8717 | PDE Loss:  -1.4992 | Function Loss:  -1.9885\n",
      "Total loss:  -0.8718 | PDE Loss:  -1.4997 | Function Loss:  -1.9885\n",
      "Total loss:  -0.8719 | PDE Loss:  -1.4998 | Function Loss:  -1.9885\n",
      "Total loss:  -0.872 | PDE Loss:  -1.5 | Function Loss:  -1.9886\n",
      "Total loss:  -0.8721 | PDE Loss:  -1.5001 | Function Loss:  -1.9887\n",
      "Total loss:  -0.8722 | PDE Loss:  -1.5 | Function Loss:  -1.9888\n",
      "Total loss:  -0.8723 | PDE Loss:  -1.5004 | Function Loss:  -1.9889\n",
      "Total loss:  -0.8725 | PDE Loss:  -1.5008 | Function Loss:  -1.989\n",
      "Total loss:  -0.8727 | PDE Loss:  -1.5015 | Function Loss:  -1.989\n",
      "Total loss:  -0.8728 | PDE Loss:  -1.5019 | Function Loss:  -1.9891\n",
      "Total loss:  -0.8729 | PDE Loss:  -1.5025 | Function Loss:  -1.9891\n",
      "Total loss:  -0.8731 | PDE Loss:  -1.5032 | Function Loss:  -1.989\n",
      "Total loss:  -0.8732 | PDE Loss:  -1.5044 | Function Loss:  -1.9888\n",
      "Total loss:  -0.8732 | PDE Loss:  -1.5047 | Function Loss:  -1.9888\n",
      "Total loss:  -0.8733 | PDE Loss:  -1.5052 | Function Loss:  -1.9888\n",
      "Total loss:  -0.8734 | PDE Loss:  -1.5056 | Function Loss:  -1.9888\n",
      "Total loss:  -0.8735 | PDE Loss:  -1.506 | Function Loss:  -1.9888\n",
      "Total loss:  -0.8736 | PDE Loss:  -1.5063 | Function Loss:  -1.9888\n",
      "Total loss:  -0.8737 | PDE Loss:  -1.5066 | Function Loss:  -1.9888\n",
      "Total loss:  -0.8738 | PDE Loss:  -1.5067 | Function Loss:  -1.9889\n",
      "Total loss:  -0.874 | PDE Loss:  -1.5071 | Function Loss:  -1.989\n",
      "Total loss:  -0.874 | PDE Loss:  -1.506 | Function Loss:  -1.9893\n",
      "Total loss:  -0.8741 | PDE Loss:  -1.5068 | Function Loss:  -1.9893\n",
      "Total loss:  -0.8743 | PDE Loss:  -1.5069 | Function Loss:  -1.9894\n",
      "Total loss:  -0.8744 | PDE Loss:  -1.5071 | Function Loss:  -1.9896\n",
      "Total loss:  -0.8745 | PDE Loss:  -1.5074 | Function Loss:  -1.9897\n",
      "Total loss:  -0.8747 | PDE Loss:  -1.5075 | Function Loss:  -1.9898\n",
      "Total loss:  -0.8748 | PDE Loss:  -1.5077 | Function Loss:  -1.9899\n",
      "Total loss:  -0.8749 | PDE Loss:  -1.5079 | Function Loss:  -1.99\n",
      "Total loss:  -0.875 | PDE Loss:  -1.5082 | Function Loss:  -1.9901\n",
      "Total loss:  -0.8751 | PDE Loss:  -1.5085 | Function Loss:  -1.9901\n",
      "Total loss:  -0.8753 | PDE Loss:  -1.5089 | Function Loss:  -1.9902\n",
      "Total loss:  -0.8754 | PDE Loss:  -1.5093 | Function Loss:  -1.9902\n",
      "Total loss:  -0.8756 | PDE Loss:  -1.5094 | Function Loss:  -1.9905\n",
      "Total loss:  -0.8757 | PDE Loss:  -1.5097 | Function Loss:  -1.9905\n",
      "Total loss:  -0.8759 | PDE Loss:  -1.5097 | Function Loss:  -1.9908\n",
      "Total loss:  -0.8761 | PDE Loss:  -1.5098 | Function Loss:  -1.991\n",
      "Total loss:  -0.8763 | PDE Loss:  -1.5097 | Function Loss:  -1.9912\n",
      "Total loss:  -0.8764 | PDE Loss:  -1.5097 | Function Loss:  -1.9914\n",
      "Total loss:  -0.8766 | PDE Loss:  -1.5096 | Function Loss:  -1.9917\n",
      "Total loss:  -0.8768 | PDE Loss:  -1.5095 | Function Loss:  -1.992\n",
      "Total loss:  -0.8768 | PDE Loss:  -1.5083 | Function Loss:  -1.9924\n",
      "Total loss:  -0.8771 | PDE Loss:  -1.5089 | Function Loss:  -1.9925\n",
      "Total loss:  -0.8773 | PDE Loss:  -1.5097 | Function Loss:  -1.9926\n",
      "Total loss:  -0.8774 | PDE Loss:  -1.5102 | Function Loss:  -1.9926\n",
      "Total loss:  -0.8776 | PDE Loss:  -1.5113 | Function Loss:  -1.9924\n",
      "Total loss:  -0.8777 | PDE Loss:  -1.5114 | Function Loss:  -1.9925\n",
      "Total loss:  -0.8777 | PDE Loss:  -1.5117 | Function Loss:  -1.9925\n",
      "Total loss:  -0.8778 | PDE Loss:  -1.512 | Function Loss:  -1.9926\n",
      "Total loss:  -0.878 | PDE Loss:  -1.5129 | Function Loss:  -1.9925\n",
      "Total loss:  -0.8781 | PDE Loss:  -1.5131 | Function Loss:  -1.9926\n",
      "Total loss:  -0.8782 | PDE Loss:  -1.5133 | Function Loss:  -1.9927\n",
      "Total loss:  -0.8784 | PDE Loss:  -1.5134 | Function Loss:  -1.9929\n",
      "Total loss:  -0.8787 | PDE Loss:  -1.5136 | Function Loss:  -1.9932\n",
      "Total loss:  -0.8789 | PDE Loss:  -1.5136 | Function Loss:  -1.9935\n",
      "Total loss:  -0.8791 | PDE Loss:  -1.5136 | Function Loss:  -1.9938\n",
      "Total loss:  -0.8793 | PDE Loss:  -1.5135 | Function Loss:  -1.994\n",
      "Total loss:  -0.8795 | PDE Loss:  -1.5137 | Function Loss:  -1.9942\n",
      "Total loss:  -0.8797 | PDE Loss:  -1.5136 | Function Loss:  -1.9945\n",
      "Total loss:  -0.8798 | PDE Loss:  -1.5138 | Function Loss:  -1.9946\n",
      "Total loss:  -0.8799 | PDE Loss:  -1.5139 | Function Loss:  -1.9946\n",
      "Total loss:  -0.8799 | PDE Loss:  -1.5142 | Function Loss:  -1.9946\n",
      "Total loss:  -0.88 | PDE Loss:  -1.5143 | Function Loss:  -1.9947\n",
      "Total loss:  -0.88 | PDE Loss:  -1.5153 | Function Loss:  -1.9944\n",
      "Total loss:  -0.8801 | PDE Loss:  -1.5156 | Function Loss:  -1.9944\n",
      "Total loss:  -0.8802 | PDE Loss:  -1.5155 | Function Loss:  -1.9945\n",
      "Total loss:  -0.8803 | PDE Loss:  -1.5156 | Function Loss:  -1.9947\n",
      "Total loss:  -0.8804 | PDE Loss:  -1.5158 | Function Loss:  -1.9948\n",
      "Total loss:  -0.8805 | PDE Loss:  -1.516 | Function Loss:  -1.9949\n",
      "Total loss:  -0.8805 | PDE Loss:  -1.5166 | Function Loss:  -1.9947\n",
      "Total loss:  -0.8806 | PDE Loss:  -1.5165 | Function Loss:  -1.9948\n",
      "Total loss:  -0.8807 | PDE Loss:  -1.5167 | Function Loss:  -1.9949\n",
      "Total loss:  -0.8808 | PDE Loss:  -1.517 | Function Loss:  -1.995\n",
      "Total loss:  -0.881 | PDE Loss:  -1.5173 | Function Loss:  -1.995\n",
      "Total loss:  -0.881 | PDE Loss:  -1.5175 | Function Loss:  -1.9951\n",
      "Total loss:  -0.8811 | PDE Loss:  -1.5176 | Function Loss:  -1.9952\n",
      "Total loss:  -0.8812 | PDE Loss:  -1.5177 | Function Loss:  -1.9952\n",
      "Total loss:  -0.8813 | PDE Loss:  -1.5176 | Function Loss:  -1.9953\n",
      "Total loss:  -0.8813 | PDE Loss:  -1.5177 | Function Loss:  -1.9954\n",
      "Total loss:  -0.8814 | PDE Loss:  -1.5175 | Function Loss:  -1.9956\n",
      "Total loss:  -0.8815 | PDE Loss:  -1.5174 | Function Loss:  -1.9957\n",
      "Total loss:  -0.8816 | PDE Loss:  -1.5174 | Function Loss:  -1.9958\n",
      "Total loss:  -0.8817 | PDE Loss:  -1.5173 | Function Loss:  -1.996\n",
      "Total loss:  -0.8818 | PDE Loss:  -1.5174 | Function Loss:  -1.9961\n",
      "Total loss:  -0.8819 | PDE Loss:  -1.5173 | Function Loss:  -1.9963\n",
      "Total loss:  -0.882 | PDE Loss:  -1.5172 | Function Loss:  -1.9964\n",
      "Total loss:  -0.8821 | PDE Loss:  -1.517 | Function Loss:  -1.9966\n",
      "Total loss:  -0.8822 | PDE Loss:  -1.5168 | Function Loss:  -1.9969\n",
      "Total loss:  -0.8823 | PDE Loss:  -1.5166 | Function Loss:  -1.997\n",
      "Total loss:  -0.8824 | PDE Loss:  -1.5166 | Function Loss:  -1.9972\n",
      "Total loss:  -0.8825 | PDE Loss:  -1.5165 | Function Loss:  -1.9973\n",
      "Total loss:  -0.8826 | PDE Loss:  -1.5166 | Function Loss:  -1.9973\n",
      "Total loss:  -0.8826 | PDE Loss:  -1.5166 | Function Loss:  -1.9974\n",
      "Total loss:  -0.8827 | PDE Loss:  -1.5168 | Function Loss:  -1.9975\n",
      "Total loss:  -0.8828 | PDE Loss:  -1.5169 | Function Loss:  -1.9976\n",
      "Total loss:  -0.883 | PDE Loss:  -1.5174 | Function Loss:  -1.9976\n",
      "Total loss:  -0.8831 | PDE Loss:  -1.5176 | Function Loss:  -1.9977\n",
      "Total loss:  -0.8832 | PDE Loss:  -1.5177 | Function Loss:  -1.9979\n",
      "Total loss:  -0.8834 | PDE Loss:  -1.5178 | Function Loss:  -1.998\n",
      "Total loss:  -0.8835 | PDE Loss:  -1.5178 | Function Loss:  -1.9982\n",
      "Total loss:  -0.8836 | PDE Loss:  -1.5178 | Function Loss:  -1.9983\n",
      "Total loss:  -0.8837 | PDE Loss:  -1.5178 | Function Loss:  -1.9985\n",
      "Total loss:  -0.8838 | PDE Loss:  -1.5177 | Function Loss:  -1.9987\n",
      "Total loss:  -0.8839 | PDE Loss:  -1.5175 | Function Loss:  -1.9989\n",
      "Total loss:  -0.8841 | PDE Loss:  -1.5173 | Function Loss:  -1.9991\n",
      "Total loss:  -0.8842 | PDE Loss:  -1.5169 | Function Loss:  -1.9994\n",
      "Total loss:  -0.8843 | PDE Loss:  -1.5169 | Function Loss:  -1.9995\n",
      "Total loss:  -0.8844 | PDE Loss:  -1.5172 | Function Loss:  -1.9996\n",
      "Total loss:  -0.8845 | PDE Loss:  -1.5172 | Function Loss:  -1.9998\n",
      "Total loss:  -0.8846 | PDE Loss:  -1.5173 | Function Loss:  -1.9998\n",
      "Total loss:  -0.8847 | PDE Loss:  -1.5172 | Function Loss:  -1.9999\n",
      "Total loss:  -0.8847 | PDE Loss:  -1.5171 | Function Loss:  -2.0\n",
      "Total loss:  -0.8848 | PDE Loss:  -1.5169 | Function Loss:  -2.0002\n",
      "Total loss:  -0.8849 | PDE Loss:  -1.5167 | Function Loss:  -2.0003\n",
      "Total loss:  -0.8849 | PDE Loss:  -1.5164 | Function Loss:  -2.0005\n",
      "Total loss:  -0.885 | PDE Loss:  -1.5163 | Function Loss:  -2.0006\n",
      "Total loss:  -0.8851 | PDE Loss:  -1.5161 | Function Loss:  -2.0008\n",
      "Total loss:  -0.8851 | PDE Loss:  -1.5161 | Function Loss:  -2.0009\n",
      "Total loss:  -0.8852 | PDE Loss:  -1.5162 | Function Loss:  -2.0009\n",
      "Total loss:  -0.8853 | PDE Loss:  -1.5162 | Function Loss:  -2.001\n",
      "Total loss:  -0.8853 | PDE Loss:  -1.5163 | Function Loss:  -2.0011\n",
      "Total loss:  -0.8854 | PDE Loss:  -1.5164 | Function Loss:  -2.0011\n",
      "Total loss:  -0.8855 | PDE Loss:  -1.5166 | Function Loss:  -2.0012\n",
      "Total loss:  -0.8857 | PDE Loss:  -1.5168 | Function Loss:  -2.0013\n",
      "Total loss:  -0.8858 | PDE Loss:  -1.517 | Function Loss:  -2.0014\n",
      "Total loss:  -0.8858 | PDE Loss:  -1.5174 | Function Loss:  -2.0013\n",
      "Total loss:  -0.8859 | PDE Loss:  -1.5174 | Function Loss:  -2.0014\n",
      "Total loss:  -0.886 | PDE Loss:  -1.5176 | Function Loss:  -2.0015\n",
      "Total loss:  -0.8861 | PDE Loss:  -1.5175 | Function Loss:  -2.0017\n",
      "Total loss:  -0.8862 | PDE Loss:  -1.5174 | Function Loss:  -2.0019\n",
      "Total loss:  -0.8863 | PDE Loss:  -1.5175 | Function Loss:  -2.002\n",
      "Total loss:  -0.8865 | PDE Loss:  -1.5175 | Function Loss:  -2.0022\n",
      "Total loss:  -0.8866 | PDE Loss:  -1.5179 | Function Loss:  -2.0022\n",
      "Total loss:  -0.8867 | PDE Loss:  -1.5181 | Function Loss:  -2.0023\n",
      "Total loss:  -0.8868 | PDE Loss:  -1.5185 | Function Loss:  -2.0024\n",
      "Total loss:  -0.8869 | PDE Loss:  -1.5193 | Function Loss:  -2.0022\n",
      "Total loss:  -0.8871 | PDE Loss:  -1.5202 | Function Loss:  -2.0021\n",
      "Total loss:  -0.8872 | PDE Loss:  -1.5202 | Function Loss:  -2.0022\n",
      "Total loss:  -0.8873 | PDE Loss:  -1.5201 | Function Loss:  -2.0024\n",
      "Total loss:  -0.8874 | PDE Loss:  -1.5198 | Function Loss:  -2.0027\n",
      "Total loss:  -0.8875 | PDE Loss:  -1.5195 | Function Loss:  -2.0029\n",
      "Total loss:  -0.8875 | PDE Loss:  -1.519 | Function Loss:  -2.0031\n",
      "Total loss:  -0.8877 | PDE Loss:  -1.5189 | Function Loss:  -2.0033\n",
      "Total loss:  -0.8878 | PDE Loss:  -1.5184 | Function Loss:  -2.0036\n",
      "Total loss:  -0.8879 | PDE Loss:  -1.5181 | Function Loss:  -2.0038\n",
      "Total loss:  -0.888 | PDE Loss:  -1.5178 | Function Loss:  -2.004\n",
      "Total loss:  -0.888 | PDE Loss:  -1.518 | Function Loss:  -2.0041\n",
      "Total loss:  -0.8881 | PDE Loss:  -1.5181 | Function Loss:  -2.0041\n",
      "Total loss:  -0.8882 | PDE Loss:  -1.5184 | Function Loss:  -2.0041\n",
      "Total loss:  -0.8883 | PDE Loss:  -1.5185 | Function Loss:  -2.0042\n",
      "Total loss:  -0.8883 | PDE Loss:  -1.5188 | Function Loss:  -2.0042\n",
      "Total loss:  -0.8884 | PDE Loss:  -1.5192 | Function Loss:  -2.0042\n",
      "Total loss:  -0.8885 | PDE Loss:  -1.5192 | Function Loss:  -2.0042\n",
      "Total loss:  -0.8885 | PDE Loss:  -1.5194 | Function Loss:  -2.0042\n",
      "Total loss:  -0.8886 | PDE Loss:  -1.5198 | Function Loss:  -2.0042\n",
      "Total loss:  -0.8887 | PDE Loss:  -1.5201 | Function Loss:  -2.0042\n",
      "Total loss:  -0.8887 | PDE Loss:  -1.5204 | Function Loss:  -2.0042\n",
      "Total loss:  -0.8888 | PDE Loss:  -1.5207 | Function Loss:  -2.0042\n",
      "Total loss:  -0.8889 | PDE Loss:  -1.5211 | Function Loss:  -2.0042\n",
      "Total loss:  -0.889 | PDE Loss:  -1.5215 | Function Loss:  -2.0043\n",
      "Total loss:  -0.8891 | PDE Loss:  -1.5219 | Function Loss:  -2.0043\n",
      "Total loss:  -0.8892 | PDE Loss:  -1.5222 | Function Loss:  -2.0043\n",
      "Total loss:  -0.8893 | PDE Loss:  -1.5224 | Function Loss:  -2.0044\n",
      "Total loss:  -0.8895 | PDE Loss:  -1.5226 | Function Loss:  -2.0045\n",
      "Total loss:  -0.8896 | PDE Loss:  -1.5226 | Function Loss:  -2.0047\n",
      "Total loss:  -0.8897 | PDE Loss:  -1.5224 | Function Loss:  -2.0048\n",
      "Total loss:  -0.8897 | PDE Loss:  -1.5223 | Function Loss:  -2.005\n",
      "Total loss:  -0.8898 | PDE Loss:  -1.5221 | Function Loss:  -2.0051\n",
      "Total loss:  -0.8899 | PDE Loss:  -1.5218 | Function Loss:  -2.0053\n",
      "Total loss:  -0.8899 | PDE Loss:  -1.5216 | Function Loss:  -2.0055\n",
      "Total loss:  -0.8901 | PDE Loss:  -1.521 | Function Loss:  -2.0058\n",
      "Total loss:  -0.89 | PDE Loss:  -1.5202 | Function Loss:  -2.0059\n",
      "Total loss:  -0.8901 | PDE Loss:  -1.5209 | Function Loss:  -2.0059\n",
      "Total loss:  -0.8902 | PDE Loss:  -1.5207 | Function Loss:  -2.006\n",
      "Total loss:  -0.8903 | PDE Loss:  -1.5205 | Function Loss:  -2.0062\n",
      "Total loss:  -0.8903 | PDE Loss:  -1.5206 | Function Loss:  -2.0063\n",
      "Total loss:  -0.8904 | PDE Loss:  -1.5205 | Function Loss:  -2.0064\n",
      "Total loss:  -0.8905 | PDE Loss:  -1.5207 | Function Loss:  -2.0064\n",
      "Total loss:  -0.8905 | PDE Loss:  -1.5207 | Function Loss:  -2.0065\n",
      "Total loss:  -0.8906 | PDE Loss:  -1.5208 | Function Loss:  -2.0065\n",
      "Total loss:  -0.8907 | PDE Loss:  -1.5208 | Function Loss:  -2.0067\n",
      "Total loss:  -0.8909 | PDE Loss:  -1.5207 | Function Loss:  -2.0069\n",
      "Total loss:  -0.891 | PDE Loss:  -1.5208 | Function Loss:  -2.007\n",
      "Total loss:  -0.891 | PDE Loss:  -1.5209 | Function Loss:  -2.0071\n",
      "Total loss:  -0.8911 | PDE Loss:  -1.521 | Function Loss:  -2.0072\n",
      "Total loss:  -0.8912 | PDE Loss:  -1.5211 | Function Loss:  -2.0072\n",
      "Total loss:  -0.8912 | PDE Loss:  -1.5212 | Function Loss:  -2.0073\n",
      "Total loss:  -0.8913 | PDE Loss:  -1.5213 | Function Loss:  -2.0073\n",
      "Total loss:  -0.8914 | PDE Loss:  -1.5218 | Function Loss:  -2.0072\n",
      "Total loss:  -0.8915 | PDE Loss:  -1.5218 | Function Loss:  -2.0074\n",
      "Total loss:  -0.8915 | PDE Loss:  -1.5217 | Function Loss:  -2.0075\n",
      "Total loss:  -0.8916 | PDE Loss:  -1.5218 | Function Loss:  -2.0076\n",
      "Total loss:  -0.8918 | PDE Loss:  -1.5217 | Function Loss:  -2.0078\n",
      "Total loss:  -0.8919 | PDE Loss:  -1.5217 | Function Loss:  -2.0079\n",
      "Total loss:  -0.892 | PDE Loss:  -1.5217 | Function Loss:  -2.0081\n",
      "Total loss:  -0.8921 | PDE Loss:  -1.5215 | Function Loss:  -2.0082\n",
      "Total loss:  -0.8922 | PDE Loss:  -1.5214 | Function Loss:  -2.0084\n",
      "Total loss:  -0.8923 | PDE Loss:  -1.5213 | Function Loss:  -2.0086\n",
      "Total loss:  -0.8925 | PDE Loss:  -1.521 | Function Loss:  -2.0089\n",
      "Total loss:  -0.8926 | PDE Loss:  -1.521 | Function Loss:  -2.0091\n",
      "Total loss:  -0.8927 | PDE Loss:  -1.5206 | Function Loss:  -2.0093\n",
      "Total loss:  -0.8927 | PDE Loss:  -1.5206 | Function Loss:  -2.0094\n",
      "Total loss:  -0.8928 | PDE Loss:  -1.5207 | Function Loss:  -2.0095\n",
      "Total loss:  -0.8929 | PDE Loss:  -1.5206 | Function Loss:  -2.0096\n",
      "Total loss:  -0.8929 | PDE Loss:  -1.5205 | Function Loss:  -2.0097\n",
      "Total loss:  -0.893 | PDE Loss:  -1.5203 | Function Loss:  -2.0098\n",
      "Total loss:  -0.893 | PDE Loss:  -1.52 | Function Loss:  -2.01\n",
      "Total loss:  -0.8931 | PDE Loss:  -1.5198 | Function Loss:  -2.0102\n",
      "Total loss:  -0.8923 | PDE Loss:  -1.5126 | Function Loss:  -2.0113\n",
      "Total loss:  -0.8932 | PDE Loss:  -1.5189 | Function Loss:  -2.0105\n",
      "Total loss:  -0.8932 | PDE Loss:  -1.5188 | Function Loss:  -2.0106\n",
      "Total loss:  -0.8933 | PDE Loss:  -1.5189 | Function Loss:  -2.0107\n",
      "Total loss:  -0.8934 | PDE Loss:  -1.5189 | Function Loss:  -2.0108\n",
      "Total loss:  -0.8935 | PDE Loss:  -1.519 | Function Loss:  -2.0109\n",
      "Total loss:  -0.8936 | PDE Loss:  -1.5191 | Function Loss:  -2.011\n",
      "Total loss:  -0.8936 | PDE Loss:  -1.5193 | Function Loss:  -2.011\n",
      "Total loss:  -0.8937 | PDE Loss:  -1.5191 | Function Loss:  -2.0111\n",
      "Total loss:  -0.8938 | PDE Loss:  -1.5194 | Function Loss:  -2.0111\n",
      "Total loss:  -0.8938 | PDE Loss:  -1.5196 | Function Loss:  -2.0112\n",
      "Total loss:  -0.8939 | PDE Loss:  -1.5197 | Function Loss:  -2.0112\n",
      "Total loss:  -0.8939 | PDE Loss:  -1.5196 | Function Loss:  -2.0113\n",
      "Total loss:  -0.894 | PDE Loss:  -1.5195 | Function Loss:  -2.0114\n",
      "Total loss:  -0.894 | PDE Loss:  -1.5193 | Function Loss:  -2.0115\n",
      "Total loss:  -0.8941 | PDE Loss:  -1.5191 | Function Loss:  -2.0116\n",
      "Total loss:  -0.8941 | PDE Loss:  -1.5188 | Function Loss:  -2.0118\n",
      "Total loss:  -0.8942 | PDE Loss:  -1.5185 | Function Loss:  -2.012\n",
      "Total loss:  -0.8943 | PDE Loss:  -1.5183 | Function Loss:  -2.0121\n",
      "Total loss:  -0.8943 | PDE Loss:  -1.518 | Function Loss:  -2.0123\n",
      "Total loss:  -0.8944 | PDE Loss:  -1.518 | Function Loss:  -2.0124\n",
      "Total loss:  -0.8944 | PDE Loss:  -1.5181 | Function Loss:  -2.0124\n",
      "Total loss:  -0.8945 | PDE Loss:  -1.5183 | Function Loss:  -2.0125\n",
      "Total loss:  -0.8946 | PDE Loss:  -1.5181 | Function Loss:  -2.0126\n",
      "Total loss:  -0.8947 | PDE Loss:  -1.5184 | Function Loss:  -2.0126\n",
      "Total loss:  -0.8948 | PDE Loss:  -1.5187 | Function Loss:  -2.0127\n",
      "Total loss:  -0.8949 | PDE Loss:  -1.5187 | Function Loss:  -2.0128\n",
      "Total loss:  -0.895 | PDE Loss:  -1.5192 | Function Loss:  -2.0128\n",
      "Total loss:  -0.895 | PDE Loss:  -1.5193 | Function Loss:  -2.0128\n",
      "Total loss:  -0.8952 | PDE Loss:  -1.5194 | Function Loss:  -2.0129\n",
      "Total loss:  -0.8953 | PDE Loss:  -1.5196 | Function Loss:  -2.013\n",
      "Total loss:  -0.8954 | PDE Loss:  -1.5195 | Function Loss:  -2.0132\n",
      "Total loss:  -0.8955 | PDE Loss:  -1.5195 | Function Loss:  -2.0134\n",
      "Total loss:  -0.8957 | PDE Loss:  -1.5196 | Function Loss:  -2.0136\n",
      "Total loss:  -0.8958 | PDE Loss:  -1.5198 | Function Loss:  -2.0137\n",
      "Total loss:  -0.896 | PDE Loss:  -1.5197 | Function Loss:  -2.0139\n",
      "Total loss:  -0.8961 | PDE Loss:  -1.5197 | Function Loss:  -2.0141\n",
      "Total loss:  -0.8962 | PDE Loss:  -1.5195 | Function Loss:  -2.0143\n",
      "Total loss:  -0.8964 | PDE Loss:  -1.5194 | Function Loss:  -2.0145\n",
      "Total loss:  -0.8965 | PDE Loss:  -1.519 | Function Loss:  -2.0148\n",
      "Total loss:  -0.8966 | PDE Loss:  -1.5187 | Function Loss:  -2.0151\n",
      "Total loss:  -0.8967 | PDE Loss:  -1.5181 | Function Loss:  -2.0154\n",
      "Total loss:  -0.8968 | PDE Loss:  -1.5175 | Function Loss:  -2.0157\n",
      "Total loss:  -0.8968 | PDE Loss:  -1.5172 | Function Loss:  -2.0158\n",
      "Total loss:  -0.8969 | PDE Loss:  -1.517 | Function Loss:  -2.016\n",
      "Total loss:  -0.897 | PDE Loss:  -1.5169 | Function Loss:  -2.0161\n",
      "Total loss:  -0.8971 | PDE Loss:  -1.5172 | Function Loss:  -2.0161\n",
      "Total loss:  -0.8972 | PDE Loss:  -1.5166 | Function Loss:  -2.0165\n",
      "Total loss:  -0.8973 | PDE Loss:  -1.517 | Function Loss:  -2.0165\n",
      "Total loss:  -0.8974 | PDE Loss:  -1.5174 | Function Loss:  -2.0165\n",
      "Total loss:  -0.8975 | PDE Loss:  -1.5179 | Function Loss:  -2.0164\n",
      "Total loss:  -0.8976 | PDE Loss:  -1.5183 | Function Loss:  -2.0164\n",
      "Total loss:  -0.8976 | PDE Loss:  -1.5187 | Function Loss:  -2.0164\n",
      "Total loss:  -0.8978 | PDE Loss:  -1.5192 | Function Loss:  -2.0165\n",
      "Total loss:  -0.8979 | PDE Loss:  -1.5194 | Function Loss:  -2.0165\n",
      "Total loss:  -0.898 | PDE Loss:  -1.5195 | Function Loss:  -2.0167\n",
      "Total loss:  -0.8981 | PDE Loss:  -1.5194 | Function Loss:  -2.0168\n",
      "Total loss:  -0.8982 | PDE Loss:  -1.5192 | Function Loss:  -2.017\n",
      "Total loss:  -0.8983 | PDE Loss:  -1.5189 | Function Loss:  -2.0172\n",
      "Total loss:  -0.8984 | PDE Loss:  -1.5185 | Function Loss:  -2.0175\n",
      "Total loss:  -0.8985 | PDE Loss:  -1.5182 | Function Loss:  -2.0177\n",
      "Total loss:  -0.8986 | PDE Loss:  -1.5181 | Function Loss:  -2.0179\n",
      "Total loss:  -0.8987 | PDE Loss:  -1.5185 | Function Loss:  -2.0179\n",
      "Total loss:  -0.8989 | PDE Loss:  -1.5184 | Function Loss:  -2.0182\n",
      "Total loss:  -0.899 | PDE Loss:  -1.519 | Function Loss:  -2.0181\n",
      "Total loss:  -0.8991 | PDE Loss:  -1.5194 | Function Loss:  -2.0181\n",
      "Total loss:  -0.8992 | PDE Loss:  -1.5196 | Function Loss:  -2.0182\n",
      "Total loss:  -0.8994 | PDE Loss:  -1.5198 | Function Loss:  -2.0184\n",
      "Total loss:  -0.8996 | PDE Loss:  -1.5197 | Function Loss:  -2.0187\n",
      "Total loss:  -0.8998 | PDE Loss:  -1.5195 | Function Loss:  -2.019\n",
      "Total loss:  -0.8999 | PDE Loss:  -1.5188 | Function Loss:  -2.0194\n",
      "Total loss:  -0.9 | PDE Loss:  -1.5184 | Function Loss:  -2.0196\n",
      "Total loss:  -0.9001 | PDE Loss:  -1.518 | Function Loss:  -2.0199\n",
      "Total loss:  -0.9002 | PDE Loss:  -1.5177 | Function Loss:  -2.0201\n",
      "Total loss:  -0.9003 | PDE Loss:  -1.5174 | Function Loss:  -2.0203\n",
      "Total loss:  -0.9003 | PDE Loss:  -1.5174 | Function Loss:  -2.0204\n",
      "Total loss:  -0.9004 | PDE Loss:  -1.5174 | Function Loss:  -2.0205\n",
      "Total loss:  -0.9005 | PDE Loss:  -1.5176 | Function Loss:  -2.0205\n",
      "Total loss:  -0.9005 | PDE Loss:  -1.518 | Function Loss:  -2.0204\n",
      "Total loss:  -0.9006 | PDE Loss:  -1.5183 | Function Loss:  -2.0205\n",
      "Total loss:  -0.9007 | PDE Loss:  -1.5188 | Function Loss:  -2.0205\n",
      "Total loss:  -0.9008 | PDE Loss:  -1.5194 | Function Loss:  -2.0204\n",
      "Total loss:  -0.9009 | PDE Loss:  -1.5201 | Function Loss:  -2.0203\n",
      "Total loss:  -0.901 | PDE Loss:  -1.5206 | Function Loss:  -2.0203\n",
      "Total loss:  -0.9011 | PDE Loss:  -1.5211 | Function Loss:  -2.0202\n",
      "Total loss:  -0.9012 | PDE Loss:  -1.5214 | Function Loss:  -2.0203\n",
      "Total loss:  -0.9013 | PDE Loss:  -1.5217 | Function Loss:  -2.0203\n",
      "Total loss:  -0.9015 | PDE Loss:  -1.5219 | Function Loss:  -2.0204\n",
      "Total loss:  -0.9014 | PDE Loss:  -1.5219 | Function Loss:  -2.0204\n",
      "Total loss:  -0.9015 | PDE Loss:  -1.5221 | Function Loss:  -2.0205\n",
      "Total loss:  -0.9017 | PDE Loss:  -1.5223 | Function Loss:  -2.0206\n",
      "Total loss:  -0.9018 | PDE Loss:  -1.522 | Function Loss:  -2.0208\n",
      "Total loss:  -0.9018 | PDE Loss:  -1.522 | Function Loss:  -2.0209\n",
      "Total loss:  -0.902 | PDE Loss:  -1.5218 | Function Loss:  -2.0212\n",
      "Total loss:  -0.902 | PDE Loss:  -1.5215 | Function Loss:  -2.0213\n",
      "Total loss:  -0.9021 | PDE Loss:  -1.5213 | Function Loss:  -2.0215\n",
      "Total loss:  -0.9021 | PDE Loss:  -1.521 | Function Loss:  -2.0216\n",
      "Total loss:  -0.9022 | PDE Loss:  -1.5207 | Function Loss:  -2.0218\n",
      "Total loss:  -0.9023 | PDE Loss:  -1.5203 | Function Loss:  -2.022\n",
      "Total loss:  -0.9023 | PDE Loss:  -1.5197 | Function Loss:  -2.0223\n",
      "Total loss:  -0.9024 | PDE Loss:  -1.5191 | Function Loss:  -2.0226\n",
      "Total loss:  -0.9025 | PDE Loss:  -1.5189 | Function Loss:  -2.0227\n",
      "Total loss:  -0.9025 | PDE Loss:  -1.5187 | Function Loss:  -2.0229\n",
      "Total loss:  -0.9026 | PDE Loss:  -1.5186 | Function Loss:  -2.023\n",
      "Total loss:  -0.9026 | PDE Loss:  -1.519 | Function Loss:  -2.0229\n",
      "Total loss:  -0.9027 | PDE Loss:  -1.5189 | Function Loss:  -2.023\n",
      "Total loss:  -0.9028 | PDE Loss:  -1.5189 | Function Loss:  -2.0231\n",
      "Total loss:  -0.9029 | PDE Loss:  -1.5189 | Function Loss:  -2.0232\n",
      "Total loss:  -0.9024 | PDE Loss:  -1.5189 | Function Loss:  -2.0226\n",
      "Total loss:  -0.9029 | PDE Loss:  -1.5191 | Function Loss:  -2.0232\n",
      "Total loss:  -0.9029 | PDE Loss:  -1.5192 | Function Loss:  -2.0232\n",
      "Total loss:  -0.903 | PDE Loss:  -1.5193 | Function Loss:  -2.0233\n",
      "Total loss:  -0.903 | PDE Loss:  -1.5196 | Function Loss:  -2.0232\n",
      "Total loss:  -0.9029 | PDE Loss:  -1.5188 | Function Loss:  -2.0234\n",
      "Total loss:  -0.9031 | PDE Loss:  -1.5195 | Function Loss:  -2.0233\n",
      "Total loss:  -0.9031 | PDE Loss:  -1.5195 | Function Loss:  -2.0233\n",
      "Total loss:  -0.9031 | PDE Loss:  -1.5196 | Function Loss:  -2.0234\n",
      "Total loss:  -0.9032 | PDE Loss:  -1.5196 | Function Loss:  -2.0235\n",
      "Total loss:  -0.9033 | PDE Loss:  -1.5195 | Function Loss:  -2.0236\n",
      "Total loss:  -0.9033 | PDE Loss:  -1.5193 | Function Loss:  -2.0237\n",
      "Total loss:  -0.9034 | PDE Loss:  -1.519 | Function Loss:  -2.0239\n",
      "Total loss:  -0.9035 | PDE Loss:  -1.5188 | Function Loss:  -2.0241\n",
      "Total loss:  -0.9035 | PDE Loss:  -1.5183 | Function Loss:  -2.0243\n",
      "Total loss:  -0.9036 | PDE Loss:  -1.5182 | Function Loss:  -2.0244\n",
      "Total loss:  -0.9037 | PDE Loss:  -1.518 | Function Loss:  -2.0246\n",
      "Total loss:  -0.9038 | PDE Loss:  -1.518 | Function Loss:  -2.0247\n",
      "Total loss:  -0.9039 | PDE Loss:  -1.5168 | Function Loss:  -2.0252\n",
      "Total loss:  -0.904 | PDE Loss:  -1.5174 | Function Loss:  -2.0253\n",
      "Total loss:  -0.9041 | PDE Loss:  -1.5177 | Function Loss:  -2.0253\n",
      "Total loss:  -0.9042 | PDE Loss:  -1.518 | Function Loss:  -2.0253\n",
      "Total loss:  -0.9043 | PDE Loss:  -1.5183 | Function Loss:  -2.0253\n",
      "Total loss:  -0.9044 | PDE Loss:  -1.5184 | Function Loss:  -2.0254\n",
      "Total loss:  -0.9044 | PDE Loss:  -1.5181 | Function Loss:  -2.0256\n",
      "Total loss:  -0.9045 | PDE Loss:  -1.518 | Function Loss:  -2.0258\n",
      "Total loss:  -0.9046 | PDE Loss:  -1.5172 | Function Loss:  -2.0261\n",
      "Total loss:  -0.9047 | PDE Loss:  -1.5167 | Function Loss:  -2.0264\n",
      "Total loss:  -0.9048 | PDE Loss:  -1.5158 | Function Loss:  -2.0268\n",
      "Total loss:  -0.9049 | PDE Loss:  -1.5151 | Function Loss:  -2.0271\n",
      "Total loss:  -0.9049 | PDE Loss:  -1.5147 | Function Loss:  -2.0273\n",
      "Total loss:  -0.905 | PDE Loss:  -1.5142 | Function Loss:  -2.0276\n",
      "Total loss:  -0.9051 | PDE Loss:  -1.5142 | Function Loss:  -2.0277\n",
      "Total loss:  -0.9052 | PDE Loss:  -1.514 | Function Loss:  -2.0279\n",
      "Total loss:  -0.9052 | PDE Loss:  -1.5142 | Function Loss:  -2.0279\n",
      "Total loss:  -0.9053 | PDE Loss:  -1.5143 | Function Loss:  -2.0279\n",
      "Total loss:  -0.9053 | PDE Loss:  -1.5145 | Function Loss:  -2.0279\n",
      "Total loss:  -0.9054 | PDE Loss:  -1.5149 | Function Loss:  -2.0279\n",
      "Total loss:  -0.9055 | PDE Loss:  -1.515 | Function Loss:  -2.028\n",
      "Total loss:  -0.9056 | PDE Loss:  -1.5154 | Function Loss:  -2.028\n",
      "Total loss:  -0.9057 | PDE Loss:  -1.5156 | Function Loss:  -2.0281\n",
      "Total loss:  -0.9058 | PDE Loss:  -1.5157 | Function Loss:  -2.0282\n",
      "Total loss:  -0.9059 | PDE Loss:  -1.5156 | Function Loss:  -2.0284\n",
      "Total loss:  -0.906 | PDE Loss:  -1.5153 | Function Loss:  -2.0286\n",
      "Total loss:  -0.9061 | PDE Loss:  -1.515 | Function Loss:  -2.0288\n",
      "Total loss:  -0.9062 | PDE Loss:  -1.5147 | Function Loss:  -2.0291\n",
      "Total loss:  -0.9063 | PDE Loss:  -1.5144 | Function Loss:  -2.0293\n",
      "Total loss:  -0.9064 | PDE Loss:  -1.514 | Function Loss:  -2.0295\n",
      "Total loss:  -0.9063 | PDE Loss:  -1.5141 | Function Loss:  -2.0294\n",
      "Total loss:  -0.9065 | PDE Loss:  -1.5143 | Function Loss:  -2.0296\n",
      "Total loss:  -0.9066 | PDE Loss:  -1.5143 | Function Loss:  -2.0296\n",
      "Total loss:  -0.9067 | PDE Loss:  -1.5146 | Function Loss:  -2.0297\n",
      "Total loss:  -0.9068 | PDE Loss:  -1.5151 | Function Loss:  -2.0297\n",
      "Total loss:  -0.9068 | PDE Loss:  -1.5159 | Function Loss:  -2.0294\n",
      "Total loss:  -0.9069 | PDE Loss:  -1.5156 | Function Loss:  -2.0296\n",
      "Total loss:  -0.907 | PDE Loss:  -1.5162 | Function Loss:  -2.0296\n",
      "Total loss:  -0.907 | PDE Loss:  -1.5174 | Function Loss:  -2.0293\n",
      "Total loss:  -0.9072 | PDE Loss:  -1.5179 | Function Loss:  -2.0293\n",
      "Total loss:  -0.9073 | PDE Loss:  -1.5183 | Function Loss:  -2.0293\n",
      "Total loss:  -0.9074 | PDE Loss:  -1.5186 | Function Loss:  -2.0293\n",
      "Total loss:  -0.9075 | PDE Loss:  -1.5188 | Function Loss:  -2.0294\n",
      "Total loss:  -0.9074 | PDE Loss:  -1.5183 | Function Loss:  -2.0295\n",
      "Total loss:  -0.9075 | PDE Loss:  -1.5187 | Function Loss:  -2.0295\n",
      "Total loss:  -0.9076 | PDE Loss:  -1.5188 | Function Loss:  -2.0295\n",
      "Total loss:  -0.9077 | PDE Loss:  -1.5188 | Function Loss:  -2.0296\n",
      "Total loss:  -0.9078 | PDE Loss:  -1.5187 | Function Loss:  -2.0298\n",
      "Total loss:  -0.9079 | PDE Loss:  -1.5185 | Function Loss:  -2.0301\n",
      "Total loss:  -0.9078 | PDE Loss:  -1.5174 | Function Loss:  -2.0302\n",
      "Total loss:  -0.908 | PDE Loss:  -1.5184 | Function Loss:  -2.0302\n",
      "Total loss:  -0.9082 | PDE Loss:  -1.5181 | Function Loss:  -2.0305\n",
      "Total loss:  -0.9083 | PDE Loss:  -1.5179 | Function Loss:  -2.0307\n",
      "Total loss:  -0.9084 | PDE Loss:  -1.5181 | Function Loss:  -2.0308\n",
      "Total loss:  -0.9085 | PDE Loss:  -1.5181 | Function Loss:  -2.031\n",
      "Total loss:  -0.9086 | PDE Loss:  -1.5184 | Function Loss:  -2.031\n",
      "Total loss:  -0.9086 | PDE Loss:  -1.5185 | Function Loss:  -2.031\n",
      "Total loss:  -0.9087 | PDE Loss:  -1.5188 | Function Loss:  -2.031\n",
      "Total loss:  -0.9088 | PDE Loss:  -1.5191 | Function Loss:  -2.031\n",
      "Total loss:  -0.9088 | PDE Loss:  -1.5195 | Function Loss:  -2.0309\n",
      "Total loss:  -0.9089 | PDE Loss:  -1.5196 | Function Loss:  -2.031\n",
      "Total loss:  -0.909 | PDE Loss:  -1.5197 | Function Loss:  -2.0311\n",
      "Total loss:  -0.9091 | PDE Loss:  -1.5198 | Function Loss:  -2.0312\n",
      "Total loss:  -0.9092 | PDE Loss:  -1.5198 | Function Loss:  -2.0313\n",
      "Total loss:  -0.9092 | PDE Loss:  -1.5197 | Function Loss:  -2.0314\n",
      "Total loss:  -0.9093 | PDE Loss:  -1.5194 | Function Loss:  -2.0316\n",
      "Total loss:  -0.9094 | PDE Loss:  -1.519 | Function Loss:  -2.0319\n",
      "Total loss:  -0.9096 | PDE Loss:  -1.5185 | Function Loss:  -2.0323\n",
      "Total loss:  -0.9097 | PDE Loss:  -1.5178 | Function Loss:  -2.0327\n",
      "Total loss:  -0.9098 | PDE Loss:  -1.5175 | Function Loss:  -2.0329\n",
      "Total loss:  -0.91 | PDE Loss:  -1.5171 | Function Loss:  -2.0332\n",
      "Total loss:  -0.9101 | PDE Loss:  -1.5171 | Function Loss:  -2.0334\n",
      "Total loss:  -0.9102 | PDE Loss:  -1.5171 | Function Loss:  -2.0335\n",
      "Total loss:  -0.9103 | PDE Loss:  -1.5173 | Function Loss:  -2.0335\n",
      "Total loss:  -0.9103 | PDE Loss:  -1.5177 | Function Loss:  -2.0335\n",
      "Total loss:  -0.9104 | PDE Loss:  -1.5179 | Function Loss:  -2.0335\n",
      "Total loss:  -0.9104 | PDE Loss:  -1.5182 | Function Loss:  -2.0335\n",
      "Total loss:  -0.9105 | PDE Loss:  -1.5185 | Function Loss:  -2.0334\n",
      "Total loss:  -0.9105 | PDE Loss:  -1.5189 | Function Loss:  -2.0334\n",
      "Total loss:  -0.9106 | PDE Loss:  -1.519 | Function Loss:  -2.0334\n",
      "Total loss:  -0.9106 | PDE Loss:  -1.5192 | Function Loss:  -2.0334\n",
      "Total loss:  -0.9107 | PDE Loss:  -1.5192 | Function Loss:  -2.0335\n",
      "Total loss:  -0.9107 | PDE Loss:  -1.5193 | Function Loss:  -2.0335\n",
      "Total loss:  -0.9108 | PDE Loss:  -1.519 | Function Loss:  -2.0337\n",
      "Total loss:  -0.9109 | PDE Loss:  -1.5187 | Function Loss:  -2.0339\n",
      "Total loss:  -0.9109 | PDE Loss:  -1.5184 | Function Loss:  -2.034\n",
      "Total loss:  -0.9109 | PDE Loss:  -1.5183 | Function Loss:  -2.0342\n",
      "Total loss:  -0.911 | PDE Loss:  -1.5181 | Function Loss:  -2.0343\n",
      "Total loss:  -0.911 | PDE Loss:  -1.5179 | Function Loss:  -2.0344\n",
      "Total loss:  -0.9111 | PDE Loss:  -1.5178 | Function Loss:  -2.0345\n",
      "Total loss:  -0.9112 | PDE Loss:  -1.5177 | Function Loss:  -2.0346\n",
      "Total loss:  -0.9112 | PDE Loss:  -1.5176 | Function Loss:  -2.0348\n",
      "Total loss:  -0.9114 | PDE Loss:  -1.5175 | Function Loss:  -2.0349\n",
      "Total loss:  -0.9114 | PDE Loss:  -1.5174 | Function Loss:  -2.0351\n",
      "Total loss:  -0.9115 | PDE Loss:  -1.5175 | Function Loss:  -2.0352\n",
      "Total loss:  -0.9116 | PDE Loss:  -1.5175 | Function Loss:  -2.0353\n",
      "Total loss:  -0.9117 | PDE Loss:  -1.5175 | Function Loss:  -2.0353\n",
      "Total loss:  -0.9116 | PDE Loss:  -1.5177 | Function Loss:  -2.0353\n",
      "Total loss:  -0.9117 | PDE Loss:  -1.5177 | Function Loss:  -2.0353\n",
      "Total loss:  -0.9118 | PDE Loss:  -1.5177 | Function Loss:  -2.0354\n",
      "Total loss:  -0.9118 | PDE Loss:  -1.5178 | Function Loss:  -2.0355\n",
      "Total loss:  -0.9119 | PDE Loss:  -1.5178 | Function Loss:  -2.0355\n",
      "Total loss:  -0.9119 | PDE Loss:  -1.5179 | Function Loss:  -2.0355\n",
      "Total loss:  -0.9119 | PDE Loss:  -1.5179 | Function Loss:  -2.0356\n",
      "Total loss:  -0.912 | PDE Loss:  -1.518 | Function Loss:  -2.0356\n",
      "Total loss:  -0.912 | PDE Loss:  -1.5181 | Function Loss:  -2.0356\n",
      "Total loss:  -0.9121 | PDE Loss:  -1.5182 | Function Loss:  -2.0357\n",
      "Total loss:  -0.9122 | PDE Loss:  -1.5182 | Function Loss:  -2.0358\n",
      "Total loss:  -0.9122 | PDE Loss:  -1.5185 | Function Loss:  -2.0358\n",
      "Total loss:  -0.9123 | PDE Loss:  -1.5186 | Function Loss:  -2.0359\n",
      "Total loss:  -0.9124 | PDE Loss:  -1.5188 | Function Loss:  -2.0359\n",
      "Total loss:  -0.9125 | PDE Loss:  -1.5189 | Function Loss:  -2.036\n",
      "Total loss:  -0.9125 | PDE Loss:  -1.5189 | Function Loss:  -2.0361\n",
      "Total loss:  -0.9126 | PDE Loss:  -1.5189 | Function Loss:  -2.0361\n",
      "Total loss:  -0.9126 | PDE Loss:  -1.519 | Function Loss:  -2.0361\n",
      "Total loss:  -0.9127 | PDE Loss:  -1.5189 | Function Loss:  -2.0363\n",
      "Total loss:  -0.9128 | PDE Loss:  -1.5189 | Function Loss:  -2.0364\n",
      "Total loss:  -0.9128 | PDE Loss:  -1.5189 | Function Loss:  -2.0365\n",
      "Total loss:  -0.9129 | PDE Loss:  -1.5189 | Function Loss:  -2.0366\n",
      "Total loss:  -0.913 | PDE Loss:  -1.5189 | Function Loss:  -2.0366\n",
      "Total loss:  -0.913 | PDE Loss:  -1.5189 | Function Loss:  -2.0367\n",
      "Total loss:  -0.9131 | PDE Loss:  -1.5189 | Function Loss:  -2.0368\n",
      "Total loss:  -0.9132 | PDE Loss:  -1.5191 | Function Loss:  -2.0368\n",
      "Total loss:  -0.9132 | PDE Loss:  -1.5191 | Function Loss:  -2.0369\n",
      "Total loss:  -0.9133 | PDE Loss:  -1.5194 | Function Loss:  -2.037\n",
      "Total loss:  -0.9134 | PDE Loss:  -1.52 | Function Loss:  -2.0369\n",
      "Total loss:  -0.9135 | PDE Loss:  -1.5202 | Function Loss:  -2.0369\n",
      "Total loss:  -0.9136 | PDE Loss:  -1.5206 | Function Loss:  -2.0369\n",
      "Total loss:  -0.9137 | PDE Loss:  -1.5209 | Function Loss:  -2.0369\n",
      "Total loss:  -0.9138 | PDE Loss:  -1.521 | Function Loss:  -2.037\n",
      "Total loss:  -0.9139 | PDE Loss:  -1.521 | Function Loss:  -2.0372\n",
      "Total loss:  -0.914 | PDE Loss:  -1.5208 | Function Loss:  -2.0374\n",
      "Total loss:  -0.914 | PDE Loss:  -1.5201 | Function Loss:  -2.0376\n",
      "Total loss:  -0.9141 | PDE Loss:  -1.5206 | Function Loss:  -2.0376\n",
      "Total loss:  -0.9142 | PDE Loss:  -1.5204 | Function Loss:  -2.0378\n",
      "Total loss:  -0.9143 | PDE Loss:  -1.5204 | Function Loss:  -2.0378\n",
      "Total loss:  -0.9143 | PDE Loss:  -1.5204 | Function Loss:  -2.0379\n",
      "Total loss:  -0.9144 | PDE Loss:  -1.5201 | Function Loss:  -2.0381\n",
      "Total loss:  -0.9145 | PDE Loss:  -1.5201 | Function Loss:  -2.0382\n",
      "Total loss:  -0.9146 | PDE Loss:  -1.5201 | Function Loss:  -2.0384\n",
      "Total loss:  -0.9147 | PDE Loss:  -1.5201 | Function Loss:  -2.0386\n",
      "Total loss:  -0.9148 | PDE Loss:  -1.5201 | Function Loss:  -2.0387\n",
      "Total loss:  -0.9149 | PDE Loss:  -1.5197 | Function Loss:  -2.0389\n",
      "Total loss:  -0.9149 | PDE Loss:  -1.5201 | Function Loss:  -2.0389\n",
      "Total loss:  -0.915 | PDE Loss:  -1.5203 | Function Loss:  -2.0389\n",
      "Total loss:  -0.9151 | PDE Loss:  -1.5205 | Function Loss:  -2.0389\n",
      "Total loss:  -0.9152 | PDE Loss:  -1.5206 | Function Loss:  -2.039\n",
      "Total loss:  -0.9153 | PDE Loss:  -1.5205 | Function Loss:  -2.0391\n",
      "Total loss:  -0.9153 | PDE Loss:  -1.5202 | Function Loss:  -2.0393\n",
      "Total loss:  -0.9154 | PDE Loss:  -1.5202 | Function Loss:  -2.0394\n",
      "Total loss:  -0.9155 | PDE Loss:  -1.5201 | Function Loss:  -2.0396\n",
      "Total loss:  -0.9155 | PDE Loss:  -1.5199 | Function Loss:  -2.0397\n",
      "Total loss:  -0.9156 | PDE Loss:  -1.5198 | Function Loss:  -2.0398\n",
      "Total loss:  -0.9156 | PDE Loss:  -1.5196 | Function Loss:  -2.0399\n",
      "Total loss:  -0.9157 | PDE Loss:  -1.5198 | Function Loss:  -2.04\n",
      "Total loss:  -0.9158 | PDE Loss:  -1.5197 | Function Loss:  -2.0401\n",
      "Total loss:  -0.9159 | PDE Loss:  -1.5197 | Function Loss:  -2.0402\n",
      "Total loss:  -0.916 | PDE Loss:  -1.52 | Function Loss:  -2.0404\n",
      "Total loss:  -0.9162 | PDE Loss:  -1.5204 | Function Loss:  -2.0404\n",
      "Total loss:  -0.9164 | PDE Loss:  -1.521 | Function Loss:  -2.0404\n",
      "Total loss:  -0.9165 | PDE Loss:  -1.5216 | Function Loss:  -2.0404\n",
      "Total loss:  -0.9166 | PDE Loss:  -1.5221 | Function Loss:  -2.0404\n",
      "Total loss:  -0.9167 | PDE Loss:  -1.5226 | Function Loss:  -2.0404\n",
      "Total loss:  -0.9168 | PDE Loss:  -1.5232 | Function Loss:  -2.0403\n",
      "Total loss:  -0.917 | PDE Loss:  -1.5238 | Function Loss:  -2.0403\n",
      "Total loss:  -0.917 | PDE Loss:  -1.5242 | Function Loss:  -2.0403\n",
      "Total loss:  -0.9171 | PDE Loss:  -1.5246 | Function Loss:  -2.0403\n",
      "Total loss:  -0.9172 | PDE Loss:  -1.5245 | Function Loss:  -2.0404\n",
      "Total loss:  -0.9172 | PDE Loss:  -1.5247 | Function Loss:  -2.0404\n",
      "Total loss:  -0.9173 | PDE Loss:  -1.5245 | Function Loss:  -2.0405\n",
      "Total loss:  -0.9173 | PDE Loss:  -1.5246 | Function Loss:  -2.0406\n",
      "Total loss:  -0.9174 | PDE Loss:  -1.5246 | Function Loss:  -2.0406\n",
      "Total loss:  -0.9174 | PDE Loss:  -1.5245 | Function Loss:  -2.0407\n",
      "Total loss:  -0.9175 | PDE Loss:  -1.5246 | Function Loss:  -2.0408\n",
      "Total loss:  -0.9176 | PDE Loss:  -1.5246 | Function Loss:  -2.0408\n",
      "Total loss:  -0.9176 | PDE Loss:  -1.5248 | Function Loss:  -2.0408\n",
      "Total loss:  -0.9176 | PDE Loss:  -1.525 | Function Loss:  -2.0408\n",
      "Total loss:  -0.9177 | PDE Loss:  -1.5253 | Function Loss:  -2.0408\n",
      "Total loss:  -0.9177 | PDE Loss:  -1.5255 | Function Loss:  -2.0408\n",
      "Total loss:  -0.9178 | PDE Loss:  -1.5258 | Function Loss:  -2.0408\n",
      "Total loss:  -0.9179 | PDE Loss:  -1.5262 | Function Loss:  -2.0407\n",
      "Total loss:  -0.9179 | PDE Loss:  -1.5264 | Function Loss:  -2.0407\n",
      "Total loss:  -0.9179 | PDE Loss:  -1.5265 | Function Loss:  -2.0407\n",
      "Total loss:  -0.918 | PDE Loss:  -1.5266 | Function Loss:  -2.0407\n",
      "Total loss:  -0.918 | PDE Loss:  -1.5266 | Function Loss:  -2.0408\n",
      "Total loss:  -0.9181 | PDE Loss:  -1.5266 | Function Loss:  -2.0409\n",
      "Total loss:  -0.9181 | PDE Loss:  -1.5265 | Function Loss:  -2.041\n",
      "Total loss:  -0.9182 | PDE Loss:  -1.5263 | Function Loss:  -2.0411\n",
      "Total loss:  -0.9183 | PDE Loss:  -1.5262 | Function Loss:  -2.0413\n",
      "Total loss:  -0.9184 | PDE Loss:  -1.526 | Function Loss:  -2.0414\n",
      "Total loss:  -0.9184 | PDE Loss:  -1.5259 | Function Loss:  -2.0415\n",
      "Total loss:  -0.9184 | PDE Loss:  -1.5258 | Function Loss:  -2.0416\n",
      "Total loss:  -0.9185 | PDE Loss:  -1.5256 | Function Loss:  -2.0417\n",
      "Total loss:  -0.9186 | PDE Loss:  -1.5255 | Function Loss:  -2.0419\n",
      "Total loss:  -0.9186 | PDE Loss:  -1.525 | Function Loss:  -2.0421\n",
      "Total loss:  -0.9187 | PDE Loss:  -1.5246 | Function Loss:  -2.0424\n",
      "Total loss:  -0.9188 | PDE Loss:  -1.5246 | Function Loss:  -2.0425\n",
      "Total loss:  -0.9188 | PDE Loss:  -1.5246 | Function Loss:  -2.0426\n",
      "Total loss:  -0.9189 | PDE Loss:  -1.5246 | Function Loss:  -2.0426\n",
      "Total loss:  -0.919 | PDE Loss:  -1.5248 | Function Loss:  -2.0427\n",
      "Total loss:  -0.919 | PDE Loss:  -1.5249 | Function Loss:  -2.0427\n",
      "Total loss:  -0.9191 | PDE Loss:  -1.5251 | Function Loss:  -2.0427\n",
      "Total loss:  -0.9191 | PDE Loss:  -1.5253 | Function Loss:  -2.0427\n",
      "Total loss:  -0.9192 | PDE Loss:  -1.5256 | Function Loss:  -2.0427\n",
      "Total loss:  -0.9193 | PDE Loss:  -1.5259 | Function Loss:  -2.0427\n",
      "Total loss:  -0.9194 | PDE Loss:  -1.5263 | Function Loss:  -2.0427\n",
      "Total loss:  -0.9194 | PDE Loss:  -1.5266 | Function Loss:  -2.0427\n",
      "Total loss:  -0.9195 | PDE Loss:  -1.5268 | Function Loss:  -2.0427\n",
      "Total loss:  -0.9196 | PDE Loss:  -1.5269 | Function Loss:  -2.0428\n",
      "Total loss:  -0.9197 | PDE Loss:  -1.527 | Function Loss:  -2.0429\n",
      "Total loss:  -0.9198 | PDE Loss:  -1.5269 | Function Loss:  -2.0431\n",
      "Total loss:  -0.9199 | PDE Loss:  -1.5269 | Function Loss:  -2.0432\n",
      "Total loss:  -0.92 | PDE Loss:  -1.5269 | Function Loss:  -2.0434\n",
      "Total loss:  -0.9201 | PDE Loss:  -1.5268 | Function Loss:  -2.0435\n",
      "Total loss:  -0.9201 | PDE Loss:  -1.5267 | Function Loss:  -2.0436\n",
      "Total loss:  -0.9202 | PDE Loss:  -1.5267 | Function Loss:  -2.0437\n",
      "Total loss:  -0.9203 | PDE Loss:  -1.5266 | Function Loss:  -2.0438\n",
      "Total loss:  -0.9203 | PDE Loss:  -1.5268 | Function Loss:  -2.0438\n",
      "Total loss:  -0.9204 | PDE Loss:  -1.5271 | Function Loss:  -2.0438\n",
      "Total loss:  -0.9205 | PDE Loss:  -1.5275 | Function Loss:  -2.0438\n",
      "Total loss:  -0.9206 | PDE Loss:  -1.5285 | Function Loss:  -2.0436\n",
      "Total loss:  -0.9206 | PDE Loss:  -1.5294 | Function Loss:  -2.0433\n",
      "Total loss:  -0.9206 | PDE Loss:  -1.5291 | Function Loss:  -2.0434\n",
      "Total loss:  -0.9207 | PDE Loss:  -1.5295 | Function Loss:  -2.0434\n",
      "Total loss:  -0.9207 | PDE Loss:  -1.5302 | Function Loss:  -2.0432\n",
      "Total loss:  -0.9208 | PDE Loss:  -1.5308 | Function Loss:  -2.0431\n",
      "Total loss:  -0.9209 | PDE Loss:  -1.5314 | Function Loss:  -2.043\n",
      "Total loss:  -0.921 | PDE Loss:  -1.5318 | Function Loss:  -2.043\n",
      "Total loss:  -0.9211 | PDE Loss:  -1.5323 | Function Loss:  -2.043\n",
      "Total loss:  -0.921 | PDE Loss:  -1.5329 | Function Loss:  -2.0427\n",
      "Total loss:  -0.9211 | PDE Loss:  -1.5327 | Function Loss:  -2.0429\n",
      "Total loss:  -0.9212 | PDE Loss:  -1.5329 | Function Loss:  -2.043\n",
      "Total loss:  -0.9213 | PDE Loss:  -1.5327 | Function Loss:  -2.0432\n",
      "Total loss:  -0.9214 | PDE Loss:  -1.5324 | Function Loss:  -2.0434\n",
      "Total loss:  -0.9215 | PDE Loss:  -1.532 | Function Loss:  -2.0436\n",
      "Total loss:  -0.9215 | PDE Loss:  -1.5315 | Function Loss:  -2.0438\n",
      "Total loss:  -0.9216 | PDE Loss:  -1.5311 | Function Loss:  -2.0441\n",
      "Total loss:  -0.9206 | PDE Loss:  -1.5266 | Function Loss:  -2.0442\n",
      "Total loss:  -0.9216 | PDE Loss:  -1.5309 | Function Loss:  -2.0442\n",
      "Total loss:  -0.9217 | PDE Loss:  -1.5305 | Function Loss:  -2.0444\n",
      "Total loss:  -0.9218 | PDE Loss:  -1.5302 | Function Loss:  -2.0446\n",
      "Total loss:  -0.9218 | PDE Loss:  -1.5301 | Function Loss:  -2.0447\n",
      "Total loss:  -0.9219 | PDE Loss:  -1.5297 | Function Loss:  -2.0449\n",
      "Total loss:  -0.922 | PDE Loss:  -1.53 | Function Loss:  -2.0449\n",
      "Total loss:  -0.9221 | PDE Loss:  -1.5302 | Function Loss:  -2.045\n",
      "Total loss:  -0.9222 | PDE Loss:  -1.5311 | Function Loss:  -2.0448\n",
      "Total loss:  -0.9222 | PDE Loss:  -1.5319 | Function Loss:  -2.0446\n",
      "Total loss:  -0.9223 | PDE Loss:  -1.5317 | Function Loss:  -2.0448\n",
      "Total loss:  -0.9224 | PDE Loss:  -1.5317 | Function Loss:  -2.0449\n",
      "Total loss:  -0.9225 | PDE Loss:  -1.5316 | Function Loss:  -2.0451\n",
      "Total loss:  -0.9225 | PDE Loss:  -1.5315 | Function Loss:  -2.0452\n",
      "Total loss:  -0.9226 | PDE Loss:  -1.5314 | Function Loss:  -2.0453\n",
      "Total loss:  -0.9227 | PDE Loss:  -1.5314 | Function Loss:  -2.0455\n",
      "Total loss:  -0.9226 | PDE Loss:  -1.5297 | Function Loss:  -2.0459\n",
      "Total loss:  -0.9227 | PDE Loss:  -1.5308 | Function Loss:  -2.0457\n",
      "Total loss:  -0.9229 | PDE Loss:  -1.5305 | Function Loss:  -2.0459\n",
      "Total loss:  -0.9229 | PDE Loss:  -1.5308 | Function Loss:  -2.0459\n",
      "Total loss:  -0.923 | PDE Loss:  -1.5313 | Function Loss:  -2.0459\n",
      "Total loss:  -0.9231 | PDE Loss:  -1.5317 | Function Loss:  -2.0459\n",
      "Total loss:  -0.9232 | PDE Loss:  -1.532 | Function Loss:  -2.0458\n",
      "Total loss:  -0.9232 | PDE Loss:  -1.5324 | Function Loss:  -2.0458\n",
      "Total loss:  -0.9233 | PDE Loss:  -1.5329 | Function Loss:  -2.0458\n",
      "Total loss:  -0.9227 | PDE Loss:  -1.5303 | Function Loss:  -2.0458\n",
      "Total loss:  -0.9234 | PDE Loss:  -1.5328 | Function Loss:  -2.0459\n",
      "Total loss:  -0.9235 | PDE Loss:  -1.5332 | Function Loss:  -2.0459\n",
      "Total loss:  -0.9236 | PDE Loss:  -1.5333 | Function Loss:  -2.046\n",
      "Total loss:  -0.9237 | PDE Loss:  -1.5333 | Function Loss:  -2.0462\n",
      "Total loss:  -0.9238 | PDE Loss:  -1.5331 | Function Loss:  -2.0463\n",
      "Total loss:  -0.9239 | PDE Loss:  -1.5329 | Function Loss:  -2.0465\n",
      "Total loss:  -0.924 | PDE Loss:  -1.5327 | Function Loss:  -2.0467\n",
      "Total loss:  -0.9241 | PDE Loss:  -1.5324 | Function Loss:  -2.047\n",
      "Total loss:  -0.9242 | PDE Loss:  -1.5322 | Function Loss:  -2.0472\n",
      "Total loss:  -0.9244 | PDE Loss:  -1.5321 | Function Loss:  -2.0475\n",
      "Total loss:  -0.9245 | PDE Loss:  -1.5319 | Function Loss:  -2.0477\n",
      "Total loss:  -0.9247 | PDE Loss:  -1.532 | Function Loss:  -2.0478\n",
      "Total loss:  -0.9248 | PDE Loss:  -1.5325 | Function Loss:  -2.0479\n",
      "Total loss:  -0.9249 | PDE Loss:  -1.5328 | Function Loss:  -2.0479\n",
      "Total loss:  -0.9249 | PDE Loss:  -1.5332 | Function Loss:  -2.0478\n",
      "Total loss:  -0.925 | PDE Loss:  -1.5337 | Function Loss:  -2.0478\n",
      "Total loss:  -0.9251 | PDE Loss:  -1.534 | Function Loss:  -2.0477\n",
      "Total loss:  -0.9251 | PDE Loss:  -1.5345 | Function Loss:  -2.0477\n",
      "Total loss:  -0.9253 | PDE Loss:  -1.5349 | Function Loss:  -2.0477\n",
      "Total loss:  -0.9254 | PDE Loss:  -1.5354 | Function Loss:  -2.0477\n",
      "Total loss:  -0.9255 | PDE Loss:  -1.5355 | Function Loss:  -2.0478\n",
      "Total loss:  -0.9256 | PDE Loss:  -1.536 | Function Loss:  -2.0478\n",
      "Total loss:  -0.9257 | PDE Loss:  -1.5359 | Function Loss:  -2.0479\n",
      "Total loss:  -0.9257 | PDE Loss:  -1.5358 | Function Loss:  -2.0481\n",
      "Total loss:  -0.9258 | PDE Loss:  -1.5358 | Function Loss:  -2.0482\n",
      "Total loss:  -0.9259 | PDE Loss:  -1.5354 | Function Loss:  -2.0484\n",
      "Total loss:  -0.926 | PDE Loss:  -1.5354 | Function Loss:  -2.0486\n",
      "Total loss:  -0.9261 | PDE Loss:  -1.5352 | Function Loss:  -2.0487\n",
      "Total loss:  -0.9262 | PDE Loss:  -1.5351 | Function Loss:  -2.0489\n",
      "Total loss:  -0.9263 | PDE Loss:  -1.5348 | Function Loss:  -2.0491\n",
      "Total loss:  -0.9264 | PDE Loss:  -1.5347 | Function Loss:  -2.0493\n",
      "Total loss:  -0.9265 | PDE Loss:  -1.5346 | Function Loss:  -2.0495\n",
      "Total loss:  -0.9266 | PDE Loss:  -1.5344 | Function Loss:  -2.0497\n",
      "Total loss:  -0.9267 | PDE Loss:  -1.5343 | Function Loss:  -2.0498\n",
      "Total loss:  -0.9268 | PDE Loss:  -1.5343 | Function Loss:  -2.05\n",
      "Total loss:  -0.9269 | PDE Loss:  -1.5336 | Function Loss:  -2.0503\n",
      "Total loss:  -0.927 | PDE Loss:  -1.534 | Function Loss:  -2.0503\n",
      "Total loss:  -0.927 | PDE Loss:  -1.534 | Function Loss:  -2.0503\n",
      "Total loss:  -0.9271 | PDE Loss:  -1.5342 | Function Loss:  -2.0504\n",
      "Total loss:  -0.9272 | PDE Loss:  -1.5343 | Function Loss:  -2.0505\n",
      "Total loss:  -0.9273 | PDE Loss:  -1.5346 | Function Loss:  -2.0505\n",
      "Total loss:  -0.9273 | PDE Loss:  -1.5347 | Function Loss:  -2.0505\n",
      "Total loss:  -0.9274 | PDE Loss:  -1.535 | Function Loss:  -2.0505\n",
      "Total loss:  -0.9275 | PDE Loss:  -1.5354 | Function Loss:  -2.0505\n",
      "Total loss:  -0.9276 | PDE Loss:  -1.536 | Function Loss:  -2.0504\n",
      "Total loss:  -0.9276 | PDE Loss:  -1.5366 | Function Loss:  -2.0503\n",
      "Total loss:  -0.9277 | PDE Loss:  -1.5369 | Function Loss:  -2.0503\n",
      "Total loss:  -0.9278 | PDE Loss:  -1.5372 | Function Loss:  -2.0504\n",
      "Total loss:  -0.9279 | PDE Loss:  -1.5374 | Function Loss:  -2.0504\n",
      "Total loss:  -0.928 | PDE Loss:  -1.5371 | Function Loss:  -2.0506\n",
      "Total loss:  -0.9281 | PDE Loss:  -1.5372 | Function Loss:  -2.0507\n",
      "Total loss:  -0.9281 | PDE Loss:  -1.5369 | Function Loss:  -2.0508\n",
      "Total loss:  -0.9282 | PDE Loss:  -1.5367 | Function Loss:  -2.051\n",
      "Total loss:  -0.9282 | PDE Loss:  -1.5363 | Function Loss:  -2.0512\n",
      "Total loss:  -0.9283 | PDE Loss:  -1.536 | Function Loss:  -2.0513\n",
      "Total loss:  -0.9283 | PDE Loss:  -1.5357 | Function Loss:  -2.0515\n",
      "Total loss:  -0.9284 | PDE Loss:  -1.5355 | Function Loss:  -2.0516\n",
      "Total loss:  -0.9284 | PDE Loss:  -1.5353 | Function Loss:  -2.0518\n",
      "Total loss:  -0.9285 | PDE Loss:  -1.535 | Function Loss:  -2.0519\n",
      "Total loss:  -0.9285 | PDE Loss:  -1.535 | Function Loss:  -2.052\n",
      "Total loss:  -0.9286 | PDE Loss:  -1.5352 | Function Loss:  -2.052\n",
      "Total loss:  -0.9286 | PDE Loss:  -1.5354 | Function Loss:  -2.052\n",
      "Total loss:  -0.9286 | PDE Loss:  -1.5356 | Function Loss:  -2.052\n",
      "Total loss:  -0.9287 | PDE Loss:  -1.5358 | Function Loss:  -2.052\n",
      "Total loss:  -0.9287 | PDE Loss:  -1.5359 | Function Loss:  -2.052\n",
      "Total loss:  -0.9283 | PDE Loss:  -1.5347 | Function Loss:  -2.0518\n",
      "Total loss:  -0.9287 | PDE Loss:  -1.5358 | Function Loss:  -2.052\n",
      "Total loss:  -0.9288 | PDE Loss:  -1.536 | Function Loss:  -2.052\n",
      "Total loss:  -0.9288 | PDE Loss:  -1.5363 | Function Loss:  -2.052\n",
      "Total loss:  -0.9289 | PDE Loss:  -1.5361 | Function Loss:  -2.0522\n",
      "Total loss:  -0.929 | PDE Loss:  -1.5361 | Function Loss:  -2.0522\n",
      "Total loss:  -0.9291 | PDE Loss:  -1.536 | Function Loss:  -2.0524\n",
      "Total loss:  -0.9291 | PDE Loss:  -1.5359 | Function Loss:  -2.0525\n",
      "Total loss:  -0.9292 | PDE Loss:  -1.5359 | Function Loss:  -2.0526\n",
      "Total loss:  -0.9292 | PDE Loss:  -1.5358 | Function Loss:  -2.0527\n",
      "Total loss:  -0.9293 | PDE Loss:  -1.5359 | Function Loss:  -2.0527\n",
      "Total loss:  -0.9293 | PDE Loss:  -1.5359 | Function Loss:  -2.0528\n",
      "Total loss:  -0.9294 | PDE Loss:  -1.536 | Function Loss:  -2.0529\n",
      "Total loss:  -0.9295 | PDE Loss:  -1.5362 | Function Loss:  -2.0529\n",
      "Total loss:  -0.9295 | PDE Loss:  -1.5361 | Function Loss:  -2.0529\n",
      "Total loss:  -0.9296 | PDE Loss:  -1.5362 | Function Loss:  -2.053\n",
      "Total loss:  -0.9296 | PDE Loss:  -1.5363 | Function Loss:  -2.053\n",
      "Total loss:  -0.9297 | PDE Loss:  -1.5364 | Function Loss:  -2.0531\n",
      "Total loss:  -0.9297 | PDE Loss:  -1.5364 | Function Loss:  -2.0532\n",
      "Total loss:  -0.9298 | PDE Loss:  -1.5365 | Function Loss:  -2.0532\n",
      "Total loss:  -0.9298 | PDE Loss:  -1.5365 | Function Loss:  -2.0533\n",
      "Total loss:  -0.9299 | PDE Loss:  -1.5369 | Function Loss:  -2.0532\n",
      "Total loss:  -0.93 | PDE Loss:  -1.5369 | Function Loss:  -2.0533\n",
      "Total loss:  -0.93 | PDE Loss:  -1.5371 | Function Loss:  -2.0532\n",
      "Total loss:  -0.9301 | PDE Loss:  -1.5376 | Function Loss:  -2.0532\n",
      "Total loss:  -0.9301 | PDE Loss:  -1.538 | Function Loss:  -2.0531\n",
      "Total loss:  -0.9301 | PDE Loss:  -1.5384 | Function Loss:  -2.053\n",
      "Total loss:  -0.9302 | PDE Loss:  -1.5388 | Function Loss:  -2.053\n",
      "Total loss:  -0.9302 | PDE Loss:  -1.5391 | Function Loss:  -2.0529\n",
      "Total loss:  -0.9302 | PDE Loss:  -1.5391 | Function Loss:  -2.0528\n",
      "Total loss:  -0.9302 | PDE Loss:  -1.5391 | Function Loss:  -2.0529\n",
      "Total loss:  -0.9303 | PDE Loss:  -1.5393 | Function Loss:  -2.0529\n",
      "Total loss:  -0.9303 | PDE Loss:  -1.5393 | Function Loss:  -2.053\n",
      "Total loss:  -0.9304 | PDE Loss:  -1.5392 | Function Loss:  -2.0531\n",
      "Total loss:  -0.9304 | PDE Loss:  -1.5389 | Function Loss:  -2.0533\n",
      "Total loss:  -0.9305 | PDE Loss:  -1.5385 | Function Loss:  -2.0535\n",
      "Total loss:  -0.9305 | PDE Loss:  -1.5381 | Function Loss:  -2.0536\n",
      "Total loss:  -0.9306 | PDE Loss:  -1.5378 | Function Loss:  -2.0538\n",
      "Total loss:  -0.9306 | PDE Loss:  -1.5374 | Function Loss:  -2.054\n",
      "Total loss:  -0.9307 | PDE Loss:  -1.5372 | Function Loss:  -2.0541\n",
      "Total loss:  -0.9307 | PDE Loss:  -1.537 | Function Loss:  -2.0543\n",
      "Total loss:  -0.9308 | PDE Loss:  -1.5358 | Function Loss:  -2.0548\n",
      "Total loss:  -0.9309 | PDE Loss:  -1.5363 | Function Loss:  -2.0547\n",
      "Total loss:  -0.931 | PDE Loss:  -1.537 | Function Loss:  -2.0546\n",
      "Total loss:  -0.9311 | PDE Loss:  -1.5375 | Function Loss:  -2.0546\n",
      "Total loss:  -0.9312 | PDE Loss:  -1.538 | Function Loss:  -2.0546\n",
      "Total loss:  -0.9313 | PDE Loss:  -1.5381 | Function Loss:  -2.0547\n",
      "Total loss:  -0.9314 | PDE Loss:  -1.5383 | Function Loss:  -2.0548\n",
      "Total loss:  -0.9315 | PDE Loss:  -1.538 | Function Loss:  -2.055\n",
      "Total loss:  -0.9316 | PDE Loss:  -1.5377 | Function Loss:  -2.0552\n",
      "Total loss:  -0.9317 | PDE Loss:  -1.5373 | Function Loss:  -2.0554\n",
      "Total loss:  -0.9318 | PDE Loss:  -1.5371 | Function Loss:  -2.0557\n",
      "Total loss:  -0.9318 | PDE Loss:  -1.5364 | Function Loss:  -2.0559\n",
      "Total loss:  -0.932 | PDE Loss:  -1.5365 | Function Loss:  -2.0561\n",
      "Total loss:  -0.9321 | PDE Loss:  -1.5367 | Function Loss:  -2.0561\n",
      "Total loss:  -0.9322 | PDE Loss:  -1.537 | Function Loss:  -2.0562\n",
      "Total loss:  -0.9322 | PDE Loss:  -1.5374 | Function Loss:  -2.0562\n",
      "Total loss:  -0.9323 | PDE Loss:  -1.5379 | Function Loss:  -2.0561\n",
      "Total loss:  -0.9324 | PDE Loss:  -1.5384 | Function Loss:  -2.0561\n",
      "Total loss:  -0.9325 | PDE Loss:  -1.539 | Function Loss:  -2.056\n",
      "Total loss:  -0.9326 | PDE Loss:  -1.5394 | Function Loss:  -2.0559\n",
      "Total loss:  -0.9326 | PDE Loss:  -1.5396 | Function Loss:  -2.0559\n",
      "Total loss:  -0.9327 | PDE Loss:  -1.5399 | Function Loss:  -2.056\n",
      "Total loss:  -0.9328 | PDE Loss:  -1.5401 | Function Loss:  -2.0561\n",
      "Total loss:  -0.9329 | PDE Loss:  -1.5402 | Function Loss:  -2.0561\n",
      "Total loss:  -0.933 | PDE Loss:  -1.5403 | Function Loss:  -2.0561\n",
      "Total loss:  -0.933 | PDE Loss:  -1.5406 | Function Loss:  -2.0561\n",
      "Total loss:  -0.9326 | PDE Loss:  -1.5398 | Function Loss:  -2.0558\n",
      "Total loss:  -0.933 | PDE Loss:  -1.5407 | Function Loss:  -2.0561\n",
      "Total loss:  -0.9331 | PDE Loss:  -1.5411 | Function Loss:  -2.0561\n",
      "Total loss:  -0.9332 | PDE Loss:  -1.5412 | Function Loss:  -2.0562\n",
      "Total loss:  -0.9333 | PDE Loss:  -1.5416 | Function Loss:  -2.0561\n",
      "Total loss:  -0.9333 | PDE Loss:  -1.542 | Function Loss:  -2.0561\n",
      "Total loss:  -0.9334 | PDE Loss:  -1.5423 | Function Loss:  -2.056\n",
      "Total loss:  -0.9334 | PDE Loss:  -1.5427 | Function Loss:  -2.056\n",
      "Total loss:  -0.9328 | PDE Loss:  -1.5394 | Function Loss:  -2.0562\n",
      "Total loss:  -0.9334 | PDE Loss:  -1.5424 | Function Loss:  -2.0561\n",
      "Total loss:  -0.9335 | PDE Loss:  -1.5428 | Function Loss:  -2.0561\n",
      "Total loss:  -0.9336 | PDE Loss:  -1.543 | Function Loss:  -2.0561\n",
      "Total loss:  -0.9336 | PDE Loss:  -1.543 | Function Loss:  -2.0562\n",
      "Total loss:  -0.9337 | PDE Loss:  -1.543 | Function Loss:  -2.0562\n",
      "Total loss:  -0.9337 | PDE Loss:  -1.5428 | Function Loss:  -2.0563\n",
      "Total loss:  -0.9338 | PDE Loss:  -1.5426 | Function Loss:  -2.0565\n",
      "Total loss:  -0.9338 | PDE Loss:  -1.5424 | Function Loss:  -2.0566\n",
      "Total loss:  -0.9339 | PDE Loss:  -1.5421 | Function Loss:  -2.0568\n",
      "Total loss:  -0.934 | PDE Loss:  -1.5418 | Function Loss:  -2.057\n",
      "Total loss:  -0.934 | PDE Loss:  -1.5417 | Function Loss:  -2.0571\n",
      "Total loss:  -0.9341 | PDE Loss:  -1.5419 | Function Loss:  -2.0572\n",
      "Total loss:  -0.9342 | PDE Loss:  -1.5424 | Function Loss:  -2.0572\n",
      "Total loss:  -0.9343 | PDE Loss:  -1.543 | Function Loss:  -2.0571\n",
      "Total loss:  -0.9344 | PDE Loss:  -1.5437 | Function Loss:  -2.057\n",
      "Total loss:  -0.9345 | PDE Loss:  -1.5454 | Function Loss:  -2.0565\n",
      "Total loss:  -0.9346 | PDE Loss:  -1.5459 | Function Loss:  -2.0565\n",
      "Total loss:  -0.9347 | PDE Loss:  -1.5464 | Function Loss:  -2.0564\n",
      "Total loss:  -0.9347 | PDE Loss:  -1.5468 | Function Loss:  -2.0563\n",
      "Total loss:  -0.9347 | PDE Loss:  -1.5471 | Function Loss:  -2.0563\n",
      "Total loss:  -0.9348 | PDE Loss:  -1.5473 | Function Loss:  -2.0563\n",
      "Total loss:  -0.9348 | PDE Loss:  -1.5475 | Function Loss:  -2.0563\n",
      "Total loss:  -0.9348 | PDE Loss:  -1.5476 | Function Loss:  -2.0563\n",
      "Total loss:  -0.9349 | PDE Loss:  -1.5478 | Function Loss:  -2.0563\n",
      "Total loss:  -0.9348 | PDE Loss:  -1.5473 | Function Loss:  -2.0564\n",
      "Total loss:  -0.9349 | PDE Loss:  -1.5477 | Function Loss:  -2.0564\n",
      "Total loss:  -0.935 | PDE Loss:  -1.5478 | Function Loss:  -2.0564\n",
      "Total loss:  -0.9351 | PDE Loss:  -1.5479 | Function Loss:  -2.0565\n",
      "Total loss:  -0.9352 | PDE Loss:  -1.5478 | Function Loss:  -2.0566\n",
      "Total loss:  -0.9352 | PDE Loss:  -1.5478 | Function Loss:  -2.0567\n",
      "Total loss:  -0.9353 | PDE Loss:  -1.5478 | Function Loss:  -2.0568\n",
      "Total loss:  -0.9353 | PDE Loss:  -1.5478 | Function Loss:  -2.0569\n",
      "Total loss:  -0.9354 | PDE Loss:  -1.5479 | Function Loss:  -2.0569\n",
      "Total loss:  -0.9355 | PDE Loss:  -1.5481 | Function Loss:  -2.057\n",
      "Total loss:  -0.9356 | PDE Loss:  -1.5483 | Function Loss:  -2.0571\n",
      "Total loss:  -0.9357 | PDE Loss:  -1.5485 | Function Loss:  -2.0571\n",
      "Total loss:  -0.9358 | PDE Loss:  -1.5492 | Function Loss:  -2.057\n",
      "Total loss:  -0.9359 | PDE Loss:  -1.5497 | Function Loss:  -2.057\n",
      "Total loss:  -0.936 | PDE Loss:  -1.5503 | Function Loss:  -2.0569\n",
      "Total loss:  -0.9361 | PDE Loss:  -1.5507 | Function Loss:  -2.0569\n",
      "Total loss:  -0.9361 | PDE Loss:  -1.5511 | Function Loss:  -2.0569\n",
      "Total loss:  -0.9362 | PDE Loss:  -1.5514 | Function Loss:  -2.0569\n",
      "Total loss:  -0.9363 | PDE Loss:  -1.5517 | Function Loss:  -2.0569\n",
      "Total loss:  -0.9364 | PDE Loss:  -1.5518 | Function Loss:  -2.0569\n",
      "Total loss:  -0.9364 | PDE Loss:  -1.552 | Function Loss:  -2.0569\n",
      "Total loss:  -0.9365 | PDE Loss:  -1.5521 | Function Loss:  -2.057\n",
      "Total loss:  -0.9365 | PDE Loss:  -1.5524 | Function Loss:  -2.057\n",
      "Total loss:  -0.9366 | PDE Loss:  -1.5526 | Function Loss:  -2.057\n",
      "Total loss:  -0.9367 | PDE Loss:  -1.5529 | Function Loss:  -2.057\n",
      "Total loss:  -0.9367 | PDE Loss:  -1.5533 | Function Loss:  -2.057\n",
      "Total loss:  -0.9368 | PDE Loss:  -1.5536 | Function Loss:  -2.0569\n",
      "Total loss:  -0.9368 | PDE Loss:  -1.5539 | Function Loss:  -2.0569\n",
      "Total loss:  -0.9369 | PDE Loss:  -1.5542 | Function Loss:  -2.0569\n",
      "Total loss:  -0.937 | PDE Loss:  -1.5547 | Function Loss:  -2.0568\n",
      "Total loss:  -0.937 | PDE Loss:  -1.5548 | Function Loss:  -2.0568\n",
      "Total loss:  -0.937 | PDE Loss:  -1.555 | Function Loss:  -2.0568\n",
      "Total loss:  -0.9371 | PDE Loss:  -1.5552 | Function Loss:  -2.0568\n",
      "Total loss:  -0.9371 | PDE Loss:  -1.5553 | Function Loss:  -2.0568\n",
      "Total loss:  -0.9372 | PDE Loss:  -1.5554 | Function Loss:  -2.0569\n",
      "Total loss:  -0.9372 | PDE Loss:  -1.5554 | Function Loss:  -2.0569\n",
      "Total loss:  -0.9372 | PDE Loss:  -1.5555 | Function Loss:  -2.0569\n",
      "Total loss:  -0.9373 | PDE Loss:  -1.5557 | Function Loss:  -2.0569\n",
      "Total loss:  -0.9374 | PDE Loss:  -1.5559 | Function Loss:  -2.0569\n",
      "Total loss:  -0.9374 | PDE Loss:  -1.5562 | Function Loss:  -2.0569\n",
      "Total loss:  -0.9375 | PDE Loss:  -1.5564 | Function Loss:  -2.0569\n",
      "Total loss:  -0.9375 | PDE Loss:  -1.5567 | Function Loss:  -2.0569\n",
      "Total loss:  -0.9376 | PDE Loss:  -1.5569 | Function Loss:  -2.0569\n",
      "Total loss:  -0.9376 | PDE Loss:  -1.5571 | Function Loss:  -2.0569\n",
      "Total loss:  -0.9377 | PDE Loss:  -1.5572 | Function Loss:  -2.0569\n",
      "Total loss:  -0.9377 | PDE Loss:  -1.5572 | Function Loss:  -2.057\n",
      "Total loss:  -0.9378 | PDE Loss:  -1.5572 | Function Loss:  -2.0571\n",
      "Total loss:  -0.9378 | PDE Loss:  -1.557 | Function Loss:  -2.0572\n",
      "Total loss:  -0.9379 | PDE Loss:  -1.5569 | Function Loss:  -2.0573\n",
      "Total loss:  -0.9379 | PDE Loss:  -1.5568 | Function Loss:  -2.0574\n",
      "Total loss:  -0.938 | PDE Loss:  -1.5568 | Function Loss:  -2.0575\n",
      "Total loss:  -0.938 | PDE Loss:  -1.5568 | Function Loss:  -2.0575\n",
      "Total loss:  -0.938 | PDE Loss:  -1.5568 | Function Loss:  -2.0575\n",
      "Total loss:  -0.9381 | PDE Loss:  -1.5569 | Function Loss:  -2.0576\n",
      "Total loss:  -0.9381 | PDE Loss:  -1.557 | Function Loss:  -2.0576\n",
      "Total loss:  -0.9381 | PDE Loss:  -1.5572 | Function Loss:  -2.0576\n",
      "Total loss:  -0.9382 | PDE Loss:  -1.5575 | Function Loss:  -2.0575\n",
      "Total loss:  -0.9382 | PDE Loss:  -1.5578 | Function Loss:  -2.0575\n",
      "Total loss:  -0.9381 | PDE Loss:  -1.5575 | Function Loss:  -2.0574\n",
      "Total loss:  -0.9383 | PDE Loss:  -1.5578 | Function Loss:  -2.0575\n",
      "Total loss:  -0.9383 | PDE Loss:  -1.5581 | Function Loss:  -2.0575\n",
      "Total loss:  -0.9383 | PDE Loss:  -1.5584 | Function Loss:  -2.0574\n",
      "Total loss:  -0.9384 | PDE Loss:  -1.5587 | Function Loss:  -2.0574\n",
      "Total loss:  -0.9384 | PDE Loss:  -1.5589 | Function Loss:  -2.0574\n",
      "Total loss:  -0.9384 | PDE Loss:  -1.5591 | Function Loss:  -2.0574\n",
      "Total loss:  -0.9385 | PDE Loss:  -1.5592 | Function Loss:  -2.0574\n",
      "Total loss:  -0.9385 | PDE Loss:  -1.5594 | Function Loss:  -2.0574\n",
      "Total loss:  -0.9386 | PDE Loss:  -1.5597 | Function Loss:  -2.0574\n",
      "Total loss:  -0.9387 | PDE Loss:  -1.5604 | Function Loss:  -2.0573\n",
      "Total loss:  -0.9388 | PDE Loss:  -1.5606 | Function Loss:  -2.0574\n",
      "Total loss:  -0.9389 | PDE Loss:  -1.5609 | Function Loss:  -2.0574\n",
      "Total loss:  -0.939 | PDE Loss:  -1.561 | Function Loss:  -2.0575\n",
      "Total loss:  -0.9391 | PDE Loss:  -1.5611 | Function Loss:  -2.0576\n",
      "Total loss:  -0.9391 | PDE Loss:  -1.5611 | Function Loss:  -2.0576\n",
      "Total loss:  -0.9392 | PDE Loss:  -1.5612 | Function Loss:  -2.0577\n",
      "Total loss:  -0.9393 | PDE Loss:  -1.561 | Function Loss:  -2.0579\n",
      "Total loss:  -0.9394 | PDE Loss:  -1.5612 | Function Loss:  -2.0579\n",
      "Total loss:  -0.9395 | PDE Loss:  -1.5613 | Function Loss:  -2.058\n",
      "Total loss:  -0.9395 | PDE Loss:  -1.5615 | Function Loss:  -2.0581\n",
      "Total loss:  -0.9396 | PDE Loss:  -1.5615 | Function Loss:  -2.0581\n",
      "Total loss:  -0.9396 | PDE Loss:  -1.5615 | Function Loss:  -2.0582\n",
      "Total loss:  -0.9397 | PDE Loss:  -1.5615 | Function Loss:  -2.0582\n",
      "Total loss:  -0.9398 | PDE Loss:  -1.5615 | Function Loss:  -2.0584\n",
      "Total loss:  -0.9399 | PDE Loss:  -1.5615 | Function Loss:  -2.0586\n",
      "Total loss:  -0.94 | PDE Loss:  -1.5611 | Function Loss:  -2.0588\n",
      "Total loss:  -0.9401 | PDE Loss:  -1.5615 | Function Loss:  -2.0588\n",
      "Total loss:  -0.9401 | PDE Loss:  -1.5616 | Function Loss:  -2.0588\n",
      "Total loss:  -0.9402 | PDE Loss:  -1.5618 | Function Loss:  -2.0589\n",
      "Total loss:  -0.9403 | PDE Loss:  -1.5621 | Function Loss:  -2.0589\n",
      "Total loss:  -0.9403 | PDE Loss:  -1.5622 | Function Loss:  -2.0589\n",
      "Total loss:  -0.9404 | PDE Loss:  -1.5627 | Function Loss:  -2.0588\n",
      "Total loss:  -0.9404 | PDE Loss:  -1.5628 | Function Loss:  -2.0588\n",
      "Total loss:  -0.9405 | PDE Loss:  -1.5631 | Function Loss:  -2.0588\n",
      "Total loss:  -0.9406 | PDE Loss:  -1.5635 | Function Loss:  -2.0589\n",
      "Total loss:  -0.9407 | PDE Loss:  -1.5633 | Function Loss:  -2.059\n",
      "Total loss:  -0.9408 | PDE Loss:  -1.5635 | Function Loss:  -2.0591\n",
      "Total loss:  -0.9409 | PDE Loss:  -1.5636 | Function Loss:  -2.0592\n",
      "Total loss:  -0.941 | PDE Loss:  -1.5636 | Function Loss:  -2.0593\n",
      "Total loss:  -0.941 | PDE Loss:  -1.5635 | Function Loss:  -2.0593\n",
      "Total loss:  -0.941 | PDE Loss:  -1.5634 | Function Loss:  -2.0594\n",
      "Total loss:  -0.9411 | PDE Loss:  -1.5633 | Function Loss:  -2.0595\n",
      "Total loss:  -0.9411 | PDE Loss:  -1.5633 | Function Loss:  -2.0596\n",
      "Total loss:  -0.9412 | PDE Loss:  -1.5632 | Function Loss:  -2.0597\n",
      "Total loss:  -0.9412 | PDE Loss:  -1.5631 | Function Loss:  -2.0598\n",
      "Total loss:  -0.9413 | PDE Loss:  -1.563 | Function Loss:  -2.0599\n",
      "Total loss:  -0.9415 | PDE Loss:  -1.5628 | Function Loss:  -2.0601\n",
      "Total loss:  -0.9416 | PDE Loss:  -1.5627 | Function Loss:  -2.0603\n",
      "Total loss:  -0.9417 | PDE Loss:  -1.5626 | Function Loss:  -2.0605\n",
      "Total loss:  -0.9418 | PDE Loss:  -1.5626 | Function Loss:  -2.0606\n",
      "Total loss:  -0.9418 | PDE Loss:  -1.5627 | Function Loss:  -2.0607\n",
      "Total loss:  -0.9419 | PDE Loss:  -1.5627 | Function Loss:  -2.0608\n",
      "Total loss:  -0.942 | PDE Loss:  -1.5628 | Function Loss:  -2.0608\n",
      "Total loss:  -0.942 | PDE Loss:  -1.5629 | Function Loss:  -2.0609\n",
      "Total loss:  -0.9421 | PDE Loss:  -1.5629 | Function Loss:  -2.061\n",
      "Total loss:  -0.9421 | PDE Loss:  -1.5628 | Function Loss:  -2.0611\n",
      "Total loss:  -0.9422 | PDE Loss:  -1.5627 | Function Loss:  -2.0612\n",
      "Total loss:  -0.9412 | PDE Loss:  -1.5606 | Function Loss:  -2.0605\n",
      "Total loss:  -0.9422 | PDE Loss:  -1.5627 | Function Loss:  -2.0612\n",
      "Total loss:  -0.9423 | PDE Loss:  -1.5626 | Function Loss:  -2.0613\n",
      "Total loss:  -0.9423 | PDE Loss:  -1.5624 | Function Loss:  -2.0614\n",
      "Total loss:  -0.9424 | PDE Loss:  -1.5623 | Function Loss:  -2.0615\n",
      "Total loss:  -0.9424 | PDE Loss:  -1.5621 | Function Loss:  -2.0616\n",
      "Total loss:  -0.9424 | PDE Loss:  -1.562 | Function Loss:  -2.0617\n",
      "Total loss:  -0.9425 | PDE Loss:  -1.5618 | Function Loss:  -2.0618\n",
      "Total loss:  -0.9423 | PDE Loss:  -1.5616 | Function Loss:  -2.0617\n",
      "Total loss:  -0.9425 | PDE Loss:  -1.5619 | Function Loss:  -2.0618\n",
      "Total loss:  -0.9425 | PDE Loss:  -1.5617 | Function Loss:  -2.0619\n",
      "Total loss:  -0.9426 | PDE Loss:  -1.5614 | Function Loss:  -2.062\n",
      "Total loss:  -0.9426 | PDE Loss:  -1.5613 | Function Loss:  -2.0622\n",
      "Total loss:  -0.9427 | PDE Loss:  -1.5614 | Function Loss:  -2.0622\n",
      "Total loss:  -0.9427 | PDE Loss:  -1.5615 | Function Loss:  -2.0622\n",
      "Total loss:  -0.9428 | PDE Loss:  -1.5616 | Function Loss:  -2.0623\n",
      "Total loss:  -0.9428 | PDE Loss:  -1.5615 | Function Loss:  -2.0623\n",
      "Total loss:  -0.9428 | PDE Loss:  -1.5616 | Function Loss:  -2.0623\n",
      "Total loss:  -0.9428 | PDE Loss:  -1.5617 | Function Loss:  -2.0623\n",
      "Total loss:  -0.9429 | PDE Loss:  -1.5618 | Function Loss:  -2.0624\n",
      "Total loss:  -0.9429 | PDE Loss:  -1.5618 | Function Loss:  -2.0624\n",
      "Total loss:  -0.943 | PDE Loss:  -1.5618 | Function Loss:  -2.0625\n",
      "Total loss:  -0.943 | PDE Loss:  -1.5617 | Function Loss:  -2.0625\n",
      "Total loss:  -0.9431 | PDE Loss:  -1.5616 | Function Loss:  -2.0626\n",
      "Total loss:  -0.9431 | PDE Loss:  -1.5614 | Function Loss:  -2.0628\n",
      "Total loss:  -0.9429 | PDE Loss:  -1.5602 | Function Loss:  -2.0629\n",
      "Total loss:  -0.9431 | PDE Loss:  -1.5613 | Function Loss:  -2.0628\n",
      "Total loss:  -0.9432 | PDE Loss:  -1.5611 | Function Loss:  -2.0629\n",
      "Total loss:  -0.9432 | PDE Loss:  -1.5609 | Function Loss:  -2.0631\n",
      "Total loss:  -0.9433 | PDE Loss:  -1.5608 | Function Loss:  -2.0632\n",
      "Total loss:  -0.9433 | PDE Loss:  -1.5607 | Function Loss:  -2.0632\n",
      "Total loss:  -0.9433 | PDE Loss:  -1.5607 | Function Loss:  -2.0633\n",
      "Total loss:  -0.9434 | PDE Loss:  -1.5608 | Function Loss:  -2.0633\n",
      "Total loss:  -0.9434 | PDE Loss:  -1.5609 | Function Loss:  -2.0633\n",
      "Total loss:  -0.9434 | PDE Loss:  -1.5611 | Function Loss:  -2.0633\n",
      "Total loss:  -0.9435 | PDE Loss:  -1.5613 | Function Loss:  -2.0632\n",
      "Total loss:  -0.9435 | PDE Loss:  -1.5615 | Function Loss:  -2.0632\n",
      "Total loss:  -0.9436 | PDE Loss:  -1.5619 | Function Loss:  -2.0632\n",
      "Total loss:  -0.9436 | PDE Loss:  -1.5622 | Function Loss:  -2.0632\n",
      "Total loss:  -0.9437 | PDE Loss:  -1.5624 | Function Loss:  -2.0632\n",
      "Total loss:  -0.9437 | PDE Loss:  -1.5626 | Function Loss:  -2.0632\n",
      "Total loss:  -0.9438 | PDE Loss:  -1.5628 | Function Loss:  -2.0633\n",
      "Total loss:  -0.9436 | PDE Loss:  -1.5618 | Function Loss:  -2.0633\n",
      "Total loss:  -0.9438 | PDE Loss:  -1.5627 | Function Loss:  -2.0633\n",
      "Total loss:  -0.9439 | PDE Loss:  -1.5628 | Function Loss:  -2.0634\n",
      "Total loss:  -0.944 | PDE Loss:  -1.5627 | Function Loss:  -2.0635\n",
      "Total loss:  -0.944 | PDE Loss:  -1.5626 | Function Loss:  -2.0636\n",
      "Total loss:  -0.9441 | PDE Loss:  -1.5623 | Function Loss:  -2.0638\n",
      "Total loss:  -0.9442 | PDE Loss:  -1.562 | Function Loss:  -2.064\n",
      "Total loss:  -0.9442 | PDE Loss:  -1.5618 | Function Loss:  -2.0641\n",
      "Total loss:  -0.9443 | PDE Loss:  -1.5615 | Function Loss:  -2.0643\n",
      "Total loss:  -0.9443 | PDE Loss:  -1.5613 | Function Loss:  -2.0644\n",
      "Total loss:  -0.9444 | PDE Loss:  -1.5613 | Function Loss:  -2.0645\n",
      "Total loss:  -0.9444 | PDE Loss:  -1.5613 | Function Loss:  -2.0645\n",
      "Total loss:  -0.9445 | PDE Loss:  -1.5614 | Function Loss:  -2.0646\n",
      "Total loss:  -0.9445 | PDE Loss:  -1.5616 | Function Loss:  -2.0646\n",
      "Total loss:  -0.9445 | PDE Loss:  -1.5616 | Function Loss:  -2.0646\n",
      "Total loss:  -0.9446 | PDE Loss:  -1.5616 | Function Loss:  -2.0646\n",
      "Total loss:  -0.9446 | PDE Loss:  -1.5617 | Function Loss:  -2.0647\n",
      "Total loss:  -0.9446 | PDE Loss:  -1.5617 | Function Loss:  -2.0647\n",
      "Total loss:  -0.9447 | PDE Loss:  -1.5617 | Function Loss:  -2.0647\n",
      "Total loss:  -0.9447 | PDE Loss:  -1.5617 | Function Loss:  -2.0648\n",
      "Total loss:  -0.9448 | PDE Loss:  -1.5616 | Function Loss:  -2.0649\n",
      "Total loss:  -0.9448 | PDE Loss:  -1.5615 | Function Loss:  -2.065\n",
      "Total loss:  -0.9449 | PDE Loss:  -1.5615 | Function Loss:  -2.065\n",
      "Total loss:  -0.9449 | PDE Loss:  -1.5615 | Function Loss:  -2.0651\n",
      "Total loss:  -0.945 | PDE Loss:  -1.5616 | Function Loss:  -2.0652\n",
      "Total loss:  -0.945 | PDE Loss:  -1.5616 | Function Loss:  -2.0653\n",
      "Total loss:  -0.9451 | PDE Loss:  -1.5616 | Function Loss:  -2.0653\n",
      "Total loss:  -0.9452 | PDE Loss:  -1.5616 | Function Loss:  -2.0654\n",
      "Total loss:  -0.9453 | PDE Loss:  -1.5613 | Function Loss:  -2.0656\n",
      "Total loss:  -0.9453 | PDE Loss:  -1.5615 | Function Loss:  -2.0657\n",
      "Total loss:  -0.9453 | PDE Loss:  -1.5614 | Function Loss:  -2.0657\n",
      "Total loss:  -0.9454 | PDE Loss:  -1.5613 | Function Loss:  -2.0658\n",
      "Total loss:  -0.9455 | PDE Loss:  -1.5612 | Function Loss:  -2.0659\n",
      "Total loss:  -0.9455 | PDE Loss:  -1.5612 | Function Loss:  -2.066\n",
      "Total loss:  -0.9456 | PDE Loss:  -1.5611 | Function Loss:  -2.0661\n",
      "Total loss:  -0.9456 | PDE Loss:  -1.5613 | Function Loss:  -2.0661\n",
      "Total loss:  -0.9457 | PDE Loss:  -1.5605 | Function Loss:  -2.0665\n",
      "Total loss:  -0.9458 | PDE Loss:  -1.5609 | Function Loss:  -2.0664\n",
      "Total loss:  -0.9458 | PDE Loss:  -1.5614 | Function Loss:  -2.0664\n",
      "Total loss:  -0.9459 | PDE Loss:  -1.5618 | Function Loss:  -2.0663\n",
      "Total loss:  -0.9459 | PDE Loss:  -1.5623 | Function Loss:  -2.0662\n",
      "Total loss:  -0.946 | PDE Loss:  -1.5624 | Function Loss:  -2.0662\n",
      "Total loss:  -0.946 | PDE Loss:  -1.5626 | Function Loss:  -2.0662\n",
      "Total loss:  -0.9461 | PDE Loss:  -1.5629 | Function Loss:  -2.0662\n",
      "Total loss:  -0.9461 | PDE Loss:  -1.5631 | Function Loss:  -2.0662\n",
      "Total loss:  -0.9462 | PDE Loss:  -1.5634 | Function Loss:  -2.0662\n",
      "Total loss:  -0.9463 | PDE Loss:  -1.5638 | Function Loss:  -2.0662\n",
      "Total loss:  -0.9464 | PDE Loss:  -1.5641 | Function Loss:  -2.0662\n",
      "Total loss:  -0.9465 | PDE Loss:  -1.5641 | Function Loss:  -2.0663\n",
      "Total loss:  -0.9465 | PDE Loss:  -1.5643 | Function Loss:  -2.0663\n",
      "Total loss:  -0.9466 | PDE Loss:  -1.5643 | Function Loss:  -2.0664\n",
      "Total loss:  -0.9466 | PDE Loss:  -1.5642 | Function Loss:  -2.0665\n",
      "Total loss:  -0.9466 | PDE Loss:  -1.5641 | Function Loss:  -2.0665\n",
      "Total loss:  -0.9467 | PDE Loss:  -1.564 | Function Loss:  -2.0666\n",
      "Total loss:  -0.9467 | PDE Loss:  -1.5639 | Function Loss:  -2.0667\n",
      "Total loss:  -0.9467 | PDE Loss:  -1.5638 | Function Loss:  -2.0668\n",
      "Total loss:  -0.9468 | PDE Loss:  -1.5637 | Function Loss:  -2.0669\n",
      "Total loss:  -0.9468 | PDE Loss:  -1.5636 | Function Loss:  -2.067\n",
      "Total loss:  -0.9469 | PDE Loss:  -1.5636 | Function Loss:  -2.067\n",
      "Total loss:  -0.9469 | PDE Loss:  -1.5638 | Function Loss:  -2.0671\n",
      "Total loss:  -0.947 | PDE Loss:  -1.5647 | Function Loss:  -2.0669\n",
      "Total loss:  -0.9471 | PDE Loss:  -1.5649 | Function Loss:  -2.0669\n",
      "Total loss:  -0.9472 | PDE Loss:  -1.5654 | Function Loss:  -2.0668\n",
      "Total loss:  -0.9472 | PDE Loss:  -1.5657 | Function Loss:  -2.0668\n",
      "Total loss:  -0.9473 | PDE Loss:  -1.5662 | Function Loss:  -2.0667\n",
      "Total loss:  -0.9473 | PDE Loss:  -1.5665 | Function Loss:  -2.0667\n",
      "Total loss:  -0.9474 | PDE Loss:  -1.5671 | Function Loss:  -2.0667\n",
      "Total loss:  -0.9475 | PDE Loss:  -1.567 | Function Loss:  -2.0668\n",
      "Total loss:  -0.9476 | PDE Loss:  -1.5674 | Function Loss:  -2.0668\n",
      "Total loss:  -0.9477 | PDE Loss:  -1.5676 | Function Loss:  -2.0668\n",
      "Total loss:  -0.9477 | PDE Loss:  -1.5677 | Function Loss:  -2.0669\n",
      "Total loss:  -0.9478 | PDE Loss:  -1.5676 | Function Loss:  -2.067\n",
      "Total loss:  -0.9478 | PDE Loss:  -1.5675 | Function Loss:  -2.0671\n",
      "Total loss:  -0.9479 | PDE Loss:  -1.5673 | Function Loss:  -2.0672\n",
      "Total loss:  -0.948 | PDE Loss:  -1.5671 | Function Loss:  -2.0674\n",
      "Total loss:  -0.9481 | PDE Loss:  -1.5668 | Function Loss:  -2.0676\n",
      "Total loss:  -0.9482 | PDE Loss:  -1.5665 | Function Loss:  -2.0678\n",
      "Total loss:  -0.9483 | PDE Loss:  -1.5662 | Function Loss:  -2.0681\n",
      "Total loss:  -0.9485 | PDE Loss:  -1.566 | Function Loss:  -2.0684\n",
      "Total loss:  -0.9485 | PDE Loss:  -1.5663 | Function Loss:  -2.0683\n",
      "Total loss:  -0.9487 | PDE Loss:  -1.567 | Function Loss:  -2.0683\n",
      "Total loss:  -0.9487 | PDE Loss:  -1.5677 | Function Loss:  -2.0682\n",
      "Total loss:  -0.9486 | PDE Loss:  -1.5682 | Function Loss:  -2.0678\n",
      "Total loss:  -0.9488 | PDE Loss:  -1.568 | Function Loss:  -2.0681\n",
      "Total loss:  -0.9489 | PDE Loss:  -1.5685 | Function Loss:  -2.0681\n",
      "Total loss:  -0.9489 | PDE Loss:  -1.5688 | Function Loss:  -2.0681\n",
      "Total loss:  -0.949 | PDE Loss:  -1.5691 | Function Loss:  -2.0681\n",
      "Total loss:  -0.9491 | PDE Loss:  -1.5692 | Function Loss:  -2.0682\n",
      "Total loss:  -0.9492 | PDE Loss:  -1.5692 | Function Loss:  -2.0683\n",
      "Total loss:  -0.9492 | PDE Loss:  -1.5696 | Function Loss:  -2.0682\n",
      "Total loss:  -0.9491 | PDE Loss:  -1.5676 | Function Loss:  -2.0688\n",
      "Total loss:  -0.9493 | PDE Loss:  -1.5691 | Function Loss:  -2.0685\n",
      "Total loss:  -0.9494 | PDE Loss:  -1.569 | Function Loss:  -2.0686\n",
      "Total loss:  -0.9494 | PDE Loss:  -1.5689 | Function Loss:  -2.0687\n",
      "Total loss:  -0.9495 | PDE Loss:  -1.5689 | Function Loss:  -2.0688\n",
      "Total loss:  -0.9495 | PDE Loss:  -1.5688 | Function Loss:  -2.0688\n",
      "Total loss:  -0.9496 | PDE Loss:  -1.5688 | Function Loss:  -2.0689\n",
      "Total loss:  -0.9495 | PDE Loss:  -1.568 | Function Loss:  -2.0691\n",
      "Total loss:  -0.9496 | PDE Loss:  -1.5685 | Function Loss:  -2.069\n",
      "Total loss:  -0.9496 | PDE Loss:  -1.5686 | Function Loss:  -2.0691\n",
      "Total loss:  -0.9497 | PDE Loss:  -1.5687 | Function Loss:  -2.0691\n",
      "Total loss:  -0.9498 | PDE Loss:  -1.5686 | Function Loss:  -2.0693\n",
      "Total loss:  -0.9498 | PDE Loss:  -1.5688 | Function Loss:  -2.0693\n",
      "Total loss:  -0.9499 | PDE Loss:  -1.5688 | Function Loss:  -2.0694\n",
      "Total loss:  -0.95 | PDE Loss:  -1.5688 | Function Loss:  -2.0695\n",
      "Total loss:  -0.9501 | PDE Loss:  -1.5686 | Function Loss:  -2.0697\n",
      "Total loss:  -0.9502 | PDE Loss:  -1.5687 | Function Loss:  -2.0698\n",
      "Total loss:  -0.9503 | PDE Loss:  -1.5685 | Function Loss:  -2.0699\n",
      "Total loss:  -0.9503 | PDE Loss:  -1.5686 | Function Loss:  -2.07\n",
      "Total loss:  -0.9503 | PDE Loss:  -1.5688 | Function Loss:  -2.07\n",
      "Total loss:  -0.9503 | PDE Loss:  -1.5688 | Function Loss:  -2.0699\n",
      "Total loss:  -0.9504 | PDE Loss:  -1.5689 | Function Loss:  -2.07\n",
      "Total loss:  -0.9504 | PDE Loss:  -1.569 | Function Loss:  -2.0699\n",
      "Total loss:  -0.9504 | PDE Loss:  -1.5691 | Function Loss:  -2.07\n",
      "Total loss:  -0.9504 | PDE Loss:  -1.5692 | Function Loss:  -2.07\n",
      "Total loss:  -0.9505 | PDE Loss:  -1.5693 | Function Loss:  -2.07\n",
      "Total loss:  -0.9505 | PDE Loss:  -1.5694 | Function Loss:  -2.07\n",
      "Total loss:  -0.9506 | PDE Loss:  -1.5696 | Function Loss:  -2.07\n",
      "Total loss:  -0.9506 | PDE Loss:  -1.5697 | Function Loss:  -2.07\n",
      "Total loss:  -0.9504 | PDE Loss:  -1.5694 | Function Loss:  -2.0699\n",
      "Total loss:  -0.9506 | PDE Loss:  -1.5698 | Function Loss:  -2.07\n",
      "Total loss:  -0.9507 | PDE Loss:  -1.5698 | Function Loss:  -2.0701\n",
      "Total loss:  -0.9507 | PDE Loss:  -1.5698 | Function Loss:  -2.0701\n",
      "Total loss:  -0.9508 | PDE Loss:  -1.5697 | Function Loss:  -2.0703\n",
      "Total loss:  -0.9509 | PDE Loss:  -1.5697 | Function Loss:  -2.0704\n",
      "Total loss:  -0.9509 | PDE Loss:  -1.5694 | Function Loss:  -2.0705\n",
      "Total loss:  -0.951 | PDE Loss:  -1.5693 | Function Loss:  -2.0707\n",
      "Total loss:  -0.951 | PDE Loss:  -1.5691 | Function Loss:  -2.0707\n",
      "Total loss:  -0.951 | PDE Loss:  -1.5693 | Function Loss:  -2.0707\n",
      "Total loss:  -0.9511 | PDE Loss:  -1.569 | Function Loss:  -2.0709\n",
      "Total loss:  -0.9511 | PDE Loss:  -1.5689 | Function Loss:  -2.071\n",
      "Total loss:  -0.9512 | PDE Loss:  -1.5687 | Function Loss:  -2.0711\n",
      "Total loss:  -0.9512 | PDE Loss:  -1.5685 | Function Loss:  -2.0712\n",
      "Total loss:  -0.9513 | PDE Loss:  -1.5686 | Function Loss:  -2.0713\n",
      "Total loss:  -0.9512 | PDE Loss:  -1.5669 | Function Loss:  -2.0717\n",
      "Total loss:  -0.9513 | PDE Loss:  -1.5682 | Function Loss:  -2.0714\n",
      "Total loss:  -0.9513 | PDE Loss:  -1.5683 | Function Loss:  -2.0714\n",
      "Total loss:  -0.9514 | PDE Loss:  -1.5685 | Function Loss:  -2.0714\n",
      "Total loss:  -0.9515 | PDE Loss:  -1.5688 | Function Loss:  -2.0714\n",
      "Total loss:  -0.9515 | PDE Loss:  -1.569 | Function Loss:  -2.0715\n",
      "Total loss:  -0.9516 | PDE Loss:  -1.5693 | Function Loss:  -2.0715\n",
      "Total loss:  -0.9515 | PDE Loss:  -1.5682 | Function Loss:  -2.0717\n",
      "Total loss:  -0.9516 | PDE Loss:  -1.569 | Function Loss:  -2.0716\n",
      "Total loss:  -0.9517 | PDE Loss:  -1.5693 | Function Loss:  -2.0716\n",
      "Total loss:  -0.9518 | PDE Loss:  -1.5697 | Function Loss:  -2.0715\n",
      "Total loss:  -0.9518 | PDE Loss:  -1.5698 | Function Loss:  -2.0716\n",
      "Total loss:  -0.9518 | PDE Loss:  -1.5698 | Function Loss:  -2.0716\n",
      "Total loss:  -0.9519 | PDE Loss:  -1.5698 | Function Loss:  -2.0717\n",
      "Total loss:  -0.9519 | PDE Loss:  -1.5698 | Function Loss:  -2.0717\n",
      "Total loss:  -0.952 | PDE Loss:  -1.5698 | Function Loss:  -2.0717\n",
      "Total loss:  -0.952 | PDE Loss:  -1.5699 | Function Loss:  -2.0718\n",
      "Total loss:  -0.952 | PDE Loss:  -1.5699 | Function Loss:  -2.0718\n",
      "Total loss:  -0.9521 | PDE Loss:  -1.57 | Function Loss:  -2.0719\n",
      "Total loss:  -0.9522 | PDE Loss:  -1.5701 | Function Loss:  -2.0719\n",
      "Total loss:  -0.9522 | PDE Loss:  -1.5701 | Function Loss:  -2.072\n",
      "Total loss:  -0.9523 | PDE Loss:  -1.5702 | Function Loss:  -2.072\n",
      "Total loss:  -0.9524 | PDE Loss:  -1.5705 | Function Loss:  -2.0721\n",
      "Total loss:  -0.9525 | PDE Loss:  -1.5707 | Function Loss:  -2.0721\n",
      "Total loss:  -0.9526 | PDE Loss:  -1.5709 | Function Loss:  -2.0722\n",
      "Total loss:  -0.9527 | PDE Loss:  -1.571 | Function Loss:  -2.0723\n",
      "Total loss:  -0.9528 | PDE Loss:  -1.5711 | Function Loss:  -2.0724\n",
      "Total loss:  -0.9523 | PDE Loss:  -1.5691 | Function Loss:  -2.0725\n",
      "Total loss:  -0.9528 | PDE Loss:  -1.5709 | Function Loss:  -2.0725\n",
      "Total loss:  -0.9529 | PDE Loss:  -1.571 | Function Loss:  -2.0726\n",
      "Total loss:  -0.953 | PDE Loss:  -1.571 | Function Loss:  -2.0727\n",
      "Total loss:  -0.9531 | PDE Loss:  -1.571 | Function Loss:  -2.0728\n",
      "Total loss:  -0.9531 | PDE Loss:  -1.571 | Function Loss:  -2.0729\n",
      "Total loss:  -0.9532 | PDE Loss:  -1.5712 | Function Loss:  -2.073\n",
      "Total loss:  -0.9533 | PDE Loss:  -1.5713 | Function Loss:  -2.0731\n",
      "Total loss:  -0.9534 | PDE Loss:  -1.5715 | Function Loss:  -2.0731\n",
      "Total loss:  -0.9535 | PDE Loss:  -1.5718 | Function Loss:  -2.0731\n",
      "Total loss:  -0.9535 | PDE Loss:  -1.5721 | Function Loss:  -2.0731\n",
      "Total loss:  -0.9536 | PDE Loss:  -1.5723 | Function Loss:  -2.0731\n",
      "Total loss:  -0.9536 | PDE Loss:  -1.5727 | Function Loss:  -2.0731\n",
      "Total loss:  -0.9537 | PDE Loss:  -1.5728 | Function Loss:  -2.0731\n",
      "Total loss:  -0.9538 | PDE Loss:  -1.5732 | Function Loss:  -2.0731\n",
      "Total loss:  -0.9538 | PDE Loss:  -1.5735 | Function Loss:  -2.073\n",
      "Total loss:  -0.9539 | PDE Loss:  -1.5741 | Function Loss:  -2.073\n",
      "Total loss:  -0.954 | PDE Loss:  -1.5744 | Function Loss:  -2.073\n",
      "Total loss:  -0.9541 | PDE Loss:  -1.5746 | Function Loss:  -2.0731\n",
      "Total loss:  -0.9542 | PDE Loss:  -1.5746 | Function Loss:  -2.0732\n",
      "Total loss:  -0.9543 | PDE Loss:  -1.5744 | Function Loss:  -2.0734\n",
      "Total loss:  -0.9543 | PDE Loss:  -1.5741 | Function Loss:  -2.0735\n",
      "Total loss:  -0.9542 | PDE Loss:  -1.5731 | Function Loss:  -2.0736\n",
      "Total loss:  -0.9544 | PDE Loss:  -1.574 | Function Loss:  -2.0736\n",
      "Total loss:  -0.9544 | PDE Loss:  -1.5738 | Function Loss:  -2.0737\n",
      "Total loss:  -0.9545 | PDE Loss:  -1.5735 | Function Loss:  -2.0739\n",
      "Total loss:  -0.9545 | PDE Loss:  -1.5734 | Function Loss:  -2.074\n",
      "Total loss:  -0.9546 | PDE Loss:  -1.5734 | Function Loss:  -2.0741\n",
      "Total loss:  -0.9546 | PDE Loss:  -1.5734 | Function Loss:  -2.0741\n",
      "Total loss:  -0.9547 | PDE Loss:  -1.5735 | Function Loss:  -2.0742\n",
      "Total loss:  -0.9547 | PDE Loss:  -1.5737 | Function Loss:  -2.0742\n",
      "Total loss:  -0.9548 | PDE Loss:  -1.5739 | Function Loss:  -2.0742\n",
      "Total loss:  -0.9549 | PDE Loss:  -1.5743 | Function Loss:  -2.0742\n",
      "Total loss:  -0.9549 | PDE Loss:  -1.5746 | Function Loss:  -2.0742\n",
      "Total loss:  -0.955 | PDE Loss:  -1.5749 | Function Loss:  -2.0742\n",
      "Total loss:  -0.9551 | PDE Loss:  -1.575 | Function Loss:  -2.0742\n",
      "Total loss:  -0.9551 | PDE Loss:  -1.575 | Function Loss:  -2.0743\n",
      "Total loss:  -0.9552 | PDE Loss:  -1.575 | Function Loss:  -2.0743\n",
      "Total loss:  -0.9552 | PDE Loss:  -1.5749 | Function Loss:  -2.0744\n",
      "Total loss:  -0.9553 | PDE Loss:  -1.5742 | Function Loss:  -2.0747\n",
      "Total loss:  -0.9554 | PDE Loss:  -1.5744 | Function Loss:  -2.0748\n",
      "Total loss:  -0.9554 | PDE Loss:  -1.5746 | Function Loss:  -2.0748\n",
      "Total loss:  -0.9555 | PDE Loss:  -1.5748 | Function Loss:  -2.0749\n",
      "Total loss:  -0.9556 | PDE Loss:  -1.5751 | Function Loss:  -2.0749\n",
      "Total loss:  -0.9557 | PDE Loss:  -1.5753 | Function Loss:  -2.0749\n",
      "Total loss:  -0.9557 | PDE Loss:  -1.5758 | Function Loss:  -2.0748\n",
      "Total loss:  -0.9558 | PDE Loss:  -1.5761 | Function Loss:  -2.0748\n",
      "Total loss:  -0.9558 | PDE Loss:  -1.5763 | Function Loss:  -2.0748\n",
      "Total loss:  -0.9559 | PDE Loss:  -1.5767 | Function Loss:  -2.0748\n",
      "Total loss:  -0.956 | PDE Loss:  -1.5769 | Function Loss:  -2.0748\n",
      "Total loss:  -0.956 | PDE Loss:  -1.5772 | Function Loss:  -2.0747\n",
      "Total loss:  -0.956 | PDE Loss:  -1.5774 | Function Loss:  -2.0747\n",
      "Total loss:  -0.9561 | PDE Loss:  -1.5775 | Function Loss:  -2.0747\n",
      "Total loss:  -0.9561 | PDE Loss:  -1.5777 | Function Loss:  -2.0747\n",
      "Total loss:  -0.9561 | PDE Loss:  -1.5778 | Function Loss:  -2.0747\n",
      "Total loss:  -0.9562 | PDE Loss:  -1.5779 | Function Loss:  -2.0747\n",
      "Total loss:  -0.9562 | PDE Loss:  -1.5781 | Function Loss:  -2.0747\n",
      "Total loss:  -0.9563 | PDE Loss:  -1.5781 | Function Loss:  -2.0748\n",
      "Total loss:  -0.9563 | PDE Loss:  -1.5781 | Function Loss:  -2.0749\n",
      "Total loss:  -0.9564 | PDE Loss:  -1.578 | Function Loss:  -2.075\n",
      "Total loss:  -0.9565 | PDE Loss:  -1.5779 | Function Loss:  -2.0751\n",
      "Total loss:  -0.9565 | PDE Loss:  -1.5779 | Function Loss:  -2.0752\n",
      "Total loss:  -0.9566 | PDE Loss:  -1.5778 | Function Loss:  -2.0753\n",
      "Total loss:  -0.9566 | PDE Loss:  -1.5777 | Function Loss:  -2.0754\n",
      "Total loss:  -0.9567 | PDE Loss:  -1.5776 | Function Loss:  -2.0755\n",
      "Total loss:  -0.9567 | PDE Loss:  -1.5775 | Function Loss:  -2.0756\n",
      "Total loss:  -0.9568 | PDE Loss:  -1.5775 | Function Loss:  -2.0757\n",
      "Total loss:  -0.9569 | PDE Loss:  -1.5773 | Function Loss:  -2.0759\n",
      "Total loss:  -0.957 | PDE Loss:  -1.5772 | Function Loss:  -2.076\n",
      "Total loss:  -0.957 | PDE Loss:  -1.5773 | Function Loss:  -2.0761\n",
      "Total loss:  -0.9571 | PDE Loss:  -1.5772 | Function Loss:  -2.0762\n",
      "Total loss:  -0.9572 | PDE Loss:  -1.5773 | Function Loss:  -2.0763\n",
      "Total loss:  -0.9572 | PDE Loss:  -1.5774 | Function Loss:  -2.0763\n",
      "Total loss:  -0.9573 | PDE Loss:  -1.5775 | Function Loss:  -2.0763\n",
      "Total loss:  -0.9573 | PDE Loss:  -1.5775 | Function Loss:  -2.0764\n",
      "Total loss:  -0.9573 | PDE Loss:  -1.5775 | Function Loss:  -2.0764\n",
      "Total loss:  -0.9574 | PDE Loss:  -1.5775 | Function Loss:  -2.0765\n",
      "Total loss:  -0.9574 | PDE Loss:  -1.5774 | Function Loss:  -2.0766\n",
      "Total loss:  -0.9575 | PDE Loss:  -1.5772 | Function Loss:  -2.0768\n",
      "Total loss:  -0.9576 | PDE Loss:  -1.5769 | Function Loss:  -2.0769\n",
      "Total loss:  -0.9576 | PDE Loss:  -1.5768 | Function Loss:  -2.077\n",
      "Total loss:  -0.9576 | PDE Loss:  -1.5767 | Function Loss:  -2.0771\n",
      "Total loss:  -0.9577 | PDE Loss:  -1.5767 | Function Loss:  -2.0771\n",
      "Total loss:  -0.9577 | PDE Loss:  -1.5767 | Function Loss:  -2.0771\n",
      "Total loss:  -0.9577 | PDE Loss:  -1.5767 | Function Loss:  -2.0772\n",
      "Total loss:  -0.9578 | PDE Loss:  -1.5767 | Function Loss:  -2.0772\n",
      "Total loss:  -0.9578 | PDE Loss:  -1.5769 | Function Loss:  -2.0772\n",
      "Total loss:  -0.9578 | PDE Loss:  -1.5769 | Function Loss:  -2.0772\n",
      "Total loss:  -0.9579 | PDE Loss:  -1.577 | Function Loss:  -2.0773\n",
      "Total loss:  -0.9579 | PDE Loss:  -1.5771 | Function Loss:  -2.0773\n",
      "Total loss:  -0.9579 | PDE Loss:  -1.5777 | Function Loss:  -2.0772\n",
      "Total loss:  -0.958 | PDE Loss:  -1.5776 | Function Loss:  -2.0772\n",
      "Total loss:  -0.958 | PDE Loss:  -1.5776 | Function Loss:  -2.0773\n",
      "Total loss:  -0.958 | PDE Loss:  -1.5776 | Function Loss:  -2.0773\n",
      "Total loss:  -0.9581 | PDE Loss:  -1.5776 | Function Loss:  -2.0773\n",
      "Total loss:  -0.9581 | PDE Loss:  -1.5776 | Function Loss:  -2.0774\n",
      "Total loss:  -0.9581 | PDE Loss:  -1.5776 | Function Loss:  -2.0774\n",
      "Total loss:  -0.9581 | PDE Loss:  -1.5766 | Function Loss:  -2.0777\n",
      "Total loss:  -0.9581 | PDE Loss:  -1.5773 | Function Loss:  -2.0775\n",
      "Total loss:  -0.9582 | PDE Loss:  -1.5774 | Function Loss:  -2.0775\n",
      "Total loss:  -0.9582 | PDE Loss:  -1.5776 | Function Loss:  -2.0775\n",
      "Total loss:  -0.9583 | PDE Loss:  -1.5778 | Function Loss:  -2.0775\n",
      "Total loss:  -0.9583 | PDE Loss:  -1.578 | Function Loss:  -2.0775\n",
      "Total loss:  -0.9584 | PDE Loss:  -1.5782 | Function Loss:  -2.0775\n",
      "Total loss:  -0.9584 | PDE Loss:  -1.5783 | Function Loss:  -2.0775\n",
      "Total loss:  -0.9584 | PDE Loss:  -1.5785 | Function Loss:  -2.0775\n",
      "Total loss:  -0.9585 | PDE Loss:  -1.5788 | Function Loss:  -2.0775\n",
      "Total loss:  -0.9586 | PDE Loss:  -1.5789 | Function Loss:  -2.0776\n",
      "Total loss:  -0.9586 | PDE Loss:  -1.5796 | Function Loss:  -2.0774\n",
      "Total loss:  -0.9587 | PDE Loss:  -1.5799 | Function Loss:  -2.0774\n",
      "Total loss:  -0.9588 | PDE Loss:  -1.5803 | Function Loss:  -2.0774\n",
      "Total loss:  -0.9589 | PDE Loss:  -1.5804 | Function Loss:  -2.0775\n",
      "Total loss:  -0.9589 | PDE Loss:  -1.5806 | Function Loss:  -2.0775\n",
      "Total loss:  -0.959 | PDE Loss:  -1.5803 | Function Loss:  -2.0776\n",
      "Total loss:  -0.959 | PDE Loss:  -1.5802 | Function Loss:  -2.0777\n",
      "Total loss:  -0.959 | PDE Loss:  -1.5801 | Function Loss:  -2.0778\n",
      "Total loss:  -0.959 | PDE Loss:  -1.5797 | Function Loss:  -2.0779\n",
      "Total loss:  -0.9591 | PDE Loss:  -1.5797 | Function Loss:  -2.078\n",
      "Total loss:  -0.9591 | PDE Loss:  -1.5796 | Function Loss:  -2.078\n",
      "Total loss:  -0.9592 | PDE Loss:  -1.5795 | Function Loss:  -2.0782\n",
      "Total loss:  -0.9592 | PDE Loss:  -1.5794 | Function Loss:  -2.0783\n",
      "Total loss:  -0.9593 | PDE Loss:  -1.5792 | Function Loss:  -2.0784\n",
      "Total loss:  -0.9594 | PDE Loss:  -1.5798 | Function Loss:  -2.0783\n",
      "Total loss:  -0.9594 | PDE Loss:  -1.5796 | Function Loss:  -2.0785\n",
      "Total loss:  -0.9595 | PDE Loss:  -1.5794 | Function Loss:  -2.0786\n",
      "Total loss:  -0.9596 | PDE Loss:  -1.5794 | Function Loss:  -2.0787\n",
      "Total loss:  -0.9596 | PDE Loss:  -1.5796 | Function Loss:  -2.0787\n",
      "Total loss:  -0.9597 | PDE Loss:  -1.5798 | Function Loss:  -2.0788\n",
      "Total loss:  -0.9598 | PDE Loss:  -1.5802 | Function Loss:  -2.0787\n",
      "Total loss:  -0.9598 | PDE Loss:  -1.5806 | Function Loss:  -2.0787\n",
      "Total loss:  -0.9599 | PDE Loss:  -1.5806 | Function Loss:  -2.0788\n",
      "Total loss:  -0.9599 | PDE Loss:  -1.5811 | Function Loss:  -2.0787\n",
      "Total loss:  -0.96 | PDE Loss:  -1.5815 | Function Loss:  -2.0786\n",
      "Total loss:  -0.96 | PDE Loss:  -1.5819 | Function Loss:  -2.0786\n",
      "Total loss:  -0.9601 | PDE Loss:  -1.5822 | Function Loss:  -2.0785\n",
      "Total loss:  -0.9602 | PDE Loss:  -1.5826 | Function Loss:  -2.0785\n",
      "Total loss:  -0.9603 | PDE Loss:  -1.583 | Function Loss:  -2.0785\n",
      "Total loss:  -0.9604 | PDE Loss:  -1.5831 | Function Loss:  -2.0786\n",
      "Total loss:  -0.9605 | PDE Loss:  -1.5835 | Function Loss:  -2.0786\n",
      "Total loss:  -0.9605 | PDE Loss:  -1.5837 | Function Loss:  -2.0787\n",
      "Total loss:  -0.9606 | PDE Loss:  -1.5838 | Function Loss:  -2.0787\n",
      "Total loss:  -0.9607 | PDE Loss:  -1.5839 | Function Loss:  -2.0788\n",
      "Total loss:  -0.9607 | PDE Loss:  -1.584 | Function Loss:  -2.0788\n",
      "Total loss:  -0.9608 | PDE Loss:  -1.584 | Function Loss:  -2.0789\n",
      "Total loss:  -0.9608 | PDE Loss:  -1.5841 | Function Loss:  -2.0789\n",
      "Total loss:  -0.9609 | PDE Loss:  -1.5841 | Function Loss:  -2.079\n",
      "Total loss:  -0.961 | PDE Loss:  -1.5841 | Function Loss:  -2.0791\n",
      "Total loss:  -0.961 | PDE Loss:  -1.5841 | Function Loss:  -2.0792\n",
      "Total loss:  -0.9611 | PDE Loss:  -1.5841 | Function Loss:  -2.0792\n",
      "Total loss:  -0.9611 | PDE Loss:  -1.5841 | Function Loss:  -2.0793\n",
      "Total loss:  -0.9611 | PDE Loss:  -1.5841 | Function Loss:  -2.0793\n",
      "Total loss:  -0.9612 | PDE Loss:  -1.5842 | Function Loss:  -2.0793\n",
      "Total loss:  -0.9612 | PDE Loss:  -1.5843 | Function Loss:  -2.0794\n",
      "Total loss:  -0.9613 | PDE Loss:  -1.5844 | Function Loss:  -2.0794\n",
      "Total loss:  -0.9613 | PDE Loss:  -1.5843 | Function Loss:  -2.0795\n",
      "Total loss:  -0.9613 | PDE Loss:  -1.5845 | Function Loss:  -2.0794\n",
      "Total loss:  -0.9614 | PDE Loss:  -1.5847 | Function Loss:  -2.0794\n",
      "Total loss:  -0.9614 | PDE Loss:  -1.5849 | Function Loss:  -2.0794\n",
      "Total loss:  -0.9614 | PDE Loss:  -1.585 | Function Loss:  -2.0794\n",
      "Total loss:  -0.9614 | PDE Loss:  -1.5851 | Function Loss:  -2.0794\n",
      "Total loss:  -0.9614 | PDE Loss:  -1.5852 | Function Loss:  -2.0794\n",
      "Total loss:  -0.9614 | PDE Loss:  -1.5854 | Function Loss:  -2.0793\n",
      "Total loss:  -0.9615 | PDE Loss:  -1.5855 | Function Loss:  -2.0793\n",
      "Total loss:  -0.9615 | PDE Loss:  -1.5857 | Function Loss:  -2.0793\n",
      "Total loss:  -0.9615 | PDE Loss:  -1.5859 | Function Loss:  -2.0793\n",
      "Total loss:  -0.9616 | PDE Loss:  -1.586 | Function Loss:  -2.0793\n",
      "Total loss:  -0.9616 | PDE Loss:  -1.586 | Function Loss:  -2.0793\n",
      "Total loss:  -0.9616 | PDE Loss:  -1.5859 | Function Loss:  -2.0794\n",
      "Total loss:  -0.9617 | PDE Loss:  -1.5858 | Function Loss:  -2.0795\n",
      "Total loss:  -0.9617 | PDE Loss:  -1.5857 | Function Loss:  -2.0796\n",
      "Total loss:  -0.9617 | PDE Loss:  -1.5856 | Function Loss:  -2.0797\n",
      "Total loss:  -0.9618 | PDE Loss:  -1.5855 | Function Loss:  -2.0798\n",
      "Total loss:  -0.9618 | PDE Loss:  -1.5854 | Function Loss:  -2.0798\n",
      "Total loss:  -0.9619 | PDE Loss:  -1.5854 | Function Loss:  -2.0799\n",
      "Total loss:  -0.9619 | PDE Loss:  -1.5855 | Function Loss:  -2.0799\n",
      "Total loss:  -0.9619 | PDE Loss:  -1.5856 | Function Loss:  -2.0799\n",
      "Total loss:  -0.9619 | PDE Loss:  -1.5858 | Function Loss:  -2.0798\n",
      "Total loss:  -0.9619 | PDE Loss:  -1.5859 | Function Loss:  -2.0798\n",
      "Total loss:  -0.962 | PDE Loss:  -1.5862 | Function Loss:  -2.0797\n",
      "Total loss:  -0.962 | PDE Loss:  -1.5865 | Function Loss:  -2.0797\n",
      "Total loss:  -0.962 | PDE Loss:  -1.5869 | Function Loss:  -2.0796\n",
      "Total loss:  -0.9621 | PDE Loss:  -1.5874 | Function Loss:  -2.0796\n",
      "Total loss:  -0.9622 | PDE Loss:  -1.588 | Function Loss:  -2.0795\n",
      "Total loss:  -0.9622 | PDE Loss:  -1.5881 | Function Loss:  -2.0795\n",
      "Total loss:  -0.9622 | PDE Loss:  -1.5881 | Function Loss:  -2.0795\n",
      "Total loss:  -0.9623 | PDE Loss:  -1.5879 | Function Loss:  -2.0796\n",
      "Total loss:  -0.9623 | PDE Loss:  -1.5878 | Function Loss:  -2.0797\n",
      "Total loss:  -0.9623 | PDE Loss:  -1.5876 | Function Loss:  -2.0798\n",
      "Total loss:  -0.9624 | PDE Loss:  -1.5874 | Function Loss:  -2.0799\n",
      "Total loss:  -0.9624 | PDE Loss:  -1.5872 | Function Loss:  -2.08\n",
      "Total loss:  -0.9625 | PDE Loss:  -1.5868 | Function Loss:  -2.0802\n",
      "Total loss:  -0.9625 | PDE Loss:  -1.5867 | Function Loss:  -2.0803\n",
      "Total loss:  -0.9626 | PDE Loss:  -1.5867 | Function Loss:  -2.0804\n",
      "Total loss:  -0.9626 | PDE Loss:  -1.5869 | Function Loss:  -2.0804\n",
      "Total loss:  -0.9627 | PDE Loss:  -1.5871 | Function Loss:  -2.0804\n",
      "Total loss:  -0.9627 | PDE Loss:  -1.5872 | Function Loss:  -2.0805\n",
      "Total loss:  -0.9628 | PDE Loss:  -1.5874 | Function Loss:  -2.0805\n",
      "Total loss:  -0.9629 | PDE Loss:  -1.5874 | Function Loss:  -2.0806\n",
      "Total loss:  -0.963 | PDE Loss:  -1.5876 | Function Loss:  -2.0806\n",
      "Total loss:  -0.963 | PDE Loss:  -1.5876 | Function Loss:  -2.0807\n",
      "Total loss:  -0.9631 | PDE Loss:  -1.5875 | Function Loss:  -2.0809\n",
      "Total loss:  -0.9632 | PDE Loss:  -1.5874 | Function Loss:  -2.081\n",
      "Total loss:  -0.9632 | PDE Loss:  -1.5873 | Function Loss:  -2.081\n",
      "Total loss:  -0.9631 | PDE Loss:  -1.5869 | Function Loss:  -2.0811\n",
      "Total loss:  -0.9632 | PDE Loss:  -1.5872 | Function Loss:  -2.0811\n",
      "Total loss:  -0.9633 | PDE Loss:  -1.5872 | Function Loss:  -2.0811\n",
      "Total loss:  -0.9633 | PDE Loss:  -1.5873 | Function Loss:  -2.0812\n",
      "Total loss:  -0.9634 | PDE Loss:  -1.5875 | Function Loss:  -2.0812\n",
      "Total loss:  -0.9634 | PDE Loss:  -1.5877 | Function Loss:  -2.0812\n",
      "Total loss:  -0.9634 | PDE Loss:  -1.588 | Function Loss:  -2.0811\n",
      "Total loss:  -0.9635 | PDE Loss:  -1.5883 | Function Loss:  -2.0811\n",
      "Total loss:  -0.9635 | PDE Loss:  -1.5885 | Function Loss:  -2.0811\n",
      "Total loss:  -0.9636 | PDE Loss:  -1.5889 | Function Loss:  -2.081\n",
      "Total loss:  -0.9636 | PDE Loss:  -1.5896 | Function Loss:  -2.0809\n",
      "Total loss:  -0.9637 | PDE Loss:  -1.5898 | Function Loss:  -2.0809\n",
      "Total loss:  -0.9638 | PDE Loss:  -1.5899 | Function Loss:  -2.081\n",
      "Total loss:  -0.9638 | PDE Loss:  -1.5899 | Function Loss:  -2.0811\n",
      "Total loss:  -0.9639 | PDE Loss:  -1.5898 | Function Loss:  -2.0811\n",
      "Total loss:  -0.9639 | PDE Loss:  -1.5897 | Function Loss:  -2.0812\n",
      "Total loss:  -0.964 | PDE Loss:  -1.5896 | Function Loss:  -2.0813\n",
      "Total loss:  -0.964 | PDE Loss:  -1.5896 | Function Loss:  -2.0814\n",
      "Total loss:  -0.964 | PDE Loss:  -1.5896 | Function Loss:  -2.0814\n",
      "Total loss:  -0.9641 | PDE Loss:  -1.5897 | Function Loss:  -2.0815\n",
      "Total loss:  -0.9642 | PDE Loss:  -1.5898 | Function Loss:  -2.0816\n",
      "Total loss:  -0.9643 | PDE Loss:  -1.59 | Function Loss:  -2.0816\n",
      "Total loss:  -0.9643 | PDE Loss:  -1.5905 | Function Loss:  -2.0814\n",
      "Total loss:  -0.9643 | PDE Loss:  -1.5904 | Function Loss:  -2.0815\n",
      "Total loss:  -0.9643 | PDE Loss:  -1.5905 | Function Loss:  -2.0815\n",
      "Total loss:  -0.9644 | PDE Loss:  -1.5908 | Function Loss:  -2.0815\n",
      "Total loss:  -0.9645 | PDE Loss:  -1.5911 | Function Loss:  -2.0815\n",
      "Total loss:  -0.9645 | PDE Loss:  -1.5913 | Function Loss:  -2.0815\n",
      "Total loss:  -0.9646 | PDE Loss:  -1.5915 | Function Loss:  -2.0816\n",
      "Total loss:  -0.9647 | PDE Loss:  -1.5916 | Function Loss:  -2.0816\n",
      "Total loss:  -0.9648 | PDE Loss:  -1.5917 | Function Loss:  -2.0817\n",
      "Total loss:  -0.9648 | PDE Loss:  -1.5915 | Function Loss:  -2.0819\n",
      "Total loss:  -0.9649 | PDE Loss:  -1.5914 | Function Loss:  -2.082\n",
      "Total loss:  -0.965 | PDE Loss:  -1.5914 | Function Loss:  -2.0821\n",
      "Total loss:  -0.965 | PDE Loss:  -1.5913 | Function Loss:  -2.0822\n",
      "Total loss:  -0.9651 | PDE Loss:  -1.5913 | Function Loss:  -2.0823\n",
      "Total loss:  -0.9652 | PDE Loss:  -1.5912 | Function Loss:  -2.0824\n",
      "Total loss:  -0.9652 | PDE Loss:  -1.5912 | Function Loss:  -2.0825\n",
      "Total loss:  -0.965 | PDE Loss:  -1.5908 | Function Loss:  -2.0823\n",
      "Total loss:  -0.9652 | PDE Loss:  -1.5912 | Function Loss:  -2.0825\n",
      "Total loss:  -0.9653 | PDE Loss:  -1.5913 | Function Loss:  -2.0825\n",
      "Total loss:  -0.9654 | PDE Loss:  -1.5913 | Function Loss:  -2.0826\n",
      "Total loss:  -0.9655 | PDE Loss:  -1.5915 | Function Loss:  -2.0827\n",
      "Total loss:  -0.9656 | PDE Loss:  -1.5916 | Function Loss:  -2.0828\n",
      "Total loss:  -0.9656 | PDE Loss:  -1.5919 | Function Loss:  -2.0828\n",
      "Total loss:  -0.9657 | PDE Loss:  -1.592 | Function Loss:  -2.0828\n",
      "Total loss:  -0.9658 | PDE Loss:  -1.5922 | Function Loss:  -2.0829\n",
      "Total loss:  -0.9659 | PDE Loss:  -1.5923 | Function Loss:  -2.083\n",
      "Total loss:  -0.966 | PDE Loss:  -1.5925 | Function Loss:  -2.083\n",
      "Total loss:  -0.966 | PDE Loss:  -1.5917 | Function Loss:  -2.0833\n",
      "Total loss:  -0.9661 | PDE Loss:  -1.5922 | Function Loss:  -2.0833\n",
      "Total loss:  -0.9661 | PDE Loss:  -1.5926 | Function Loss:  -2.0832\n",
      "Total loss:  -0.9662 | PDE Loss:  -1.5928 | Function Loss:  -2.0832\n",
      "Total loss:  -0.9662 | PDE Loss:  -1.5929 | Function Loss:  -2.0832\n",
      "Total loss:  -0.9662 | PDE Loss:  -1.5929 | Function Loss:  -2.0833\n",
      "Total loss:  -0.9663 | PDE Loss:  -1.5929 | Function Loss:  -2.0834\n",
      "Total loss:  -0.9664 | PDE Loss:  -1.5927 | Function Loss:  -2.0835\n",
      "Total loss:  -0.9664 | PDE Loss:  -1.5921 | Function Loss:  -2.0837\n",
      "Total loss:  -0.9665 | PDE Loss:  -1.5922 | Function Loss:  -2.0838\n",
      "Total loss:  -0.9665 | PDE Loss:  -1.5921 | Function Loss:  -2.0839\n",
      "Total loss:  -0.9666 | PDE Loss:  -1.5921 | Function Loss:  -2.084\n",
      "Total loss:  -0.9666 | PDE Loss:  -1.592 | Function Loss:  -2.084\n",
      "Total loss:  -0.9667 | PDE Loss:  -1.5919 | Function Loss:  -2.0841\n",
      "Total loss:  -0.9667 | PDE Loss:  -1.5919 | Function Loss:  -2.0842\n",
      "Total loss:  -0.9663 | PDE Loss:  -1.5883 | Function Loss:  -2.0847\n",
      "Total loss:  -0.9668 | PDE Loss:  -1.5915 | Function Loss:  -2.0844\n",
      "Total loss:  -0.9668 | PDE Loss:  -1.5916 | Function Loss:  -2.0844\n",
      "Total loss:  -0.9669 | PDE Loss:  -1.5917 | Function Loss:  -2.0845\n",
      "Total loss:  -0.9669 | PDE Loss:  -1.5918 | Function Loss:  -2.0845\n",
      "Total loss:  -0.967 | PDE Loss:  -1.5918 | Function Loss:  -2.0846\n",
      "Total loss:  -0.9671 | PDE Loss:  -1.5922 | Function Loss:  -2.0846\n",
      "Total loss:  -0.9671 | PDE Loss:  -1.5917 | Function Loss:  -2.0848\n",
      "Total loss:  -0.9672 | PDE Loss:  -1.5916 | Function Loss:  -2.0849\n",
      "Total loss:  -0.9673 | PDE Loss:  -1.5913 | Function Loss:  -2.0852\n",
      "Total loss:  -0.9674 | PDE Loss:  -1.5911 | Function Loss:  -2.0854\n",
      "Total loss:  -0.9675 | PDE Loss:  -1.5908 | Function Loss:  -2.0855\n",
      "Total loss:  -0.9675 | PDE Loss:  -1.5907 | Function Loss:  -2.0857\n",
      "Total loss:  -0.9676 | PDE Loss:  -1.5906 | Function Loss:  -2.0858\n",
      "Total loss:  -0.9677 | PDE Loss:  -1.5903 | Function Loss:  -2.086\n",
      "Total loss:  -0.9678 | PDE Loss:  -1.5903 | Function Loss:  -2.0861\n",
      "Total loss:  -0.9679 | PDE Loss:  -1.5901 | Function Loss:  -2.0863\n",
      "Total loss:  -0.9679 | PDE Loss:  -1.59 | Function Loss:  -2.0864\n",
      "Total loss:  -0.968 | PDE Loss:  -1.59 | Function Loss:  -2.0865\n",
      "Total loss:  -0.9679 | PDE Loss:  -1.5899 | Function Loss:  -2.0864\n",
      "Total loss:  -0.968 | PDE Loss:  -1.5901 | Function Loss:  -2.0865\n",
      "Total loss:  -0.9681 | PDE Loss:  -1.59 | Function Loss:  -2.0866\n",
      "Total loss:  -0.9682 | PDE Loss:  -1.5899 | Function Loss:  -2.0867\n",
      "Total loss:  -0.9682 | PDE Loss:  -1.5898 | Function Loss:  -2.0869\n",
      "Total loss:  -0.9683 | PDE Loss:  -1.5896 | Function Loss:  -2.087\n",
      "Total loss:  -0.9684 | PDE Loss:  -1.5893 | Function Loss:  -2.0872\n",
      "Total loss:  -0.9685 | PDE Loss:  -1.589 | Function Loss:  -2.0875\n",
      "Total loss:  -0.9686 | PDE Loss:  -1.5886 | Function Loss:  -2.0877\n",
      "Total loss:  -0.968 | PDE Loss:  -1.5857 | Function Loss:  -2.0879\n",
      "Total loss:  -0.9687 | PDE Loss:  -1.5883 | Function Loss:  -2.0879\n",
      "Total loss:  -0.9687 | PDE Loss:  -1.5882 | Function Loss:  -2.0881\n",
      "Total loss:  -0.9688 | PDE Loss:  -1.5881 | Function Loss:  -2.0882\n",
      "Total loss:  -0.9689 | PDE Loss:  -1.588 | Function Loss:  -2.0884\n",
      "Total loss:  -0.969 | PDE Loss:  -1.5879 | Function Loss:  -2.0885\n",
      "Total loss:  -0.9691 | PDE Loss:  -1.5877 | Function Loss:  -2.0886\n",
      "Total loss:  -0.9691 | PDE Loss:  -1.5875 | Function Loss:  -2.0888\n",
      "Total loss:  -0.9692 | PDE Loss:  -1.5872 | Function Loss:  -2.089\n",
      "Total loss:  -0.9649 | PDE Loss:  -1.5741 | Function Loss:  -2.0876\n",
      "Total loss:  -0.9692 | PDE Loss:  -1.5868 | Function Loss:  -2.0891\n",
      "Total loss:  -0.9693 | PDE Loss:  -1.5866 | Function Loss:  -2.0893\n",
      "Total loss:  -0.9694 | PDE Loss:  -1.5865 | Function Loss:  -2.0894\n",
      "Total loss:  -0.9695 | PDE Loss:  -1.5863 | Function Loss:  -2.0896\n",
      "Total loss:  -0.9696 | PDE Loss:  -1.5863 | Function Loss:  -2.0897\n",
      "Total loss:  -0.9696 | PDE Loss:  -1.5862 | Function Loss:  -2.0898\n",
      "Total loss:  -0.9697 | PDE Loss:  -1.5862 | Function Loss:  -2.0899\n",
      "Total loss:  -0.9698 | PDE Loss:  -1.5862 | Function Loss:  -2.09\n",
      "Total loss:  -0.9698 | PDE Loss:  -1.5865 | Function Loss:  -2.09\n",
      "Total loss:  -0.9699 | PDE Loss:  -1.5867 | Function Loss:  -2.09\n",
      "Total loss:  -0.97 | PDE Loss:  -1.5869 | Function Loss:  -2.09\n",
      "Total loss:  -0.97 | PDE Loss:  -1.5876 | Function Loss:  -2.0899\n",
      "Total loss:  -0.9701 | PDE Loss:  -1.5877 | Function Loss:  -2.09\n",
      "Total loss:  -0.9702 | PDE Loss:  -1.5876 | Function Loss:  -2.0901\n",
      "Total loss:  -0.9702 | PDE Loss:  -1.5874 | Function Loss:  -2.0902\n",
      "Total loss:  -0.9703 | PDE Loss:  -1.5873 | Function Loss:  -2.0904\n",
      "Total loss:  -0.9704 | PDE Loss:  -1.5872 | Function Loss:  -2.0905\n",
      "Total loss:  -0.9705 | PDE Loss:  -1.587 | Function Loss:  -2.0907\n",
      "Total loss:  -0.9706 | PDE Loss:  -1.5869 | Function Loss:  -2.0909\n",
      "Total loss:  -0.9704 | PDE Loss:  -1.585 | Function Loss:  -2.0912\n",
      "Total loss:  -0.9706 | PDE Loss:  -1.5866 | Function Loss:  -2.091\n",
      "Total loss:  -0.9707 | PDE Loss:  -1.5866 | Function Loss:  -2.0912\n",
      "Total loss:  -0.9708 | PDE Loss:  -1.5866 | Function Loss:  -2.0913\n",
      "Total loss:  -0.9709 | PDE Loss:  -1.5866 | Function Loss:  -2.0914\n",
      "Total loss:  -0.971 | PDE Loss:  -1.5865 | Function Loss:  -2.0916\n",
      "Total loss:  -0.9711 | PDE Loss:  -1.5866 | Function Loss:  -2.0916\n",
      "Total loss:  -0.9711 | PDE Loss:  -1.5866 | Function Loss:  -2.0917\n",
      "Total loss:  -0.9712 | PDE Loss:  -1.5867 | Function Loss:  -2.0917\n",
      "Total loss:  -0.9712 | PDE Loss:  -1.5866 | Function Loss:  -2.0918\n",
      "Total loss:  -0.9713 | PDE Loss:  -1.5867 | Function Loss:  -2.0918\n",
      "Total loss:  -0.9713 | PDE Loss:  -1.5866 | Function Loss:  -2.0919\n",
      "Total loss:  -0.9714 | PDE Loss:  -1.5865 | Function Loss:  -2.092\n",
      "Total loss:  -0.9714 | PDE Loss:  -1.5862 | Function Loss:  -2.0922\n",
      "Total loss:  -0.9715 | PDE Loss:  -1.5861 | Function Loss:  -2.0923\n",
      "Total loss:  -0.9715 | PDE Loss:  -1.5859 | Function Loss:  -2.0925\n",
      "Total loss:  -0.9716 | PDE Loss:  -1.5857 | Function Loss:  -2.0926\n",
      "Total loss:  -0.9717 | PDE Loss:  -1.5856 | Function Loss:  -2.0927\n",
      "Total loss:  -0.9717 | PDE Loss:  -1.5851 | Function Loss:  -2.093\n",
      "Total loss:  -0.9718 | PDE Loss:  -1.5852 | Function Loss:  -2.093\n",
      "Total loss:  -0.9719 | PDE Loss:  -1.5853 | Function Loss:  -2.0931\n",
      "Total loss:  -0.972 | PDE Loss:  -1.5855 | Function Loss:  -2.0931\n",
      "Total loss:  -0.9721 | PDE Loss:  -1.5857 | Function Loss:  -2.0932\n",
      "Total loss:  -0.9722 | PDE Loss:  -1.5859 | Function Loss:  -2.0934\n",
      "Total loss:  -0.9724 | PDE Loss:  -1.5861 | Function Loss:  -2.0935\n",
      "Total loss:  -0.9725 | PDE Loss:  -1.5863 | Function Loss:  -2.0936\n",
      "Total loss:  -0.9726 | PDE Loss:  -1.5864 | Function Loss:  -2.0938\n",
      "Total loss:  -0.9727 | PDE Loss:  -1.5863 | Function Loss:  -2.0939\n",
      "Total loss:  -0.9728 | PDE Loss:  -1.5862 | Function Loss:  -2.094\n",
      "Total loss:  -0.9728 | PDE Loss:  -1.5861 | Function Loss:  -2.0941\n",
      "Total loss:  -0.9729 | PDE Loss:  -1.586 | Function Loss:  -2.0942\n",
      "Total loss:  -0.9729 | PDE Loss:  -1.586 | Function Loss:  -2.0943\n",
      "Total loss:  -0.973 | PDE Loss:  -1.586 | Function Loss:  -2.0944\n",
      "Total loss:  -0.9731 | PDE Loss:  -1.5859 | Function Loss:  -2.0945\n",
      "Total loss:  -0.9732 | PDE Loss:  -1.5862 | Function Loss:  -2.0946\n",
      "Total loss:  -0.9733 | PDE Loss:  -1.5864 | Function Loss:  -2.0947\n",
      "Total loss:  -0.9734 | PDE Loss:  -1.5864 | Function Loss:  -2.0947\n",
      "Total loss:  -0.9735 | PDE Loss:  -1.5868 | Function Loss:  -2.0947\n",
      "Total loss:  -0.9735 | PDE Loss:  -1.5871 | Function Loss:  -2.0947\n",
      "Total loss:  -0.9736 | PDE Loss:  -1.5876 | Function Loss:  -2.0946\n",
      "Total loss:  -0.9736 | PDE Loss:  -1.5877 | Function Loss:  -2.0946\n",
      "Total loss:  -0.9736 | PDE Loss:  -1.5879 | Function Loss:  -2.0946\n",
      "Total loss:  -0.9737 | PDE Loss:  -1.5879 | Function Loss:  -2.0947\n",
      "Total loss:  -0.9738 | PDE Loss:  -1.5878 | Function Loss:  -2.0948\n",
      "Total loss:  -0.9738 | PDE Loss:  -1.5877 | Function Loss:  -2.0949\n",
      "Total loss:  -0.9739 | PDE Loss:  -1.5874 | Function Loss:  -2.0951\n",
      "Total loss:  -0.9739 | PDE Loss:  -1.5873 | Function Loss:  -2.0951\n",
      "Total loss:  -0.9739 | PDE Loss:  -1.5872 | Function Loss:  -2.0952\n",
      "Total loss:  -0.974 | PDE Loss:  -1.5872 | Function Loss:  -2.0953\n",
      "Total loss:  -0.974 | PDE Loss:  -1.5871 | Function Loss:  -2.0954\n",
      "Total loss:  -0.9741 | PDE Loss:  -1.5871 | Function Loss:  -2.0954\n",
      "Total loss:  -0.9741 | PDE Loss:  -1.587 | Function Loss:  -2.0955\n",
      "Total loss:  -0.9742 | PDE Loss:  -1.5871 | Function Loss:  -2.0955\n",
      "Total loss:  -0.9742 | PDE Loss:  -1.5871 | Function Loss:  -2.0956\n",
      "Total loss:  -0.9742 | PDE Loss:  -1.5871 | Function Loss:  -2.0956\n",
      "Total loss:  -0.9743 | PDE Loss:  -1.5871 | Function Loss:  -2.0957\n",
      "Total loss:  -0.9743 | PDE Loss:  -1.5871 | Function Loss:  -2.0957\n",
      "Total loss:  -0.9743 | PDE Loss:  -1.587 | Function Loss:  -2.0958\n",
      "Total loss:  -0.9744 | PDE Loss:  -1.5868 | Function Loss:  -2.0959\n",
      "Total loss:  -0.9744 | PDE Loss:  -1.5867 | Function Loss:  -2.096\n",
      "Total loss:  -0.9744 | PDE Loss:  -1.5857 | Function Loss:  -2.0963\n",
      "Total loss:  -0.9745 | PDE Loss:  -1.5863 | Function Loss:  -2.0962\n",
      "Total loss:  -0.9745 | PDE Loss:  -1.5861 | Function Loss:  -2.0963\n",
      "Total loss:  -0.9745 | PDE Loss:  -1.5855 | Function Loss:  -2.0965\n",
      "Total loss:  -0.9745 | PDE Loss:  -1.5859 | Function Loss:  -2.0964\n",
      "Total loss:  -0.9746 | PDE Loss:  -1.5859 | Function Loss:  -2.0965\n",
      "Total loss:  -0.9747 | PDE Loss:  -1.5857 | Function Loss:  -2.0966\n",
      "Total loss:  -0.9748 | PDE Loss:  -1.5858 | Function Loss:  -2.0968\n",
      "Total loss:  -0.9749 | PDE Loss:  -1.5859 | Function Loss:  -2.0969\n",
      "Total loss:  -0.975 | PDE Loss:  -1.5861 | Function Loss:  -2.097\n",
      "Total loss:  -0.9751 | PDE Loss:  -1.5863 | Function Loss:  -2.097\n",
      "Total loss:  -0.9752 | PDE Loss:  -1.5859 | Function Loss:  -2.0973\n",
      "Total loss:  -0.9753 | PDE Loss:  -1.5861 | Function Loss:  -2.0973\n",
      "Total loss:  -0.9754 | PDE Loss:  -1.5866 | Function Loss:  -2.0974\n",
      "Total loss:  -0.9755 | PDE Loss:  -1.5867 | Function Loss:  -2.0974\n",
      "Total loss:  -0.9756 | PDE Loss:  -1.5868 | Function Loss:  -2.0975\n",
      "Total loss:  -0.9757 | PDE Loss:  -1.5868 | Function Loss:  -2.0976\n",
      "Total loss:  -0.9757 | PDE Loss:  -1.5869 | Function Loss:  -2.0977\n",
      "Total loss:  -0.9758 | PDE Loss:  -1.5869 | Function Loss:  -2.0978\n",
      "Total loss:  -0.9759 | PDE Loss:  -1.587 | Function Loss:  -2.0979\n",
      "Total loss:  -0.9759 | PDE Loss:  -1.587 | Function Loss:  -2.0979\n",
      "Total loss:  -0.976 | PDE Loss:  -1.5872 | Function Loss:  -2.098\n",
      "Total loss:  -0.9762 | PDE Loss:  -1.5876 | Function Loss:  -2.098\n",
      "Total loss:  -0.9763 | PDE Loss:  -1.5878 | Function Loss:  -2.0981\n",
      "Total loss:  -0.9764 | PDE Loss:  -1.5881 | Function Loss:  -2.0982\n",
      "Total loss:  -0.9765 | PDE Loss:  -1.5886 | Function Loss:  -2.0982\n",
      "Total loss:  -0.9767 | PDE Loss:  -1.5891 | Function Loss:  -2.0982\n",
      "Total loss:  -0.9768 | PDE Loss:  -1.5897 | Function Loss:  -2.0982\n",
      "Total loss:  -0.977 | PDE Loss:  -1.59 | Function Loss:  -2.0983\n",
      "Total loss:  -0.977 | PDE Loss:  -1.5908 | Function Loss:  -2.0982\n",
      "Total loss:  -0.9771 | PDE Loss:  -1.591 | Function Loss:  -2.0982\n",
      "Total loss:  -0.9772 | PDE Loss:  -1.5913 | Function Loss:  -2.0982\n",
      "Total loss:  -0.9774 | PDE Loss:  -1.5919 | Function Loss:  -2.0982\n",
      "Total loss:  -0.9776 | PDE Loss:  -1.5926 | Function Loss:  -2.0983\n",
      "Total loss:  -0.9777 | PDE Loss:  -1.5932 | Function Loss:  -2.0983\n",
      "Total loss:  -0.9779 | PDE Loss:  -1.5936 | Function Loss:  -2.0984\n",
      "Total loss:  -0.978 | PDE Loss:  -1.5937 | Function Loss:  -2.0985\n",
      "Total loss:  -0.978 | PDE Loss:  -1.5946 | Function Loss:  -2.0982\n",
      "Total loss:  -0.9781 | PDE Loss:  -1.5943 | Function Loss:  -2.0984\n",
      "Total loss:  -0.9782 | PDE Loss:  -1.5941 | Function Loss:  -2.0986\n",
      "Total loss:  -0.9782 | PDE Loss:  -1.5939 | Function Loss:  -2.0987\n",
      "Total loss:  -0.9783 | PDE Loss:  -1.5937 | Function Loss:  -2.0989\n",
      "Total loss:  -0.9784 | PDE Loss:  -1.5933 | Function Loss:  -2.0991\n",
      "Total loss:  -0.9785 | PDE Loss:  -1.593 | Function Loss:  -2.0994\n",
      "Total loss:  -0.9786 | PDE Loss:  -1.5923 | Function Loss:  -2.0997\n",
      "Total loss:  -0.9775 | PDE Loss:  -1.5913 | Function Loss:  -2.0986\n",
      "Total loss:  -0.9786 | PDE Loss:  -1.5926 | Function Loss:  -2.0996\n",
      "Total loss:  -0.9787 | PDE Loss:  -1.5921 | Function Loss:  -2.0999\n",
      "Total loss:  -0.9788 | PDE Loss:  -1.5916 | Function Loss:  -2.1002\n",
      "Total loss:  -0.9788 | PDE Loss:  -1.5913 | Function Loss:  -2.1004\n",
      "Total loss:  -0.9789 | PDE Loss:  -1.591 | Function Loss:  -2.1006\n",
      "Total loss:  -0.979 | PDE Loss:  -1.5909 | Function Loss:  -2.1007\n",
      "Total loss:  -0.979 | PDE Loss:  -1.5909 | Function Loss:  -2.1007\n",
      "Total loss:  -0.979 | PDE Loss:  -1.5909 | Function Loss:  -2.1008\n",
      "Total loss:  -0.9791 | PDE Loss:  -1.591 | Function Loss:  -2.1008\n",
      "Total loss:  -0.9791 | PDE Loss:  -1.591 | Function Loss:  -2.1009\n",
      "Total loss:  -0.9792 | PDE Loss:  -1.5909 | Function Loss:  -2.101\n",
      "Total loss:  -0.9792 | PDE Loss:  -1.5898 | Function Loss:  -2.1014\n",
      "Total loss:  -0.9794 | PDE Loss:  -1.5899 | Function Loss:  -2.1015\n",
      "Total loss:  -0.9794 | PDE Loss:  -1.5899 | Function Loss:  -2.1016\n",
      "Total loss:  -0.9795 | PDE Loss:  -1.59 | Function Loss:  -2.1016\n",
      "Total loss:  -0.9795 | PDE Loss:  -1.5899 | Function Loss:  -2.1017\n",
      "Total loss:  -0.9796 | PDE Loss:  -1.5899 | Function Loss:  -2.1018\n",
      "Total loss:  -0.9796 | PDE Loss:  -1.59 | Function Loss:  -2.1018\n",
      "Total loss:  -0.9796 | PDE Loss:  -1.5899 | Function Loss:  -2.1018\n",
      "Total loss:  -0.9797 | PDE Loss:  -1.5899 | Function Loss:  -2.1019\n",
      "Total loss:  -0.9797 | PDE Loss:  -1.5899 | Function Loss:  -2.102\n",
      "Total loss:  -0.9798 | PDE Loss:  -1.5901 | Function Loss:  -2.102\n",
      "Total loss:  -0.9799 | PDE Loss:  -1.5907 | Function Loss:  -2.102\n",
      "Total loss:  -0.98 | PDE Loss:  -1.591 | Function Loss:  -2.1021\n",
      "Total loss:  -0.9801 | PDE Loss:  -1.592 | Function Loss:  -2.1018\n",
      "Total loss:  -0.9802 | PDE Loss:  -1.5922 | Function Loss:  -2.1018\n",
      "Total loss:  -0.9803 | PDE Loss:  -1.5925 | Function Loss:  -2.1019\n",
      "Total loss:  -0.9803 | PDE Loss:  -1.5927 | Function Loss:  -2.1019\n",
      "Total loss:  -0.9804 | PDE Loss:  -1.593 | Function Loss:  -2.1019\n",
      "Total loss:  -0.9804 | PDE Loss:  -1.5934 | Function Loss:  -2.1018\n",
      "Total loss:  -0.9805 | PDE Loss:  -1.5929 | Function Loss:  -2.102\n",
      "Total loss:  -0.9806 | PDE Loss:  -1.5945 | Function Loss:  -2.1017\n",
      "Total loss:  -0.9807 | PDE Loss:  -1.5943 | Function Loss:  -2.1018\n",
      "Total loss:  -0.9808 | PDE Loss:  -1.5939 | Function Loss:  -2.1021\n",
      "Total loss:  -0.9809 | PDE Loss:  -1.5937 | Function Loss:  -2.1023\n",
      "Total loss:  -0.9809 | PDE Loss:  -1.5936 | Function Loss:  -2.1024\n",
      "Total loss:  -0.981 | PDE Loss:  -1.5935 | Function Loss:  -2.1025\n",
      "Total loss:  -0.981 | PDE Loss:  -1.5933 | Function Loss:  -2.1026\n",
      "Total loss:  -0.9811 | PDE Loss:  -1.5932 | Function Loss:  -2.1027\n",
      "Total loss:  -0.9812 | PDE Loss:  -1.5932 | Function Loss:  -2.1028\n",
      "Total loss:  -0.9812 | PDE Loss:  -1.5927 | Function Loss:  -2.103\n",
      "Total loss:  -0.9812 | PDE Loss:  -1.5931 | Function Loss:  -2.1029\n",
      "Total loss:  -0.9813 | PDE Loss:  -1.5931 | Function Loss:  -2.103\n",
      "Total loss:  -0.9813 | PDE Loss:  -1.5931 | Function Loss:  -2.1031\n",
      "Total loss:  -0.9814 | PDE Loss:  -1.593 | Function Loss:  -2.1031\n",
      "Total loss:  -0.9814 | PDE Loss:  -1.5931 | Function Loss:  -2.1032\n",
      "Total loss:  -0.9814 | PDE Loss:  -1.5929 | Function Loss:  -2.1033\n",
      "Total loss:  -0.9814 | PDE Loss:  -1.593 | Function Loss:  -2.1033\n",
      "Total loss:  -0.9815 | PDE Loss:  -1.593 | Function Loss:  -2.1033\n",
      "Total loss:  -0.9815 | PDE Loss:  -1.5934 | Function Loss:  -2.1033\n",
      "Total loss:  -0.9816 | PDE Loss:  -1.5933 | Function Loss:  -2.1034\n",
      "Total loss:  -0.9817 | PDE Loss:  -1.5937 | Function Loss:  -2.1033\n",
      "Total loss:  -0.9817 | PDE Loss:  -1.5938 | Function Loss:  -2.1034\n",
      "Total loss:  -0.9818 | PDE Loss:  -1.5941 | Function Loss:  -2.1034\n",
      "Total loss:  -0.9819 | PDE Loss:  -1.5945 | Function Loss:  -2.1033\n",
      "Total loss:  -0.9819 | PDE Loss:  -1.5947 | Function Loss:  -2.1033\n",
      "Total loss:  -0.9819 | PDE Loss:  -1.5949 | Function Loss:  -2.1033\n",
      "Total loss:  -0.982 | PDE Loss:  -1.5951 | Function Loss:  -2.1033\n",
      "Total loss:  -0.982 | PDE Loss:  -1.5954 | Function Loss:  -2.1032\n",
      "Total loss:  -0.982 | PDE Loss:  -1.5956 | Function Loss:  -2.1032\n",
      "Total loss:  -0.9821 | PDE Loss:  -1.5959 | Function Loss:  -2.1032\n",
      "Total loss:  -0.9821 | PDE Loss:  -1.5961 | Function Loss:  -2.1032\n",
      "Total loss:  -0.9822 | PDE Loss:  -1.5963 | Function Loss:  -2.1032\n",
      "Total loss:  -0.9822 | PDE Loss:  -1.5963 | Function Loss:  -2.1032\n",
      "Total loss:  -0.9822 | PDE Loss:  -1.5963 | Function Loss:  -2.1032\n",
      "Total loss:  -0.9822 | PDE Loss:  -1.5962 | Function Loss:  -2.1033\n",
      "Total loss:  -0.9822 | PDE Loss:  -1.5961 | Function Loss:  -2.1033\n",
      "Total loss:  -0.9823 | PDE Loss:  -1.5959 | Function Loss:  -2.1034\n",
      "Total loss:  -0.9824 | PDE Loss:  -1.5959 | Function Loss:  -2.1035\n",
      "Total loss:  -0.9823 | PDE Loss:  -1.5937 | Function Loss:  -2.1042\n",
      "Total loss:  -0.9824 | PDE Loss:  -1.5951 | Function Loss:  -2.1039\n",
      "Total loss:  -0.9825 | PDE Loss:  -1.5953 | Function Loss:  -2.1039\n",
      "Total loss:  -0.9826 | PDE Loss:  -1.5957 | Function Loss:  -2.1039\n",
      "Total loss:  -0.9827 | PDE Loss:  -1.5961 | Function Loss:  -2.1039\n",
      "Total loss:  -0.9827 | PDE Loss:  -1.5964 | Function Loss:  -2.1039\n",
      "Total loss:  -0.9828 | PDE Loss:  -1.5966 | Function Loss:  -2.1039\n",
      "Total loss:  -0.9829 | PDE Loss:  -1.5965 | Function Loss:  -2.104\n",
      "Total loss:  -0.983 | PDE Loss:  -1.5964 | Function Loss:  -2.1042\n",
      "Total loss:  -0.9829 | PDE Loss:  -1.5963 | Function Loss:  -2.1042\n",
      "Total loss:  -0.983 | PDE Loss:  -1.5965 | Function Loss:  -2.1042\n",
      "Total loss:  -0.9831 | PDE Loss:  -1.5964 | Function Loss:  -2.1043\n",
      "Total loss:  -0.9832 | PDE Loss:  -1.5963 | Function Loss:  -2.1045\n",
      "Total loss:  -0.9833 | PDE Loss:  -1.5962 | Function Loss:  -2.1046\n",
      "Total loss:  -0.9834 | PDE Loss:  -1.5963 | Function Loss:  -2.1047\n",
      "Total loss:  -0.9834 | PDE Loss:  -1.5966 | Function Loss:  -2.1048\n",
      "Total loss:  -0.9835 | PDE Loss:  -1.5965 | Function Loss:  -2.1049\n",
      "Total loss:  -0.9836 | PDE Loss:  -1.5972 | Function Loss:  -2.1048\n",
      "Total loss:  -0.9837 | PDE Loss:  -1.5975 | Function Loss:  -2.1047\n",
      "Total loss:  -0.9837 | PDE Loss:  -1.598 | Function Loss:  -2.1047\n",
      "Total loss:  -0.9838 | PDE Loss:  -1.5985 | Function Loss:  -2.1046\n",
      "Total loss:  -0.9839 | PDE Loss:  -1.5988 | Function Loss:  -2.1046\n",
      "Total loss:  -0.984 | PDE Loss:  -1.5991 | Function Loss:  -2.1047\n",
      "Total loss:  -0.984 | PDE Loss:  -1.5988 | Function Loss:  -2.1047\n",
      "Total loss:  -0.984 | PDE Loss:  -1.599 | Function Loss:  -2.1047\n",
      "Total loss:  -0.9841 | PDE Loss:  -1.5993 | Function Loss:  -2.1048\n",
      "Total loss:  -0.9842 | PDE Loss:  -1.5993 | Function Loss:  -2.1048\n",
      "Total loss:  -0.9842 | PDE Loss:  -1.5992 | Function Loss:  -2.105\n",
      "Total loss:  -0.9843 | PDE Loss:  -1.5992 | Function Loss:  -2.105\n",
      "Total loss:  -0.9843 | PDE Loss:  -1.5992 | Function Loss:  -2.1051\n",
      "Total loss:  -0.9844 | PDE Loss:  -1.5991 | Function Loss:  -2.1052\n",
      "Total loss:  -0.9844 | PDE Loss:  -1.5992 | Function Loss:  -2.1052\n",
      "Total loss:  -0.9844 | PDE Loss:  -1.5992 | Function Loss:  -2.1052\n",
      "Total loss:  -0.9845 | PDE Loss:  -1.5993 | Function Loss:  -2.1052\n",
      "Total loss:  -0.9845 | PDE Loss:  -1.5994 | Function Loss:  -2.1052\n",
      "Total loss:  -0.9845 | PDE Loss:  -1.5995 | Function Loss:  -2.1052\n",
      "Total loss:  -0.9845 | PDE Loss:  -1.5996 | Function Loss:  -2.1052\n",
      "Total loss:  -0.9846 | PDE Loss:  -1.5996 | Function Loss:  -2.1052\n",
      "Total loss:  -0.9845 | PDE Loss:  -1.5995 | Function Loss:  -2.1052\n",
      "Total loss:  -0.9846 | PDE Loss:  -1.5997 | Function Loss:  -2.1053\n",
      "Total loss:  -0.9846 | PDE Loss:  -1.5997 | Function Loss:  -2.1053\n",
      "Total loss:  -0.9847 | PDE Loss:  -1.5997 | Function Loss:  -2.1053\n",
      "Total loss:  -0.9847 | PDE Loss:  -1.5998 | Function Loss:  -2.1054\n",
      "Total loss:  -0.9848 | PDE Loss:  -1.5998 | Function Loss:  -2.1055\n",
      "Total loss:  -0.9849 | PDE Loss:  -1.5999 | Function Loss:  -2.1056\n",
      "Total loss:  -0.9849 | PDE Loss:  -1.6006 | Function Loss:  -2.1054\n",
      "Total loss:  -0.9849 | PDE Loss:  -1.6003 | Function Loss:  -2.1055\n",
      "Total loss:  -0.985 | PDE Loss:  -1.6004 | Function Loss:  -2.1056\n",
      "Total loss:  -0.985 | PDE Loss:  -1.6005 | Function Loss:  -2.1056\n",
      "Total loss:  -0.9849 | PDE Loss:  -1.6 | Function Loss:  -2.1055\n",
      "Total loss:  -0.985 | PDE Loss:  -1.6005 | Function Loss:  -2.1056\n",
      "Total loss:  -0.9851 | PDE Loss:  -1.6007 | Function Loss:  -2.1056\n",
      "Total loss:  -0.9851 | PDE Loss:  -1.601 | Function Loss:  -2.1056\n",
      "Total loss:  -0.9852 | PDE Loss:  -1.6013 | Function Loss:  -2.1055\n",
      "Total loss:  -0.9852 | PDE Loss:  -1.6017 | Function Loss:  -2.1055\n",
      "Total loss:  -0.9853 | PDE Loss:  -1.6021 | Function Loss:  -2.1054\n",
      "Total loss:  -0.9853 | PDE Loss:  -1.6025 | Function Loss:  -2.1054\n",
      "Total loss:  -0.9854 | PDE Loss:  -1.603 | Function Loss:  -2.1053\n",
      "Total loss:  -0.9855 | PDE Loss:  -1.6031 | Function Loss:  -2.1053\n",
      "Total loss:  -0.9855 | PDE Loss:  -1.6031 | Function Loss:  -2.1054\n",
      "Total loss:  -0.9856 | PDE Loss:  -1.603 | Function Loss:  -2.1055\n",
      "Total loss:  -0.9856 | PDE Loss:  -1.6029 | Function Loss:  -2.1056\n",
      "Total loss:  -0.9857 | PDE Loss:  -1.6028 | Function Loss:  -2.1057\n",
      "Total loss:  -0.9857 | PDE Loss:  -1.6029 | Function Loss:  -2.1058\n",
      "Total loss:  -0.9858 | PDE Loss:  -1.603 | Function Loss:  -2.1058\n",
      "Total loss:  -0.9858 | PDE Loss:  -1.6031 | Function Loss:  -2.1058\n",
      "Total loss:  -0.9859 | PDE Loss:  -1.6033 | Function Loss:  -2.1058\n",
      "Total loss:  -0.9859 | PDE Loss:  -1.6036 | Function Loss:  -2.1058\n",
      "Total loss:  -0.986 | PDE Loss:  -1.6038 | Function Loss:  -2.1058\n",
      "Total loss:  -0.986 | PDE Loss:  -1.6041 | Function Loss:  -2.1058\n",
      "Total loss:  -0.9861 | PDE Loss:  -1.6043 | Function Loss:  -2.1057\n",
      "Total loss:  -0.9861 | PDE Loss:  -1.6045 | Function Loss:  -2.1057\n",
      "Total loss:  -0.9861 | PDE Loss:  -1.605 | Function Loss:  -2.1056\n",
      "Total loss:  -0.9862 | PDE Loss:  -1.605 | Function Loss:  -2.1058\n",
      "Total loss:  -0.9863 | PDE Loss:  -1.6051 | Function Loss:  -2.1058\n",
      "Total loss:  -0.9864 | PDE Loss:  -1.6048 | Function Loss:  -2.106\n",
      "Total loss:  -0.9864 | PDE Loss:  -1.6046 | Function Loss:  -2.1061\n",
      "Total loss:  -0.9864 | PDE Loss:  -1.6044 | Function Loss:  -2.1062\n",
      "Total loss:  -0.9865 | PDE Loss:  -1.6044 | Function Loss:  -2.1063\n",
      "Total loss:  -0.9865 | PDE Loss:  -1.6043 | Function Loss:  -2.1063\n",
      "Total loss:  -0.9866 | PDE Loss:  -1.6044 | Function Loss:  -2.1064\n",
      "Total loss:  -0.9866 | PDE Loss:  -1.6045 | Function Loss:  -2.1064\n",
      "Total loss:  -0.9867 | PDE Loss:  -1.605 | Function Loss:  -2.1064\n",
      "Total loss:  -0.9868 | PDE Loss:  -1.605 | Function Loss:  -2.1065\n",
      "Total loss:  -0.9869 | PDE Loss:  -1.6055 | Function Loss:  -2.1065\n",
      "Total loss:  -0.987 | PDE Loss:  -1.6063 | Function Loss:  -2.1064\n",
      "Total loss:  -0.9871 | PDE Loss:  -1.6067 | Function Loss:  -2.1063\n",
      "Total loss:  -0.9872 | PDE Loss:  -1.6074 | Function Loss:  -2.1062\n",
      "Total loss:  -0.9873 | PDE Loss:  -1.6074 | Function Loss:  -2.1064\n",
      "Total loss:  -0.9873 | PDE Loss:  -1.6075 | Function Loss:  -2.1064\n",
      "Total loss:  -0.9874 | PDE Loss:  -1.6078 | Function Loss:  -2.1064\n",
      "Total loss:  -0.9875 | PDE Loss:  -1.608 | Function Loss:  -2.1064\n",
      "Total loss:  -0.9875 | PDE Loss:  -1.6081 | Function Loss:  -2.1065\n",
      "Total loss:  -0.9876 | PDE Loss:  -1.6081 | Function Loss:  -2.1066\n",
      "Total loss:  -0.9877 | PDE Loss:  -1.6081 | Function Loss:  -2.1066\n",
      "Total loss:  -0.9877 | PDE Loss:  -1.6084 | Function Loss:  -2.1066\n",
      "Total loss:  -0.9878 | PDE Loss:  -1.6072 | Function Loss:  -2.1071\n",
      "Total loss:  -0.9879 | PDE Loss:  -1.608 | Function Loss:  -2.107\n",
      "Total loss:  -0.988 | PDE Loss:  -1.6084 | Function Loss:  -2.107\n",
      "Total loss:  -0.9881 | PDE Loss:  -1.6087 | Function Loss:  -2.107\n",
      "Total loss:  -0.9882 | PDE Loss:  -1.6088 | Function Loss:  -2.1071\n",
      "Total loss:  -0.9883 | PDE Loss:  -1.6088 | Function Loss:  -2.1073\n",
      "Total loss:  -0.9884 | PDE Loss:  -1.6084 | Function Loss:  -2.1076\n",
      "Total loss:  -0.9886 | PDE Loss:  -1.6083 | Function Loss:  -2.1077\n",
      "Total loss:  -0.9887 | PDE Loss:  -1.6078 | Function Loss:  -2.1081\n",
      "Total loss:  -0.9888 | PDE Loss:  -1.6073 | Function Loss:  -2.1084\n",
      "Total loss:  -0.9889 | PDE Loss:  -1.6067 | Function Loss:  -2.1087\n",
      "Total loss:  -0.989 | PDE Loss:  -1.6065 | Function Loss:  -2.1089\n",
      "Total loss:  -0.989 | PDE Loss:  -1.6065 | Function Loss:  -2.1089\n",
      "Total loss:  -0.989 | PDE Loss:  -1.6066 | Function Loss:  -2.1089\n",
      "Total loss:  -0.9891 | PDE Loss:  -1.6067 | Function Loss:  -2.1089\n",
      "Total loss:  -0.9892 | PDE Loss:  -1.6069 | Function Loss:  -2.109\n",
      "Total loss:  -0.9893 | PDE Loss:  -1.6071 | Function Loss:  -2.109\n",
      "Total loss:  -0.9894 | PDE Loss:  -1.6071 | Function Loss:  -2.1092\n",
      "Total loss:  -0.9895 | PDE Loss:  -1.6081 | Function Loss:  -2.109\n",
      "Total loss:  -0.9896 | PDE Loss:  -1.6076 | Function Loss:  -2.1094\n",
      "Total loss:  -0.9897 | PDE Loss:  -1.6071 | Function Loss:  -2.1097\n",
      "Total loss:  -0.9898 | PDE Loss:  -1.6068 | Function Loss:  -2.1099\n",
      "Total loss:  -0.9899 | PDE Loss:  -1.6065 | Function Loss:  -2.1101\n",
      "Total loss:  -0.99 | PDE Loss:  -1.6062 | Function Loss:  -2.1103\n",
      "Total loss:  -0.9901 | PDE Loss:  -1.6061 | Function Loss:  -2.1105\n",
      "Total loss:  -0.9902 | PDE Loss:  -1.605 | Function Loss:  -2.1109\n",
      "Total loss:  -0.9902 | PDE Loss:  -1.6052 | Function Loss:  -2.1109\n",
      "Total loss:  -0.9903 | PDE Loss:  -1.6055 | Function Loss:  -2.1109\n",
      "Total loss:  -0.9903 | PDE Loss:  -1.6058 | Function Loss:  -2.1109\n",
      "Total loss:  -0.9904 | PDE Loss:  -1.606 | Function Loss:  -2.1109\n",
      "Total loss:  -0.9904 | PDE Loss:  -1.6062 | Function Loss:  -2.1109\n",
      "Total loss:  -0.9905 | PDE Loss:  -1.6063 | Function Loss:  -2.1109\n",
      "Total loss:  -0.9905 | PDE Loss:  -1.6063 | Function Loss:  -2.1109\n",
      "Total loss:  -0.9905 | PDE Loss:  -1.6061 | Function Loss:  -2.111\n",
      "Total loss:  -0.9905 | PDE Loss:  -1.6063 | Function Loss:  -2.111\n",
      "Total loss:  -0.9905 | PDE Loss:  -1.6062 | Function Loss:  -2.111\n",
      "Total loss:  -0.9906 | PDE Loss:  -1.606 | Function Loss:  -2.1112\n",
      "Total loss:  -0.9906 | PDE Loss:  -1.6058 | Function Loss:  -2.1113\n",
      "Total loss:  -0.9907 | PDE Loss:  -1.6056 | Function Loss:  -2.1114\n",
      "Total loss:  -0.9907 | PDE Loss:  -1.6054 | Function Loss:  -2.1115\n",
      "Total loss:  -0.9907 | PDE Loss:  -1.6053 | Function Loss:  -2.1115\n",
      "Total loss:  -0.9907 | PDE Loss:  -1.6052 | Function Loss:  -2.1116\n",
      "Total loss:  -0.9908 | PDE Loss:  -1.6052 | Function Loss:  -2.1117\n",
      "Total loss:  -0.9908 | PDE Loss:  -1.6052 | Function Loss:  -2.1117\n",
      "Total loss:  -0.9909 | PDE Loss:  -1.6054 | Function Loss:  -2.1117\n",
      "Total loss:  -0.9909 | PDE Loss:  -1.6055 | Function Loss:  -2.1118\n",
      "Total loss:  -0.991 | PDE Loss:  -1.6057 | Function Loss:  -2.1118\n",
      "Total loss:  -0.9911 | PDE Loss:  -1.6061 | Function Loss:  -2.1118\n",
      "Total loss:  -0.9912 | PDE Loss:  -1.6062 | Function Loss:  -2.1119\n",
      "Total loss:  -0.9912 | PDE Loss:  -1.6062 | Function Loss:  -2.112\n",
      "Total loss:  -0.9913 | PDE Loss:  -1.6062 | Function Loss:  -2.1121\n",
      "Total loss:  -0.9914 | PDE Loss:  -1.606 | Function Loss:  -2.1122\n",
      "Total loss:  -0.9914 | PDE Loss:  -1.6059 | Function Loss:  -2.1123\n",
      "Total loss:  -0.9915 | PDE Loss:  -1.6054 | Function Loss:  -2.1125\n",
      "Total loss:  -0.9915 | PDE Loss:  -1.6054 | Function Loss:  -2.1125\n",
      "Total loss:  -0.9915 | PDE Loss:  -1.6054 | Function Loss:  -2.1126\n",
      "Total loss:  -0.9916 | PDE Loss:  -1.6053 | Function Loss:  -2.1127\n",
      "Total loss:  -0.9916 | PDE Loss:  -1.6056 | Function Loss:  -2.1126\n",
      "Total loss:  -0.9916 | PDE Loss:  -1.6056 | Function Loss:  -2.1126\n",
      "Total loss:  -0.9916 | PDE Loss:  -1.6054 | Function Loss:  -2.1127\n",
      "Total loss:  -0.9917 | PDE Loss:  -1.6053 | Function Loss:  -2.1128\n",
      "Total loss:  -0.9918 | PDE Loss:  -1.6051 | Function Loss:  -2.113\n",
      "Total loss:  -0.9918 | PDE Loss:  -1.605 | Function Loss:  -2.1132\n",
      "Total loss:  -0.9919 | PDE Loss:  -1.6049 | Function Loss:  -2.1133\n",
      "Total loss:  -0.992 | PDE Loss:  -1.605 | Function Loss:  -2.1134\n",
      "Total loss:  -0.9921 | PDE Loss:  -1.6053 | Function Loss:  -2.1135\n",
      "Total loss:  -0.9922 | PDE Loss:  -1.6058 | Function Loss:  -2.1134\n",
      "Total loss:  -0.9923 | PDE Loss:  -1.6063 | Function Loss:  -2.1134\n",
      "Total loss:  -0.9924 | PDE Loss:  -1.6069 | Function Loss:  -2.1133\n",
      "Total loss:  -0.9925 | PDE Loss:  -1.608 | Function Loss:  -2.1131\n",
      "Total loss:  -0.9926 | PDE Loss:  -1.6088 | Function Loss:  -2.1129\n",
      "Total loss:  -0.9927 | PDE Loss:  -1.6095 | Function Loss:  -2.1128\n",
      "Total loss:  -0.9928 | PDE Loss:  -1.6102 | Function Loss:  -2.1127\n",
      "Total loss:  -0.9928 | PDE Loss:  -1.6107 | Function Loss:  -2.1126\n",
      "Total loss:  -0.9929 | PDE Loss:  -1.6111 | Function Loss:  -2.1126\n",
      "Total loss:  -0.9929 | PDE Loss:  -1.6112 | Function Loss:  -2.1126\n",
      "Total loss:  -0.993 | PDE Loss:  -1.6113 | Function Loss:  -2.1126\n",
      "Total loss:  -0.993 | PDE Loss:  -1.6112 | Function Loss:  -2.1127\n",
      "Total loss:  -0.993 | PDE Loss:  -1.6112 | Function Loss:  -2.1127\n",
      "Total loss:  -0.9931 | PDE Loss:  -1.611 | Function Loss:  -2.1129\n",
      "Total loss:  -0.9932 | PDE Loss:  -1.6108 | Function Loss:  -2.1131\n",
      "Total loss:  -0.9933 | PDE Loss:  -1.6106 | Function Loss:  -2.1133\n",
      "Total loss:  -0.9934 | PDE Loss:  -1.6104 | Function Loss:  -2.1135\n",
      "Total loss:  -0.9935 | PDE Loss:  -1.6104 | Function Loss:  -2.1136\n",
      "Total loss:  -0.9936 | PDE Loss:  -1.6103 | Function Loss:  -2.1138\n",
      "Total loss:  -0.9936 | PDE Loss:  -1.6107 | Function Loss:  -2.1137\n",
      "Total loss:  -0.9937 | PDE Loss:  -1.6106 | Function Loss:  -2.1138\n",
      "Total loss:  -0.9938 | PDE Loss:  -1.6107 | Function Loss:  -2.1139\n",
      "Total loss:  -0.9938 | PDE Loss:  -1.6107 | Function Loss:  -2.1139\n",
      "Total loss:  -0.9939 | PDE Loss:  -1.6108 | Function Loss:  -2.114\n",
      "Total loss:  -0.9939 | PDE Loss:  -1.6111 | Function Loss:  -2.1139\n",
      "Total loss:  -0.9939 | PDE Loss:  -1.6111 | Function Loss:  -2.1139\n",
      "Total loss:  -0.9939 | PDE Loss:  -1.611 | Function Loss:  -2.114\n",
      "Total loss:  -0.9939 | PDE Loss:  -1.611 | Function Loss:  -2.114\n",
      "Total loss:  -0.994 | PDE Loss:  -1.6108 | Function Loss:  -2.1141\n",
      "Total loss:  -0.994 | PDE Loss:  -1.6107 | Function Loss:  -2.1142\n",
      "Total loss:  -0.994 | PDE Loss:  -1.6105 | Function Loss:  -2.1142\n",
      "Total loss:  -0.994 | PDE Loss:  -1.6105 | Function Loss:  -2.1143\n",
      "Total loss:  -0.9916 | PDE Loss:  -1.6052 | Function Loss:  -2.1128\n",
      "Total loss:  -0.994 | PDE Loss:  -1.6105 | Function Loss:  -2.1143\n",
      "Total loss:  -0.9941 | PDE Loss:  -1.6105 | Function Loss:  -2.1143\n",
      "Total loss:  -0.9941 | PDE Loss:  -1.6107 | Function Loss:  -2.1143\n",
      "Total loss:  -0.9941 | PDE Loss:  -1.6109 | Function Loss:  -2.1143\n",
      "Total loss:  -0.9941 | PDE Loss:  -1.6112 | Function Loss:  -2.1142\n",
      "Total loss:  -0.9942 | PDE Loss:  -1.6115 | Function Loss:  -2.1141\n",
      "Total loss:  -0.9942 | PDE Loss:  -1.6118 | Function Loss:  -2.1141\n",
      "Total loss:  -0.9942 | PDE Loss:  -1.6122 | Function Loss:  -2.114\n",
      "Total loss:  -0.9943 | PDE Loss:  -1.6126 | Function Loss:  -2.1139\n",
      "Total loss:  -0.9943 | PDE Loss:  -1.6129 | Function Loss:  -2.1138\n",
      "Total loss:  -0.9943 | PDE Loss:  -1.6132 | Function Loss:  -2.1138\n",
      "Total loss:  -0.9944 | PDE Loss:  -1.6134 | Function Loss:  -2.1138\n",
      "Total loss:  -0.9944 | PDE Loss:  -1.6135 | Function Loss:  -2.1138\n",
      "Total loss:  -0.9944 | PDE Loss:  -1.6136 | Function Loss:  -2.1138\n",
      "Total loss:  -0.9945 | PDE Loss:  -1.6138 | Function Loss:  -2.1138\n",
      "Total loss:  -0.9945 | PDE Loss:  -1.6136 | Function Loss:  -2.1139\n",
      "Total loss:  -0.9945 | PDE Loss:  -1.6138 | Function Loss:  -2.1139\n",
      "Total loss:  -0.9946 | PDE Loss:  -1.614 | Function Loss:  -2.1139\n",
      "Total loss:  -0.9946 | PDE Loss:  -1.6142 | Function Loss:  -2.1139\n",
      "Total loss:  -0.9946 | PDE Loss:  -1.6144 | Function Loss:  -2.1138\n",
      "Total loss:  -0.9947 | PDE Loss:  -1.6145 | Function Loss:  -2.1138\n",
      "Total loss:  -0.9947 | PDE Loss:  -1.6149 | Function Loss:  -2.1137\n",
      "Total loss:  -0.9947 | PDE Loss:  -1.6149 | Function Loss:  -2.1138\n",
      "Total loss:  -0.9948 | PDE Loss:  -1.6149 | Function Loss:  -2.1138\n",
      "Total loss:  -0.9948 | PDE Loss:  -1.615 | Function Loss:  -2.1138\n",
      "Total loss:  -0.9948 | PDE Loss:  -1.6149 | Function Loss:  -2.1139\n",
      "Total loss:  -0.9948 | PDE Loss:  -1.6149 | Function Loss:  -2.1139\n",
      "Total loss:  -0.9949 | PDE Loss:  -1.6149 | Function Loss:  -2.114\n",
      "Total loss:  -0.9949 | PDE Loss:  -1.6152 | Function Loss:  -2.1139\n",
      "Total loss:  -0.9949 | PDE Loss:  -1.6154 | Function Loss:  -2.1139\n",
      "Total loss:  -0.995 | PDE Loss:  -1.6155 | Function Loss:  -2.114\n",
      "Total loss:  -0.9951 | PDE Loss:  -1.6158 | Function Loss:  -2.114\n",
      "Total loss:  -0.9951 | PDE Loss:  -1.6162 | Function Loss:  -2.1139\n",
      "Total loss:  -0.9952 | PDE Loss:  -1.6165 | Function Loss:  -2.1139\n",
      "Total loss:  -0.9953 | PDE Loss:  -1.6173 | Function Loss:  -2.1137\n",
      "Total loss:  -0.9953 | PDE Loss:  -1.6176 | Function Loss:  -2.1137\n",
      "Total loss:  -0.9954 | PDE Loss:  -1.6178 | Function Loss:  -2.1138\n",
      "Total loss:  -0.9955 | PDE Loss:  -1.618 | Function Loss:  -2.1139\n",
      "Total loss:  -0.9957 | PDE Loss:  -1.6183 | Function Loss:  -2.1139\n",
      "Total loss:  -0.9957 | PDE Loss:  -1.6182 | Function Loss:  -2.1141\n",
      "Total loss:  -0.9958 | PDE Loss:  -1.6181 | Function Loss:  -2.1142\n",
      "Total loss:  -0.9959 | PDE Loss:  -1.6181 | Function Loss:  -2.1143\n",
      "Total loss:  -0.996 | PDE Loss:  -1.6181 | Function Loss:  -2.1144\n",
      "Total loss:  -0.996 | PDE Loss:  -1.618 | Function Loss:  -2.1145\n",
      "Total loss:  -0.9961 | PDE Loss:  -1.6182 | Function Loss:  -2.1145\n",
      "Total loss:  -0.9961 | PDE Loss:  -1.6186 | Function Loss:  -2.1145\n",
      "Total loss:  -0.9962 | PDE Loss:  -1.619 | Function Loss:  -2.1144\n",
      "Total loss:  -0.9963 | PDE Loss:  -1.6194 | Function Loss:  -2.1144\n",
      "Total loss:  -0.9963 | PDE Loss:  -1.6197 | Function Loss:  -2.1144\n",
      "Total loss:  -0.9963 | PDE Loss:  -1.6194 | Function Loss:  -2.1145\n",
      "Total loss:  -0.9964 | PDE Loss:  -1.6199 | Function Loss:  -2.1145\n",
      "Total loss:  -0.9965 | PDE Loss:  -1.6201 | Function Loss:  -2.1145\n",
      "Total loss:  -0.9966 | PDE Loss:  -1.6202 | Function Loss:  -2.1146\n",
      "Total loss:  -0.9966 | PDE Loss:  -1.6201 | Function Loss:  -2.1147\n",
      "Total loss:  -0.9967 | PDE Loss:  -1.6199 | Function Loss:  -2.1148\n",
      "Total loss:  -0.9968 | PDE Loss:  -1.6196 | Function Loss:  -2.115\n",
      "Total loss:  -0.9968 | PDE Loss:  -1.6193 | Function Loss:  -2.1151\n",
      "Total loss:  -0.9969 | PDE Loss:  -1.619 | Function Loss:  -2.1153\n",
      "Total loss:  -0.9969 | PDE Loss:  -1.6189 | Function Loss:  -2.1154\n",
      "Total loss:  -0.997 | PDE Loss:  -1.6187 | Function Loss:  -2.1156\n",
      "Total loss:  -0.9971 | PDE Loss:  -1.6186 | Function Loss:  -2.1157\n",
      "Total loss:  -0.9971 | PDE Loss:  -1.6188 | Function Loss:  -2.1157\n",
      "Total loss:  -0.9972 | PDE Loss:  -1.619 | Function Loss:  -2.1157\n",
      "Total loss:  -0.9972 | PDE Loss:  -1.6192 | Function Loss:  -2.1157\n",
      "Total loss:  -0.9973 | PDE Loss:  -1.6194 | Function Loss:  -2.1157\n",
      "Total loss:  -0.9973 | PDE Loss:  -1.6197 | Function Loss:  -2.1157\n",
      "Total loss:  -0.9973 | PDE Loss:  -1.6198 | Function Loss:  -2.1157\n",
      "Total loss:  -0.9974 | PDE Loss:  -1.6199 | Function Loss:  -2.1157\n",
      "Total loss:  -0.9974 | PDE Loss:  -1.62 | Function Loss:  -2.1157\n",
      "Total loss:  -0.9975 | PDE Loss:  -1.6201 | Function Loss:  -2.1158\n",
      "Total loss:  -0.9975 | PDE Loss:  -1.6201 | Function Loss:  -2.1158\n",
      "Total loss:  -0.9975 | PDE Loss:  -1.6201 | Function Loss:  -2.1158\n",
      "Total loss:  -0.9975 | PDE Loss:  -1.6201 | Function Loss:  -2.1158\n",
      "Total loss:  -0.9975 | PDE Loss:  -1.6201 | Function Loss:  -2.1159\n",
      "Total loss:  -0.9976 | PDE Loss:  -1.6207 | Function Loss:  -2.1157\n",
      "Total loss:  -0.9976 | PDE Loss:  -1.6204 | Function Loss:  -2.1159\n",
      "Total loss:  -0.9977 | PDE Loss:  -1.6204 | Function Loss:  -2.1159\n",
      "Total loss:  -0.9977 | PDE Loss:  -1.6204 | Function Loss:  -2.116\n",
      "Total loss:  -0.9978 | PDE Loss:  -1.6206 | Function Loss:  -2.116\n",
      "Total loss:  -0.9979 | PDE Loss:  -1.6208 | Function Loss:  -2.116\n",
      "Total loss:  -0.9979 | PDE Loss:  -1.621 | Function Loss:  -2.116\n",
      "Total loss:  -0.998 | PDE Loss:  -1.6215 | Function Loss:  -2.116\n",
      "Total loss:  -0.998 | PDE Loss:  -1.6216 | Function Loss:  -2.1159\n",
      "Total loss:  -0.998 | PDE Loss:  -1.6216 | Function Loss:  -2.116\n",
      "Total loss:  -0.998 | PDE Loss:  -1.6219 | Function Loss:  -2.1159\n",
      "Total loss:  -0.9981 | PDE Loss:  -1.6222 | Function Loss:  -2.1159\n",
      "Total loss:  -0.9981 | PDE Loss:  -1.6223 | Function Loss:  -2.1159\n",
      "Total loss:  -0.9982 | PDE Loss:  -1.6224 | Function Loss:  -2.1159\n",
      "Total loss:  -0.9982 | PDE Loss:  -1.6224 | Function Loss:  -2.116\n",
      "Total loss:  -0.9982 | PDE Loss:  -1.6224 | Function Loss:  -2.1161\n",
      "Total loss:  -0.9983 | PDE Loss:  -1.6222 | Function Loss:  -2.1161\n",
      "Total loss:  -0.9983 | PDE Loss:  -1.6221 | Function Loss:  -2.1162\n",
      "Total loss:  -0.9983 | PDE Loss:  -1.622 | Function Loss:  -2.1163\n",
      "Total loss:  -0.9984 | PDE Loss:  -1.6219 | Function Loss:  -2.1164\n",
      "Total loss:  -0.9984 | PDE Loss:  -1.6218 | Function Loss:  -2.1164\n",
      "Total loss:  -0.9984 | PDE Loss:  -1.6218 | Function Loss:  -2.1165\n",
      "Total loss:  -0.9985 | PDE Loss:  -1.6218 | Function Loss:  -2.1165\n",
      "Total loss:  -0.9985 | PDE Loss:  -1.6219 | Function Loss:  -2.1166\n",
      "Total loss:  -0.9984 | PDE Loss:  -1.6219 | Function Loss:  -2.1164\n",
      "Total loss:  -0.9985 | PDE Loss:  -1.622 | Function Loss:  -2.1165\n",
      "Total loss:  -0.9986 | PDE Loss:  -1.6222 | Function Loss:  -2.1166\n",
      "Total loss:  -0.9986 | PDE Loss:  -1.6224 | Function Loss:  -2.1165\n",
      "Total loss:  -0.9987 | PDE Loss:  -1.6226 | Function Loss:  -2.1166\n",
      "Total loss:  -0.9987 | PDE Loss:  -1.6231 | Function Loss:  -2.1165\n",
      "Total loss:  -0.9988 | PDE Loss:  -1.6233 | Function Loss:  -2.1165\n",
      "Total loss:  -0.9989 | PDE Loss:  -1.6235 | Function Loss:  -2.1165\n",
      "Total loss:  -0.9989 | PDE Loss:  -1.6237 | Function Loss:  -2.1166\n",
      "Total loss:  -0.999 | PDE Loss:  -1.6237 | Function Loss:  -2.1166\n",
      "Total loss:  -0.999 | PDE Loss:  -1.6237 | Function Loss:  -2.1167\n",
      "Total loss:  -0.9991 | PDE Loss:  -1.6237 | Function Loss:  -2.1167\n",
      "Total loss:  -0.9991 | PDE Loss:  -1.6238 | Function Loss:  -2.1167\n",
      "Total loss:  -0.999 | PDE Loss:  -1.6218 | Function Loss:  -2.1172\n",
      "Total loss:  -0.9991 | PDE Loss:  -1.6233 | Function Loss:  -2.1169\n",
      "Total loss:  -0.9991 | PDE Loss:  -1.6235 | Function Loss:  -2.1169\n",
      "Total loss:  -0.9992 | PDE Loss:  -1.6237 | Function Loss:  -2.1169\n",
      "Total loss:  -0.9991 | PDE Loss:  -1.6241 | Function Loss:  -2.1167\n",
      "Total loss:  -0.9992 | PDE Loss:  -1.6239 | Function Loss:  -2.1169\n",
      "Total loss:  -0.9993 | PDE Loss:  -1.6241 | Function Loss:  -2.1169\n",
      "Total loss:  -0.9993 | PDE Loss:  -1.6243 | Function Loss:  -2.1169\n",
      "Total loss:  -0.9994 | PDE Loss:  -1.6246 | Function Loss:  -2.1168\n",
      "Total loss:  -0.9994 | PDE Loss:  -1.625 | Function Loss:  -2.1168\n",
      "Total loss:  -0.9995 | PDE Loss:  -1.6253 | Function Loss:  -2.1168\n",
      "Total loss:  -0.9995 | PDE Loss:  -1.6257 | Function Loss:  -2.1167\n",
      "Total loss:  -0.9996 | PDE Loss:  -1.626 | Function Loss:  -2.1167\n",
      "Total loss:  -0.9997 | PDE Loss:  -1.6263 | Function Loss:  -2.1167\n",
      "Total loss:  -0.9997 | PDE Loss:  -1.6266 | Function Loss:  -2.1167\n",
      "Total loss:  -0.9998 | PDE Loss:  -1.6269 | Function Loss:  -2.1168\n",
      "Total loss:  -0.9999 | PDE Loss:  -1.6272 | Function Loss:  -2.1167\n",
      "Total loss:  -0.9999 | PDE Loss:  -1.627 | Function Loss:  -2.1169\n",
      "Total loss:  -1.0 | PDE Loss:  -1.6271 | Function Loss:  -2.1169\n",
      "Total loss:  -1.0 | PDE Loss:  -1.6271 | Function Loss:  -2.117\n",
      "Total loss:  -1.0001 | PDE Loss:  -1.6271 | Function Loss:  -2.117\n",
      "Total loss:  -1.0001 | PDE Loss:  -1.6271 | Function Loss:  -2.117\n",
      "Total loss:  -1.0001 | PDE Loss:  -1.6271 | Function Loss:  -2.1171\n",
      "Total loss:  -1.0002 | PDE Loss:  -1.6271 | Function Loss:  -2.1171\n",
      "Total loss:  -1.0002 | PDE Loss:  -1.627 | Function Loss:  -2.1172\n",
      "Total loss:  -1.0002 | PDE Loss:  -1.6271 | Function Loss:  -2.1172\n",
      "Total loss:  -1.0003 | PDE Loss:  -1.6271 | Function Loss:  -2.1173\n",
      "Total loss:  -1.0003 | PDE Loss:  -1.6276 | Function Loss:  -2.1171\n",
      "Total loss:  -1.0003 | PDE Loss:  -1.6274 | Function Loss:  -2.1172\n",
      "Total loss:  -1.0003 | PDE Loss:  -1.6275 | Function Loss:  -2.1172\n",
      "Total loss:  -1.0004 | PDE Loss:  -1.6276 | Function Loss:  -2.1172\n",
      "Total loss:  -1.0004 | PDE Loss:  -1.6278 | Function Loss:  -2.1172\n",
      "Total loss:  -1.0004 | PDE Loss:  -1.628 | Function Loss:  -2.1172\n",
      "Total loss:  -1.0005 | PDE Loss:  -1.6282 | Function Loss:  -2.1172\n",
      "Total loss:  -1.0005 | PDE Loss:  -1.6284 | Function Loss:  -2.1171\n",
      "Total loss:  -1.0005 | PDE Loss:  -1.6287 | Function Loss:  -2.1171\n",
      "Total loss:  -1.0005 | PDE Loss:  -1.6287 | Function Loss:  -2.1171\n",
      "Total loss:  -1.0005 | PDE Loss:  -1.6287 | Function Loss:  -2.1171\n",
      "Total loss:  -1.0006 | PDE Loss:  -1.6288 | Function Loss:  -2.1171\n",
      "Total loss:  -1.0006 | PDE Loss:  -1.6289 | Function Loss:  -2.1171\n",
      "Total loss:  -1.0006 | PDE Loss:  -1.6289 | Function Loss:  -2.1171\n",
      "Total loss:  -1.0006 | PDE Loss:  -1.6289 | Function Loss:  -2.1171\n",
      "Total loss:  -1.0006 | PDE Loss:  -1.629 | Function Loss:  -2.1171\n",
      "Total loss:  -1.0006 | PDE Loss:  -1.629 | Function Loss:  -2.1172\n",
      "Total loss:  -1.0006 | PDE Loss:  -1.6291 | Function Loss:  -2.1171\n",
      "Total loss:  -1.0007 | PDE Loss:  -1.6291 | Function Loss:  -2.1171\n",
      "Total loss:  -1.0007 | PDE Loss:  -1.6292 | Function Loss:  -2.1171\n",
      "Total loss:  -1.0007 | PDE Loss:  -1.6293 | Function Loss:  -2.1171\n",
      "Total loss:  -1.0007 | PDE Loss:  -1.6294 | Function Loss:  -2.1171\n",
      "Total loss:  -1.0007 | PDE Loss:  -1.6296 | Function Loss:  -2.1171\n",
      "Total loss:  -1.0008 | PDE Loss:  -1.6298 | Function Loss:  -2.1171\n",
      "Total loss:  -1.0008 | PDE Loss:  -1.6303 | Function Loss:  -2.1169\n",
      "Total loss:  -1.0008 | PDE Loss:  -1.6306 | Function Loss:  -2.1169\n",
      "Total loss:  -1.0009 | PDE Loss:  -1.6309 | Function Loss:  -2.1169\n",
      "Total loss:  -1.0009 | PDE Loss:  -1.6313 | Function Loss:  -2.1168\n",
      "Total loss:  -1.001 | PDE Loss:  -1.6326 | Function Loss:  -2.1165\n",
      "Total loss:  -1.0011 | PDE Loss:  -1.6327 | Function Loss:  -2.1166\n",
      "Total loss:  -1.0011 | PDE Loss:  -1.6326 | Function Loss:  -2.1167\n",
      "Total loss:  -1.0011 | PDE Loss:  -1.6325 | Function Loss:  -2.1167\n",
      "Total loss:  -1.0012 | PDE Loss:  -1.6324 | Function Loss:  -2.1168\n",
      "Total loss:  -1.0012 | PDE Loss:  -1.6323 | Function Loss:  -2.1169\n",
      "Total loss:  -1.0012 | PDE Loss:  -1.6321 | Function Loss:  -2.117\n",
      "Total loss:  -1.0013 | PDE Loss:  -1.6319 | Function Loss:  -2.1171\n",
      "Total loss:  -1.0013 | PDE Loss:  -1.6314 | Function Loss:  -2.1173\n",
      "Total loss:  -1.0014 | PDE Loss:  -1.6311 | Function Loss:  -2.1175\n",
      "Total loss:  -1.0014 | PDE Loss:  -1.6298 | Function Loss:  -2.1179\n",
      "Total loss:  -1.0014 | PDE Loss:  -1.6306 | Function Loss:  -2.1177\n",
      "Total loss:  -1.0015 | PDE Loss:  -1.6306 | Function Loss:  -2.1177\n",
      "Total loss:  -1.0015 | PDE Loss:  -1.6308 | Function Loss:  -2.1177\n",
      "Total loss:  -1.0015 | PDE Loss:  -1.631 | Function Loss:  -2.1177\n",
      "Total loss:  -1.0016 | PDE Loss:  -1.6312 | Function Loss:  -2.1177\n",
      "Total loss:  -1.0016 | PDE Loss:  -1.6313 | Function Loss:  -2.1177\n",
      "Total loss:  -1.0016 | PDE Loss:  -1.6315 | Function Loss:  -2.1176\n",
      "Total loss:  -1.0016 | PDE Loss:  -1.6316 | Function Loss:  -2.1176\n",
      "Total loss:  -1.0016 | PDE Loss:  -1.6317 | Function Loss:  -2.1176\n",
      "Total loss:  -1.0017 | PDE Loss:  -1.6318 | Function Loss:  -2.1176\n",
      "Total loss:  -1.0017 | PDE Loss:  -1.6318 | Function Loss:  -2.1177\n",
      "Total loss:  -1.0017 | PDE Loss:  -1.6317 | Function Loss:  -2.1177\n",
      "Total loss:  -1.0017 | PDE Loss:  -1.6317 | Function Loss:  -2.1177\n",
      "Total loss:  -1.0017 | PDE Loss:  -1.6317 | Function Loss:  -2.1178\n",
      "Total loss:  -1.0018 | PDE Loss:  -1.6317 | Function Loss:  -2.1178\n",
      "Total loss:  -1.0018 | PDE Loss:  -1.6318 | Function Loss:  -2.1178\n",
      "Total loss:  -1.0018 | PDE Loss:  -1.6319 | Function Loss:  -2.1178\n",
      "Total loss:  -1.0018 | PDE Loss:  -1.6321 | Function Loss:  -2.1178\n",
      "Total loss:  -1.0019 | PDE Loss:  -1.6322 | Function Loss:  -2.1178\n",
      "Total loss:  -1.0019 | PDE Loss:  -1.6324 | Function Loss:  -2.1178\n",
      "Total loss:  -1.002 | PDE Loss:  -1.6326 | Function Loss:  -2.1178\n",
      "Total loss:  -1.002 | PDE Loss:  -1.6329 | Function Loss:  -2.1177\n",
      "Total loss:  -1.002 | PDE Loss:  -1.6332 | Function Loss:  -2.1177\n",
      "Total loss:  -1.0016 | PDE Loss:  -1.6343 | Function Loss:  -2.1169\n",
      "Total loss:  -1.002 | PDE Loss:  -1.6334 | Function Loss:  -2.1176\n",
      "Total loss:  -1.0021 | PDE Loss:  -1.6337 | Function Loss:  -2.1176\n",
      "Total loss:  -1.0021 | PDE Loss:  -1.6338 | Function Loss:  -2.1176\n",
      "Total loss:  -1.0021 | PDE Loss:  -1.634 | Function Loss:  -2.1176\n",
      "Total loss:  -1.0022 | PDE Loss:  -1.634 | Function Loss:  -2.1176\n",
      "Total loss:  -1.0022 | PDE Loss:  -1.634 | Function Loss:  -2.1177\n",
      "Total loss:  -1.0023 | PDE Loss:  -1.634 | Function Loss:  -2.1178\n",
      "Total loss:  -1.0023 | PDE Loss:  -1.6339 | Function Loss:  -2.1179\n",
      "Total loss:  -1.0024 | PDE Loss:  -1.6337 | Function Loss:  -2.1181\n",
      "Total loss:  -1.0025 | PDE Loss:  -1.6335 | Function Loss:  -2.1182\n",
      "Total loss:  -1.0025 | PDE Loss:  -1.6334 | Function Loss:  -2.1183\n",
      "Total loss:  -1.0026 | PDE Loss:  -1.6333 | Function Loss:  -2.1184\n",
      "Total loss:  -1.0026 | PDE Loss:  -1.6331 | Function Loss:  -2.1184\n",
      "Total loss:  -1.0026 | PDE Loss:  -1.633 | Function Loss:  -2.1185\n",
      "Total loss:  -1.0026 | PDE Loss:  -1.6329 | Function Loss:  -2.1186\n",
      "Total loss:  -1.0027 | PDE Loss:  -1.6328 | Function Loss:  -2.1186\n",
      "Total loss:  -1.0027 | PDE Loss:  -1.6329 | Function Loss:  -2.1187\n",
      "Total loss:  -1.0028 | PDE Loss:  -1.6328 | Function Loss:  -2.1188\n",
      "Total loss:  -1.0029 | PDE Loss:  -1.6329 | Function Loss:  -2.1189\n",
      "Total loss:  -1.003 | PDE Loss:  -1.633 | Function Loss:  -2.1189\n",
      "Total loss:  -1.003 | PDE Loss:  -1.6332 | Function Loss:  -2.119\n",
      "Total loss:  -1.0031 | PDE Loss:  -1.6334 | Function Loss:  -2.119\n",
      "Total loss:  -1.0031 | PDE Loss:  -1.6336 | Function Loss:  -2.1189\n",
      "Total loss:  -1.0032 | PDE Loss:  -1.634 | Function Loss:  -2.1189\n",
      "Total loss:  -1.0032 | PDE Loss:  -1.6341 | Function Loss:  -2.1189\n",
      "Total loss:  -1.0032 | PDE Loss:  -1.6344 | Function Loss:  -2.1189\n",
      "Total loss:  -1.0033 | PDE Loss:  -1.6346 | Function Loss:  -2.1189\n",
      "Total loss:  -1.0033 | PDE Loss:  -1.6348 | Function Loss:  -2.1189\n",
      "Total loss:  -1.0033 | PDE Loss:  -1.6348 | Function Loss:  -2.1189\n",
      "Total loss:  -1.0034 | PDE Loss:  -1.6349 | Function Loss:  -2.1189\n",
      "Total loss:  -1.0034 | PDE Loss:  -1.6349 | Function Loss:  -2.1189\n",
      "Total loss:  -1.0035 | PDE Loss:  -1.635 | Function Loss:  -2.119\n",
      "Total loss:  -1.0035 | PDE Loss:  -1.6357 | Function Loss:  -2.1188\n",
      "Total loss:  -1.0036 | PDE Loss:  -1.6358 | Function Loss:  -2.1189\n",
      "Total loss:  -1.0036 | PDE Loss:  -1.6359 | Function Loss:  -2.119\n",
      "Total loss:  -1.0037 | PDE Loss:  -1.6361 | Function Loss:  -2.119\n",
      "Total loss:  -1.0037 | PDE Loss:  -1.6363 | Function Loss:  -2.119\n",
      "Total loss:  -1.0038 | PDE Loss:  -1.6365 | Function Loss:  -2.119\n",
      "Total loss:  -1.0038 | PDE Loss:  -1.6368 | Function Loss:  -2.1189\n",
      "Total loss:  -1.0039 | PDE Loss:  -1.6372 | Function Loss:  -2.1189\n",
      "Total loss:  -1.004 | PDE Loss:  -1.6377 | Function Loss:  -2.1189\n",
      "Total loss:  -1.004 | PDE Loss:  -1.638 | Function Loss:  -2.1188\n",
      "Total loss:  -1.0041 | PDE Loss:  -1.6382 | Function Loss:  -2.1188\n",
      "Total loss:  -1.0041 | PDE Loss:  -1.6384 | Function Loss:  -2.1189\n",
      "Total loss:  -1.0042 | PDE Loss:  -1.6385 | Function Loss:  -2.1189\n",
      "Total loss:  -1.0042 | PDE Loss:  -1.6385 | Function Loss:  -2.119\n",
      "Total loss:  -1.0043 | PDE Loss:  -1.6384 | Function Loss:  -2.119\n",
      "Total loss:  -1.0043 | PDE Loss:  -1.6383 | Function Loss:  -2.1191\n",
      "Total loss:  -1.0044 | PDE Loss:  -1.6382 | Function Loss:  -2.1192\n",
      "Total loss:  -1.0044 | PDE Loss:  -1.6382 | Function Loss:  -2.1193\n",
      "Total loss:  -1.0045 | PDE Loss:  -1.6386 | Function Loss:  -2.1193\n",
      "Total loss:  -1.0046 | PDE Loss:  -1.6386 | Function Loss:  -2.1194\n",
      "Total loss:  -1.0046 | PDE Loss:  -1.6388 | Function Loss:  -2.1194\n",
      "Total loss:  -1.0047 | PDE Loss:  -1.6391 | Function Loss:  -2.1193\n",
      "Total loss:  -1.0047 | PDE Loss:  -1.6395 | Function Loss:  -2.1193\n",
      "Total loss:  -1.0048 | PDE Loss:  -1.6398 | Function Loss:  -2.1192\n",
      "Total loss:  -1.0048 | PDE Loss:  -1.6401 | Function Loss:  -2.1192\n",
      "Total loss:  -1.0048 | PDE Loss:  -1.6404 | Function Loss:  -2.1192\n",
      "Total loss:  -1.0049 | PDE Loss:  -1.6406 | Function Loss:  -2.1191\n",
      "Total loss:  -1.0049 | PDE Loss:  -1.6407 | Function Loss:  -2.1191\n",
      "Total loss:  -1.0049 | PDE Loss:  -1.6408 | Function Loss:  -2.1191\n",
      "Total loss:  -1.0049 | PDE Loss:  -1.6409 | Function Loss:  -2.1191\n",
      "Total loss:  -1.005 | PDE Loss:  -1.641 | Function Loss:  -2.1192\n",
      "Total loss:  -1.005 | PDE Loss:  -1.6411 | Function Loss:  -2.1192\n",
      "Total loss:  -1.0051 | PDE Loss:  -1.6412 | Function Loss:  -2.1193\n",
      "Total loss:  -1.0052 | PDE Loss:  -1.6412 | Function Loss:  -2.1194\n",
      "Total loss:  -1.0052 | PDE Loss:  -1.6412 | Function Loss:  -2.1194\n",
      "Total loss:  -1.0053 | PDE Loss:  -1.6411 | Function Loss:  -2.1195\n",
      "Total loss:  -1.0053 | PDE Loss:  -1.6414 | Function Loss:  -2.1195\n",
      "Total loss:  -1.0054 | PDE Loss:  -1.6414 | Function Loss:  -2.1195\n",
      "Total loss:  -1.0054 | PDE Loss:  -1.6419 | Function Loss:  -2.1194\n",
      "Total loss:  -1.0054 | PDE Loss:  -1.6417 | Function Loss:  -2.1195\n",
      "Total loss:  -1.0054 | PDE Loss:  -1.6416 | Function Loss:  -2.1195\n",
      "Total loss:  -1.0054 | PDE Loss:  -1.6416 | Function Loss:  -2.1196\n",
      "Total loss:  -1.0055 | PDE Loss:  -1.6415 | Function Loss:  -2.1197\n",
      "Total loss:  -1.0056 | PDE Loss:  -1.6415 | Function Loss:  -2.1198\n",
      "Total loss:  -1.0057 | PDE Loss:  -1.6415 | Function Loss:  -2.12\n",
      "Total loss:  -1.0058 | PDE Loss:  -1.6413 | Function Loss:  -2.1201\n",
      "Total loss:  -1.0058 | PDE Loss:  -1.6413 | Function Loss:  -2.1202\n",
      "Total loss:  -1.0059 | PDE Loss:  -1.6413 | Function Loss:  -2.1203\n",
      "Total loss:  -1.006 | PDE Loss:  -1.6414 | Function Loss:  -2.1203\n",
      "Total loss:  -1.006 | PDE Loss:  -1.6414 | Function Loss:  -2.1204\n",
      "Total loss:  -1.0061 | PDE Loss:  -1.6414 | Function Loss:  -2.1205\n",
      "Total loss:  -1.0061 | PDE Loss:  -1.6414 | Function Loss:  -2.1205\n",
      "Total loss:  -1.0062 | PDE Loss:  -1.6413 | Function Loss:  -2.1206\n",
      "Total loss:  -1.0062 | PDE Loss:  -1.6413 | Function Loss:  -2.1207\n",
      "Total loss:  -1.0063 | PDE Loss:  -1.641 | Function Loss:  -2.1209\n",
      "Total loss:  -1.0063 | PDE Loss:  -1.6411 | Function Loss:  -2.1209\n",
      "Total loss:  -1.0063 | PDE Loss:  -1.6412 | Function Loss:  -2.1209\n",
      "Total loss:  -1.0064 | PDE Loss:  -1.6414 | Function Loss:  -2.1209\n",
      "Total loss:  -1.0064 | PDE Loss:  -1.6416 | Function Loss:  -2.1209\n",
      "Total loss:  -1.0065 | PDE Loss:  -1.6418 | Function Loss:  -2.1208\n",
      "Total loss:  -1.0065 | PDE Loss:  -1.642 | Function Loss:  -2.1208\n",
      "Total loss:  -1.0065 | PDE Loss:  -1.6422 | Function Loss:  -2.1208\n",
      "Total loss:  -1.0066 | PDE Loss:  -1.6424 | Function Loss:  -2.1208\n",
      "Total loss:  -1.0067 | PDE Loss:  -1.6428 | Function Loss:  -2.1208\n",
      "Total loss:  -1.0067 | PDE Loss:  -1.6429 | Function Loss:  -2.1208\n",
      "Total loss:  -1.0068 | PDE Loss:  -1.6429 | Function Loss:  -2.1209\n",
      "Total loss:  -1.0068 | PDE Loss:  -1.6429 | Function Loss:  -2.121\n",
      "Total loss:  -1.0069 | PDE Loss:  -1.643 | Function Loss:  -2.121\n",
      "Total loss:  -1.0069 | PDE Loss:  -1.6421 | Function Loss:  -2.1213\n",
      "Total loss:  -1.0069 | PDE Loss:  -1.6425 | Function Loss:  -2.1213\n",
      "Total loss:  -1.007 | PDE Loss:  -1.6426 | Function Loss:  -2.1213\n",
      "Total loss:  -1.007 | PDE Loss:  -1.6428 | Function Loss:  -2.1212\n",
      "Total loss:  -1.007 | PDE Loss:  -1.6431 | Function Loss:  -2.1212\n",
      "Total loss:  -1.0071 | PDE Loss:  -1.6433 | Function Loss:  -2.1212\n",
      "Total loss:  -1.0071 | PDE Loss:  -1.6434 | Function Loss:  -2.1212\n",
      "Total loss:  -1.0072 | PDE Loss:  -1.6434 | Function Loss:  -2.1213\n",
      "Total loss:  -1.0072 | PDE Loss:  -1.6429 | Function Loss:  -2.1215\n",
      "Total loss:  -1.0072 | PDE Loss:  -1.6429 | Function Loss:  -2.1215\n",
      "Total loss:  -1.0073 | PDE Loss:  -1.6429 | Function Loss:  -2.1216\n",
      "Total loss:  -1.0073 | PDE Loss:  -1.6429 | Function Loss:  -2.1216\n",
      "Total loss:  -1.0073 | PDE Loss:  -1.6428 | Function Loss:  -2.1217\n",
      "Total loss:  -1.0073 | PDE Loss:  -1.6423 | Function Loss:  -2.1218\n",
      "Total loss:  -1.0074 | PDE Loss:  -1.6428 | Function Loss:  -2.1217\n",
      "Total loss:  -1.0074 | PDE Loss:  -1.6428 | Function Loss:  -2.1218\n",
      "Total loss:  -1.0074 | PDE Loss:  -1.6429 | Function Loss:  -2.1218\n",
      "Total loss:  -1.0075 | PDE Loss:  -1.643 | Function Loss:  -2.1218\n",
      "Total loss:  -1.0075 | PDE Loss:  -1.6433 | Function Loss:  -2.1218\n",
      "Total loss:  -1.0076 | PDE Loss:  -1.6435 | Function Loss:  -2.1218\n",
      "Total loss:  -1.0076 | PDE Loss:  -1.6438 | Function Loss:  -2.1218\n",
      "Total loss:  -1.0077 | PDE Loss:  -1.644 | Function Loss:  -2.1217\n",
      "Total loss:  -1.0077 | PDE Loss:  -1.6443 | Function Loss:  -2.1217\n",
      "Total loss:  -1.0078 | PDE Loss:  -1.6444 | Function Loss:  -2.1218\n",
      "Total loss:  -1.0078 | PDE Loss:  -1.6445 | Function Loss:  -2.1218\n",
      "Total loss:  -1.0079 | PDE Loss:  -1.6445 | Function Loss:  -2.1219\n",
      "Total loss:  -1.0079 | PDE Loss:  -1.6444 | Function Loss:  -2.1219\n",
      "Total loss:  -1.0079 | PDE Loss:  -1.6444 | Function Loss:  -2.122\n",
      "Total loss:  -1.0079 | PDE Loss:  -1.6443 | Function Loss:  -2.122\n",
      "Total loss:  -1.008 | PDE Loss:  -1.6442 | Function Loss:  -2.1221\n",
      "Total loss:  -1.008 | PDE Loss:  -1.6447 | Function Loss:  -2.122\n",
      "Total loss:  -1.0081 | PDE Loss:  -1.6444 | Function Loss:  -2.1221\n",
      "Total loss:  -1.0081 | PDE Loss:  -1.6444 | Function Loss:  -2.1222\n",
      "Total loss:  -1.0081 | PDE Loss:  -1.6446 | Function Loss:  -2.1222\n",
      "Total loss:  -1.0082 | PDE Loss:  -1.6448 | Function Loss:  -2.1222\n",
      "Total loss:  -1.0082 | PDE Loss:  -1.6449 | Function Loss:  -2.1222\n",
      "Total loss:  -1.0083 | PDE Loss:  -1.645 | Function Loss:  -2.1222\n",
      "Total loss:  -1.0083 | PDE Loss:  -1.6451 | Function Loss:  -2.1223\n",
      "Total loss:  -1.0084 | PDE Loss:  -1.6452 | Function Loss:  -2.1223\n",
      "Total loss:  -1.0084 | PDE Loss:  -1.6453 | Function Loss:  -2.1223\n",
      "Total loss:  -1.0084 | PDE Loss:  -1.6453 | Function Loss:  -2.1224\n",
      "Total loss:  -1.0085 | PDE Loss:  -1.6453 | Function Loss:  -2.1224\n",
      "Total loss:  -1.0085 | PDE Loss:  -1.6453 | Function Loss:  -2.1224\n",
      "Total loss:  -1.0085 | PDE Loss:  -1.6454 | Function Loss:  -2.1224\n",
      "Total loss:  -1.0085 | PDE Loss:  -1.6454 | Function Loss:  -2.1225\n",
      "Total loss:  -1.0086 | PDE Loss:  -1.6454 | Function Loss:  -2.1225\n",
      "Total loss:  -1.0086 | PDE Loss:  -1.6454 | Function Loss:  -2.1225\n",
      "Total loss:  -1.0086 | PDE Loss:  -1.6454 | Function Loss:  -2.1226\n",
      "Total loss:  -1.0087 | PDE Loss:  -1.6455 | Function Loss:  -2.1226\n",
      "Total loss:  -1.0087 | PDE Loss:  -1.6455 | Function Loss:  -2.1227\n",
      "Total loss:  -1.0087 | PDE Loss:  -1.6455 | Function Loss:  -2.1227\n",
      "Total loss:  -1.0087 | PDE Loss:  -1.6457 | Function Loss:  -2.1226\n",
      "Total loss:  -1.0088 | PDE Loss:  -1.6458 | Function Loss:  -2.1226\n",
      "Total loss:  -1.0088 | PDE Loss:  -1.646 | Function Loss:  -2.1226\n",
      "Total loss:  -1.0087 | PDE Loss:  -1.6463 | Function Loss:  -2.1224\n",
      "Total loss:  -1.0088 | PDE Loss:  -1.6461 | Function Loss:  -2.1226\n",
      "Total loss:  -1.0089 | PDE Loss:  -1.6462 | Function Loss:  -2.1227\n",
      "Total loss:  -1.0089 | PDE Loss:  -1.6461 | Function Loss:  -2.1227\n",
      "Total loss:  -1.0089 | PDE Loss:  -1.6461 | Function Loss:  -2.1228\n",
      "Total loss:  -1.009 | PDE Loss:  -1.6459 | Function Loss:  -2.1229\n",
      "Total loss:  -1.009 | PDE Loss:  -1.6458 | Function Loss:  -2.1229\n",
      "Total loss:  -1.009 | PDE Loss:  -1.6456 | Function Loss:  -2.123\n",
      "Total loss:  -1.009 | PDE Loss:  -1.6455 | Function Loss:  -2.1231\n",
      "Total loss:  -1.009 | PDE Loss:  -1.6452 | Function Loss:  -2.1232\n",
      "Total loss:  -1.0091 | PDE Loss:  -1.6448 | Function Loss:  -2.1233\n",
      "Total loss:  -1.0091 | PDE Loss:  -1.6446 | Function Loss:  -2.1234\n",
      "Total loss:  -1.0091 | PDE Loss:  -1.6446 | Function Loss:  -2.1235\n",
      "Total loss:  -1.0092 | PDE Loss:  -1.6447 | Function Loss:  -2.1236\n",
      "Total loss:  -1.0093 | PDE Loss:  -1.6449 | Function Loss:  -2.1236\n",
      "Total loss:  -1.0093 | PDE Loss:  -1.6452 | Function Loss:  -2.1235\n",
      "Total loss:  -1.0093 | PDE Loss:  -1.6454 | Function Loss:  -2.1235\n",
      "Total loss:  -1.0094 | PDE Loss:  -1.6458 | Function Loss:  -2.1235\n",
      "Total loss:  -1.0095 | PDE Loss:  -1.6463 | Function Loss:  -2.1234\n",
      "Total loss:  -1.0096 | PDE Loss:  -1.6469 | Function Loss:  -2.1233\n",
      "Total loss:  -1.0097 | PDE Loss:  -1.6481 | Function Loss:  -2.1231\n",
      "Total loss:  -1.0098 | PDE Loss:  -1.6483 | Function Loss:  -2.1232\n",
      "Total loss:  -1.0099 | PDE Loss:  -1.6485 | Function Loss:  -2.1233\n",
      "Total loss:  -1.0099 | PDE Loss:  -1.6485 | Function Loss:  -2.1233\n",
      "Total loss:  -1.01 | PDE Loss:  -1.6485 | Function Loss:  -2.1234\n",
      "Total loss:  -1.01 | PDE Loss:  -1.6485 | Function Loss:  -2.1235\n",
      "Total loss:  -1.01 | PDE Loss:  -1.6482 | Function Loss:  -2.1236\n",
      "Total loss:  -1.0101 | PDE Loss:  -1.6483 | Function Loss:  -2.1236\n",
      "Total loss:  -1.0101 | PDE Loss:  -1.6484 | Function Loss:  -2.1236\n",
      "Total loss:  -1.0102 | PDE Loss:  -1.6485 | Function Loss:  -2.1236\n",
      "Total loss:  -1.0102 | PDE Loss:  -1.6486 | Function Loss:  -2.1236\n",
      "Total loss:  -1.0102 | PDE Loss:  -1.6487 | Function Loss:  -2.1236\n",
      "Total loss:  -1.0102 | PDE Loss:  -1.6488 | Function Loss:  -2.1237\n",
      "Total loss:  -1.0103 | PDE Loss:  -1.6488 | Function Loss:  -2.1237\n",
      "Total loss:  -1.0104 | PDE Loss:  -1.6488 | Function Loss:  -2.1238\n",
      "Total loss:  -1.0105 | PDE Loss:  -1.6488 | Function Loss:  -2.124\n",
      "Total loss:  -1.0106 | PDE Loss:  -1.6488 | Function Loss:  -2.1242\n",
      "Total loss:  -1.0107 | PDE Loss:  -1.6484 | Function Loss:  -2.1244\n",
      "Total loss:  -1.0108 | PDE Loss:  -1.6485 | Function Loss:  -2.1245\n",
      "Total loss:  -1.0109 | PDE Loss:  -1.6484 | Function Loss:  -2.1246\n",
      "Total loss:  -1.0109 | PDE Loss:  -1.6484 | Function Loss:  -2.1246\n",
      "Total loss:  -1.0109 | PDE Loss:  -1.6485 | Function Loss:  -2.1247\n",
      "Total loss:  -1.011 | PDE Loss:  -1.6485 | Function Loss:  -2.1247\n",
      "Total loss:  -1.011 | PDE Loss:  -1.6484 | Function Loss:  -2.1248\n",
      "Total loss:  -1.0111 | PDE Loss:  -1.6483 | Function Loss:  -2.1249\n",
      "Total loss:  -1.0112 | PDE Loss:  -1.6478 | Function Loss:  -2.1252\n",
      "Total loss:  -1.0113 | PDE Loss:  -1.6475 | Function Loss:  -2.1255\n",
      "Total loss:  -1.0114 | PDE Loss:  -1.6467 | Function Loss:  -2.1258\n",
      "Total loss:  -1.0114 | PDE Loss:  -1.6466 | Function Loss:  -2.1259\n",
      "Total loss:  -1.0115 | PDE Loss:  -1.6466 | Function Loss:  -2.126\n",
      "Total loss:  -1.0116 | PDE Loss:  -1.6467 | Function Loss:  -2.126\n",
      "Total loss:  -1.0116 | PDE Loss:  -1.6468 | Function Loss:  -2.126\n",
      "Total loss:  -1.0116 | PDE Loss:  -1.6466 | Function Loss:  -2.1261\n",
      "Total loss:  -1.0117 | PDE Loss:  -1.6468 | Function Loss:  -2.1261\n",
      "Total loss:  -1.0117 | PDE Loss:  -1.647 | Function Loss:  -2.1261\n",
      "Total loss:  -1.0118 | PDE Loss:  -1.6472 | Function Loss:  -2.1261\n",
      "Total loss:  -1.0118 | PDE Loss:  -1.6472 | Function Loss:  -2.1261\n",
      "Total loss:  -1.0118 | PDE Loss:  -1.6474 | Function Loss:  -2.1261\n",
      "Total loss:  -1.0118 | PDE Loss:  -1.6475 | Function Loss:  -2.1261\n",
      "Total loss:  -1.0118 | PDE Loss:  -1.6475 | Function Loss:  -2.1261\n",
      "Total loss:  -1.0118 | PDE Loss:  -1.6475 | Function Loss:  -2.1261\n",
      "Total loss:  -1.0119 | PDE Loss:  -1.6475 | Function Loss:  -2.1261\n",
      "Total loss:  -1.0119 | PDE Loss:  -1.6475 | Function Loss:  -2.1262\n",
      "Total loss:  -1.0119 | PDE Loss:  -1.6475 | Function Loss:  -2.1262\n",
      "Total loss:  -1.012 | PDE Loss:  -1.6477 | Function Loss:  -2.1262\n",
      "Total loss:  -1.012 | PDE Loss:  -1.6474 | Function Loss:  -2.1264\n",
      "Total loss:  -1.012 | PDE Loss:  -1.6475 | Function Loss:  -2.1264\n",
      "Total loss:  -1.0121 | PDE Loss:  -1.6475 | Function Loss:  -2.1264\n",
      "Total loss:  -1.0121 | PDE Loss:  -1.6475 | Function Loss:  -2.1264\n",
      "Total loss:  -1.0121 | PDE Loss:  -1.6476 | Function Loss:  -2.1264\n",
      "Total loss:  -1.0121 | PDE Loss:  -1.6476 | Function Loss:  -2.1264\n",
      "Total loss:  -1.0121 | PDE Loss:  -1.6477 | Function Loss:  -2.1264\n",
      "Total loss:  -1.0122 | PDE Loss:  -1.6479 | Function Loss:  -2.1264\n",
      "Total loss:  -1.0122 | PDE Loss:  -1.6483 | Function Loss:  -2.1264\n",
      "Total loss:  -1.0114 | PDE Loss:  -1.6446 | Function Loss:  -2.1265\n",
      "Total loss:  -1.0122 | PDE Loss:  -1.648 | Function Loss:  -2.1265\n",
      "Total loss:  -1.0123 | PDE Loss:  -1.6485 | Function Loss:  -2.1264\n",
      "Total loss:  -1.0124 | PDE Loss:  -1.649 | Function Loss:  -2.1264\n",
      "Total loss:  -1.0125 | PDE Loss:  -1.6494 | Function Loss:  -2.1264\n",
      "Total loss:  -1.0125 | PDE Loss:  -1.6498 | Function Loss:  -2.1263\n",
      "Total loss:  -1.0126 | PDE Loss:  -1.6499 | Function Loss:  -2.1263\n",
      "Total loss:  -1.0126 | PDE Loss:  -1.65 | Function Loss:  -2.1264\n",
      "Total loss:  -1.0127 | PDE Loss:  -1.6501 | Function Loss:  -2.1264\n",
      "Total loss:  -1.0127 | PDE Loss:  -1.65 | Function Loss:  -2.1265\n",
      "Total loss:  -1.0128 | PDE Loss:  -1.65 | Function Loss:  -2.1266\n",
      "Total loss:  -1.0128 | PDE Loss:  -1.6499 | Function Loss:  -2.1266\n",
      "Total loss:  -1.0128 | PDE Loss:  -1.6499 | Function Loss:  -2.1267\n",
      "Total loss:  -1.0129 | PDE Loss:  -1.6498 | Function Loss:  -2.1268\n",
      "Total loss:  -1.0129 | PDE Loss:  -1.6497 | Function Loss:  -2.1268\n",
      "Total loss:  -1.0129 | PDE Loss:  -1.6496 | Function Loss:  -2.1269\n",
      "Total loss:  -1.013 | PDE Loss:  -1.6496 | Function Loss:  -2.127\n",
      "Total loss:  -1.013 | PDE Loss:  -1.6495 | Function Loss:  -2.127\n",
      "Total loss:  -1.0131 | PDE Loss:  -1.6493 | Function Loss:  -2.1272\n",
      "Total loss:  -1.0131 | PDE Loss:  -1.6495 | Function Loss:  -2.1272\n",
      "Total loss:  -1.0132 | PDE Loss:  -1.6493 | Function Loss:  -2.1273\n",
      "Total loss:  -1.0132 | PDE Loss:  -1.6492 | Function Loss:  -2.1274\n",
      "Total loss:  -1.0133 | PDE Loss:  -1.6492 | Function Loss:  -2.1275\n",
      "Total loss:  -1.0133 | PDE Loss:  -1.6493 | Function Loss:  -2.1275\n",
      "Total loss:  -1.0134 | PDE Loss:  -1.6494 | Function Loss:  -2.1276\n",
      "Total loss:  -1.0134 | PDE Loss:  -1.6495 | Function Loss:  -2.1276\n",
      "Total loss:  -1.0135 | PDE Loss:  -1.6497 | Function Loss:  -2.1276\n",
      "Total loss:  -1.0136 | PDE Loss:  -1.6499 | Function Loss:  -2.1277\n",
      "Total loss:  -1.0135 | PDE Loss:  -1.65 | Function Loss:  -2.1276\n",
      "Total loss:  -1.0136 | PDE Loss:  -1.65 | Function Loss:  -2.1277\n",
      "Total loss:  -1.0137 | PDE Loss:  -1.6502 | Function Loss:  -2.1277\n",
      "Total loss:  -1.0137 | PDE Loss:  -1.6501 | Function Loss:  -2.1278\n",
      "Total loss:  -1.0138 | PDE Loss:  -1.6501 | Function Loss:  -2.1279\n",
      "Total loss:  -1.0138 | PDE Loss:  -1.65 | Function Loss:  -2.1279\n",
      "Total loss:  -1.0138 | PDE Loss:  -1.6499 | Function Loss:  -2.128\n",
      "Total loss:  -1.0138 | PDE Loss:  -1.6498 | Function Loss:  -2.128\n",
      "Total loss:  -1.0138 | PDE Loss:  -1.6497 | Function Loss:  -2.1281\n",
      "Total loss:  -1.0139 | PDE Loss:  -1.6496 | Function Loss:  -2.1281\n",
      "Total loss:  -1.0138 | PDE Loss:  -1.6493 | Function Loss:  -2.1282\n",
      "Total loss:  -1.0139 | PDE Loss:  -1.6496 | Function Loss:  -2.1282\n",
      "Total loss:  -1.0139 | PDE Loss:  -1.6495 | Function Loss:  -2.1282\n",
      "Total loss:  -1.0139 | PDE Loss:  -1.6494 | Function Loss:  -2.1283\n",
      "Total loss:  -1.014 | PDE Loss:  -1.6495 | Function Loss:  -2.1283\n",
      "Total loss:  -1.014 | PDE Loss:  -1.6495 | Function Loss:  -2.1284\n",
      "Total loss:  -1.014 | PDE Loss:  -1.6496 | Function Loss:  -2.1283\n",
      "Total loss:  -1.0141 | PDE Loss:  -1.65 | Function Loss:  -2.1283\n",
      "Total loss:  -1.0141 | PDE Loss:  -1.6502 | Function Loss:  -2.1282\n",
      "Total loss:  -1.0142 | PDE Loss:  -1.6505 | Function Loss:  -2.1282\n",
      "Total loss:  -1.014 | PDE Loss:  -1.6501 | Function Loss:  -2.1281\n",
      "Total loss:  -1.0142 | PDE Loss:  -1.6506 | Function Loss:  -2.1282\n",
      "Total loss:  -1.0143 | PDE Loss:  -1.6509 | Function Loss:  -2.1282\n",
      "Total loss:  -1.0143 | PDE Loss:  -1.6511 | Function Loss:  -2.1283\n",
      "Total loss:  -1.0144 | PDE Loss:  -1.6516 | Function Loss:  -2.1282\n",
      "Total loss:  -1.0145 | PDE Loss:  -1.6516 | Function Loss:  -2.1283\n",
      "Total loss:  -1.0145 | PDE Loss:  -1.6516 | Function Loss:  -2.1284\n",
      "Total loss:  -1.0146 | PDE Loss:  -1.6516 | Function Loss:  -2.1285\n",
      "Total loss:  -1.0146 | PDE Loss:  -1.6515 | Function Loss:  -2.1285\n",
      "Total loss:  -1.0147 | PDE Loss:  -1.6515 | Function Loss:  -2.1286\n",
      "Total loss:  -1.0147 | PDE Loss:  -1.6514 | Function Loss:  -2.1287\n",
      "Total loss:  -1.0148 | PDE Loss:  -1.6513 | Function Loss:  -2.1288\n",
      "Total loss:  -1.0149 | PDE Loss:  -1.6508 | Function Loss:  -2.1291\n",
      "Total loss:  -1.0145 | PDE Loss:  -1.6502 | Function Loss:  -2.1288\n",
      "Total loss:  -1.0149 | PDE Loss:  -1.651 | Function Loss:  -2.1291\n",
      "Total loss:  -1.015 | PDE Loss:  -1.6507 | Function Loss:  -2.1293\n",
      "Total loss:  -1.0151 | PDE Loss:  -1.6505 | Function Loss:  -2.1295\n",
      "Total loss:  -1.0152 | PDE Loss:  -1.6503 | Function Loss:  -2.1296\n",
      "Total loss:  -1.0152 | PDE Loss:  -1.6502 | Function Loss:  -2.1297\n",
      "Total loss:  -1.0152 | PDE Loss:  -1.6502 | Function Loss:  -2.1297\n",
      "Total loss:  -1.0153 | PDE Loss:  -1.6502 | Function Loss:  -2.1298\n",
      "Total loss:  -1.0153 | PDE Loss:  -1.6502 | Function Loss:  -2.1299\n",
      "Total loss:  -1.0153 | PDE Loss:  -1.6497 | Function Loss:  -2.13\n",
      "Total loss:  -1.0154 | PDE Loss:  -1.6497 | Function Loss:  -2.1301\n",
      "Total loss:  -1.0155 | PDE Loss:  -1.6497 | Function Loss:  -2.1302\n",
      "Total loss:  -1.0155 | PDE Loss:  -1.6497 | Function Loss:  -2.1302\n",
      "Total loss:  -1.0156 | PDE Loss:  -1.6497 | Function Loss:  -2.1303\n",
      "Total loss:  -1.0156 | PDE Loss:  -1.6496 | Function Loss:  -2.1304\n",
      "Total loss:  -1.0156 | PDE Loss:  -1.6496 | Function Loss:  -2.1304\n",
      "Total loss:  -1.0152 | PDE Loss:  -1.6473 | Function Loss:  -2.1306\n",
      "Total loss:  -1.0156 | PDE Loss:  -1.6494 | Function Loss:  -2.1305\n",
      "Total loss:  -1.0157 | PDE Loss:  -1.6494 | Function Loss:  -2.1305\n",
      "Total loss:  -1.0157 | PDE Loss:  -1.6494 | Function Loss:  -2.1306\n",
      "Total loss:  -1.0157 | PDE Loss:  -1.6494 | Function Loss:  -2.1306\n",
      "Total loss:  -1.0158 | PDE Loss:  -1.6494 | Function Loss:  -2.1307\n",
      "Total loss:  -1.0158 | PDE Loss:  -1.6494 | Function Loss:  -2.1307\n",
      "Total loss:  -1.0158 | PDE Loss:  -1.6494 | Function Loss:  -2.1307\n",
      "Total loss:  -1.0158 | PDE Loss:  -1.6494 | Function Loss:  -2.1307\n",
      "Total loss:  -1.0158 | PDE Loss:  -1.6495 | Function Loss:  -2.1308\n",
      "Total loss:  -1.0159 | PDE Loss:  -1.6495 | Function Loss:  -2.1308\n",
      "Total loss:  -1.0159 | PDE Loss:  -1.6496 | Function Loss:  -2.1308\n",
      "Total loss:  -1.0159 | PDE Loss:  -1.6497 | Function Loss:  -2.1308\n",
      "Total loss:  -1.016 | PDE Loss:  -1.6498 | Function Loss:  -2.1309\n",
      "Total loss:  -1.016 | PDE Loss:  -1.6499 | Function Loss:  -2.1309\n",
      "Total loss:  -1.0161 | PDE Loss:  -1.6499 | Function Loss:  -2.1309\n",
      "Total loss:  -1.0161 | PDE Loss:  -1.6499 | Function Loss:  -2.1309\n",
      "Total loss:  -1.0161 | PDE Loss:  -1.6499 | Function Loss:  -2.131\n",
      "Total loss:  -1.0162 | PDE Loss:  -1.6499 | Function Loss:  -2.131\n",
      "Total loss:  -1.0162 | PDE Loss:  -1.6498 | Function Loss:  -2.1312\n",
      "Total loss:  -1.0163 | PDE Loss:  -1.6497 | Function Loss:  -2.1313\n",
      "Total loss:  -1.0163 | PDE Loss:  -1.6497 | Function Loss:  -2.1313\n",
      "Total loss:  -1.0164 | PDE Loss:  -1.6496 | Function Loss:  -2.1314\n",
      "Total loss:  -1.0164 | PDE Loss:  -1.6495 | Function Loss:  -2.1315\n",
      "Total loss:  -1.0165 | PDE Loss:  -1.6494 | Function Loss:  -2.1316\n",
      "Total loss:  -1.0165 | PDE Loss:  -1.6492 | Function Loss:  -2.1317\n",
      "Total loss:  -1.0166 | PDE Loss:  -1.649 | Function Loss:  -2.1319\n",
      "Total loss:  -1.0167 | PDE Loss:  -1.6485 | Function Loss:  -2.1322\n",
      "Total loss:  -1.0168 | PDE Loss:  -1.6481 | Function Loss:  -2.1325\n",
      "Total loss:  -1.0169 | PDE Loss:  -1.6475 | Function Loss:  -2.1328\n",
      "Total loss:  -1.017 | PDE Loss:  -1.6474 | Function Loss:  -2.1329\n",
      "Total loss:  -1.0171 | PDE Loss:  -1.6472 | Function Loss:  -2.133\n",
      "Total loss:  -1.0171 | PDE Loss:  -1.6471 | Function Loss:  -2.1331\n",
      "Total loss:  -1.017 | PDE Loss:  -1.6461 | Function Loss:  -2.1334\n",
      "Total loss:  -1.0171 | PDE Loss:  -1.6469 | Function Loss:  -2.1332\n",
      "Total loss:  -1.0172 | PDE Loss:  -1.6469 | Function Loss:  -2.1333\n",
      "Total loss:  -1.0172 | PDE Loss:  -1.6469 | Function Loss:  -2.1333\n",
      "Total loss:  -1.0173 | PDE Loss:  -1.6468 | Function Loss:  -2.1334\n",
      "Total loss:  -1.0173 | PDE Loss:  -1.6467 | Function Loss:  -2.1335\n",
      "Total loss:  -1.0174 | PDE Loss:  -1.6467 | Function Loss:  -2.1336\n",
      "Total loss:  -1.0174 | PDE Loss:  -1.6465 | Function Loss:  -2.1337\n",
      "Total loss:  -1.0175 | PDE Loss:  -1.6465 | Function Loss:  -2.1338\n",
      "Total loss:  -1.0175 | PDE Loss:  -1.6464 | Function Loss:  -2.1338\n",
      "Total loss:  -1.0175 | PDE Loss:  -1.6465 | Function Loss:  -2.1339\n",
      "Total loss:  -1.0176 | PDE Loss:  -1.6465 | Function Loss:  -2.1339\n",
      "Total loss:  -1.0176 | PDE Loss:  -1.6466 | Function Loss:  -2.1339\n",
      "Total loss:  -1.0176 | PDE Loss:  -1.6467 | Function Loss:  -2.1339\n",
      "Total loss:  -1.0176 | PDE Loss:  -1.6468 | Function Loss:  -2.1339\n",
      "Total loss:  -1.0177 | PDE Loss:  -1.6469 | Function Loss:  -2.1339\n",
      "Total loss:  -1.0177 | PDE Loss:  -1.647 | Function Loss:  -2.1339\n",
      "Total loss:  -1.0177 | PDE Loss:  -1.647 | Function Loss:  -2.1339\n",
      "Total loss:  -1.0177 | PDE Loss:  -1.647 | Function Loss:  -2.134\n",
      "Total loss:  -1.0178 | PDE Loss:  -1.6469 | Function Loss:  -2.134\n",
      "Total loss:  -1.0178 | PDE Loss:  -1.6469 | Function Loss:  -2.1341\n",
      "Total loss:  -1.0178 | PDE Loss:  -1.6468 | Function Loss:  -2.1342\n",
      "Total loss:  -1.0179 | PDE Loss:  -1.6468 | Function Loss:  -2.1342\n",
      "Total loss:  -1.0179 | PDE Loss:  -1.6468 | Function Loss:  -2.1343\n",
      "Total loss:  -1.018 | PDE Loss:  -1.647 | Function Loss:  -2.1343\n",
      "Total loss:  -1.018 | PDE Loss:  -1.647 | Function Loss:  -2.1344\n",
      "Total loss:  -1.0181 | PDE Loss:  -1.6471 | Function Loss:  -2.1344\n",
      "Total loss:  -1.0181 | PDE Loss:  -1.6471 | Function Loss:  -2.1344\n",
      "Total loss:  -1.0182 | PDE Loss:  -1.6473 | Function Loss:  -2.1345\n",
      "Total loss:  -1.0183 | PDE Loss:  -1.6474 | Function Loss:  -2.1345\n",
      "Total loss:  -1.0183 | PDE Loss:  -1.6475 | Function Loss:  -2.1346\n",
      "Total loss:  -1.0184 | PDE Loss:  -1.6476 | Function Loss:  -2.1347\n",
      "Total loss:  -1.0185 | PDE Loss:  -1.6474 | Function Loss:  -2.1348\n",
      "Total loss:  -1.0185 | PDE Loss:  -1.6474 | Function Loss:  -2.1349\n",
      "Total loss:  -1.0186 | PDE Loss:  -1.6474 | Function Loss:  -2.1349\n",
      "Total loss:  -1.0186 | PDE Loss:  -1.6473 | Function Loss:  -2.135\n",
      "Total loss:  -1.0186 | PDE Loss:  -1.6471 | Function Loss:  -2.1351\n",
      "Total loss:  -1.0184 | PDE Loss:  -1.6465 | Function Loss:  -2.135\n",
      "Total loss:  -1.0187 | PDE Loss:  -1.6472 | Function Loss:  -2.1352\n",
      "Total loss:  -1.0187 | PDE Loss:  -1.6473 | Function Loss:  -2.1352\n",
      "Total loss:  -1.0188 | PDE Loss:  -1.6476 | Function Loss:  -2.1352\n",
      "Total loss:  -1.0188 | PDE Loss:  -1.6479 | Function Loss:  -2.1351\n",
      "Total loss:  -1.0189 | PDE Loss:  -1.6482 | Function Loss:  -2.1351\n",
      "Total loss:  -1.0189 | PDE Loss:  -1.6485 | Function Loss:  -2.1351\n",
      "Total loss:  -1.019 | PDE Loss:  -1.649 | Function Loss:  -2.135\n",
      "Total loss:  -1.019 | PDE Loss:  -1.6495 | Function Loss:  -2.1349\n",
      "Total loss:  -1.0191 | PDE Loss:  -1.6497 | Function Loss:  -2.1349\n",
      "Total loss:  -1.0191 | PDE Loss:  -1.65 | Function Loss:  -2.1349\n",
      "Total loss:  -1.0192 | PDE Loss:  -1.6502 | Function Loss:  -2.1349\n",
      "Total loss:  -1.0193 | PDE Loss:  -1.6503 | Function Loss:  -2.135\n",
      "Total loss:  -1.0193 | PDE Loss:  -1.6503 | Function Loss:  -2.135\n",
      "Total loss:  -1.0193 | PDE Loss:  -1.6504 | Function Loss:  -2.135\n",
      "Total loss:  -1.0194 | PDE Loss:  -1.6503 | Function Loss:  -2.1351\n",
      "Total loss:  -1.0195 | PDE Loss:  -1.6512 | Function Loss:  -2.1349\n",
      "Total loss:  -1.0195 | PDE Loss:  -1.6509 | Function Loss:  -2.1352\n",
      "Total loss:  -1.0196 | PDE Loss:  -1.6507 | Function Loss:  -2.1353\n",
      "Total loss:  -1.0197 | PDE Loss:  -1.6507 | Function Loss:  -2.1354\n",
      "Total loss:  -1.0199 | PDE Loss:  -1.6509 | Function Loss:  -2.1356\n",
      "Total loss:  -1.02 | PDE Loss:  -1.6512 | Function Loss:  -2.1356\n",
      "Total loss:  -1.0201 | PDE Loss:  -1.6514 | Function Loss:  -2.1357\n",
      "Total loss:  -1.0201 | PDE Loss:  -1.6516 | Function Loss:  -2.1357\n",
      "Total loss:  -1.0202 | PDE Loss:  -1.6516 | Function Loss:  -2.1357\n",
      "Total loss:  -1.0202 | PDE Loss:  -1.6519 | Function Loss:  -2.1357\n",
      "Total loss:  -1.0203 | PDE Loss:  -1.6518 | Function Loss:  -2.1358\n",
      "Total loss:  -1.0203 | PDE Loss:  -1.6519 | Function Loss:  -2.1358\n",
      "Total loss:  -1.0204 | PDE Loss:  -1.6517 | Function Loss:  -2.136\n",
      "Total loss:  -1.0204 | PDE Loss:  -1.6514 | Function Loss:  -2.1361\n",
      "Total loss:  -1.0205 | PDE Loss:  -1.6512 | Function Loss:  -2.1363\n",
      "Total loss:  -1.0206 | PDE Loss:  -1.6511 | Function Loss:  -2.1364\n",
      "Total loss:  -1.0206 | PDE Loss:  -1.6503 | Function Loss:  -2.1367\n",
      "Total loss:  -1.0207 | PDE Loss:  -1.6505 | Function Loss:  -2.1367\n",
      "Total loss:  -1.0207 | PDE Loss:  -1.6507 | Function Loss:  -2.1367\n",
      "Total loss:  -1.0207 | PDE Loss:  -1.651 | Function Loss:  -2.1367\n",
      "Total loss:  -1.0208 | PDE Loss:  -1.6512 | Function Loss:  -2.1366\n",
      "Total loss:  -1.0208 | PDE Loss:  -1.6515 | Function Loss:  -2.1366\n",
      "Total loss:  -1.0208 | PDE Loss:  -1.6517 | Function Loss:  -2.1366\n",
      "Total loss:  -1.0209 | PDE Loss:  -1.6519 | Function Loss:  -2.1366\n",
      "Total loss:  -1.0209 | PDE Loss:  -1.652 | Function Loss:  -2.1366\n",
      "Total loss:  -1.021 | PDE Loss:  -1.6525 | Function Loss:  -2.1366\n",
      "Total loss:  -1.021 | PDE Loss:  -1.6526 | Function Loss:  -2.1366\n",
      "Total loss:  -1.0211 | PDE Loss:  -1.6528 | Function Loss:  -2.1366\n",
      "Total loss:  -1.0212 | PDE Loss:  -1.6529 | Function Loss:  -2.1366\n",
      "Total loss:  -1.0212 | PDE Loss:  -1.653 | Function Loss:  -2.1367\n",
      "Total loss:  -1.0213 | PDE Loss:  -1.6531 | Function Loss:  -2.1367\n",
      "Total loss:  -1.0212 | PDE Loss:  -1.6531 | Function Loss:  -2.1367\n",
      "Total loss:  -1.0213 | PDE Loss:  -1.6533 | Function Loss:  -2.1367\n",
      "Total loss:  -1.0214 | PDE Loss:  -1.6532 | Function Loss:  -2.1369\n",
      "Total loss:  -1.0215 | PDE Loss:  -1.6531 | Function Loss:  -2.137\n",
      "Total loss:  -1.0215 | PDE Loss:  -1.6531 | Function Loss:  -2.137\n",
      "Total loss:  -1.0215 | PDE Loss:  -1.6531 | Function Loss:  -2.1371\n",
      "Total loss:  -1.0216 | PDE Loss:  -1.6531 | Function Loss:  -2.1372\n",
      "Total loss:  -1.0215 | PDE Loss:  -1.6527 | Function Loss:  -2.1372\n",
      "Total loss:  -1.0216 | PDE Loss:  -1.653 | Function Loss:  -2.1372\n",
      "Total loss:  -1.0216 | PDE Loss:  -1.653 | Function Loss:  -2.1373\n",
      "Total loss:  -1.0217 | PDE Loss:  -1.653 | Function Loss:  -2.1373\n",
      "Total loss:  -1.0218 | PDE Loss:  -1.653 | Function Loss:  -2.1374\n",
      "Total loss:  -1.0219 | PDE Loss:  -1.653 | Function Loss:  -2.1375\n",
      "Total loss:  -1.0219 | PDE Loss:  -1.653 | Function Loss:  -2.1376\n",
      "Total loss:  -1.022 | PDE Loss:  -1.6531 | Function Loss:  -2.1376\n",
      "Total loss:  -1.022 | PDE Loss:  -1.6532 | Function Loss:  -2.1376\n",
      "Total loss:  -1.0221 | PDE Loss:  -1.6534 | Function Loss:  -2.1377\n",
      "Total loss:  -1.0221 | PDE Loss:  -1.6537 | Function Loss:  -2.1376\n",
      "Total loss:  -1.0222 | PDE Loss:  -1.6539 | Function Loss:  -2.1377\n",
      "Total loss:  -1.0223 | PDE Loss:  -1.6542 | Function Loss:  -2.1377\n",
      "Total loss:  -1.0223 | PDE Loss:  -1.6544 | Function Loss:  -2.1377\n",
      "Total loss:  -1.0224 | PDE Loss:  -1.6546 | Function Loss:  -2.1377\n",
      "Total loss:  -1.0225 | PDE Loss:  -1.6547 | Function Loss:  -2.1378\n",
      "Total loss:  -1.0225 | PDE Loss:  -1.6547 | Function Loss:  -2.1378\n",
      "Total loss:  -1.0226 | PDE Loss:  -1.6547 | Function Loss:  -2.1379\n",
      "Total loss:  -1.0226 | PDE Loss:  -1.6545 | Function Loss:  -2.1381\n",
      "Total loss:  -1.0227 | PDE Loss:  -1.6546 | Function Loss:  -2.1381\n",
      "Total loss:  -1.0228 | PDE Loss:  -1.6544 | Function Loss:  -2.1383\n",
      "Total loss:  -1.0228 | PDE Loss:  -1.6541 | Function Loss:  -2.1384\n",
      "Total loss:  -1.0229 | PDE Loss:  -1.654 | Function Loss:  -2.1386\n",
      "Total loss:  -1.023 | PDE Loss:  -1.654 | Function Loss:  -2.1387\n",
      "Total loss:  -1.0231 | PDE Loss:  -1.6539 | Function Loss:  -2.1388\n",
      "Total loss:  -1.0231 | PDE Loss:  -1.6541 | Function Loss:  -2.1388\n",
      "Total loss:  -1.0232 | PDE Loss:  -1.6544 | Function Loss:  -2.1388\n",
      "Total loss:  -1.0233 | PDE Loss:  -1.6545 | Function Loss:  -2.1389\n",
      "Total loss:  -1.0233 | PDE Loss:  -1.6548 | Function Loss:  -2.1389\n",
      "Total loss:  -1.0234 | PDE Loss:  -1.6551 | Function Loss:  -2.1389\n",
      "Total loss:  -1.0234 | PDE Loss:  -1.6553 | Function Loss:  -2.1389\n",
      "Total loss:  -1.0235 | PDE Loss:  -1.6554 | Function Loss:  -2.1389\n",
      "Total loss:  -1.0235 | PDE Loss:  -1.6542 | Function Loss:  -2.1393\n",
      "Total loss:  -1.0236 | PDE Loss:  -1.655 | Function Loss:  -2.1391\n",
      "Total loss:  -1.0236 | PDE Loss:  -1.6551 | Function Loss:  -2.1392\n",
      "Total loss:  -1.0237 | PDE Loss:  -1.6551 | Function Loss:  -2.1393\n",
      "Total loss:  -1.0237 | PDE Loss:  -1.655 | Function Loss:  -2.1393\n",
      "Total loss:  -1.0238 | PDE Loss:  -1.6549 | Function Loss:  -2.1394\n",
      "Total loss:  -1.0238 | PDE Loss:  -1.6548 | Function Loss:  -2.1395\n",
      "Total loss:  -1.0238 | PDE Loss:  -1.6547 | Function Loss:  -2.1396\n",
      "Total loss:  -1.0239 | PDE Loss:  -1.6547 | Function Loss:  -2.1396\n",
      "Total loss:  -1.0239 | PDE Loss:  -1.6547 | Function Loss:  -2.1397\n",
      "Total loss:  -1.0216 | PDE Loss:  -1.6527 | Function Loss:  -2.1373\n",
      "Total loss:  -1.0239 | PDE Loss:  -1.6548 | Function Loss:  -2.1396\n",
      "Total loss:  -1.0239 | PDE Loss:  -1.6548 | Function Loss:  -2.1397\n",
      "Total loss:  -1.024 | PDE Loss:  -1.6549 | Function Loss:  -2.1397\n",
      "Total loss:  -1.024 | PDE Loss:  -1.655 | Function Loss:  -2.1398\n",
      "Total loss:  -1.024 | PDE Loss:  -1.6548 | Function Loss:  -2.1398\n",
      "Total loss:  -1.0241 | PDE Loss:  -1.655 | Function Loss:  -2.1398\n",
      "Total loss:  -1.0241 | PDE Loss:  -1.6551 | Function Loss:  -2.1398\n",
      "Total loss:  -1.0241 | PDE Loss:  -1.6551 | Function Loss:  -2.1398\n",
      "Total loss:  -1.0242 | PDE Loss:  -1.655 | Function Loss:  -2.1399\n",
      "Total loss:  -1.0242 | PDE Loss:  -1.655 | Function Loss:  -2.14\n",
      "Total loss:  -1.0242 | PDE Loss:  -1.6548 | Function Loss:  -2.1401\n",
      "Total loss:  -1.0243 | PDE Loss:  -1.6546 | Function Loss:  -2.1402\n",
      "Total loss:  -1.0243 | PDE Loss:  -1.6535 | Function Loss:  -2.1406\n",
      "Total loss:  -1.0244 | PDE Loss:  -1.6537 | Function Loss:  -2.1406\n",
      "Total loss:  -1.0245 | PDE Loss:  -1.6537 | Function Loss:  -2.1407\n",
      "Total loss:  -1.0245 | PDE Loss:  -1.6537 | Function Loss:  -2.1408\n",
      "Total loss:  -1.0246 | PDE Loss:  -1.6535 | Function Loss:  -2.1409\n",
      "Total loss:  -1.0246 | PDE Loss:  -1.6533 | Function Loss:  -2.1411\n",
      "Total loss:  -1.0247 | PDE Loss:  -1.653 | Function Loss:  -2.1412\n",
      "Total loss:  -1.0248 | PDE Loss:  -1.6528 | Function Loss:  -2.1414\n",
      "Total loss:  -1.0249 | PDE Loss:  -1.6517 | Function Loss:  -2.1418\n",
      "Total loss:  -1.0249 | PDE Loss:  -1.6515 | Function Loss:  -2.142\n",
      "Total loss:  -1.025 | PDE Loss:  -1.6513 | Function Loss:  -2.1421\n",
      "Total loss:  -1.025 | PDE Loss:  -1.6513 | Function Loss:  -2.1422\n",
      "Total loss:  -1.025 | PDE Loss:  -1.6504 | Function Loss:  -2.1424\n",
      "Total loss:  -1.025 | PDE Loss:  -1.651 | Function Loss:  -2.1423\n",
      "Total loss:  -1.0251 | PDE Loss:  -1.6511 | Function Loss:  -2.1423\n",
      "Total loss:  -1.0251 | PDE Loss:  -1.6511 | Function Loss:  -2.1424\n",
      "Total loss:  -1.0252 | PDE Loss:  -1.6512 | Function Loss:  -2.1424\n",
      "Total loss:  -1.0252 | PDE Loss:  -1.6512 | Function Loss:  -2.1425\n",
      "Total loss:  -1.0253 | PDE Loss:  -1.6513 | Function Loss:  -2.1425\n",
      "Total loss:  -1.0254 | PDE Loss:  -1.651 | Function Loss:  -2.1427\n",
      "Total loss:  -1.0254 | PDE Loss:  -1.651 | Function Loss:  -2.1428\n",
      "Total loss:  -1.0255 | PDE Loss:  -1.6512 | Function Loss:  -2.1428\n",
      "Total loss:  -1.0256 | PDE Loss:  -1.6514 | Function Loss:  -2.1428\n",
      "Total loss:  -1.0256 | PDE Loss:  -1.6516 | Function Loss:  -2.1429\n",
      "Total loss:  -1.0257 | PDE Loss:  -1.6518 | Function Loss:  -2.1429\n",
      "Total loss:  -1.0257 | PDE Loss:  -1.652 | Function Loss:  -2.1429\n",
      "Total loss:  -1.0257 | PDE Loss:  -1.6523 | Function Loss:  -2.1428\n",
      "Total loss:  -1.0257 | PDE Loss:  -1.6523 | Function Loss:  -2.1428\n",
      "Total loss:  -1.0258 | PDE Loss:  -1.6524 | Function Loss:  -2.1428\n",
      "Total loss:  -1.0258 | PDE Loss:  -1.6525 | Function Loss:  -2.1428\n",
      "Total loss:  -1.0259 | PDE Loss:  -1.6526 | Function Loss:  -2.1429\n",
      "Total loss:  -1.0259 | PDE Loss:  -1.6527 | Function Loss:  -2.1429\n",
      "Total loss:  -1.026 | PDE Loss:  -1.6529 | Function Loss:  -2.1429\n",
      "Total loss:  -1.026 | PDE Loss:  -1.6529 | Function Loss:  -2.143\n",
      "Total loss:  -1.0261 | PDE Loss:  -1.653 | Function Loss:  -2.143\n",
      "Total loss:  -1.0261 | PDE Loss:  -1.6529 | Function Loss:  -2.1431\n",
      "Total loss:  -1.0262 | PDE Loss:  -1.6529 | Function Loss:  -2.1432\n",
      "Total loss:  -1.0263 | PDE Loss:  -1.6529 | Function Loss:  -2.1433\n",
      "Total loss:  -1.0263 | PDE Loss:  -1.6531 | Function Loss:  -2.1434\n",
      "Total loss:  -1.0264 | PDE Loss:  -1.6531 | Function Loss:  -2.1434\n",
      "Total loss:  -1.0264 | PDE Loss:  -1.6532 | Function Loss:  -2.1434\n",
      "Total loss:  -1.0265 | PDE Loss:  -1.6536 | Function Loss:  -2.1434\n",
      "Total loss:  -1.0265 | PDE Loss:  -1.6539 | Function Loss:  -2.1433\n",
      "Total loss:  -1.0266 | PDE Loss:  -1.6542 | Function Loss:  -2.1433\n",
      "Total loss:  -1.0266 | PDE Loss:  -1.6546 | Function Loss:  -2.1432\n",
      "Total loss:  -1.0266 | PDE Loss:  -1.6551 | Function Loss:  -2.1431\n",
      "Total loss:  -1.0266 | PDE Loss:  -1.6552 | Function Loss:  -2.1431\n",
      "Total loss:  -1.0267 | PDE Loss:  -1.6553 | Function Loss:  -2.1431\n",
      "Total loss:  -1.0267 | PDE Loss:  -1.6553 | Function Loss:  -2.1431\n",
      "Total loss:  -1.0267 | PDE Loss:  -1.6554 | Function Loss:  -2.1431\n",
      "Total loss:  -1.0267 | PDE Loss:  -1.6555 | Function Loss:  -2.1431\n",
      "Total loss:  -1.0267 | PDE Loss:  -1.6558 | Function Loss:  -2.1431\n",
      "Total loss:  -1.0268 | PDE Loss:  -1.6561 | Function Loss:  -2.143\n",
      "Total loss:  -1.0268 | PDE Loss:  -1.6568 | Function Loss:  -2.1428\n",
      "Total loss:  -1.0268 | PDE Loss:  -1.6565 | Function Loss:  -2.1429\n",
      "Total loss:  -1.0268 | PDE Loss:  -1.6567 | Function Loss:  -2.1429\n",
      "Total loss:  -1.0269 | PDE Loss:  -1.6571 | Function Loss:  -2.1428\n",
      "Total loss:  -1.0269 | PDE Loss:  -1.6573 | Function Loss:  -2.1428\n",
      "Total loss:  -1.0269 | PDE Loss:  -1.6575 | Function Loss:  -2.1428\n",
      "Total loss:  -1.027 | PDE Loss:  -1.6576 | Function Loss:  -2.1428\n",
      "Total loss:  -1.027 | PDE Loss:  -1.6577 | Function Loss:  -2.1428\n",
      "Total loss:  -1.027 | PDE Loss:  -1.6579 | Function Loss:  -2.1428\n",
      "Total loss:  -1.0271 | PDE Loss:  -1.658 | Function Loss:  -2.1428\n",
      "Total loss:  -1.0271 | PDE Loss:  -1.6582 | Function Loss:  -2.1429\n",
      "Total loss:  -1.0272 | PDE Loss:  -1.6582 | Function Loss:  -2.1429\n",
      "Total loss:  -1.0272 | PDE Loss:  -1.6583 | Function Loss:  -2.1429\n",
      "Total loss:  -1.0273 | PDE Loss:  -1.6584 | Function Loss:  -2.143\n",
      "Total loss:  -1.0273 | PDE Loss:  -1.6585 | Function Loss:  -2.143\n",
      "Total loss:  -1.0274 | PDE Loss:  -1.6586 | Function Loss:  -2.143\n",
      "Total loss:  -1.0274 | PDE Loss:  -1.6587 | Function Loss:  -2.143\n",
      "Total loss:  -1.0275 | PDE Loss:  -1.6587 | Function Loss:  -2.1431\n",
      "Total loss:  -1.0275 | PDE Loss:  -1.6591 | Function Loss:  -2.143\n",
      "Total loss:  -1.0276 | PDE Loss:  -1.659 | Function Loss:  -2.1432\n",
      "Total loss:  -1.0276 | PDE Loss:  -1.659 | Function Loss:  -2.1432\n",
      "Total loss:  -1.0277 | PDE Loss:  -1.6589 | Function Loss:  -2.1433\n",
      "Total loss:  -1.0277 | PDE Loss:  -1.6589 | Function Loss:  -2.1434\n",
      "Total loss:  -1.0278 | PDE Loss:  -1.6589 | Function Loss:  -2.1434\n",
      "Total loss:  -1.0278 | PDE Loss:  -1.6591 | Function Loss:  -2.1435\n",
      "Total loss:  -1.0279 | PDE Loss:  -1.659 | Function Loss:  -2.1436\n",
      "Total loss:  -1.0279 | PDE Loss:  -1.6591 | Function Loss:  -2.1436\n",
      "Total loss:  -1.028 | PDE Loss:  -1.6594 | Function Loss:  -2.1436\n",
      "Total loss:  -1.028 | PDE Loss:  -1.6596 | Function Loss:  -2.1436\n",
      "Total loss:  -1.0281 | PDE Loss:  -1.6598 | Function Loss:  -2.1436\n",
      "Total loss:  -1.0281 | PDE Loss:  -1.6599 | Function Loss:  -2.1436\n",
      "Total loss:  -1.0281 | PDE Loss:  -1.6599 | Function Loss:  -2.1436\n",
      "Total loss:  -1.0281 | PDE Loss:  -1.6605 | Function Loss:  -2.1434\n",
      "Total loss:  -1.0282 | PDE Loss:  -1.6604 | Function Loss:  -2.1435\n",
      "Total loss:  -1.0282 | PDE Loss:  -1.6602 | Function Loss:  -2.1436\n",
      "Total loss:  -1.0282 | PDE Loss:  -1.6601 | Function Loss:  -2.1437\n",
      "Total loss:  -1.0282 | PDE Loss:  -1.66 | Function Loss:  -2.1437\n",
      "Total loss:  -1.0283 | PDE Loss:  -1.6599 | Function Loss:  -2.1438\n",
      "Total loss:  -1.0283 | PDE Loss:  -1.6599 | Function Loss:  -2.1438\n",
      "Total loss:  -1.0283 | PDE Loss:  -1.6598 | Function Loss:  -2.1438\n",
      "Total loss:  -1.0283 | PDE Loss:  -1.6597 | Function Loss:  -2.1439\n",
      "Total loss:  -1.0284 | PDE Loss:  -1.6598 | Function Loss:  -2.1439\n",
      "Total loss:  -1.0284 | PDE Loss:  -1.6599 | Function Loss:  -2.1439\n",
      "Total loss:  -1.0284 | PDE Loss:  -1.66 | Function Loss:  -2.1439\n",
      "Total loss:  -1.0284 | PDE Loss:  -1.6601 | Function Loss:  -2.1439\n",
      "Total loss:  -1.0285 | PDE Loss:  -1.6602 | Function Loss:  -2.1439\n",
      "Total loss:  -1.0285 | PDE Loss:  -1.6602 | Function Loss:  -2.144\n",
      "Total loss:  -1.0285 | PDE Loss:  -1.6601 | Function Loss:  -2.144\n",
      "Total loss:  -1.0285 | PDE Loss:  -1.6601 | Function Loss:  -2.144\n",
      "Total loss:  -1.0285 | PDE Loss:  -1.6597 | Function Loss:  -2.1442\n",
      "Total loss:  -1.0286 | PDE Loss:  -1.6596 | Function Loss:  -2.1443\n",
      "Total loss:  -1.0286 | PDE Loss:  -1.6594 | Function Loss:  -2.1443\n",
      "Total loss:  -1.0286 | PDE Loss:  -1.6592 | Function Loss:  -2.1445\n",
      "Total loss:  -1.0287 | PDE Loss:  -1.659 | Function Loss:  -2.1446\n",
      "Total loss:  -1.0287 | PDE Loss:  -1.6589 | Function Loss:  -2.1447\n",
      "Total loss:  -1.0287 | PDE Loss:  -1.6587 | Function Loss:  -2.1448\n",
      "Total loss:  -1.0287 | PDE Loss:  -1.6586 | Function Loss:  -2.1448\n",
      "Total loss:  -1.0288 | PDE Loss:  -1.6587 | Function Loss:  -2.1448\n",
      "Total loss:  -1.0288 | PDE Loss:  -1.6588 | Function Loss:  -2.1448\n",
      "Total loss:  -1.0289 | PDE Loss:  -1.6589 | Function Loss:  -2.1448\n",
      "Total loss:  -1.0289 | PDE Loss:  -1.6591 | Function Loss:  -2.1448\n",
      "Total loss:  -1.0289 | PDE Loss:  -1.6592 | Function Loss:  -2.1448\n",
      "Total loss:  -1.0289 | PDE Loss:  -1.6593 | Function Loss:  -2.1448\n",
      "Total loss:  -1.0289 | PDE Loss:  -1.6594 | Function Loss:  -2.1448\n",
      "Total loss:  -1.029 | PDE Loss:  -1.6595 | Function Loss:  -2.1448\n",
      "Total loss:  -1.029 | PDE Loss:  -1.6601 | Function Loss:  -2.1447\n",
      "Total loss:  -1.029 | PDE Loss:  -1.6599 | Function Loss:  -2.1448\n",
      "Total loss:  -1.0291 | PDE Loss:  -1.6598 | Function Loss:  -2.1449\n",
      "Total loss:  -1.0291 | PDE Loss:  -1.6596 | Function Loss:  -2.145\n",
      "Total loss:  -1.0292 | PDE Loss:  -1.6596 | Function Loss:  -2.1451\n",
      "Total loss:  -1.0292 | PDE Loss:  -1.6596 | Function Loss:  -2.1451\n",
      "Total loss:  -1.0293 | PDE Loss:  -1.6596 | Function Loss:  -2.1452\n",
      "Total loss:  -1.0294 | PDE Loss:  -1.6598 | Function Loss:  -2.1452\n",
      "Total loss:  -1.0294 | PDE Loss:  -1.6602 | Function Loss:  -2.1452\n",
      "Total loss:  -1.0295 | PDE Loss:  -1.6606 | Function Loss:  -2.1452\n",
      "Total loss:  -1.0296 | PDE Loss:  -1.661 | Function Loss:  -2.1451\n",
      "Total loss:  -1.0296 | PDE Loss:  -1.6611 | Function Loss:  -2.1452\n",
      "Total loss:  -1.0296 | PDE Loss:  -1.6613 | Function Loss:  -2.1451\n",
      "Total loss:  -1.0297 | PDE Loss:  -1.6614 | Function Loss:  -2.1451\n",
      "Total loss:  -1.0297 | PDE Loss:  -1.6616 | Function Loss:  -2.1451\n",
      "Total loss:  -1.0298 | PDE Loss:  -1.6619 | Function Loss:  -2.1451\n",
      "Total loss:  -1.0298 | PDE Loss:  -1.6618 | Function Loss:  -2.1452\n",
      "Total loss:  -1.0299 | PDE Loss:  -1.6619 | Function Loss:  -2.1452\n",
      "Total loss:  -1.0299 | PDE Loss:  -1.662 | Function Loss:  -2.1452\n",
      "Total loss:  -1.0299 | PDE Loss:  -1.6621 | Function Loss:  -2.1453\n",
      "Total loss:  -1.03 | PDE Loss:  -1.6621 | Function Loss:  -2.1453\n",
      "Total loss:  -1.03 | PDE Loss:  -1.6622 | Function Loss:  -2.1453\n",
      "Total loss:  -1.03 | PDE Loss:  -1.6623 | Function Loss:  -2.1453\n",
      "Total loss:  -1.03 | PDE Loss:  -1.6624 | Function Loss:  -2.1453\n",
      "Total loss:  -1.0301 | PDE Loss:  -1.6626 | Function Loss:  -2.1453\n",
      "Total loss:  -1.0295 | PDE Loss:  -1.6618 | Function Loss:  -2.1448\n",
      "Total loss:  -1.0301 | PDE Loss:  -1.6626 | Function Loss:  -2.1453\n",
      "Total loss:  -1.0301 | PDE Loss:  -1.6629 | Function Loss:  -2.1453\n",
      "Total loss:  -1.0301 | PDE Loss:  -1.6632 | Function Loss:  -2.1452\n",
      "Total loss:  -1.0302 | PDE Loss:  -1.6636 | Function Loss:  -2.1451\n",
      "Total loss:  -1.0302 | PDE Loss:  -1.664 | Function Loss:  -2.1451\n",
      "Total loss:  -1.0303 | PDE Loss:  -1.6643 | Function Loss:  -2.1451\n",
      "Total loss:  -1.0303 | PDE Loss:  -1.6644 | Function Loss:  -2.1451\n",
      "Total loss:  -1.0304 | PDE Loss:  -1.6642 | Function Loss:  -2.1452\n",
      "Total loss:  -1.0304 | PDE Loss:  -1.6641 | Function Loss:  -2.1453\n",
      "Total loss:  -1.0305 | PDE Loss:  -1.6639 | Function Loss:  -2.1454\n",
      "Total loss:  -1.0305 | PDE Loss:  -1.6638 | Function Loss:  -2.1455\n",
      "Total loss:  -1.0306 | PDE Loss:  -1.6636 | Function Loss:  -2.1456\n",
      "Total loss:  -1.0306 | PDE Loss:  -1.6636 | Function Loss:  -2.1457\n",
      "Total loss:  -1.0307 | PDE Loss:  -1.6634 | Function Loss:  -2.1458\n",
      "Total loss:  -1.0307 | PDE Loss:  -1.6633 | Function Loss:  -2.1459\n",
      "Total loss:  -1.0308 | PDE Loss:  -1.6633 | Function Loss:  -2.146\n",
      "Total loss:  -1.0308 | PDE Loss:  -1.6633 | Function Loss:  -2.1461\n",
      "Total loss:  -1.0309 | PDE Loss:  -1.6632 | Function Loss:  -2.1462\n",
      "Total loss:  -1.0309 | PDE Loss:  -1.6635 | Function Loss:  -2.1462\n",
      "Total loss:  -1.031 | PDE Loss:  -1.6633 | Function Loss:  -2.1463\n",
      "Total loss:  -1.031 | PDE Loss:  -1.6636 | Function Loss:  -2.1463\n",
      "Total loss:  -1.0311 | PDE Loss:  -1.6636 | Function Loss:  -2.1463\n",
      "Total loss:  -1.0311 | PDE Loss:  -1.6635 | Function Loss:  -2.1464\n",
      "Total loss:  -1.0301 | PDE Loss:  -1.6632 | Function Loss:  -2.1451\n",
      "Total loss:  -1.0311 | PDE Loss:  -1.6637 | Function Loss:  -2.1464\n",
      "Total loss:  -1.0312 | PDE Loss:  -1.6636 | Function Loss:  -2.1464\n",
      "Total loss:  -1.0312 | PDE Loss:  -1.6635 | Function Loss:  -2.1465\n",
      "Total loss:  -1.0312 | PDE Loss:  -1.6634 | Function Loss:  -2.1466\n",
      "Total loss:  -1.0313 | PDE Loss:  -1.6634 | Function Loss:  -2.1467\n",
      "Total loss:  -1.0313 | PDE Loss:  -1.6633 | Function Loss:  -2.1467\n",
      "Total loss:  -1.0313 | PDE Loss:  -1.6634 | Function Loss:  -2.1467\n",
      "Total loss:  -1.0314 | PDE Loss:  -1.6634 | Function Loss:  -2.1468\n",
      "Total loss:  -1.0314 | PDE Loss:  -1.6638 | Function Loss:  -2.1467\n",
      "Total loss:  -1.0314 | PDE Loss:  -1.6637 | Function Loss:  -2.1467\n",
      "Total loss:  -1.0315 | PDE Loss:  -1.6636 | Function Loss:  -2.1468\n",
      "Total loss:  -1.0315 | PDE Loss:  -1.6634 | Function Loss:  -2.1469\n",
      "Total loss:  -1.0316 | PDE Loss:  -1.6632 | Function Loss:  -2.1471\n",
      "Total loss:  -1.0316 | PDE Loss:  -1.6631 | Function Loss:  -2.1472\n",
      "Total loss:  -1.0315 | PDE Loss:  -1.6621 | Function Loss:  -2.1473\n",
      "Total loss:  -1.0316 | PDE Loss:  -1.6629 | Function Loss:  -2.1472\n",
      "Total loss:  -1.0316 | PDE Loss:  -1.6629 | Function Loss:  -2.1473\n",
      "Total loss:  -1.0317 | PDE Loss:  -1.663 | Function Loss:  -2.1473\n",
      "Total loss:  -1.0317 | PDE Loss:  -1.6628 | Function Loss:  -2.1474\n",
      "Total loss:  -1.0318 | PDE Loss:  -1.6632 | Function Loss:  -2.1474\n",
      "Total loss:  -1.0318 | PDE Loss:  -1.6632 | Function Loss:  -2.1474\n",
      "Total loss:  -1.0318 | PDE Loss:  -1.6633 | Function Loss:  -2.1474\n",
      "Total loss:  -1.0319 | PDE Loss:  -1.6635 | Function Loss:  -2.1474\n",
      "Total loss:  -1.0319 | PDE Loss:  -1.6637 | Function Loss:  -2.1474\n",
      "Total loss:  -1.0319 | PDE Loss:  -1.6638 | Function Loss:  -2.1474\n",
      "Total loss:  -1.032 | PDE Loss:  -1.664 | Function Loss:  -2.1474\n",
      "Total loss:  -1.0321 | PDE Loss:  -1.6643 | Function Loss:  -2.1474\n",
      "Total loss:  -1.0321 | PDE Loss:  -1.6643 | Function Loss:  -2.1475\n",
      "Total loss:  -1.0322 | PDE Loss:  -1.6643 | Function Loss:  -2.1476\n",
      "Total loss:  -1.0322 | PDE Loss:  -1.6642 | Function Loss:  -2.1476\n",
      "Total loss:  -1.0323 | PDE Loss:  -1.6641 | Function Loss:  -2.1477\n",
      "Total loss:  -1.0323 | PDE Loss:  -1.664 | Function Loss:  -2.1478\n",
      "Total loss:  -1.0323 | PDE Loss:  -1.664 | Function Loss:  -2.1479\n",
      "Total loss:  -1.0324 | PDE Loss:  -1.6639 | Function Loss:  -2.1479\n",
      "Total loss:  -1.0324 | PDE Loss:  -1.6639 | Function Loss:  -2.148\n",
      "Total loss:  -1.0324 | PDE Loss:  -1.6638 | Function Loss:  -2.148\n",
      "Total loss:  -1.0325 | PDE Loss:  -1.6641 | Function Loss:  -2.148\n",
      "Total loss:  -1.0325 | PDE Loss:  -1.6641 | Function Loss:  -2.148\n",
      "Total loss:  -1.0326 | PDE Loss:  -1.6643 | Function Loss:  -2.148\n",
      "Total loss:  -1.0326 | PDE Loss:  -1.6643 | Function Loss:  -2.1481\n",
      "Total loss:  -1.0327 | PDE Loss:  -1.6643 | Function Loss:  -2.1482\n",
      "Total loss:  -1.0327 | PDE Loss:  -1.6637 | Function Loss:  -2.1484\n",
      "Total loss:  -1.0328 | PDE Loss:  -1.6637 | Function Loss:  -2.1485\n",
      "Total loss:  -1.0328 | PDE Loss:  -1.6635 | Function Loss:  -2.1486\n",
      "Total loss:  -1.0329 | PDE Loss:  -1.6634 | Function Loss:  -2.1487\n",
      "Total loss:  -1.0329 | PDE Loss:  -1.6633 | Function Loss:  -2.1488\n",
      "Total loss:  -1.0329 | PDE Loss:  -1.6632 | Function Loss:  -2.1489\n",
      "Total loss:  -1.033 | PDE Loss:  -1.6633 | Function Loss:  -2.1489\n",
      "Total loss:  -1.033 | PDE Loss:  -1.6626 | Function Loss:  -2.1491\n",
      "Total loss:  -1.033 | PDE Loss:  -1.663 | Function Loss:  -2.1491\n",
      "Total loss:  -1.0331 | PDE Loss:  -1.6634 | Function Loss:  -2.149\n",
      "Total loss:  -1.0332 | PDE Loss:  -1.6638 | Function Loss:  -2.149\n",
      "Total loss:  -1.0332 | PDE Loss:  -1.6641 | Function Loss:  -2.149\n",
      "Total loss:  -1.0332 | PDE Loss:  -1.6644 | Function Loss:  -2.1489\n",
      "Total loss:  -1.0333 | PDE Loss:  -1.6646 | Function Loss:  -2.1489\n",
      "Total loss:  -1.0333 | PDE Loss:  -1.6646 | Function Loss:  -2.1489\n",
      "Total loss:  -1.0333 | PDE Loss:  -1.6649 | Function Loss:  -2.1488\n",
      "Total loss:  -1.0333 | PDE Loss:  -1.6649 | Function Loss:  -2.1489\n",
      "Total loss:  -1.0334 | PDE Loss:  -1.6648 | Function Loss:  -2.1489\n",
      "Total loss:  -1.0334 | PDE Loss:  -1.6645 | Function Loss:  -2.1491\n",
      "Total loss:  -1.0335 | PDE Loss:  -1.6642 | Function Loss:  -2.1493\n",
      "Total loss:  -1.0335 | PDE Loss:  -1.6639 | Function Loss:  -2.1495\n",
      "Total loss:  -1.0336 | PDE Loss:  -1.6637 | Function Loss:  -2.1496\n",
      "Total loss:  -1.0336 | PDE Loss:  -1.6637 | Function Loss:  -2.1496\n",
      "Total loss:  -1.0337 | PDE Loss:  -1.6638 | Function Loss:  -2.1496\n",
      "Total loss:  -1.0337 | PDE Loss:  -1.6642 | Function Loss:  -2.1496\n",
      "Total loss:  -1.0338 | PDE Loss:  -1.6645 | Function Loss:  -2.1495\n",
      "Total loss:  -1.0338 | PDE Loss:  -1.6648 | Function Loss:  -2.1495\n",
      "Total loss:  -1.0339 | PDE Loss:  -1.6656 | Function Loss:  -2.1494\n",
      "Total loss:  -1.0339 | PDE Loss:  -1.6656 | Function Loss:  -2.1494\n",
      "Total loss:  -1.0339 | PDE Loss:  -1.6656 | Function Loss:  -2.1494\n",
      "Total loss:  -1.034 | PDE Loss:  -1.6655 | Function Loss:  -2.1495\n",
      "Total loss:  -1.034 | PDE Loss:  -1.6654 | Function Loss:  -2.1496\n",
      "Total loss:  -1.034 | PDE Loss:  -1.6652 | Function Loss:  -2.1497\n",
      "Total loss:  -1.0341 | PDE Loss:  -1.6651 | Function Loss:  -2.1498\n",
      "Total loss:  -1.0341 | PDE Loss:  -1.6645 | Function Loss:  -2.15\n",
      "Total loss:  -1.0341 | PDE Loss:  -1.6645 | Function Loss:  -2.15\n",
      "Total loss:  -1.0341 | PDE Loss:  -1.6646 | Function Loss:  -2.15\n",
      "Total loss:  -1.0341 | PDE Loss:  -1.6647 | Function Loss:  -2.15\n",
      "Total loss:  -1.0342 | PDE Loss:  -1.6648 | Function Loss:  -2.15\n",
      "Total loss:  -1.0342 | PDE Loss:  -1.6649 | Function Loss:  -2.15\n",
      "Total loss:  -1.0342 | PDE Loss:  -1.665 | Function Loss:  -2.15\n",
      "Total loss:  -1.0342 | PDE Loss:  -1.6651 | Function Loss:  -2.15\n",
      "Total loss:  -1.0343 | PDE Loss:  -1.6653 | Function Loss:  -2.15\n",
      "Total loss:  -1.0343 | PDE Loss:  -1.6657 | Function Loss:  -2.1499\n",
      "Total loss:  -1.0344 | PDE Loss:  -1.6658 | Function Loss:  -2.15\n",
      "Total loss:  -1.0344 | PDE Loss:  -1.6659 | Function Loss:  -2.15\n",
      "Total loss:  -1.0345 | PDE Loss:  -1.6661 | Function Loss:  -2.15\n",
      "Total loss:  -1.0346 | PDE Loss:  -1.6663 | Function Loss:  -2.1501\n",
      "Total loss:  -1.0346 | PDE Loss:  -1.6665 | Function Loss:  -2.1501\n",
      "Total loss:  -1.0346 | PDE Loss:  -1.6663 | Function Loss:  -2.1501\n",
      "Total loss:  -1.0346 | PDE Loss:  -1.6664 | Function Loss:  -2.1501\n",
      "Total loss:  -1.0347 | PDE Loss:  -1.6667 | Function Loss:  -2.1501\n",
      "Total loss:  -1.0347 | PDE Loss:  -1.6669 | Function Loss:  -2.1501\n",
      "Total loss:  -1.0348 | PDE Loss:  -1.6672 | Function Loss:  -2.15\n",
      "Total loss:  -1.0348 | PDE Loss:  -1.6674 | Function Loss:  -2.15\n",
      "Total loss:  -1.0348 | PDE Loss:  -1.6675 | Function Loss:  -2.15\n",
      "Total loss:  -1.0348 | PDE Loss:  -1.6676 | Function Loss:  -2.15\n",
      "Total loss:  -1.0349 | PDE Loss:  -1.6677 | Function Loss:  -2.15\n",
      "Total loss:  -1.0349 | PDE Loss:  -1.6679 | Function Loss:  -2.15\n",
      "Total loss:  -1.0349 | PDE Loss:  -1.6679 | Function Loss:  -2.15\n",
      "Total loss:  -1.035 | PDE Loss:  -1.668 | Function Loss:  -2.1501\n",
      "Total loss:  -1.035 | PDE Loss:  -1.668 | Function Loss:  -2.1501\n",
      "Total loss:  -1.0351 | PDE Loss:  -1.668 | Function Loss:  -2.1502\n",
      "Total loss:  -1.0351 | PDE Loss:  -1.668 | Function Loss:  -2.1502\n",
      "Total loss:  -1.0351 | PDE Loss:  -1.668 | Function Loss:  -2.1502\n",
      "Total loss:  -1.0351 | PDE Loss:  -1.668 | Function Loss:  -2.1503\n",
      "Total loss:  -1.0352 | PDE Loss:  -1.668 | Function Loss:  -2.1503\n",
      "Total loss:  -1.0352 | PDE Loss:  -1.6678 | Function Loss:  -2.1504\n",
      "Total loss:  -1.0352 | PDE Loss:  -1.6678 | Function Loss:  -2.1505\n",
      "Total loss:  -1.0353 | PDE Loss:  -1.6678 | Function Loss:  -2.1505\n",
      "Total loss:  -1.0353 | PDE Loss:  -1.6678 | Function Loss:  -2.1505\n",
      "Total loss:  -1.0353 | PDE Loss:  -1.6678 | Function Loss:  -2.1506\n",
      "Total loss:  -1.0354 | PDE Loss:  -1.6677 | Function Loss:  -2.1506\n",
      "Total loss:  -1.0354 | PDE Loss:  -1.6677 | Function Loss:  -2.1507\n",
      "Total loss:  -1.0354 | PDE Loss:  -1.6674 | Function Loss:  -2.1508\n",
      "Total loss:  -1.0354 | PDE Loss:  -1.6673 | Function Loss:  -2.1509\n",
      "Total loss:  -1.0355 | PDE Loss:  -1.6673 | Function Loss:  -2.1509\n",
      "Total loss:  -1.0355 | PDE Loss:  -1.6667 | Function Loss:  -2.1511\n",
      "Total loss:  -1.0355 | PDE Loss:  -1.6669 | Function Loss:  -2.1511\n",
      "Total loss:  -1.0355 | PDE Loss:  -1.667 | Function Loss:  -2.1511\n",
      "Total loss:  -1.0355 | PDE Loss:  -1.6671 | Function Loss:  -2.1511\n",
      "Total loss:  -1.0356 | PDE Loss:  -1.6672 | Function Loss:  -2.1511\n",
      "Total loss:  -1.0356 | PDE Loss:  -1.6673 | Function Loss:  -2.151\n",
      "Total loss:  -1.0356 | PDE Loss:  -1.6675 | Function Loss:  -2.151\n",
      "Total loss:  -1.0356 | PDE Loss:  -1.6682 | Function Loss:  -2.1508\n",
      "Total loss:  -1.0356 | PDE Loss:  -1.6678 | Function Loss:  -2.1509\n",
      "Total loss:  -1.0356 | PDE Loss:  -1.668 | Function Loss:  -2.1509\n",
      "Total loss:  -1.0356 | PDE Loss:  -1.6683 | Function Loss:  -2.1509\n",
      "Total loss:  -1.0357 | PDE Loss:  -1.6686 | Function Loss:  -2.1508\n",
      "Total loss:  -1.0357 | PDE Loss:  -1.6689 | Function Loss:  -2.1507\n",
      "Total loss:  -1.0357 | PDE Loss:  -1.6692 | Function Loss:  -2.1507\n",
      "Total loss:  -1.0357 | PDE Loss:  -1.6694 | Function Loss:  -2.1506\n",
      "Total loss:  -1.0357 | PDE Loss:  -1.6697 | Function Loss:  -2.1505\n",
      "Total loss:  -1.0358 | PDE Loss:  -1.6698 | Function Loss:  -2.1505\n",
      "Total loss:  -1.0358 | PDE Loss:  -1.6698 | Function Loss:  -2.1506\n",
      "Total loss:  -1.0358 | PDE Loss:  -1.6698 | Function Loss:  -2.1506\n",
      "Total loss:  -1.0358 | PDE Loss:  -1.6698 | Function Loss:  -2.1506\n",
      "Total loss:  -1.0358 | PDE Loss:  -1.67 | Function Loss:  -2.1506\n",
      "Total loss:  -1.0359 | PDE Loss:  -1.67 | Function Loss:  -2.1506\n",
      "Total loss:  -1.0359 | PDE Loss:  -1.6702 | Function Loss:  -2.1506\n",
      "Total loss:  -1.0359 | PDE Loss:  -1.67 | Function Loss:  -2.1507\n",
      "Total loss:  -1.036 | PDE Loss:  -1.6702 | Function Loss:  -2.1507\n",
      "Total loss:  -1.036 | PDE Loss:  -1.6704 | Function Loss:  -2.1506\n",
      "Total loss:  -1.036 | PDE Loss:  -1.6706 | Function Loss:  -2.1506\n",
      "Total loss:  -1.0361 | PDE Loss:  -1.6708 | Function Loss:  -2.1506\n",
      "Total loss:  -1.0361 | PDE Loss:  -1.6708 | Function Loss:  -2.1507\n",
      "Total loss:  -1.0361 | PDE Loss:  -1.6709 | Function Loss:  -2.1507\n",
      "Total loss:  -1.0361 | PDE Loss:  -1.6709 | Function Loss:  -2.1507\n",
      "Total loss:  -1.0361 | PDE Loss:  -1.6711 | Function Loss:  -2.1506\n",
      "Total loss:  -1.0362 | PDE Loss:  -1.6711 | Function Loss:  -2.1507\n",
      "Total loss:  -1.0362 | PDE Loss:  -1.6711 | Function Loss:  -2.1507\n",
      "Total loss:  -1.0362 | PDE Loss:  -1.6711 | Function Loss:  -2.1508\n",
      "Total loss:  -1.0363 | PDE Loss:  -1.6712 | Function Loss:  -2.1508\n",
      "Total loss:  -1.0363 | PDE Loss:  -1.6713 | Function Loss:  -2.1508\n",
      "Total loss:  -1.0364 | PDE Loss:  -1.6715 | Function Loss:  -2.1508\n",
      "Total loss:  -1.0364 | PDE Loss:  -1.671 | Function Loss:  -2.1509\n",
      "Total loss:  -1.0364 | PDE Loss:  -1.6713 | Function Loss:  -2.1509\n",
      "Total loss:  -1.0364 | PDE Loss:  -1.6716 | Function Loss:  -2.1509\n",
      "Total loss:  -1.0365 | PDE Loss:  -1.6719 | Function Loss:  -2.1508\n",
      "Total loss:  -1.0365 | PDE Loss:  -1.6722 | Function Loss:  -2.1508\n",
      "Total loss:  -1.0365 | PDE Loss:  -1.6724 | Function Loss:  -2.1508\n",
      "Total loss:  -1.0366 | PDE Loss:  -1.6726 | Function Loss:  -2.1508\n",
      "Total loss:  -1.0366 | PDE Loss:  -1.6727 | Function Loss:  -2.1508\n",
      "Total loss:  -1.0367 | PDE Loss:  -1.6729 | Function Loss:  -2.1508\n",
      "Total loss:  -1.0368 | PDE Loss:  -1.6729 | Function Loss:  -2.1509\n",
      "Total loss:  -1.0368 | PDE Loss:  -1.6729 | Function Loss:  -2.151\n",
      "Total loss:  -1.0369 | PDE Loss:  -1.6729 | Function Loss:  -2.1511\n",
      "Total loss:  -1.037 | PDE Loss:  -1.6728 | Function Loss:  -2.1512\n",
      "Total loss:  -1.037 | PDE Loss:  -1.6727 | Function Loss:  -2.1513\n",
      "Total loss:  -1.037 | PDE Loss:  -1.673 | Function Loss:  -2.1512\n",
      "Total loss:  -1.0371 | PDE Loss:  -1.6729 | Function Loss:  -2.1513\n",
      "Total loss:  -1.0371 | PDE Loss:  -1.673 | Function Loss:  -2.1513\n",
      "Total loss:  -1.0372 | PDE Loss:  -1.6733 | Function Loss:  -2.1513\n",
      "Total loss:  -1.0372 | PDE Loss:  -1.6735 | Function Loss:  -2.1513\n",
      "Total loss:  -1.0372 | PDE Loss:  -1.6731 | Function Loss:  -2.1514\n",
      "Total loss:  -1.0372 | PDE Loss:  -1.6734 | Function Loss:  -2.1514\n",
      "Total loss:  -1.0373 | PDE Loss:  -1.6737 | Function Loss:  -2.1513\n",
      "Total loss:  -1.0373 | PDE Loss:  -1.6739 | Function Loss:  -2.1513\n",
      "Total loss:  -1.0373 | PDE Loss:  -1.674 | Function Loss:  -2.1513\n",
      "Total loss:  -1.0374 | PDE Loss:  -1.6741 | Function Loss:  -2.1513\n",
      "Total loss:  -1.0374 | PDE Loss:  -1.6741 | Function Loss:  -2.1513\n",
      "Total loss:  -1.0374 | PDE Loss:  -1.6741 | Function Loss:  -2.1514\n",
      "Total loss:  -1.0374 | PDE Loss:  -1.6741 | Function Loss:  -2.1514\n",
      "Total loss:  -1.0374 | PDE Loss:  -1.674 | Function Loss:  -2.1515\n",
      "Total loss:  -1.0375 | PDE Loss:  -1.6739 | Function Loss:  -2.1515\n",
      "Total loss:  -1.0375 | PDE Loss:  -1.6739 | Function Loss:  -2.1516\n",
      "Total loss:  -1.0376 | PDE Loss:  -1.6738 | Function Loss:  -2.1517\n",
      "Total loss:  -1.0376 | PDE Loss:  -1.6738 | Function Loss:  -2.1518\n",
      "Total loss:  -1.0377 | PDE Loss:  -1.6739 | Function Loss:  -2.1518\n",
      "Total loss:  -1.0377 | PDE Loss:  -1.6741 | Function Loss:  -2.1518\n",
      "Total loss:  -1.0378 | PDE Loss:  -1.6744 | Function Loss:  -2.1518\n",
      "Total loss:  -1.0378 | PDE Loss:  -1.6747 | Function Loss:  -2.1517\n",
      "Total loss:  -1.0378 | PDE Loss:  -1.6749 | Function Loss:  -2.1517\n",
      "Total loss:  -1.0379 | PDE Loss:  -1.6751 | Function Loss:  -2.1517\n",
      "Total loss:  -1.0379 | PDE Loss:  -1.6751 | Function Loss:  -2.1517\n",
      "Total loss:  -1.0379 | PDE Loss:  -1.6752 | Function Loss:  -2.1517\n",
      "Total loss:  -1.0379 | PDE Loss:  -1.6751 | Function Loss:  -2.1518\n",
      "Total loss:  -1.038 | PDE Loss:  -1.675 | Function Loss:  -2.1519\n",
      "Total loss:  -1.038 | PDE Loss:  -1.6748 | Function Loss:  -2.1519\n",
      "Total loss:  -1.038 | PDE Loss:  -1.6747 | Function Loss:  -2.152\n",
      "Total loss:  -1.038 | PDE Loss:  -1.6747 | Function Loss:  -2.152\n",
      "Total loss:  -1.038 | PDE Loss:  -1.6746 | Function Loss:  -2.1521\n",
      "Total loss:  -1.0381 | PDE Loss:  -1.6748 | Function Loss:  -2.1521\n",
      "Total loss:  -1.0381 | PDE Loss:  -1.6741 | Function Loss:  -2.1522\n",
      "Total loss:  -1.0381 | PDE Loss:  -1.6745 | Function Loss:  -2.1522\n",
      "Total loss:  -1.0381 | PDE Loss:  -1.6748 | Function Loss:  -2.1521\n",
      "Total loss:  -1.0381 | PDE Loss:  -1.6751 | Function Loss:  -2.152\n",
      "Total loss:  -1.0382 | PDE Loss:  -1.6754 | Function Loss:  -2.152\n",
      "Total loss:  -1.0382 | PDE Loss:  -1.6758 | Function Loss:  -2.1519\n",
      "Total loss:  -1.0382 | PDE Loss:  -1.676 | Function Loss:  -2.1518\n",
      "Total loss:  -1.0382 | PDE Loss:  -1.6763 | Function Loss:  -2.1518\n",
      "Total loss:  -1.0382 | PDE Loss:  -1.6765 | Function Loss:  -2.1517\n",
      "Total loss:  -1.0383 | PDE Loss:  -1.6769 | Function Loss:  -2.1517\n",
      "Total loss:  -1.0383 | PDE Loss:  -1.6772 | Function Loss:  -2.1517\n",
      "Total loss:  -1.0384 | PDE Loss:  -1.6774 | Function Loss:  -2.1517\n",
      "Total loss:  -1.0385 | PDE Loss:  -1.6778 | Function Loss:  -2.1517\n",
      "Total loss:  -1.0386 | PDE Loss:  -1.6778 | Function Loss:  -2.1518\n",
      "Total loss:  -1.0387 | PDE Loss:  -1.6777 | Function Loss:  -2.1519\n",
      "Total loss:  -1.0387 | PDE Loss:  -1.6776 | Function Loss:  -2.1521\n",
      "Total loss:  -1.0388 | PDE Loss:  -1.6775 | Function Loss:  -2.1522\n",
      "Total loss:  -1.0389 | PDE Loss:  -1.6775 | Function Loss:  -2.1523\n",
      "Total loss:  -1.039 | PDE Loss:  -1.6775 | Function Loss:  -2.1524\n",
      "Total loss:  -1.039 | PDE Loss:  -1.6778 | Function Loss:  -2.1524\n",
      "Total loss:  -1.0391 | PDE Loss:  -1.6779 | Function Loss:  -2.1525\n",
      "Total loss:  -1.0392 | PDE Loss:  -1.6784 | Function Loss:  -2.1524\n",
      "Total loss:  -1.0392 | PDE Loss:  -1.6788 | Function Loss:  -2.1523\n",
      "Total loss:  -1.0393 | PDE Loss:  -1.6793 | Function Loss:  -2.1523\n",
      "Total loss:  -1.0393 | PDE Loss:  -1.6797 | Function Loss:  -2.1522\n",
      "Total loss:  -1.0394 | PDE Loss:  -1.68 | Function Loss:  -2.1522\n",
      "Total loss:  -1.0394 | PDE Loss:  -1.6803 | Function Loss:  -2.1522\n",
      "Total loss:  -1.0395 | PDE Loss:  -1.6806 | Function Loss:  -2.1522\n",
      "Total loss:  -1.0396 | PDE Loss:  -1.6805 | Function Loss:  -2.1523\n",
      "Total loss:  -1.0396 | PDE Loss:  -1.6806 | Function Loss:  -2.1523\n",
      "Total loss:  -1.0396 | PDE Loss:  -1.6804 | Function Loss:  -2.1524\n",
      "Total loss:  -1.0397 | PDE Loss:  -1.6803 | Function Loss:  -2.1525\n",
      "Total loss:  -1.0397 | PDE Loss:  -1.6803 | Function Loss:  -2.1526\n",
      "Total loss:  -1.0397 | PDE Loss:  -1.6803 | Function Loss:  -2.1526\n",
      "Total loss:  -1.0398 | PDE Loss:  -1.6806 | Function Loss:  -2.1526\n",
      "Total loss:  -1.0399 | PDE Loss:  -1.6811 | Function Loss:  -2.1525\n",
      "Total loss:  -1.04 | PDE Loss:  -1.6819 | Function Loss:  -2.1524\n",
      "Total loss:  -1.04 | PDE Loss:  -1.6825 | Function Loss:  -2.1523\n",
      "Total loss:  -1.0401 | PDE Loss:  -1.6827 | Function Loss:  -2.1523\n",
      "Total loss:  -1.0401 | PDE Loss:  -1.6831 | Function Loss:  -2.1522\n",
      "Total loss:  -1.0401 | PDE Loss:  -1.6835 | Function Loss:  -2.1521\n",
      "Total loss:  -1.0402 | PDE Loss:  -1.6838 | Function Loss:  -2.1521\n",
      "Total loss:  -1.0402 | PDE Loss:  -1.684 | Function Loss:  -2.1521\n",
      "Total loss:  -1.0402 | PDE Loss:  -1.6843 | Function Loss:  -2.152\n",
      "Total loss:  -1.0403 | PDE Loss:  -1.6844 | Function Loss:  -2.152\n",
      "Total loss:  -1.0403 | PDE Loss:  -1.6843 | Function Loss:  -2.1521\n",
      "Total loss:  -1.0403 | PDE Loss:  -1.6842 | Function Loss:  -2.1522\n",
      "Total loss:  -1.0403 | PDE Loss:  -1.684 | Function Loss:  -2.1522\n",
      "Total loss:  -1.0404 | PDE Loss:  -1.684 | Function Loss:  -2.1523\n",
      "Total loss:  -1.0404 | PDE Loss:  -1.6838 | Function Loss:  -2.1523\n",
      "Total loss:  -1.0404 | PDE Loss:  -1.6837 | Function Loss:  -2.1524\n",
      "Total loss:  -1.0404 | PDE Loss:  -1.6835 | Function Loss:  -2.1525\n",
      "Total loss:  -1.0404 | PDE Loss:  -1.6833 | Function Loss:  -2.1526\n",
      "Total loss:  -1.0405 | PDE Loss:  -1.6832 | Function Loss:  -2.1527\n",
      "Total loss:  -1.0405 | PDE Loss:  -1.683 | Function Loss:  -2.1528\n",
      "Total loss:  -1.0405 | PDE Loss:  -1.6829 | Function Loss:  -2.1528\n",
      "Total loss:  -1.0405 | PDE Loss:  -1.6827 | Function Loss:  -2.1529\n",
      "Total loss:  -1.0406 | PDE Loss:  -1.6826 | Function Loss:  -2.153\n",
      "Total loss:  -1.0406 | PDE Loss:  -1.6826 | Function Loss:  -2.153\n",
      "Total loss:  -1.0406 | PDE Loss:  -1.6827 | Function Loss:  -2.153\n",
      "Total loss:  -1.0407 | PDE Loss:  -1.6827 | Function Loss:  -2.153\n",
      "Total loss:  -1.0407 | PDE Loss:  -1.6829 | Function Loss:  -2.153\n",
      "Total loss:  -1.0407 | PDE Loss:  -1.6831 | Function Loss:  -2.153\n",
      "Total loss:  -1.0407 | PDE Loss:  -1.6833 | Function Loss:  -2.153\n",
      "Total loss:  -1.0408 | PDE Loss:  -1.6836 | Function Loss:  -2.153\n",
      "Total loss:  -1.0408 | PDE Loss:  -1.6838 | Function Loss:  -2.1529\n",
      "Total loss:  -1.0409 | PDE Loss:  -1.6839 | Function Loss:  -2.153\n",
      "Total loss:  -1.0409 | PDE Loss:  -1.6838 | Function Loss:  -2.153\n",
      "Total loss:  -1.0409 | PDE Loss:  -1.6838 | Function Loss:  -2.1531\n",
      "Total loss:  -1.041 | PDE Loss:  -1.6837 | Function Loss:  -2.1532\n",
      "Total loss:  -1.041 | PDE Loss:  -1.6836 | Function Loss:  -2.1532\n",
      "Total loss:  -1.0406 | PDE Loss:  -1.6807 | Function Loss:  -2.1536\n",
      "Total loss:  -1.041 | PDE Loss:  -1.6834 | Function Loss:  -2.1533\n",
      "Total loss:  -1.041 | PDE Loss:  -1.6834 | Function Loss:  -2.1533\n",
      "Total loss:  -1.0411 | PDE Loss:  -1.6835 | Function Loss:  -2.1533\n",
      "Total loss:  -1.0411 | PDE Loss:  -1.6834 | Function Loss:  -2.1534\n",
      "Total loss:  -1.0412 | PDE Loss:  -1.6835 | Function Loss:  -2.1534\n",
      "Total loss:  -1.0412 | PDE Loss:  -1.6837 | Function Loss:  -2.1535\n",
      "Total loss:  -1.0413 | PDE Loss:  -1.6834 | Function Loss:  -2.1536\n",
      "Total loss:  -1.0413 | PDE Loss:  -1.6838 | Function Loss:  -2.1536\n",
      "Total loss:  -1.0413 | PDE Loss:  -1.6839 | Function Loss:  -2.1535\n",
      "Total loss:  -1.0413 | PDE Loss:  -1.6841 | Function Loss:  -2.1535\n",
      "Total loss:  -1.0414 | PDE Loss:  -1.6842 | Function Loss:  -2.1535\n",
      "Total loss:  -1.0414 | PDE Loss:  -1.6844 | Function Loss:  -2.1535\n",
      "Total loss:  -1.0414 | PDE Loss:  -1.6843 | Function Loss:  -2.1536\n",
      "Total loss:  -1.0415 | PDE Loss:  -1.6843 | Function Loss:  -2.1536\n",
      "Total loss:  -1.0415 | PDE Loss:  -1.6838 | Function Loss:  -2.1538\n",
      "Total loss:  -1.0415 | PDE Loss:  -1.6841 | Function Loss:  -2.1537\n",
      "Total loss:  -1.0415 | PDE Loss:  -1.684 | Function Loss:  -2.1538\n",
      "Total loss:  -1.0415 | PDE Loss:  -1.6839 | Function Loss:  -2.1538\n",
      "Total loss:  -1.0416 | PDE Loss:  -1.6838 | Function Loss:  -2.1539\n",
      "Total loss:  -1.0416 | PDE Loss:  -1.6837 | Function Loss:  -2.154\n",
      "Total loss:  -1.0416 | PDE Loss:  -1.6836 | Function Loss:  -2.154\n",
      "Total loss:  -1.0417 | PDE Loss:  -1.6835 | Function Loss:  -2.1541\n",
      "Total loss:  -1.0417 | PDE Loss:  -1.6835 | Function Loss:  -2.1542\n",
      "Total loss:  -1.0417 | PDE Loss:  -1.6834 | Function Loss:  -2.1543\n",
      "Total loss:  -1.0418 | PDE Loss:  -1.6835 | Function Loss:  -2.1543\n",
      "Total loss:  -1.0418 | PDE Loss:  -1.6832 | Function Loss:  -2.1544\n",
      "Total loss:  -1.0419 | PDE Loss:  -1.6834 | Function Loss:  -2.1544\n",
      "Total loss:  -1.0419 | PDE Loss:  -1.6837 | Function Loss:  -2.1544\n",
      "Total loss:  -1.042 | PDE Loss:  -1.6839 | Function Loss:  -2.1544\n",
      "Total loss:  -1.042 | PDE Loss:  -1.6841 | Function Loss:  -2.1543\n",
      "Total loss:  -1.042 | PDE Loss:  -1.6842 | Function Loss:  -2.1543\n",
      "Total loss:  -1.042 | PDE Loss:  -1.6843 | Function Loss:  -2.1543\n",
      "Total loss:  -1.042 | PDE Loss:  -1.6844 | Function Loss:  -2.1543\n",
      "Total loss:  -1.0421 | PDE Loss:  -1.6846 | Function Loss:  -2.1543\n",
      "Total loss:  -1.0421 | PDE Loss:  -1.6848 | Function Loss:  -2.1543\n",
      "Total loss:  -1.0422 | PDE Loss:  -1.6851 | Function Loss:  -2.1543\n",
      "Total loss:  -1.0422 | PDE Loss:  -1.6853 | Function Loss:  -2.1543\n",
      "Total loss:  -1.042 | PDE Loss:  -1.6854 | Function Loss:  -2.154\n",
      "Total loss:  -1.0422 | PDE Loss:  -1.6854 | Function Loss:  -2.1543\n",
      "Total loss:  -1.0423 | PDE Loss:  -1.6855 | Function Loss:  -2.1543\n",
      "Total loss:  -1.0423 | PDE Loss:  -1.6858 | Function Loss:  -2.1543\n",
      "Total loss:  -1.0424 | PDE Loss:  -1.686 | Function Loss:  -2.1543\n",
      "Total loss:  -1.0424 | PDE Loss:  -1.6861 | Function Loss:  -2.1543\n",
      "Total loss:  -1.0424 | PDE Loss:  -1.6864 | Function Loss:  -2.1543\n",
      "Total loss:  -1.0425 | PDE Loss:  -1.6864 | Function Loss:  -2.1543\n",
      "Total loss:  -1.0425 | PDE Loss:  -1.6865 | Function Loss:  -2.1543\n",
      "Total loss:  -1.0426 | PDE Loss:  -1.687 | Function Loss:  -2.1543\n",
      "Total loss:  -1.0426 | PDE Loss:  -1.687 | Function Loss:  -2.1543\n",
      "Total loss:  -1.0426 | PDE Loss:  -1.6871 | Function Loss:  -2.1543\n",
      "Total loss:  -1.0427 | PDE Loss:  -1.6873 | Function Loss:  -2.1543\n",
      "Total loss:  -1.0427 | PDE Loss:  -1.6874 | Function Loss:  -2.1543\n",
      "Total loss:  -1.0427 | PDE Loss:  -1.6878 | Function Loss:  -2.1543\n",
      "Total loss:  -1.0428 | PDE Loss:  -1.6879 | Function Loss:  -2.1543\n",
      "Total loss:  -1.0428 | PDE Loss:  -1.6882 | Function Loss:  -2.1543\n",
      "Total loss:  -1.0429 | PDE Loss:  -1.6884 | Function Loss:  -2.1543\n",
      "Total loss:  -1.043 | PDE Loss:  -1.6886 | Function Loss:  -2.1543\n",
      "Total loss:  -1.043 | PDE Loss:  -1.6887 | Function Loss:  -2.1543\n",
      "Total loss:  -1.0431 | PDE Loss:  -1.6886 | Function Loss:  -2.1544\n",
      "Total loss:  -1.0431 | PDE Loss:  -1.6886 | Function Loss:  -2.1545\n",
      "Total loss:  -1.0431 | PDE Loss:  -1.6885 | Function Loss:  -2.1546\n",
      "Total loss:  -1.0432 | PDE Loss:  -1.6884 | Function Loss:  -2.1547\n",
      "Total loss:  -1.0433 | PDE Loss:  -1.6882 | Function Loss:  -2.1548\n",
      "Total loss:  -1.0433 | PDE Loss:  -1.6879 | Function Loss:  -2.1549\n",
      "Total loss:  -1.0433 | PDE Loss:  -1.6879 | Function Loss:  -2.155\n",
      "Total loss:  -1.0434 | PDE Loss:  -1.688 | Function Loss:  -2.155\n",
      "Total loss:  -1.0434 | PDE Loss:  -1.688 | Function Loss:  -2.155\n",
      "Total loss:  -1.0434 | PDE Loss:  -1.6881 | Function Loss:  -2.1551\n",
      "Total loss:  -1.0435 | PDE Loss:  -1.6883 | Function Loss:  -2.1551\n",
      "Total loss:  -1.0435 | PDE Loss:  -1.6884 | Function Loss:  -2.1551\n",
      "Total loss:  -1.0435 | PDE Loss:  -1.6886 | Function Loss:  -2.1549\n",
      "Total loss:  -1.0435 | PDE Loss:  -1.6886 | Function Loss:  -2.155\n",
      "Total loss:  -1.0436 | PDE Loss:  -1.6887 | Function Loss:  -2.155\n",
      "Total loss:  -1.0436 | PDE Loss:  -1.6888 | Function Loss:  -2.155\n",
      "Total loss:  -1.0436 | PDE Loss:  -1.6889 | Function Loss:  -2.1551\n",
      "Total loss:  -1.0437 | PDE Loss:  -1.6889 | Function Loss:  -2.1551\n",
      "Total loss:  -1.0436 | PDE Loss:  -1.6892 | Function Loss:  -2.1549\n",
      "Total loss:  -1.0437 | PDE Loss:  -1.6891 | Function Loss:  -2.1551\n",
      "Total loss:  -1.0437 | PDE Loss:  -1.6891 | Function Loss:  -2.1551\n",
      "Total loss:  -1.0437 | PDE Loss:  -1.6891 | Function Loss:  -2.1552\n",
      "Total loss:  -1.0438 | PDE Loss:  -1.6891 | Function Loss:  -2.1552\n",
      "Total loss:  -1.0438 | PDE Loss:  -1.6892 | Function Loss:  -2.1552\n",
      "Total loss:  -1.0439 | PDE Loss:  -1.6893 | Function Loss:  -2.1553\n",
      "Total loss:  -1.0439 | PDE Loss:  -1.6896 | Function Loss:  -2.1553\n",
      "Total loss:  -1.044 | PDE Loss:  -1.6897 | Function Loss:  -2.1553\n",
      "Total loss:  -1.044 | PDE Loss:  -1.6902 | Function Loss:  -2.1552\n",
      "Total loss:  -1.0441 | PDE Loss:  -1.6907 | Function Loss:  -2.1552\n",
      "Total loss:  -1.0442 | PDE Loss:  -1.691 | Function Loss:  -2.1552\n",
      "Total loss:  -1.0442 | PDE Loss:  -1.6916 | Function Loss:  -2.1551\n",
      "Total loss:  -1.0443 | PDE Loss:  -1.6916 | Function Loss:  -2.1551\n",
      "Total loss:  -1.0443 | PDE Loss:  -1.6917 | Function Loss:  -2.1552\n",
      "Total loss:  -1.0444 | PDE Loss:  -1.6917 | Function Loss:  -2.1552\n",
      "Total loss:  -1.0444 | PDE Loss:  -1.6917 | Function Loss:  -2.1552\n",
      "Total loss:  -1.0444 | PDE Loss:  -1.6917 | Function Loss:  -2.1553\n",
      "Total loss:  -1.0445 | PDE Loss:  -1.6917 | Function Loss:  -2.1554\n",
      "Total loss:  -1.0445 | PDE Loss:  -1.6916 | Function Loss:  -2.1555\n",
      "Total loss:  -1.0446 | PDE Loss:  -1.6914 | Function Loss:  -2.1556\n",
      "Total loss:  -1.0444 | PDE Loss:  -1.69 | Function Loss:  -2.1557\n",
      "Total loss:  -1.0446 | PDE Loss:  -1.6913 | Function Loss:  -2.1557\n",
      "Total loss:  -1.0447 | PDE Loss:  -1.6912 | Function Loss:  -2.1558\n",
      "Total loss:  -1.0448 | PDE Loss:  -1.6911 | Function Loss:  -2.1559\n",
      "Total loss:  -1.0449 | PDE Loss:  -1.691 | Function Loss:  -2.1561\n",
      "Total loss:  -1.045 | PDE Loss:  -1.6909 | Function Loss:  -2.1562\n",
      "Total loss:  -1.045 | PDE Loss:  -1.6909 | Function Loss:  -2.1563\n",
      "Total loss:  -1.0451 | PDE Loss:  -1.6912 | Function Loss:  -2.1564\n",
      "Total loss:  -1.0453 | PDE Loss:  -1.6914 | Function Loss:  -2.1565\n",
      "Total loss:  -1.0453 | PDE Loss:  -1.6916 | Function Loss:  -2.1565\n",
      "Total loss:  -1.0454 | PDE Loss:  -1.6918 | Function Loss:  -2.1565\n",
      "Total loss:  -1.0455 | PDE Loss:  -1.692 | Function Loss:  -2.1565\n",
      "Total loss:  -1.0455 | PDE Loss:  -1.6922 | Function Loss:  -2.1565\n",
      "Total loss:  -1.0456 | PDE Loss:  -1.6924 | Function Loss:  -2.1566\n",
      "Total loss:  -1.0457 | PDE Loss:  -1.6928 | Function Loss:  -2.1566\n",
      "Total loss:  -1.0457 | PDE Loss:  -1.6931 | Function Loss:  -2.1565\n",
      "Total loss:  -1.0458 | PDE Loss:  -1.6934 | Function Loss:  -2.1566\n",
      "Total loss:  -1.0459 | PDE Loss:  -1.6936 | Function Loss:  -2.1566\n",
      "Total loss:  -1.0459 | PDE Loss:  -1.6937 | Function Loss:  -2.1566\n",
      "Total loss:  -1.046 | PDE Loss:  -1.6939 | Function Loss:  -2.1566\n",
      "Total loss:  -1.046 | PDE Loss:  -1.6941 | Function Loss:  -2.1566\n",
      "Total loss:  -1.046 | PDE Loss:  -1.6937 | Function Loss:  -2.1567\n",
      "Total loss:  -1.046 | PDE Loss:  -1.694 | Function Loss:  -2.1567\n",
      "Total loss:  -1.0461 | PDE Loss:  -1.6942 | Function Loss:  -2.1567\n",
      "Total loss:  -1.0461 | PDE Loss:  -1.6944 | Function Loss:  -2.1567\n",
      "Total loss:  -1.0462 | PDE Loss:  -1.6947 | Function Loss:  -2.1567\n",
      "Total loss:  -1.0462 | PDE Loss:  -1.6949 | Function Loss:  -2.1567\n",
      "Total loss:  -1.0463 | PDE Loss:  -1.6952 | Function Loss:  -2.1567\n",
      "Total loss:  -1.0463 | PDE Loss:  -1.6954 | Function Loss:  -2.1567\n",
      "Total loss:  -1.0464 | PDE Loss:  -1.6956 | Function Loss:  -2.1567\n",
      "Total loss:  -1.0465 | PDE Loss:  -1.6959 | Function Loss:  -2.1567\n",
      "Total loss:  -1.0466 | PDE Loss:  -1.696 | Function Loss:  -2.1568\n",
      "Total loss:  -1.0466 | PDE Loss:  -1.6961 | Function Loss:  -2.1568\n",
      "Total loss:  -1.0467 | PDE Loss:  -1.6961 | Function Loss:  -2.1569\n",
      "Total loss:  -1.0467 | PDE Loss:  -1.6961 | Function Loss:  -2.157\n",
      "Total loss:  -1.0467 | PDE Loss:  -1.6962 | Function Loss:  -2.157\n",
      "Total loss:  -1.0468 | PDE Loss:  -1.6963 | Function Loss:  -2.157\n",
      "Total loss:  -1.0468 | PDE Loss:  -1.6963 | Function Loss:  -2.157\n",
      "Total loss:  -1.0468 | PDE Loss:  -1.6964 | Function Loss:  -2.157\n",
      "Total loss:  -1.0469 | PDE Loss:  -1.6964 | Function Loss:  -2.1571\n",
      "Total loss:  -1.0469 | PDE Loss:  -1.6965 | Function Loss:  -2.1571\n",
      "Total loss:  -1.0469 | PDE Loss:  -1.6966 | Function Loss:  -2.1571\n",
      "Total loss:  -1.047 | PDE Loss:  -1.6966 | Function Loss:  -2.1571\n",
      "Total loss:  -1.047 | PDE Loss:  -1.6966 | Function Loss:  -2.1572\n",
      "Total loss:  -1.0469 | PDE Loss:  -1.6976 | Function Loss:  -2.1568\n",
      "Total loss:  -1.047 | PDE Loss:  -1.697 | Function Loss:  -2.1571\n",
      "Total loss:  -1.047 | PDE Loss:  -1.6969 | Function Loss:  -2.1571\n",
      "Total loss:  -1.0471 | PDE Loss:  -1.6968 | Function Loss:  -2.1572\n",
      "Total loss:  -1.0471 | PDE Loss:  -1.6967 | Function Loss:  -2.1573\n",
      "Total loss:  -1.0471 | PDE Loss:  -1.6966 | Function Loss:  -2.1574\n",
      "Total loss:  -1.0472 | PDE Loss:  -1.6965 | Function Loss:  -2.1574\n",
      "Total loss:  -1.0472 | PDE Loss:  -1.6964 | Function Loss:  -2.1575\n",
      "Total loss:  -1.0472 | PDE Loss:  -1.6963 | Function Loss:  -2.1576\n",
      "Total loss:  -1.0472 | PDE Loss:  -1.6963 | Function Loss:  -2.1576\n",
      "Total loss:  -1.0473 | PDE Loss:  -1.6964 | Function Loss:  -2.1576\n",
      "Total loss:  -1.0473 | PDE Loss:  -1.6963 | Function Loss:  -2.1577\n",
      "Total loss:  -1.0474 | PDE Loss:  -1.6963 | Function Loss:  -2.1577\n",
      "Total loss:  -1.0474 | PDE Loss:  -1.6964 | Function Loss:  -2.1578\n",
      "Total loss:  -1.0474 | PDE Loss:  -1.6964 | Function Loss:  -2.1578\n",
      "Total loss:  -1.0474 | PDE Loss:  -1.6965 | Function Loss:  -2.1578\n",
      "Total loss:  -1.0475 | PDE Loss:  -1.6966 | Function Loss:  -2.1578\n",
      "Total loss:  -1.0475 | PDE Loss:  -1.6969 | Function Loss:  -2.1577\n",
      "Total loss:  -1.0475 | PDE Loss:  -1.6969 | Function Loss:  -2.1577\n",
      "Total loss:  -1.0475 | PDE Loss:  -1.697 | Function Loss:  -2.1577\n",
      "Total loss:  -1.0475 | PDE Loss:  -1.697 | Function Loss:  -2.1578\n",
      "Total loss:  -1.0476 | PDE Loss:  -1.6972 | Function Loss:  -2.1577\n",
      "Total loss:  -1.0476 | PDE Loss:  -1.6973 | Function Loss:  -2.1577\n",
      "Total loss:  -1.0476 | PDE Loss:  -1.6974 | Function Loss:  -2.1577\n",
      "Total loss:  -1.0476 | PDE Loss:  -1.6974 | Function Loss:  -2.1578\n",
      "Total loss:  -1.0477 | PDE Loss:  -1.6976 | Function Loss:  -2.1578\n",
      "Total loss:  -1.0477 | PDE Loss:  -1.6976 | Function Loss:  -2.1578\n",
      "Total loss:  -1.0477 | PDE Loss:  -1.697 | Function Loss:  -2.158\n",
      "Total loss:  -1.0477 | PDE Loss:  -1.6974 | Function Loss:  -2.1579\n",
      "Total loss:  -1.0478 | PDE Loss:  -1.6974 | Function Loss:  -2.1579\n",
      "Total loss:  -1.0478 | PDE Loss:  -1.6975 | Function Loss:  -2.1579\n",
      "Total loss:  -1.0478 | PDE Loss:  -1.6976 | Function Loss:  -2.158\n",
      "Total loss:  -1.0479 | PDE Loss:  -1.6976 | Function Loss:  -2.158\n",
      "Total loss:  -1.0479 | PDE Loss:  -1.6978 | Function Loss:  -2.158\n",
      "Total loss:  -1.0479 | PDE Loss:  -1.6974 | Function Loss:  -2.1581\n",
      "Total loss:  -1.048 | PDE Loss:  -1.6976 | Function Loss:  -2.1581\n",
      "Total loss:  -1.048 | PDE Loss:  -1.6978 | Function Loss:  -2.1581\n",
      "Total loss:  -1.048 | PDE Loss:  -1.6979 | Function Loss:  -2.1582\n",
      "Total loss:  -1.0481 | PDE Loss:  -1.6979 | Function Loss:  -2.1582\n",
      "Total loss:  -1.0481 | PDE Loss:  -1.6979 | Function Loss:  -2.1582\n",
      "Total loss:  -1.0481 | PDE Loss:  -1.6979 | Function Loss:  -2.1583\n",
      "Total loss:  -1.0482 | PDE Loss:  -1.6978 | Function Loss:  -2.1584\n",
      "Total loss:  -1.0482 | PDE Loss:  -1.6978 | Function Loss:  -2.1584\n",
      "Total loss:  -1.0483 | PDE Loss:  -1.6978 | Function Loss:  -2.1585\n",
      "Total loss:  -1.0483 | PDE Loss:  -1.6979 | Function Loss:  -2.1585\n",
      "Total loss:  -1.0484 | PDE Loss:  -1.698 | Function Loss:  -2.1585\n",
      "Total loss:  -1.0484 | PDE Loss:  -1.6985 | Function Loss:  -2.1585\n",
      "Total loss:  -1.0484 | PDE Loss:  -1.6986 | Function Loss:  -2.1585\n",
      "Total loss:  -1.0485 | PDE Loss:  -1.6987 | Function Loss:  -2.1585\n",
      "Total loss:  -1.0485 | PDE Loss:  -1.6988 | Function Loss:  -2.1585\n",
      "Total loss:  -1.0486 | PDE Loss:  -1.6989 | Function Loss:  -2.1585\n",
      "Total loss:  -1.0486 | PDE Loss:  -1.6992 | Function Loss:  -2.1585\n",
      "Total loss:  -1.0487 | PDE Loss:  -1.6993 | Function Loss:  -2.1586\n",
      "Total loss:  -1.0487 | PDE Loss:  -1.6998 | Function Loss:  -2.1585\n",
      "Total loss:  -1.0488 | PDE Loss:  -1.7 | Function Loss:  -2.1585\n",
      "Total loss:  -1.0488 | PDE Loss:  -1.7001 | Function Loss:  -2.1585\n",
      "Total loss:  -1.0488 | PDE Loss:  -1.7002 | Function Loss:  -2.1585\n",
      "Total loss:  -1.0489 | PDE Loss:  -1.7002 | Function Loss:  -2.1585\n",
      "Total loss:  -1.0489 | PDE Loss:  -1.7002 | Function Loss:  -2.1586\n",
      "Total loss:  -1.0489 | PDE Loss:  -1.7003 | Function Loss:  -2.1586\n",
      "Total loss:  -1.0489 | PDE Loss:  -1.7 | Function Loss:  -2.1587\n",
      "Total loss:  -1.0489 | PDE Loss:  -1.7002 | Function Loss:  -2.1586\n",
      "Total loss:  -1.049 | PDE Loss:  -1.7003 | Function Loss:  -2.1586\n",
      "Total loss:  -1.049 | PDE Loss:  -1.7004 | Function Loss:  -2.1586\n",
      "Total loss:  -1.049 | PDE Loss:  -1.7005 | Function Loss:  -2.1587\n",
      "Total loss:  -1.049 | PDE Loss:  -1.7005 | Function Loss:  -2.1587\n",
      "Total loss:  -1.0491 | PDE Loss:  -1.7005 | Function Loss:  -2.1587\n",
      "Total loss:  -1.0491 | PDE Loss:  -1.7004 | Function Loss:  -2.1588\n",
      "Total loss:  -1.0491 | PDE Loss:  -1.7004 | Function Loss:  -2.1588\n",
      "Total loss:  -1.0492 | PDE Loss:  -1.7003 | Function Loss:  -2.1589\n",
      "Total loss:  -1.0492 | PDE Loss:  -1.6996 | Function Loss:  -2.1591\n",
      "Total loss:  -1.0493 | PDE Loss:  -1.6998 | Function Loss:  -2.1592\n",
      "Total loss:  -1.0493 | PDE Loss:  -1.7 | Function Loss:  -2.1592\n",
      "Total loss:  -1.0494 | PDE Loss:  -1.7001 | Function Loss:  -2.1593\n",
      "Total loss:  -1.0495 | PDE Loss:  -1.7002 | Function Loss:  -2.1593\n",
      "Total loss:  -1.0495 | PDE Loss:  -1.7003 | Function Loss:  -2.1593\n",
      "Total loss:  -1.0495 | PDE Loss:  -1.7004 | Function Loss:  -2.1593\n",
      "Total loss:  -1.0496 | PDE Loss:  -1.7004 | Function Loss:  -2.1594\n",
      "Total loss:  -1.0495 | PDE Loss:  -1.7004 | Function Loss:  -2.1593\n",
      "Total loss:  -1.0496 | PDE Loss:  -1.7005 | Function Loss:  -2.1594\n",
      "Total loss:  -1.0496 | PDE Loss:  -1.7005 | Function Loss:  -2.1595\n",
      "Total loss:  -1.0497 | PDE Loss:  -1.7005 | Function Loss:  -2.1595\n",
      "Total loss:  -1.0497 | PDE Loss:  -1.7004 | Function Loss:  -2.1596\n",
      "Total loss:  -1.0497 | PDE Loss:  -1.7003 | Function Loss:  -2.1596\n",
      "Total loss:  -1.0498 | PDE Loss:  -1.7002 | Function Loss:  -2.1597\n",
      "Total loss:  -1.0497 | PDE Loss:  -1.7001 | Function Loss:  -2.1597\n",
      "Total loss:  -1.0498 | PDE Loss:  -1.7002 | Function Loss:  -2.1597\n",
      "Total loss:  -1.0498 | PDE Loss:  -1.7002 | Function Loss:  -2.1597\n",
      "Total loss:  -1.0498 | PDE Loss:  -1.7001 | Function Loss:  -2.1598\n",
      "Total loss:  -1.0498 | PDE Loss:  -1.7001 | Function Loss:  -2.1598\n",
      "Total loss:  -1.0498 | PDE Loss:  -1.7001 | Function Loss:  -2.1598\n",
      "Total loss:  -1.0499 | PDE Loss:  -1.7001 | Function Loss:  -2.1599\n",
      "Total loss:  -1.0499 | PDE Loss:  -1.7002 | Function Loss:  -2.1599\n",
      "Total loss:  -1.0499 | PDE Loss:  -1.7002 | Function Loss:  -2.1599\n",
      "Total loss:  -1.0499 | PDE Loss:  -1.7003 | Function Loss:  -2.1599\n",
      "Total loss:  -1.0499 | PDE Loss:  -1.7003 | Function Loss:  -2.1599\n",
      "Total loss:  -1.0499 | PDE Loss:  -1.7004 | Function Loss:  -2.1599\n",
      "Total loss:  -1.0499 | PDE Loss:  -1.7004 | Function Loss:  -2.1599\n",
      "Total loss:  -1.05 | PDE Loss:  -1.7005 | Function Loss:  -2.1599\n",
      "Total loss:  -1.05 | PDE Loss:  -1.7005 | Function Loss:  -2.1599\n",
      "Total loss:  -1.05 | PDE Loss:  -1.7006 | Function Loss:  -2.1599\n",
      "Total loss:  -1.05 | PDE Loss:  -1.7005 | Function Loss:  -2.1599\n",
      "Total loss:  -1.05 | PDE Loss:  -1.7004 | Function Loss:  -2.16\n",
      "Total loss:  -1.0501 | PDE Loss:  -1.7002 | Function Loss:  -2.1601\n",
      "Total loss:  -1.0501 | PDE Loss:  -1.7002 | Function Loss:  -2.1602\n",
      "Total loss:  -1.0501 | PDE Loss:  -1.7001 | Function Loss:  -2.1602\n",
      "Total loss:  -1.0502 | PDE Loss:  -1.7 | Function Loss:  -2.1603\n",
      "Total loss:  -1.0502 | PDE Loss:  -1.7 | Function Loss:  -2.1603\n",
      "Total loss:  -1.0503 | PDE Loss:  -1.7001 | Function Loss:  -2.1604\n",
      "Total loss:  -1.0503 | PDE Loss:  -1.7002 | Function Loss:  -2.1604\n",
      "Total loss:  -1.0498 | PDE Loss:  -1.6985 | Function Loss:  -2.1602\n",
      "Total loss:  -1.0503 | PDE Loss:  -1.7001 | Function Loss:  -2.1604\n",
      "Total loss:  -1.0503 | PDE Loss:  -1.7003 | Function Loss:  -2.1604\n",
      "Total loss:  -1.0503 | PDE Loss:  -1.7005 | Function Loss:  -2.1604\n",
      "Total loss:  -1.0504 | PDE Loss:  -1.7007 | Function Loss:  -2.1604\n",
      "Total loss:  -1.0504 | PDE Loss:  -1.7008 | Function Loss:  -2.1604\n",
      "Total loss:  -1.0504 | PDE Loss:  -1.7009 | Function Loss:  -2.1604\n",
      "Total loss:  -1.0505 | PDE Loss:  -1.7012 | Function Loss:  -2.1603\n",
      "Total loss:  -1.0505 | PDE Loss:  -1.7012 | Function Loss:  -2.1604\n",
      "Total loss:  -1.0505 | PDE Loss:  -1.7011 | Function Loss:  -2.1604\n",
      "Total loss:  -1.0506 | PDE Loss:  -1.701 | Function Loss:  -2.1605\n",
      "Total loss:  -1.0506 | PDE Loss:  -1.7009 | Function Loss:  -2.1606\n",
      "Total loss:  -1.0507 | PDE Loss:  -1.7008 | Function Loss:  -2.1607\n",
      "Total loss:  -1.0507 | PDE Loss:  -1.7007 | Function Loss:  -2.1607\n",
      "Total loss:  -1.0507 | PDE Loss:  -1.7007 | Function Loss:  -2.1608\n",
      "Total loss:  -1.0508 | PDE Loss:  -1.7007 | Function Loss:  -2.1609\n",
      "Total loss:  -1.0508 | PDE Loss:  -1.7008 | Function Loss:  -2.1609\n",
      "Total loss:  -1.0509 | PDE Loss:  -1.701 | Function Loss:  -2.1609\n",
      "Total loss:  -1.0509 | PDE Loss:  -1.7012 | Function Loss:  -2.1609\n",
      "Total loss:  -1.051 | PDE Loss:  -1.7015 | Function Loss:  -2.1609\n",
      "Total loss:  -1.051 | PDE Loss:  -1.7017 | Function Loss:  -2.1608\n",
      "Total loss:  -1.051 | PDE Loss:  -1.702 | Function Loss:  -2.1608\n",
      "Total loss:  -1.0511 | PDE Loss:  -1.7022 | Function Loss:  -2.1608\n",
      "Total loss:  -1.0511 | PDE Loss:  -1.7024 | Function Loss:  -2.1608\n",
      "Total loss:  -1.0512 | PDE Loss:  -1.7026 | Function Loss:  -2.1608\n",
      "Total loss:  -1.0512 | PDE Loss:  -1.7026 | Function Loss:  -2.1608\n",
      "Total loss:  -1.0512 | PDE Loss:  -1.7028 | Function Loss:  -2.1609\n",
      "Total loss:  -1.0513 | PDE Loss:  -1.7028 | Function Loss:  -2.1609\n",
      "Total loss:  -1.0513 | PDE Loss:  -1.7029 | Function Loss:  -2.1609\n",
      "Total loss:  -1.0513 | PDE Loss:  -1.703 | Function Loss:  -2.1609\n",
      "Total loss:  -1.0514 | PDE Loss:  -1.7032 | Function Loss:  -2.161\n",
      "Total loss:  -1.0515 | PDE Loss:  -1.7034 | Function Loss:  -2.161\n",
      "Total loss:  -1.0515 | PDE Loss:  -1.7036 | Function Loss:  -2.161\n",
      "Total loss:  -1.0515 | PDE Loss:  -1.7038 | Function Loss:  -2.161\n",
      "Total loss:  -1.0516 | PDE Loss:  -1.7041 | Function Loss:  -2.161\n",
      "Total loss:  -1.0517 | PDE Loss:  -1.7043 | Function Loss:  -2.161\n",
      "Total loss:  -1.0517 | PDE Loss:  -1.7044 | Function Loss:  -2.161\n",
      "Total loss:  -1.0517 | PDE Loss:  -1.7044 | Function Loss:  -2.161\n",
      "Total loss:  -1.0518 | PDE Loss:  -1.7045 | Function Loss:  -2.1611\n",
      "Total loss:  -1.0518 | PDE Loss:  -1.7046 | Function Loss:  -2.1611\n",
      "Total loss:  -1.0518 | PDE Loss:  -1.7046 | Function Loss:  -2.1611\n",
      "Total loss:  -1.0519 | PDE Loss:  -1.7047 | Function Loss:  -2.1611\n",
      "Total loss:  -1.0519 | PDE Loss:  -1.7049 | Function Loss:  -2.1611\n",
      "Total loss:  -1.052 | PDE Loss:  -1.7052 | Function Loss:  -2.1612\n",
      "Total loss:  -1.052 | PDE Loss:  -1.7053 | Function Loss:  -2.1612\n",
      "Total loss:  -1.0521 | PDE Loss:  -1.7055 | Function Loss:  -2.1612\n",
      "Total loss:  -1.0521 | PDE Loss:  -1.7056 | Function Loss:  -2.1612\n",
      "Total loss:  -1.0522 | PDE Loss:  -1.7057 | Function Loss:  -2.1612\n",
      "Total loss:  -1.0522 | PDE Loss:  -1.7059 | Function Loss:  -2.1612\n",
      "Total loss:  -1.0521 | PDE Loss:  -1.7046 | Function Loss:  -2.1614\n",
      "Total loss:  -1.0522 | PDE Loss:  -1.7056 | Function Loss:  -2.1613\n",
      "Total loss:  -1.0522 | PDE Loss:  -1.7058 | Function Loss:  -2.1613\n",
      "Total loss:  -1.0523 | PDE Loss:  -1.706 | Function Loss:  -2.1613\n",
      "Total loss:  -1.0523 | PDE Loss:  -1.7062 | Function Loss:  -2.1613\n",
      "Total loss:  -1.0524 | PDE Loss:  -1.7064 | Function Loss:  -2.1613\n",
      "Total loss:  -1.0524 | PDE Loss:  -1.7065 | Function Loss:  -2.1613\n",
      "Total loss:  -1.0525 | PDE Loss:  -1.7066 | Function Loss:  -2.1613\n",
      "Total loss:  -1.0525 | PDE Loss:  -1.7066 | Function Loss:  -2.1614\n",
      "Total loss:  -1.0525 | PDE Loss:  -1.7067 | Function Loss:  -2.1614\n",
      "Total loss:  -1.0526 | PDE Loss:  -1.7067 | Function Loss:  -2.1614\n",
      "Total loss:  -1.0526 | PDE Loss:  -1.7069 | Function Loss:  -2.1615\n",
      "Total loss:  -1.0527 | PDE Loss:  -1.707 | Function Loss:  -2.1615\n",
      "Total loss:  -1.0527 | PDE Loss:  -1.7071 | Function Loss:  -2.1615\n",
      "Total loss:  -1.0528 | PDE Loss:  -1.7072 | Function Loss:  -2.1616\n",
      "Total loss:  -1.0528 | PDE Loss:  -1.7074 | Function Loss:  -2.1616\n",
      "Total loss:  -1.0529 | PDE Loss:  -1.7075 | Function Loss:  -2.1616\n",
      "Total loss:  -1.0529 | PDE Loss:  -1.7077 | Function Loss:  -2.1616\n",
      "Total loss:  -1.0529 | PDE Loss:  -1.7078 | Function Loss:  -2.1616\n",
      "Total loss:  -1.053 | PDE Loss:  -1.7079 | Function Loss:  -2.1617\n",
      "Total loss:  -1.053 | PDE Loss:  -1.708 | Function Loss:  -2.1617\n",
      "Total loss:  -1.0531 | PDE Loss:  -1.7082 | Function Loss:  -2.1617\n",
      "Total loss:  -1.0532 | PDE Loss:  -1.7084 | Function Loss:  -2.1618\n",
      "Total loss:  -1.0532 | PDE Loss:  -1.7086 | Function Loss:  -2.1618\n",
      "Total loss:  -1.0533 | PDE Loss:  -1.7089 | Function Loss:  -2.1618\n",
      "Total loss:  -1.0533 | PDE Loss:  -1.7096 | Function Loss:  -2.1616\n",
      "Total loss:  -1.0534 | PDE Loss:  -1.7097 | Function Loss:  -2.1616\n",
      "Total loss:  -1.0534 | PDE Loss:  -1.7099 | Function Loss:  -2.1616\n",
      "Total loss:  -1.0535 | PDE Loss:  -1.7101 | Function Loss:  -2.1616\n",
      "Total loss:  -1.0535 | PDE Loss:  -1.7104 | Function Loss:  -2.1616\n",
      "Total loss:  -1.0536 | PDE Loss:  -1.7107 | Function Loss:  -2.1616\n",
      "Total loss:  -1.0536 | PDE Loss:  -1.711 | Function Loss:  -2.1616\n",
      "Total loss:  -1.0537 | PDE Loss:  -1.7114 | Function Loss:  -2.1615\n",
      "Total loss:  -1.0537 | PDE Loss:  -1.7115 | Function Loss:  -2.1615\n",
      "Total loss:  -1.0538 | PDE Loss:  -1.7117 | Function Loss:  -2.1616\n",
      "Total loss:  -1.0538 | PDE Loss:  -1.7117 | Function Loss:  -2.1616\n",
      "Total loss:  -1.0538 | PDE Loss:  -1.7117 | Function Loss:  -2.1617\n",
      "Total loss:  -1.0539 | PDE Loss:  -1.7117 | Function Loss:  -2.1617\n",
      "Total loss:  -1.054 | PDE Loss:  -1.7119 | Function Loss:  -2.1618\n",
      "Total loss:  -1.0541 | PDE Loss:  -1.7118 | Function Loss:  -2.1619\n",
      "Total loss:  -1.0541 | PDE Loss:  -1.7119 | Function Loss:  -2.162\n",
      "Total loss:  -1.0542 | PDE Loss:  -1.7118 | Function Loss:  -2.1621\n",
      "Total loss:  -1.0543 | PDE Loss:  -1.7119 | Function Loss:  -2.1621\n",
      "Total loss:  -1.0543 | PDE Loss:  -1.7122 | Function Loss:  -2.1621\n",
      "Total loss:  -1.0543 | PDE Loss:  -1.7125 | Function Loss:  -2.1621\n",
      "Total loss:  -1.0544 | PDE Loss:  -1.7128 | Function Loss:  -2.162\n",
      "Total loss:  -1.0544 | PDE Loss:  -1.7131 | Function Loss:  -2.162\n",
      "Total loss:  -1.0545 | PDE Loss:  -1.7133 | Function Loss:  -2.1621\n",
      "Total loss:  -1.0545 | PDE Loss:  -1.7133 | Function Loss:  -2.1621\n",
      "Total loss:  -1.0546 | PDE Loss:  -1.7136 | Function Loss:  -2.1621\n",
      "Total loss:  -1.0546 | PDE Loss:  -1.7136 | Function Loss:  -2.1621\n",
      "Total loss:  -1.0546 | PDE Loss:  -1.7136 | Function Loss:  -2.1622\n",
      "Total loss:  -1.0547 | PDE Loss:  -1.7136 | Function Loss:  -2.1622\n",
      "Total loss:  -1.0547 | PDE Loss:  -1.7136 | Function Loss:  -2.1622\n",
      "Total loss:  -1.0548 | PDE Loss:  -1.7137 | Function Loss:  -2.1623\n",
      "Total loss:  -1.0548 | PDE Loss:  -1.7137 | Function Loss:  -2.1623\n",
      "Total loss:  -1.0548 | PDE Loss:  -1.7138 | Function Loss:  -2.1624\n",
      "Total loss:  -1.0548 | PDE Loss:  -1.714 | Function Loss:  -2.1623\n",
      "Total loss:  -1.0549 | PDE Loss:  -1.7141 | Function Loss:  -2.1624\n",
      "Total loss:  -1.0549 | PDE Loss:  -1.7141 | Function Loss:  -2.1624\n",
      "Total loss:  -1.0549 | PDE Loss:  -1.7142 | Function Loss:  -2.1624\n",
      "Total loss:  -1.055 | PDE Loss:  -1.7142 | Function Loss:  -2.1624\n",
      "Total loss:  -1.055 | PDE Loss:  -1.7142 | Function Loss:  -2.1624\n",
      "Total loss:  -1.055 | PDE Loss:  -1.7143 | Function Loss:  -2.1624\n",
      "Total loss:  -1.055 | PDE Loss:  -1.7143 | Function Loss:  -2.1624\n",
      "Total loss:  -1.055 | PDE Loss:  -1.7143 | Function Loss:  -2.1624\n",
      "Total loss:  -1.0547 | PDE Loss:  -1.7134 | Function Loss:  -2.1623\n",
      "Total loss:  -1.055 | PDE Loss:  -1.7143 | Function Loss:  -2.1624\n",
      "Total loss:  -1.055 | PDE Loss:  -1.7143 | Function Loss:  -2.1625\n",
      "Total loss:  -1.055 | PDE Loss:  -1.7143 | Function Loss:  -2.1625\n",
      "Total loss:  -1.0551 | PDE Loss:  -1.7143 | Function Loss:  -2.1625\n",
      "Total loss:  -1.0551 | PDE Loss:  -1.7143 | Function Loss:  -2.1625\n",
      "Total loss:  -1.0551 | PDE Loss:  -1.7143 | Function Loss:  -2.1625\n",
      "Total loss:  -1.0551 | PDE Loss:  -1.7144 | Function Loss:  -2.1625\n",
      "Total loss:  -1.0551 | PDE Loss:  -1.7145 | Function Loss:  -2.1625\n",
      "Total loss:  -1.0552 | PDE Loss:  -1.7146 | Function Loss:  -2.1625\n",
      "Total loss:  -1.0551 | PDE Loss:  -1.7138 | Function Loss:  -2.1626\n",
      "Total loss:  -1.0552 | PDE Loss:  -1.7145 | Function Loss:  -2.1626\n",
      "Total loss:  -1.0552 | PDE Loss:  -1.7146 | Function Loss:  -2.1626\n",
      "Total loss:  -1.0553 | PDE Loss:  -1.7147 | Function Loss:  -2.1626\n",
      "Total loss:  -1.0553 | PDE Loss:  -1.7148 | Function Loss:  -2.1626\n",
      "Total loss:  -1.0553 | PDE Loss:  -1.7149 | Function Loss:  -2.1626\n",
      "Total loss:  -1.0553 | PDE Loss:  -1.7149 | Function Loss:  -2.1627\n",
      "Total loss:  -1.0553 | PDE Loss:  -1.715 | Function Loss:  -2.1627\n",
      "Total loss:  -1.0554 | PDE Loss:  -1.7149 | Function Loss:  -2.1627\n",
      "Total loss:  -1.0554 | PDE Loss:  -1.7149 | Function Loss:  -2.1627\n",
      "Total loss:  -1.0554 | PDE Loss:  -1.715 | Function Loss:  -2.1628\n",
      "Total loss:  -1.0555 | PDE Loss:  -1.715 | Function Loss:  -2.1628\n",
      "Total loss:  -1.0555 | PDE Loss:  -1.7151 | Function Loss:  -2.1629\n",
      "Total loss:  -1.0555 | PDE Loss:  -1.7152 | Function Loss:  -2.1629\n",
      "Total loss:  -1.0556 | PDE Loss:  -1.7152 | Function Loss:  -2.1629\n",
      "Total loss:  -1.0556 | PDE Loss:  -1.7154 | Function Loss:  -2.1628\n",
      "Total loss:  -1.0556 | PDE Loss:  -1.7155 | Function Loss:  -2.1628\n",
      "Total loss:  -1.0556 | PDE Loss:  -1.7156 | Function Loss:  -2.1628\n",
      "Total loss:  -1.0556 | PDE Loss:  -1.7159 | Function Loss:  -2.1628\n",
      "Total loss:  -1.0557 | PDE Loss:  -1.7163 | Function Loss:  -2.1627\n",
      "Total loss:  -1.0557 | PDE Loss:  -1.7163 | Function Loss:  -2.1627\n",
      "Total loss:  -1.0557 | PDE Loss:  -1.7164 | Function Loss:  -2.1627\n",
      "Total loss:  -1.0557 | PDE Loss:  -1.7167 | Function Loss:  -2.1627\n",
      "Total loss:  -1.0557 | PDE Loss:  -1.717 | Function Loss:  -2.1626\n",
      "Total loss:  -1.0558 | PDE Loss:  -1.7173 | Function Loss:  -2.1625\n",
      "Total loss:  -1.0558 | PDE Loss:  -1.7177 | Function Loss:  -2.1625\n",
      "Total loss:  -1.0558 | PDE Loss:  -1.7178 | Function Loss:  -2.1625\n",
      "Total loss:  -1.0558 | PDE Loss:  -1.718 | Function Loss:  -2.1624\n",
      "Total loss:  -1.0559 | PDE Loss:  -1.7183 | Function Loss:  -2.1624\n",
      "Total loss:  -1.0559 | PDE Loss:  -1.7186 | Function Loss:  -2.1624\n",
      "Total loss:  -1.0559 | PDE Loss:  -1.719 | Function Loss:  -2.1623\n",
      "Total loss:  -1.056 | PDE Loss:  -1.7191 | Function Loss:  -2.1623\n",
      "Total loss:  -1.056 | PDE Loss:  -1.7193 | Function Loss:  -2.1623\n",
      "Total loss:  -1.056 | PDE Loss:  -1.7195 | Function Loss:  -2.1623\n",
      "Total loss:  -1.056 | PDE Loss:  -1.72 | Function Loss:  -2.1622\n",
      "Total loss:  -1.0561 | PDE Loss:  -1.7201 | Function Loss:  -2.1622\n",
      "Total loss:  -1.0561 | PDE Loss:  -1.7201 | Function Loss:  -2.1622\n",
      "Total loss:  -1.0561 | PDE Loss:  -1.7203 | Function Loss:  -2.1622\n",
      "Total loss:  -1.0561 | PDE Loss:  -1.7203 | Function Loss:  -2.1622\n",
      "Total loss:  -1.0561 | PDE Loss:  -1.7205 | Function Loss:  -2.1622\n",
      "Total loss:  -1.0562 | PDE Loss:  -1.7205 | Function Loss:  -2.1622\n",
      "Total loss:  -1.0562 | PDE Loss:  -1.7208 | Function Loss:  -2.1622\n",
      "Total loss:  -1.0563 | PDE Loss:  -1.7209 | Function Loss:  -2.1622\n",
      "Total loss:  -1.0563 | PDE Loss:  -1.7211 | Function Loss:  -2.1622\n",
      "Total loss:  -1.0564 | PDE Loss:  -1.7211 | Function Loss:  -2.1622\n",
      "Total loss:  -1.0564 | PDE Loss:  -1.7212 | Function Loss:  -2.1623\n",
      "Total loss:  -1.0564 | PDE Loss:  -1.7212 | Function Loss:  -2.1623\n",
      "Total loss:  -1.0565 | PDE Loss:  -1.7212 | Function Loss:  -2.1624\n",
      "Total loss:  -1.0565 | PDE Loss:  -1.721 | Function Loss:  -2.1624\n",
      "Total loss:  -1.0565 | PDE Loss:  -1.7211 | Function Loss:  -2.1625\n",
      "Total loss:  -1.0565 | PDE Loss:  -1.7212 | Function Loss:  -2.1625\n",
      "Total loss:  -1.0566 | PDE Loss:  -1.7212 | Function Loss:  -2.1625\n",
      "Total loss:  -1.0566 | PDE Loss:  -1.7213 | Function Loss:  -2.1625\n",
      "Total loss:  -1.0566 | PDE Loss:  -1.7214 | Function Loss:  -2.1625\n",
      "Total loss:  -1.0566 | PDE Loss:  -1.7215 | Function Loss:  -2.1625\n",
      "Total loss:  -1.0567 | PDE Loss:  -1.7215 | Function Loss:  -2.1626\n",
      "Total loss:  -1.0567 | PDE Loss:  -1.7215 | Function Loss:  -2.1626\n",
      "Total loss:  -1.0567 | PDE Loss:  -1.7215 | Function Loss:  -2.1626\n",
      "Total loss:  -1.0568 | PDE Loss:  -1.7214 | Function Loss:  -2.1627\n",
      "Total loss:  -1.0568 | PDE Loss:  -1.7213 | Function Loss:  -2.1628\n",
      "Total loss:  -1.0568 | PDE Loss:  -1.7212 | Function Loss:  -2.1628\n",
      "Total loss:  -1.0569 | PDE Loss:  -1.7211 | Function Loss:  -2.1629\n",
      "Total loss:  -1.0569 | PDE Loss:  -1.721 | Function Loss:  -2.163\n",
      "Total loss:  -1.0569 | PDE Loss:  -1.7208 | Function Loss:  -2.1631\n",
      "Total loss:  -1.057 | PDE Loss:  -1.7204 | Function Loss:  -2.1633\n",
      "Total loss:  -1.057 | PDE Loss:  -1.7202 | Function Loss:  -2.1634\n",
      "Total loss:  -1.0571 | PDE Loss:  -1.7201 | Function Loss:  -2.1634\n",
      "Total loss:  -1.0571 | PDE Loss:  -1.7201 | Function Loss:  -2.1635\n",
      "Total loss:  -1.0571 | PDE Loss:  -1.7201 | Function Loss:  -2.1635\n",
      "Total loss:  -1.0572 | PDE Loss:  -1.7201 | Function Loss:  -2.1636\n",
      "Total loss:  -1.0572 | PDE Loss:  -1.7201 | Function Loss:  -2.1637\n",
      "Total loss:  -1.0573 | PDE Loss:  -1.7201 | Function Loss:  -2.1637\n",
      "Total loss:  -1.0573 | PDE Loss:  -1.7201 | Function Loss:  -2.1638\n",
      "Total loss:  -1.0574 | PDE Loss:  -1.72 | Function Loss:  -2.1638\n",
      "Total loss:  -1.0574 | PDE Loss:  -1.72 | Function Loss:  -2.1639\n",
      "Total loss:  -1.0574 | PDE Loss:  -1.7199 | Function Loss:  -2.1639\n",
      "Total loss:  -1.0574 | PDE Loss:  -1.7199 | Function Loss:  -2.1639\n",
      "Total loss:  -1.0574 | PDE Loss:  -1.7199 | Function Loss:  -2.164\n",
      "Total loss:  -1.0574 | PDE Loss:  -1.7199 | Function Loss:  -2.164\n",
      "Total loss:  -1.0575 | PDE Loss:  -1.72 | Function Loss:  -2.164\n",
      "Total loss:  -1.0575 | PDE Loss:  -1.7199 | Function Loss:  -2.1641\n",
      "Total loss:  -1.0574 | PDE Loss:  -1.7204 | Function Loss:  -2.1638\n",
      "Total loss:  -1.0576 | PDE Loss:  -1.7202 | Function Loss:  -2.1641\n",
      "Total loss:  -1.0576 | PDE Loss:  -1.7202 | Function Loss:  -2.1641\n",
      "Total loss:  -1.0577 | PDE Loss:  -1.7204 | Function Loss:  -2.1641\n",
      "Total loss:  -1.0577 | PDE Loss:  -1.7205 | Function Loss:  -2.1641\n",
      "Total loss:  -1.0577 | PDE Loss:  -1.7206 | Function Loss:  -2.1641\n",
      "Total loss:  -1.0577 | PDE Loss:  -1.7207 | Function Loss:  -2.1641\n",
      "Total loss:  -1.0578 | PDE Loss:  -1.7208 | Function Loss:  -2.1641\n",
      "Total loss:  -1.0578 | PDE Loss:  -1.721 | Function Loss:  -2.1641\n",
      "Total loss:  -1.0578 | PDE Loss:  -1.7211 | Function Loss:  -2.1641\n",
      "Total loss:  -1.0578 | PDE Loss:  -1.7212 | Function Loss:  -2.1641\n",
      "Total loss:  -1.0577 | PDE Loss:  -1.7211 | Function Loss:  -2.164\n",
      "Total loss:  -1.0579 | PDE Loss:  -1.7213 | Function Loss:  -2.1641\n",
      "Total loss:  -1.0579 | PDE Loss:  -1.7214 | Function Loss:  -2.1641\n",
      "Total loss:  -1.0579 | PDE Loss:  -1.7214 | Function Loss:  -2.1641\n",
      "Total loss:  -1.0579 | PDE Loss:  -1.7215 | Function Loss:  -2.1642\n",
      "Total loss:  -1.0579 | PDE Loss:  -1.7215 | Function Loss:  -2.1642\n",
      "Total loss:  -1.058 | PDE Loss:  -1.7215 | Function Loss:  -2.1642\n",
      "Total loss:  -1.058 | PDE Loss:  -1.7215 | Function Loss:  -2.1642\n",
      "Total loss:  -1.058 | PDE Loss:  -1.7215 | Function Loss:  -2.1642\n",
      "Total loss:  -1.058 | PDE Loss:  -1.7215 | Function Loss:  -2.1643\n",
      "Total loss:  -1.0581 | PDE Loss:  -1.7215 | Function Loss:  -2.1644\n",
      "Total loss:  -1.0581 | PDE Loss:  -1.7215 | Function Loss:  -2.1644\n",
      "Total loss:  -1.0581 | PDE Loss:  -1.7216 | Function Loss:  -2.1644\n",
      "Total loss:  -1.0582 | PDE Loss:  -1.7217 | Function Loss:  -2.1644\n",
      "Total loss:  -1.0582 | PDE Loss:  -1.7219 | Function Loss:  -2.1644\n",
      "Total loss:  -1.0582 | PDE Loss:  -1.7221 | Function Loss:  -2.1644\n",
      "Total loss:  -1.058 | PDE Loss:  -1.7229 | Function Loss:  -2.1638\n",
      "Total loss:  -1.0582 | PDE Loss:  -1.7223 | Function Loss:  -2.1643\n",
      "Total loss:  -1.0582 | PDE Loss:  -1.7224 | Function Loss:  -2.1643\n",
      "Total loss:  -1.0583 | PDE Loss:  -1.7226 | Function Loss:  -2.1643\n",
      "Total loss:  -1.0583 | PDE Loss:  -1.7228 | Function Loss:  -2.1643\n",
      "Total loss:  -1.0584 | PDE Loss:  -1.723 | Function Loss:  -2.1643\n",
      "Total loss:  -1.0584 | PDE Loss:  -1.723 | Function Loss:  -2.1644\n",
      "Total loss:  -1.0584 | PDE Loss:  -1.723 | Function Loss:  -2.1644\n",
      "Total loss:  -1.0585 | PDE Loss:  -1.7231 | Function Loss:  -2.1644\n",
      "Total loss:  -1.0585 | PDE Loss:  -1.7231 | Function Loss:  -2.1645\n",
      "Total loss:  -1.0586 | PDE Loss:  -1.723 | Function Loss:  -2.1646\n",
      "Total loss:  -1.0587 | PDE Loss:  -1.7229 | Function Loss:  -2.1647\n",
      "Total loss:  -1.0587 | PDE Loss:  -1.7228 | Function Loss:  -2.1648\n",
      "Total loss:  -1.0588 | PDE Loss:  -1.7227 | Function Loss:  -2.1649\n",
      "Total loss:  -1.0588 | PDE Loss:  -1.7221 | Function Loss:  -2.1652\n",
      "Total loss:  -1.0589 | PDE Loss:  -1.7221 | Function Loss:  -2.1652\n",
      "Total loss:  -1.0589 | PDE Loss:  -1.7221 | Function Loss:  -2.1652\n",
      "Total loss:  -1.0589 | PDE Loss:  -1.7222 | Function Loss:  -2.1652\n",
      "Total loss:  -1.0589 | PDE Loss:  -1.7222 | Function Loss:  -2.1652\n",
      "Total loss:  -1.059 | PDE Loss:  -1.7223 | Function Loss:  -2.1653\n",
      "Total loss:  -1.059 | PDE Loss:  -1.7223 | Function Loss:  -2.1653\n",
      "Total loss:  -1.059 | PDE Loss:  -1.7224 | Function Loss:  -2.1653\n",
      "Total loss:  -1.0591 | PDE Loss:  -1.7225 | Function Loss:  -2.1653\n",
      "Total loss:  -1.0591 | PDE Loss:  -1.7227 | Function Loss:  -2.1653\n",
      "Total loss:  -1.0591 | PDE Loss:  -1.7228 | Function Loss:  -2.1653\n",
      "Total loss:  -1.0592 | PDE Loss:  -1.7229 | Function Loss:  -2.1653\n",
      "Total loss:  -1.0592 | PDE Loss:  -1.7231 | Function Loss:  -2.1653\n",
      "Total loss:  -1.0593 | PDE Loss:  -1.7233 | Function Loss:  -2.1654\n",
      "Total loss:  -1.0593 | PDE Loss:  -1.7237 | Function Loss:  -2.1653\n",
      "Total loss:  -1.0594 | PDE Loss:  -1.7238 | Function Loss:  -2.1654\n",
      "Total loss:  -1.0594 | PDE Loss:  -1.724 | Function Loss:  -2.1654\n",
      "Total loss:  -1.0595 | PDE Loss:  -1.724 | Function Loss:  -2.1654\n",
      "Total loss:  -1.0595 | PDE Loss:  -1.7241 | Function Loss:  -2.1655\n",
      "Total loss:  -1.0595 | PDE Loss:  -1.7241 | Function Loss:  -2.1655\n",
      "Total loss:  -1.0596 | PDE Loss:  -1.724 | Function Loss:  -2.1656\n",
      "Total loss:  -1.0596 | PDE Loss:  -1.7239 | Function Loss:  -2.1656\n",
      "Total loss:  -1.0596 | PDE Loss:  -1.7236 | Function Loss:  -2.1657\n",
      "Total loss:  -1.0591 | PDE Loss:  -1.7215 | Function Loss:  -2.1656\n",
      "Total loss:  -1.0596 | PDE Loss:  -1.7235 | Function Loss:  -2.1658\n",
      "Total loss:  -1.0597 | PDE Loss:  -1.7233 | Function Loss:  -2.1659\n",
      "Total loss:  -1.0597 | PDE Loss:  -1.7232 | Function Loss:  -2.166\n",
      "Total loss:  -1.0597 | PDE Loss:  -1.723 | Function Loss:  -2.1661\n",
      "Total loss:  -1.0598 | PDE Loss:  -1.7229 | Function Loss:  -2.1661\n",
      "Total loss:  -1.0598 | PDE Loss:  -1.723 | Function Loss:  -2.1662\n",
      "Total loss:  -1.0598 | PDE Loss:  -1.7231 | Function Loss:  -2.1662\n",
      "Total loss:  -1.0599 | PDE Loss:  -1.7232 | Function Loss:  -2.1661\n",
      "Total loss:  -1.0599 | PDE Loss:  -1.7234 | Function Loss:  -2.1661\n",
      "Total loss:  -1.0599 | PDE Loss:  -1.7237 | Function Loss:  -2.1661\n",
      "Total loss:  -1.0599 | PDE Loss:  -1.7244 | Function Loss:  -2.1659\n",
      "Total loss:  -1.06 | PDE Loss:  -1.7242 | Function Loss:  -2.166\n",
      "Total loss:  -1.06 | PDE Loss:  -1.7244 | Function Loss:  -2.166\n",
      "Total loss:  -1.0601 | PDE Loss:  -1.7247 | Function Loss:  -2.1661\n",
      "Total loss:  -1.0602 | PDE Loss:  -1.7248 | Function Loss:  -2.1661\n",
      "Total loss:  -1.0602 | PDE Loss:  -1.7249 | Function Loss:  -2.1661\n",
      "Total loss:  -1.0603 | PDE Loss:  -1.7249 | Function Loss:  -2.1662\n",
      "Total loss:  -1.0603 | PDE Loss:  -1.7248 | Function Loss:  -2.1663\n",
      "Total loss:  -1.0603 | PDE Loss:  -1.7248 | Function Loss:  -2.1663\n",
      "Total loss:  -1.0603 | PDE Loss:  -1.7245 | Function Loss:  -2.1664\n",
      "Total loss:  -1.0604 | PDE Loss:  -1.7246 | Function Loss:  -2.1664\n",
      "Total loss:  -1.0604 | PDE Loss:  -1.7246 | Function Loss:  -2.1664\n",
      "Total loss:  -1.0604 | PDE Loss:  -1.7247 | Function Loss:  -2.1664\n",
      "Total loss:  -1.0604 | PDE Loss:  -1.7248 | Function Loss:  -2.1664\n",
      "Total loss:  -1.0604 | PDE Loss:  -1.7249 | Function Loss:  -2.1664\n",
      "Total loss:  -1.0605 | PDE Loss:  -1.725 | Function Loss:  -2.1664\n",
      "Total loss:  -1.0605 | PDE Loss:  -1.7252 | Function Loss:  -2.1664\n",
      "Total loss:  -1.0605 | PDE Loss:  -1.7254 | Function Loss:  -2.1664\n",
      "Total loss:  -1.0605 | PDE Loss:  -1.7261 | Function Loss:  -2.1662\n",
      "Total loss:  -1.0605 | PDE Loss:  -1.726 | Function Loss:  -2.1663\n",
      "Total loss:  -1.0606 | PDE Loss:  -1.7259 | Function Loss:  -2.1663\n",
      "Total loss:  -1.0606 | PDE Loss:  -1.7258 | Function Loss:  -2.1664\n",
      "Total loss:  -1.0606 | PDE Loss:  -1.7257 | Function Loss:  -2.1664\n",
      "Total loss:  -1.0606 | PDE Loss:  -1.7257 | Function Loss:  -2.1665\n",
      "Total loss:  -1.0607 | PDE Loss:  -1.7256 | Function Loss:  -2.1665\n",
      "Total loss:  -1.0607 | PDE Loss:  -1.7256 | Function Loss:  -2.1666\n",
      "Total loss:  -1.0607 | PDE Loss:  -1.7255 | Function Loss:  -2.1666\n",
      "Total loss:  -1.0607 | PDE Loss:  -1.7255 | Function Loss:  -2.1666\n",
      "Total loss:  -1.0608 | PDE Loss:  -1.7256 | Function Loss:  -2.1667\n",
      "Total loss:  -1.0608 | PDE Loss:  -1.7256 | Function Loss:  -2.1667\n",
      "Total loss:  -1.0608 | PDE Loss:  -1.7258 | Function Loss:  -2.1667\n",
      "Total loss:  -1.0608 | PDE Loss:  -1.726 | Function Loss:  -2.1666\n",
      "Total loss:  -1.0609 | PDE Loss:  -1.7262 | Function Loss:  -2.1666\n",
      "Total loss:  -1.0609 | PDE Loss:  -1.7263 | Function Loss:  -2.1666\n",
      "Total loss:  -1.0609 | PDE Loss:  -1.7265 | Function Loss:  -2.1666\n",
      "Total loss:  -1.0609 | PDE Loss:  -1.7267 | Function Loss:  -2.1666\n",
      "Total loss:  -1.061 | PDE Loss:  -1.7268 | Function Loss:  -2.1665\n",
      "Total loss:  -1.061 | PDE Loss:  -1.727 | Function Loss:  -2.1665\n",
      "Total loss:  -1.061 | PDE Loss:  -1.7272 | Function Loss:  -2.1665\n",
      "Total loss:  -1.061 | PDE Loss:  -1.7273 | Function Loss:  -2.1665\n",
      "Total loss:  -1.0611 | PDE Loss:  -1.7275 | Function Loss:  -2.1665\n",
      "Total loss:  -1.0611 | PDE Loss:  -1.7275 | Function Loss:  -2.1666\n",
      "Total loss:  -1.0611 | PDE Loss:  -1.7275 | Function Loss:  -2.1666\n",
      "Total loss:  -1.0612 | PDE Loss:  -1.7275 | Function Loss:  -2.1666\n",
      "Total loss:  -1.0612 | PDE Loss:  -1.7275 | Function Loss:  -2.1667\n",
      "Total loss:  -1.0612 | PDE Loss:  -1.7274 | Function Loss:  -2.1667\n",
      "Total loss:  -1.0612 | PDE Loss:  -1.7274 | Function Loss:  -2.1668\n",
      "Total loss:  -1.0613 | PDE Loss:  -1.7274 | Function Loss:  -2.1668\n",
      "Total loss:  -1.0613 | PDE Loss:  -1.727 | Function Loss:  -2.1669\n",
      "Total loss:  -1.0613 | PDE Loss:  -1.7273 | Function Loss:  -2.1668\n",
      "Total loss:  -1.0613 | PDE Loss:  -1.7273 | Function Loss:  -2.1669\n",
      "Total loss:  -1.0613 | PDE Loss:  -1.7273 | Function Loss:  -2.1669\n",
      "Total loss:  -1.0614 | PDE Loss:  -1.7272 | Function Loss:  -2.167\n",
      "Total loss:  -1.0614 | PDE Loss:  -1.7271 | Function Loss:  -2.167\n",
      "Total loss:  -1.0614 | PDE Loss:  -1.727 | Function Loss:  -2.1671\n",
      "Total loss:  -1.0614 | PDE Loss:  -1.7268 | Function Loss:  -2.1672\n",
      "Total loss:  -1.0615 | PDE Loss:  -1.7263 | Function Loss:  -2.1674\n",
      "Total loss:  -1.0615 | PDE Loss:  -1.726 | Function Loss:  -2.1675\n",
      "Total loss:  -1.0615 | PDE Loss:  -1.7254 | Function Loss:  -2.1677\n",
      "Total loss:  -1.0616 | PDE Loss:  -1.7252 | Function Loss:  -2.1678\n",
      "Total loss:  -1.0616 | PDE Loss:  -1.7251 | Function Loss:  -2.1679\n",
      "Total loss:  -1.0616 | PDE Loss:  -1.725 | Function Loss:  -2.1679\n",
      "Total loss:  -1.0617 | PDE Loss:  -1.7249 | Function Loss:  -2.168\n",
      "Total loss:  -1.0617 | PDE Loss:  -1.7248 | Function Loss:  -2.1681\n",
      "Total loss:  -1.0617 | PDE Loss:  -1.7243 | Function Loss:  -2.1682\n",
      "Total loss:  -1.0618 | PDE Loss:  -1.7242 | Function Loss:  -2.1683\n",
      "Total loss:  -1.0618 | PDE Loss:  -1.7242 | Function Loss:  -2.1683\n",
      "Total loss:  -1.0618 | PDE Loss:  -1.7243 | Function Loss:  -2.1684\n",
      "Total loss:  -1.0619 | PDE Loss:  -1.7242 | Function Loss:  -2.1684\n",
      "Total loss:  -1.0619 | PDE Loss:  -1.7244 | Function Loss:  -2.1684\n",
      "Total loss:  -1.0619 | PDE Loss:  -1.7243 | Function Loss:  -2.1685\n",
      "Total loss:  -1.062 | PDE Loss:  -1.7242 | Function Loss:  -2.1685\n",
      "Total loss:  -1.062 | PDE Loss:  -1.7242 | Function Loss:  -2.1686\n",
      "Total loss:  -1.0621 | PDE Loss:  -1.7241 | Function Loss:  -2.1687\n",
      "Total loss:  -1.0621 | PDE Loss:  -1.7239 | Function Loss:  -2.1688\n",
      "Total loss:  -1.0621 | PDE Loss:  -1.724 | Function Loss:  -2.1688\n",
      "Total loss:  -1.0622 | PDE Loss:  -1.7241 | Function Loss:  -2.1689\n",
      "Total loss:  -1.0622 | PDE Loss:  -1.7243 | Function Loss:  -2.1689\n",
      "Total loss:  -1.0623 | PDE Loss:  -1.7245 | Function Loss:  -2.1688\n",
      "Total loss:  -1.0623 | PDE Loss:  -1.7247 | Function Loss:  -2.1688\n",
      "Total loss:  -1.0623 | PDE Loss:  -1.7249 | Function Loss:  -2.1688\n",
      "Total loss:  -1.0623 | PDE Loss:  -1.7251 | Function Loss:  -2.1688\n",
      "Total loss:  -1.0624 | PDE Loss:  -1.7252 | Function Loss:  -2.1688\n",
      "Total loss:  -1.0624 | PDE Loss:  -1.7259 | Function Loss:  -2.1686\n",
      "Total loss:  -1.0625 | PDE Loss:  -1.7258 | Function Loss:  -2.1688\n",
      "Total loss:  -1.0625 | PDE Loss:  -1.7257 | Function Loss:  -2.1688\n",
      "Total loss:  -1.0626 | PDE Loss:  -1.7256 | Function Loss:  -2.169\n",
      "Total loss:  -1.0627 | PDE Loss:  -1.7256 | Function Loss:  -2.1691\n",
      "Total loss:  -1.0628 | PDE Loss:  -1.7257 | Function Loss:  -2.1692\n",
      "Total loss:  -1.0628 | PDE Loss:  -1.7256 | Function Loss:  -2.1693\n",
      "Total loss:  -1.0629 | PDE Loss:  -1.7257 | Function Loss:  -2.1693\n",
      "Total loss:  -1.063 | PDE Loss:  -1.7262 | Function Loss:  -2.1693\n",
      "Total loss:  -1.063 | PDE Loss:  -1.7266 | Function Loss:  -2.1693\n",
      "Total loss:  -1.0631 | PDE Loss:  -1.727 | Function Loss:  -2.1692\n",
      "Total loss:  -1.0631 | PDE Loss:  -1.7274 | Function Loss:  -2.1692\n",
      "Total loss:  -1.0632 | PDE Loss:  -1.7277 | Function Loss:  -2.1691\n",
      "Total loss:  -1.0632 | PDE Loss:  -1.728 | Function Loss:  -2.1691\n",
      "Total loss:  -1.0633 | PDE Loss:  -1.7282 | Function Loss:  -2.1692\n",
      "Total loss:  -1.0634 | PDE Loss:  -1.7283 | Function Loss:  -2.1692\n",
      "Total loss:  -1.0634 | PDE Loss:  -1.7283 | Function Loss:  -2.1693\n",
      "Total loss:  -1.0635 | PDE Loss:  -1.7283 | Function Loss:  -2.1694\n",
      "Total loss:  -1.0635 | PDE Loss:  -1.7282 | Function Loss:  -2.1694\n",
      "Total loss:  -1.0636 | PDE Loss:  -1.7281 | Function Loss:  -2.1695\n",
      "Total loss:  -1.0636 | PDE Loss:  -1.728 | Function Loss:  -2.1696\n",
      "Total loss:  -1.0636 | PDE Loss:  -1.7279 | Function Loss:  -2.1697\n",
      "Total loss:  -1.0637 | PDE Loss:  -1.7277 | Function Loss:  -2.1698\n",
      "Total loss:  -1.0637 | PDE Loss:  -1.7277 | Function Loss:  -2.1698\n",
      "Total loss:  -1.0637 | PDE Loss:  -1.7276 | Function Loss:  -2.1699\n",
      "Total loss:  -1.0638 | PDE Loss:  -1.7279 | Function Loss:  -2.1698\n",
      "Total loss:  -1.0638 | PDE Loss:  -1.728 | Function Loss:  -2.1698\n",
      "Total loss:  -1.0638 | PDE Loss:  -1.7283 | Function Loss:  -2.1698\n",
      "Total loss:  -1.0639 | PDE Loss:  -1.7284 | Function Loss:  -2.1698\n",
      "Total loss:  -1.0638 | PDE Loss:  -1.7285 | Function Loss:  -2.1697\n",
      "Total loss:  -1.0639 | PDE Loss:  -1.7285 | Function Loss:  -2.1698\n",
      "Total loss:  -1.0639 | PDE Loss:  -1.7285 | Function Loss:  -2.1699\n",
      "Total loss:  -1.0639 | PDE Loss:  -1.7285 | Function Loss:  -2.1699\n",
      "Total loss:  -1.064 | PDE Loss:  -1.7283 | Function Loss:  -2.17\n",
      "Total loss:  -1.064 | PDE Loss:  -1.7281 | Function Loss:  -2.1701\n",
      "Total loss:  -1.064 | PDE Loss:  -1.728 | Function Loss:  -2.1702\n",
      "Total loss:  -1.0641 | PDE Loss:  -1.7278 | Function Loss:  -2.1702\n",
      "Total loss:  -1.0641 | PDE Loss:  -1.7276 | Function Loss:  -2.1703\n",
      "Total loss:  -1.0641 | PDE Loss:  -1.7276 | Function Loss:  -2.1704\n",
      "Total loss:  -1.0641 | PDE Loss:  -1.7276 | Function Loss:  -2.1704\n",
      "Total loss:  -1.0642 | PDE Loss:  -1.7277 | Function Loss:  -2.1704\n",
      "Total loss:  -1.0642 | PDE Loss:  -1.7278 | Function Loss:  -2.1704\n",
      "Total loss:  -1.0642 | PDE Loss:  -1.728 | Function Loss:  -2.1704\n",
      "Total loss:  -1.0642 | PDE Loss:  -1.7282 | Function Loss:  -2.1704\n",
      "Total loss:  -1.0643 | PDE Loss:  -1.7283 | Function Loss:  -2.1704\n",
      "Total loss:  -1.0643 | PDE Loss:  -1.7288 | Function Loss:  -2.1703\n",
      "Total loss:  -1.0643 | PDE Loss:  -1.7288 | Function Loss:  -2.1703\n",
      "Total loss:  -1.0644 | PDE Loss:  -1.7287 | Function Loss:  -2.1704\n",
      "Total loss:  -1.0644 | PDE Loss:  -1.7285 | Function Loss:  -2.1705\n",
      "Total loss:  -1.0644 | PDE Loss:  -1.7284 | Function Loss:  -2.1706\n",
      "Total loss:  -1.0645 | PDE Loss:  -1.7283 | Function Loss:  -2.1706\n",
      "Total loss:  -1.0645 | PDE Loss:  -1.7282 | Function Loss:  -2.1707\n",
      "Total loss:  -1.0646 | PDE Loss:  -1.7281 | Function Loss:  -2.1708\n",
      "Total loss:  -1.0645 | PDE Loss:  -1.7277 | Function Loss:  -2.1709\n",
      "Total loss:  -1.0646 | PDE Loss:  -1.728 | Function Loss:  -2.1708\n",
      "Total loss:  -1.0646 | PDE Loss:  -1.728 | Function Loss:  -2.1709\n",
      "Total loss:  -1.0647 | PDE Loss:  -1.7281 | Function Loss:  -2.171\n",
      "Total loss:  -1.0647 | PDE Loss:  -1.7282 | Function Loss:  -2.171\n",
      "Total loss:  -1.0648 | PDE Loss:  -1.7282 | Function Loss:  -2.171\n",
      "Total loss:  -1.0648 | PDE Loss:  -1.7284 | Function Loss:  -2.171\n",
      "Total loss:  -1.0648 | PDE Loss:  -1.7286 | Function Loss:  -2.171\n",
      "Total loss:  -1.0649 | PDE Loss:  -1.7289 | Function Loss:  -2.171\n",
      "Total loss:  -1.0649 | PDE Loss:  -1.729 | Function Loss:  -2.171\n",
      "Total loss:  -1.065 | PDE Loss:  -1.7292 | Function Loss:  -2.171\n",
      "Total loss:  -1.065 | PDE Loss:  -1.7292 | Function Loss:  -2.1711\n",
      "Total loss:  -1.065 | PDE Loss:  -1.7293 | Function Loss:  -2.1711\n",
      "Total loss:  -1.0651 | PDE Loss:  -1.7293 | Function Loss:  -2.1711\n",
      "Total loss:  -1.0651 | PDE Loss:  -1.7294 | Function Loss:  -2.1711\n",
      "Total loss:  -1.0651 | PDE Loss:  -1.7294 | Function Loss:  -2.1712\n",
      "Total loss:  -1.0652 | PDE Loss:  -1.7295 | Function Loss:  -2.1712\n",
      "Total loss:  -1.0652 | PDE Loss:  -1.7296 | Function Loss:  -2.1712\n",
      "Total loss:  -1.0652 | PDE Loss:  -1.7296 | Function Loss:  -2.1713\n",
      "Total loss:  -1.0653 | PDE Loss:  -1.7297 | Function Loss:  -2.1713\n",
      "Total loss:  -1.0653 | PDE Loss:  -1.7298 | Function Loss:  -2.1713\n",
      "Total loss:  -1.0653 | PDE Loss:  -1.73 | Function Loss:  -2.1713\n",
      "Total loss:  -1.0654 | PDE Loss:  -1.7301 | Function Loss:  -2.1713\n",
      "Total loss:  -1.0654 | PDE Loss:  -1.7304 | Function Loss:  -2.1713\n",
      "Total loss:  -1.0655 | PDE Loss:  -1.7305 | Function Loss:  -2.1713\n",
      "Total loss:  -1.0655 | PDE Loss:  -1.7307 | Function Loss:  -2.1713\n",
      "Total loss:  -1.0656 | PDE Loss:  -1.7309 | Function Loss:  -2.1713\n",
      "Total loss:  -1.0656 | PDE Loss:  -1.731 | Function Loss:  -2.1714\n",
      "Total loss:  -1.0657 | PDE Loss:  -1.7311 | Function Loss:  -2.1714\n",
      "Total loss:  -1.0657 | PDE Loss:  -1.7312 | Function Loss:  -2.1714\n",
      "Total loss:  -1.0657 | PDE Loss:  -1.7312 | Function Loss:  -2.1714\n",
      "Total loss:  -1.0658 | PDE Loss:  -1.7312 | Function Loss:  -2.1715\n",
      "Total loss:  -1.0658 | PDE Loss:  -1.7313 | Function Loss:  -2.1715\n",
      "Total loss:  -1.0659 | PDE Loss:  -1.7313 | Function Loss:  -2.1716\n",
      "Total loss:  -1.0659 | PDE Loss:  -1.7314 | Function Loss:  -2.1716\n",
      "Total loss:  -1.0659 | PDE Loss:  -1.7314 | Function Loss:  -2.1716\n",
      "Total loss:  -1.0659 | PDE Loss:  -1.7314 | Function Loss:  -2.1717\n",
      "Total loss:  -1.066 | PDE Loss:  -1.7313 | Function Loss:  -2.1717\n",
      "Total loss:  -1.066 | PDE Loss:  -1.7317 | Function Loss:  -2.1717\n",
      "Total loss:  -1.0661 | PDE Loss:  -1.7315 | Function Loss:  -2.1718\n",
      "Total loss:  -1.0661 | PDE Loss:  -1.7314 | Function Loss:  -2.1719\n",
      "Total loss:  -1.0661 | PDE Loss:  -1.7313 | Function Loss:  -2.1719\n",
      "Total loss:  -1.0662 | PDE Loss:  -1.7313 | Function Loss:  -2.172\n",
      "Total loss:  -1.0662 | PDE Loss:  -1.7312 | Function Loss:  -2.172\n",
      "Total loss:  -1.0662 | PDE Loss:  -1.7312 | Function Loss:  -2.172\n",
      "Total loss:  -1.0662 | PDE Loss:  -1.7311 | Function Loss:  -2.1721\n",
      "Total loss:  -1.0663 | PDE Loss:  -1.7311 | Function Loss:  -2.1721\n",
      "Total loss:  -1.0663 | PDE Loss:  -1.7309 | Function Loss:  -2.1722\n",
      "Total loss:  -1.0663 | PDE Loss:  -1.7306 | Function Loss:  -2.1723\n",
      "Total loss:  -1.0663 | PDE Loss:  -1.7308 | Function Loss:  -2.1723\n",
      "Total loss:  -1.0663 | PDE Loss:  -1.7306 | Function Loss:  -2.1724\n",
      "Total loss:  -1.0664 | PDE Loss:  -1.7306 | Function Loss:  -2.1724\n",
      "Total loss:  -1.0664 | PDE Loss:  -1.7306 | Function Loss:  -2.1725\n",
      "Total loss:  -1.0665 | PDE Loss:  -1.7306 | Function Loss:  -2.1725\n",
      "Total loss:  -1.0665 | PDE Loss:  -1.7306 | Function Loss:  -2.1725\n",
      "Total loss:  -1.0665 | PDE Loss:  -1.7306 | Function Loss:  -2.1725\n",
      "Total loss:  -1.0665 | PDE Loss:  -1.7307 | Function Loss:  -2.1726\n",
      "Total loss:  -1.0665 | PDE Loss:  -1.7307 | Function Loss:  -2.1726\n",
      "Total loss:  -1.0665 | PDE Loss:  -1.7309 | Function Loss:  -2.1726\n",
      "Total loss:  -1.0666 | PDE Loss:  -1.731 | Function Loss:  -2.1726\n",
      "Total loss:  -1.0666 | PDE Loss:  -1.7311 | Function Loss:  -2.1726\n",
      "Total loss:  -1.0667 | PDE Loss:  -1.7313 | Function Loss:  -2.1726\n",
      "Total loss:  -1.0665 | PDE Loss:  -1.7313 | Function Loss:  -2.1723\n",
      "Total loss:  -1.0667 | PDE Loss:  -1.7314 | Function Loss:  -2.1726\n",
      "Total loss:  -1.0667 | PDE Loss:  -1.7315 | Function Loss:  -2.1726\n",
      "Total loss:  -1.0667 | PDE Loss:  -1.7315 | Function Loss:  -2.1726\n",
      "Total loss:  -1.0668 | PDE Loss:  -1.7316 | Function Loss:  -2.1727\n",
      "Total loss:  -1.0668 | PDE Loss:  -1.7316 | Function Loss:  -2.1727\n",
      "Total loss:  -1.0668 | PDE Loss:  -1.7315 | Function Loss:  -2.1728\n",
      "Total loss:  -1.0669 | PDE Loss:  -1.7314 | Function Loss:  -2.1728\n",
      "Total loss:  -1.0669 | PDE Loss:  -1.7314 | Function Loss:  -2.1728\n",
      "Total loss:  -1.0669 | PDE Loss:  -1.7312 | Function Loss:  -2.1729\n",
      "Total loss:  -1.0669 | PDE Loss:  -1.7311 | Function Loss:  -2.173\n",
      "Total loss:  -1.067 | PDE Loss:  -1.731 | Function Loss:  -2.1731\n",
      "Total loss:  -1.067 | PDE Loss:  -1.731 | Function Loss:  -2.1731\n",
      "Total loss:  -1.067 | PDE Loss:  -1.7309 | Function Loss:  -2.1732\n",
      "Total loss:  -1.0671 | PDE Loss:  -1.7308 | Function Loss:  -2.1733\n",
      "Total loss:  -1.0671 | PDE Loss:  -1.7308 | Function Loss:  -2.1733\n",
      "Total loss:  -1.0672 | PDE Loss:  -1.7307 | Function Loss:  -2.1734\n",
      "Total loss:  -1.0672 | PDE Loss:  -1.7309 | Function Loss:  -2.1734\n",
      "Total loss:  -1.0673 | PDE Loss:  -1.7308 | Function Loss:  -2.1735\n",
      "Total loss:  -1.0673 | PDE Loss:  -1.731 | Function Loss:  -2.1735\n",
      "Total loss:  -1.0674 | PDE Loss:  -1.7311 | Function Loss:  -2.1736\n",
      "Total loss:  -1.0675 | PDE Loss:  -1.7313 | Function Loss:  -2.1736\n",
      "Total loss:  -1.0675 | PDE Loss:  -1.7314 | Function Loss:  -2.1736\n",
      "Total loss:  -1.0675 | PDE Loss:  -1.7314 | Function Loss:  -2.1736\n",
      "Total loss:  -1.0675 | PDE Loss:  -1.7315 | Function Loss:  -2.1737\n",
      "Total loss:  -1.0676 | PDE Loss:  -1.7315 | Function Loss:  -2.1737\n",
      "Total loss:  -1.0676 | PDE Loss:  -1.7315 | Function Loss:  -2.1737\n",
      "Total loss:  -1.0676 | PDE Loss:  -1.7316 | Function Loss:  -2.1737\n",
      "Total loss:  -1.0676 | PDE Loss:  -1.7317 | Function Loss:  -2.1737\n",
      "Total loss:  -1.0676 | PDE Loss:  -1.7318 | Function Loss:  -2.1737\n",
      "Total loss:  -1.0677 | PDE Loss:  -1.7321 | Function Loss:  -2.1737\n",
      "Total loss:  -1.0677 | PDE Loss:  -1.7321 | Function Loss:  -2.1737\n",
      "Total loss:  -1.0678 | PDE Loss:  -1.7321 | Function Loss:  -2.1738\n",
      "Total loss:  -1.0678 | PDE Loss:  -1.7322 | Function Loss:  -2.1738\n",
      "Total loss:  -1.0679 | PDE Loss:  -1.7322 | Function Loss:  -2.1739\n",
      "Total loss:  -1.0679 | PDE Loss:  -1.7323 | Function Loss:  -2.1739\n",
      "Total loss:  -1.0679 | PDE Loss:  -1.7324 | Function Loss:  -2.1739\n",
      "Total loss:  -1.068 | PDE Loss:  -1.7326 | Function Loss:  -2.1739\n",
      "Total loss:  -1.068 | PDE Loss:  -1.7328 | Function Loss:  -2.1739\n",
      "Total loss:  -1.0681 | PDE Loss:  -1.7328 | Function Loss:  -2.174\n",
      "Total loss:  -1.0681 | PDE Loss:  -1.7331 | Function Loss:  -2.174\n",
      "Total loss:  -1.0682 | PDE Loss:  -1.7332 | Function Loss:  -2.174\n",
      "Total loss:  -1.0682 | PDE Loss:  -1.7335 | Function Loss:  -2.174\n",
      "Total loss:  -1.0682 | PDE Loss:  -1.7335 | Function Loss:  -2.174\n",
      "Total loss:  -1.0683 | PDE Loss:  -1.7335 | Function Loss:  -2.1741\n",
      "Total loss:  -1.0683 | PDE Loss:  -1.7334 | Function Loss:  -2.1741\n",
      "Total loss:  -1.0683 | PDE Loss:  -1.7334 | Function Loss:  -2.1741\n",
      "Total loss:  -1.0683 | PDE Loss:  -1.7333 | Function Loss:  -2.1742\n",
      "Total loss:  -1.0684 | PDE Loss:  -1.7333 | Function Loss:  -2.1742\n",
      "Total loss:  -1.0684 | PDE Loss:  -1.7332 | Function Loss:  -2.1743\n",
      "Total loss:  -1.0685 | PDE Loss:  -1.733 | Function Loss:  -2.1744\n",
      "Total loss:  -1.0685 | PDE Loss:  -1.733 | Function Loss:  -2.1745\n",
      "Total loss:  -1.0685 | PDE Loss:  -1.733 | Function Loss:  -2.1745\n",
      "Total loss:  -1.0686 | PDE Loss:  -1.733 | Function Loss:  -2.1745\n",
      "Total loss:  -1.0686 | PDE Loss:  -1.733 | Function Loss:  -2.1746\n",
      "Total loss:  -1.0686 | PDE Loss:  -1.733 | Function Loss:  -2.1746\n",
      "Total loss:  -1.0686 | PDE Loss:  -1.7331 | Function Loss:  -2.1746\n",
      "Total loss:  -1.0686 | PDE Loss:  -1.7332 | Function Loss:  -2.1746\n",
      "Total loss:  -1.0686 | PDE Loss:  -1.7333 | Function Loss:  -2.1746\n",
      "Total loss:  -1.0687 | PDE Loss:  -1.7334 | Function Loss:  -2.1746\n",
      "Total loss:  -1.0687 | PDE Loss:  -1.7337 | Function Loss:  -2.1745\n",
      "Total loss:  -1.0687 | PDE Loss:  -1.7339 | Function Loss:  -2.1745\n",
      "Total loss:  -1.0688 | PDE Loss:  -1.7343 | Function Loss:  -2.1745\n",
      "Total loss:  -1.0688 | PDE Loss:  -1.735 | Function Loss:  -2.1743\n",
      "Total loss:  -1.0688 | PDE Loss:  -1.7351 | Function Loss:  -2.1743\n",
      "Total loss:  -1.0688 | PDE Loss:  -1.7352 | Function Loss:  -2.1743\n",
      "Total loss:  -1.0689 | PDE Loss:  -1.7353 | Function Loss:  -2.1743\n",
      "Total loss:  -1.0689 | PDE Loss:  -1.7354 | Function Loss:  -2.1743\n",
      "Total loss:  -1.0689 | PDE Loss:  -1.7355 | Function Loss:  -2.1743\n",
      "Total loss:  -1.0689 | PDE Loss:  -1.7356 | Function Loss:  -2.1743\n",
      "Total loss:  -1.069 | PDE Loss:  -1.7358 | Function Loss:  -2.1743\n",
      "Total loss:  -1.069 | PDE Loss:  -1.736 | Function Loss:  -2.1743\n",
      "Total loss:  -1.0691 | PDE Loss:  -1.7363 | Function Loss:  -2.1743\n",
      "Total loss:  -1.0691 | PDE Loss:  -1.7363 | Function Loss:  -2.1744\n",
      "Total loss:  -1.0692 | PDE Loss:  -1.7365 | Function Loss:  -2.1744\n",
      "Total loss:  -1.0692 | PDE Loss:  -1.7366 | Function Loss:  -2.1744\n",
      "Total loss:  -1.0693 | PDE Loss:  -1.7367 | Function Loss:  -2.1744\n",
      "Total loss:  -1.0693 | PDE Loss:  -1.7369 | Function Loss:  -2.1745\n",
      "Total loss:  -1.0694 | PDE Loss:  -1.7371 | Function Loss:  -2.1745\n",
      "Total loss:  -1.0694 | PDE Loss:  -1.7373 | Function Loss:  -2.1745\n",
      "Total loss:  -1.0694 | PDE Loss:  -1.7374 | Function Loss:  -2.1745\n",
      "Total loss:  -1.0695 | PDE Loss:  -1.7376 | Function Loss:  -2.1745\n",
      "Total loss:  -1.0696 | PDE Loss:  -1.7378 | Function Loss:  -2.1745\n",
      "Total loss:  -1.0696 | PDE Loss:  -1.7381 | Function Loss:  -2.1745\n",
      "Total loss:  -1.0697 | PDE Loss:  -1.7384 | Function Loss:  -2.1745\n",
      "Total loss:  -1.0698 | PDE Loss:  -1.7386 | Function Loss:  -2.1745\n",
      "Total loss:  -1.0698 | PDE Loss:  -1.7394 | Function Loss:  -2.1743\n",
      "Total loss:  -1.0698 | PDE Loss:  -1.7393 | Function Loss:  -2.1744\n",
      "Total loss:  -1.0698 | PDE Loss:  -1.7393 | Function Loss:  -2.1744\n",
      "Total loss:  -1.0699 | PDE Loss:  -1.7393 | Function Loss:  -2.1745\n",
      "Total loss:  -1.0699 | PDE Loss:  -1.7394 | Function Loss:  -2.1745\n",
      "Total loss:  -1.0699 | PDE Loss:  -1.7395 | Function Loss:  -2.1745\n",
      "Total loss:  -1.0699 | PDE Loss:  -1.7397 | Function Loss:  -2.1745\n",
      "Total loss:  -1.07 | PDE Loss:  -1.74 | Function Loss:  -2.1744\n",
      "Total loss:  -1.07 | PDE Loss:  -1.7404 | Function Loss:  -2.1744\n",
      "Total loss:  -1.0698 | PDE Loss:  -1.7406 | Function Loss:  -2.1741\n",
      "Total loss:  -1.0701 | PDE Loss:  -1.7407 | Function Loss:  -2.1744\n",
      "Total loss:  -1.0701 | PDE Loss:  -1.7409 | Function Loss:  -2.1744\n",
      "Total loss:  -1.0702 | PDE Loss:  -1.741 | Function Loss:  -2.1744\n",
      "Total loss:  -1.0703 | PDE Loss:  -1.7411 | Function Loss:  -2.1745\n",
      "Total loss:  -1.0703 | PDE Loss:  -1.741 | Function Loss:  -2.1746\n",
      "Total loss:  -1.0703 | PDE Loss:  -1.741 | Function Loss:  -2.1746\n",
      "Total loss:  -1.0704 | PDE Loss:  -1.7409 | Function Loss:  -2.1747\n",
      "Total loss:  -1.0704 | PDE Loss:  -1.7408 | Function Loss:  -2.1748\n",
      "Total loss:  -1.0704 | PDE Loss:  -1.7407 | Function Loss:  -2.1748\n",
      "Total loss:  -1.0704 | PDE Loss:  -1.7406 | Function Loss:  -2.1749\n",
      "Total loss:  -1.0705 | PDE Loss:  -1.7405 | Function Loss:  -2.1749\n",
      "Total loss:  -1.0705 | PDE Loss:  -1.7405 | Function Loss:  -2.1749\n",
      "Total loss:  -1.0705 | PDE Loss:  -1.7405 | Function Loss:  -2.175\n",
      "Total loss:  -1.0705 | PDE Loss:  -1.7406 | Function Loss:  -2.175\n",
      "Total loss:  -1.0705 | PDE Loss:  -1.7407 | Function Loss:  -2.175\n",
      "Total loss:  -1.0706 | PDE Loss:  -1.7407 | Function Loss:  -2.175\n",
      "Total loss:  -1.0706 | PDE Loss:  -1.7408 | Function Loss:  -2.175\n",
      "Total loss:  -1.0706 | PDE Loss:  -1.7409 | Function Loss:  -2.175\n",
      "Total loss:  -1.0707 | PDE Loss:  -1.7409 | Function Loss:  -2.175\n",
      "Total loss:  -1.0707 | PDE Loss:  -1.7409 | Function Loss:  -2.1751\n",
      "Total loss:  -1.0707 | PDE Loss:  -1.7409 | Function Loss:  -2.1751\n",
      "Total loss:  -1.0707 | PDE Loss:  -1.7408 | Function Loss:  -2.1751\n",
      "Total loss:  -1.0707 | PDE Loss:  -1.7408 | Function Loss:  -2.1752\n",
      "Total loss:  -1.0708 | PDE Loss:  -1.7408 | Function Loss:  -2.1752\n",
      "Total loss:  -1.0708 | PDE Loss:  -1.7408 | Function Loss:  -2.1753\n",
      "Total loss:  -1.0709 | PDE Loss:  -1.7408 | Function Loss:  -2.1753\n",
      "Total loss:  -1.0709 | PDE Loss:  -1.7408 | Function Loss:  -2.1754\n",
      "Total loss:  -1.0709 | PDE Loss:  -1.7408 | Function Loss:  -2.1754\n",
      "Total loss:  -1.0709 | PDE Loss:  -1.7408 | Function Loss:  -2.1754\n",
      "Total loss:  -1.0709 | PDE Loss:  -1.7409 | Function Loss:  -2.1754\n",
      "Total loss:  -1.071 | PDE Loss:  -1.7409 | Function Loss:  -2.1754\n",
      "Total loss:  -1.0708 | PDE Loss:  -1.7413 | Function Loss:  -2.1752\n",
      "Total loss:  -1.071 | PDE Loss:  -1.7411 | Function Loss:  -2.1754\n",
      "Total loss:  -1.071 | PDE Loss:  -1.7412 | Function Loss:  -2.1754\n",
      "Total loss:  -1.071 | PDE Loss:  -1.7413 | Function Loss:  -2.1754\n",
      "Total loss:  -1.0711 | PDE Loss:  -1.7415 | Function Loss:  -2.1754\n",
      "Total loss:  -1.0711 | PDE Loss:  -1.7416 | Function Loss:  -2.1754\n",
      "Total loss:  -1.0711 | PDE Loss:  -1.7417 | Function Loss:  -2.1754\n",
      "Total loss:  -1.0711 | PDE Loss:  -1.7418 | Function Loss:  -2.1754\n",
      "Total loss:  -1.0711 | PDE Loss:  -1.7419 | Function Loss:  -2.1754\n",
      "Total loss:  -1.0712 | PDE Loss:  -1.7419 | Function Loss:  -2.1754\n",
      "Total loss:  -1.0712 | PDE Loss:  -1.7422 | Function Loss:  -2.1754\n",
      "Total loss:  -1.0712 | PDE Loss:  -1.7422 | Function Loss:  -2.1754\n",
      "Total loss:  -1.0713 | PDE Loss:  -1.7422 | Function Loss:  -2.1755\n",
      "Total loss:  -1.0713 | PDE Loss:  -1.7421 | Function Loss:  -2.1755\n",
      "Total loss:  -1.0713 | PDE Loss:  -1.7421 | Function Loss:  -2.1756\n",
      "Total loss:  -1.0713 | PDE Loss:  -1.742 | Function Loss:  -2.1756\n",
      "Total loss:  -1.0713 | PDE Loss:  -1.742 | Function Loss:  -2.1756\n",
      "Total loss:  -1.0714 | PDE Loss:  -1.742 | Function Loss:  -2.1756\n",
      "Total loss:  -1.0714 | PDE Loss:  -1.7423 | Function Loss:  -2.1756\n",
      "Total loss:  -1.0714 | PDE Loss:  -1.7423 | Function Loss:  -2.1756\n",
      "Total loss:  -1.0714 | PDE Loss:  -1.7423 | Function Loss:  -2.1756\n",
      "Total loss:  -1.0714 | PDE Loss:  -1.7423 | Function Loss:  -2.1757\n",
      "Total loss:  -1.0714 | PDE Loss:  -1.7423 | Function Loss:  -2.1757\n",
      "Total loss:  -1.0714 | PDE Loss:  -1.7424 | Function Loss:  -2.1757\n",
      "Total loss:  -1.0715 | PDE Loss:  -1.7424 | Function Loss:  -2.1757\n",
      "Total loss:  -1.0715 | PDE Loss:  -1.7424 | Function Loss:  -2.1757\n",
      "Total loss:  -1.0715 | PDE Loss:  -1.7424 | Function Loss:  -2.1757\n",
      "Total loss:  -1.0714 | PDE Loss:  -1.7425 | Function Loss:  -2.1756\n",
      "Total loss:  -1.0715 | PDE Loss:  -1.7424 | Function Loss:  -2.1757\n",
      "Total loss:  -1.0715 | PDE Loss:  -1.7424 | Function Loss:  -2.1757\n",
      "Total loss:  -1.0715 | PDE Loss:  -1.7424 | Function Loss:  -2.1757\n",
      "Total loss:  -1.0715 | PDE Loss:  -1.7423 | Function Loss:  -2.1758\n",
      "Total loss:  -1.0715 | PDE Loss:  -1.7423 | Function Loss:  -2.1758\n",
      "Total loss:  -1.0715 | PDE Loss:  -1.7423 | Function Loss:  -2.1758\n",
      "Total loss:  -1.0716 | PDE Loss:  -1.7423 | Function Loss:  -2.1758\n",
      "Total loss:  -1.0716 | PDE Loss:  -1.7423 | Function Loss:  -2.1758\n",
      "Total loss:  -1.0715 | PDE Loss:  -1.742 | Function Loss:  -2.1759\n",
      "Total loss:  -1.0716 | PDE Loss:  -1.7423 | Function Loss:  -2.1759\n",
      "Total loss:  -1.0716 | PDE Loss:  -1.7424 | Function Loss:  -2.1758\n",
      "Total loss:  -1.0716 | PDE Loss:  -1.7425 | Function Loss:  -2.1759\n",
      "Total loss:  -1.0717 | PDE Loss:  -1.7427 | Function Loss:  -2.1758\n",
      "Total loss:  -1.0717 | PDE Loss:  -1.7427 | Function Loss:  -2.1759\n",
      "Total loss:  -1.0717 | PDE Loss:  -1.7428 | Function Loss:  -2.1759\n",
      "Total loss:  -1.0717 | PDE Loss:  -1.7428 | Function Loss:  -2.1759\n",
      "Total loss:  -1.0717 | PDE Loss:  -1.7428 | Function Loss:  -2.1759\n",
      "Total loss:  -1.0718 | PDE Loss:  -1.7428 | Function Loss:  -2.1759\n",
      "Total loss:  -1.0718 | PDE Loss:  -1.7426 | Function Loss:  -2.176\n",
      "Total loss:  -1.0718 | PDE Loss:  -1.7425 | Function Loss:  -2.1761\n",
      "Total loss:  -1.0718 | PDE Loss:  -1.7423 | Function Loss:  -2.1762\n",
      "Total loss:  -1.0719 | PDE Loss:  -1.7421 | Function Loss:  -2.1763\n",
      "Total loss:  -1.0719 | PDE Loss:  -1.742 | Function Loss:  -2.1763\n",
      "Total loss:  -1.0719 | PDE Loss:  -1.742 | Function Loss:  -2.1764\n",
      "Total loss:  -1.0719 | PDE Loss:  -1.742 | Function Loss:  -2.1763\n",
      "Total loss:  -1.0719 | PDE Loss:  -1.742 | Function Loss:  -2.1764\n",
      "Total loss:  -1.072 | PDE Loss:  -1.7421 | Function Loss:  -2.1764\n",
      "Total loss:  -1.072 | PDE Loss:  -1.7421 | Function Loss:  -2.1764\n",
      "Total loss:  -1.072 | PDE Loss:  -1.7423 | Function Loss:  -2.1764\n",
      "Total loss:  -1.0721 | PDE Loss:  -1.7425 | Function Loss:  -2.1764\n",
      "Total loss:  -1.0721 | PDE Loss:  -1.7426 | Function Loss:  -2.1764\n",
      "Total loss:  -1.0721 | PDE Loss:  -1.7428 | Function Loss:  -2.1764\n",
      "Total loss:  -1.0722 | PDE Loss:  -1.7428 | Function Loss:  -2.1765\n",
      "Total loss:  -1.0722 | PDE Loss:  -1.7429 | Function Loss:  -2.1765\n",
      "Total loss:  -1.0722 | PDE Loss:  -1.7431 | Function Loss:  -2.1765\n",
      "Total loss:  -1.0723 | PDE Loss:  -1.7431 | Function Loss:  -2.1765\n",
      "Total loss:  -1.0723 | PDE Loss:  -1.7431 | Function Loss:  -2.1765\n",
      "Total loss:  -1.0723 | PDE Loss:  -1.7431 | Function Loss:  -2.1766\n",
      "Total loss:  -1.0723 | PDE Loss:  -1.7431 | Function Loss:  -2.1766\n",
      "Total loss:  -1.0723 | PDE Loss:  -1.743 | Function Loss:  -2.1766\n",
      "Total loss:  -1.0724 | PDE Loss:  -1.7431 | Function Loss:  -2.1766\n",
      "Total loss:  -1.0724 | PDE Loss:  -1.743 | Function Loss:  -2.1767\n",
      "Total loss:  -1.0724 | PDE Loss:  -1.743 | Function Loss:  -2.1767\n",
      "Total loss:  -1.0724 | PDE Loss:  -1.7431 | Function Loss:  -2.1767\n",
      "Total loss:  -1.0724 | PDE Loss:  -1.7431 | Function Loss:  -2.1767\n",
      "Total loss:  -1.0724 | PDE Loss:  -1.7432 | Function Loss:  -2.1767\n",
      "Total loss:  -1.0725 | PDE Loss:  -1.7433 | Function Loss:  -2.1767\n",
      "Total loss:  -1.0725 | PDE Loss:  -1.7434 | Function Loss:  -2.1767\n",
      "Total loss:  -1.0725 | PDE Loss:  -1.7441 | Function Loss:  -2.1766\n",
      "Total loss:  -1.0725 | PDE Loss:  -1.7443 | Function Loss:  -2.1765\n",
      "Total loss:  -1.0726 | PDE Loss:  -1.7447 | Function Loss:  -2.1765\n",
      "Total loss:  -1.0726 | PDE Loss:  -1.7459 | Function Loss:  -2.1761\n",
      "Total loss:  -1.0726 | PDE Loss:  -1.7453 | Function Loss:  -2.1764\n",
      "Total loss:  -1.0726 | PDE Loss:  -1.7454 | Function Loss:  -2.1763\n",
      "Total loss:  -1.0727 | PDE Loss:  -1.7458 | Function Loss:  -2.1763\n",
      "Total loss:  -1.0727 | PDE Loss:  -1.746 | Function Loss:  -2.1763\n",
      "Total loss:  -1.0727 | PDE Loss:  -1.7463 | Function Loss:  -2.1762\n",
      "Total loss:  -1.0728 | PDE Loss:  -1.7466 | Function Loss:  -2.1762\n",
      "Total loss:  -1.0728 | PDE Loss:  -1.7467 | Function Loss:  -2.1762\n",
      "Total loss:  -1.0728 | PDE Loss:  -1.7469 | Function Loss:  -2.1762\n",
      "Total loss:  -1.0728 | PDE Loss:  -1.747 | Function Loss:  -2.1762\n",
      "Total loss:  -1.0729 | PDE Loss:  -1.7471 | Function Loss:  -2.1762\n",
      "Total loss:  -1.0729 | PDE Loss:  -1.7472 | Function Loss:  -2.1762\n",
      "Total loss:  -1.0729 | PDE Loss:  -1.7472 | Function Loss:  -2.1762\n",
      "Total loss:  -1.0729 | PDE Loss:  -1.7473 | Function Loss:  -2.1762\n",
      "Total loss:  -1.073 | PDE Loss:  -1.7474 | Function Loss:  -2.1762\n",
      "Total loss:  -1.073 | PDE Loss:  -1.7475 | Function Loss:  -2.1762\n",
      "Total loss:  -1.073 | PDE Loss:  -1.7476 | Function Loss:  -2.1762\n",
      "Total loss:  -1.073 | PDE Loss:  -1.7476 | Function Loss:  -2.1763\n",
      "Total loss:  -1.073 | PDE Loss:  -1.7477 | Function Loss:  -2.1762\n",
      "Total loss:  -1.0731 | PDE Loss:  -1.7479 | Function Loss:  -2.1762\n",
      "Total loss:  -1.0731 | PDE Loss:  -1.7481 | Function Loss:  -2.1762\n",
      "Total loss:  -1.0731 | PDE Loss:  -1.7482 | Function Loss:  -2.1762\n",
      "Total loss:  -1.0731 | PDE Loss:  -1.7484 | Function Loss:  -2.1762\n",
      "Total loss:  -1.0731 | PDE Loss:  -1.7485 | Function Loss:  -2.1761\n",
      "Total loss:  -1.0731 | PDE Loss:  -1.7486 | Function Loss:  -2.1761\n",
      "Total loss:  -1.0732 | PDE Loss:  -1.7487 | Function Loss:  -2.1761\n",
      "Total loss:  -1.0732 | PDE Loss:  -1.7488 | Function Loss:  -2.1761\n",
      "Total loss:  -1.0732 | PDE Loss:  -1.7489 | Function Loss:  -2.1761\n",
      "Total loss:  -1.0732 | PDE Loss:  -1.7491 | Function Loss:  -2.1761\n",
      "Total loss:  -1.0733 | PDE Loss:  -1.7491 | Function Loss:  -2.1761\n",
      "Total loss:  -1.0733 | PDE Loss:  -1.7493 | Function Loss:  -2.1761\n",
      "Total loss:  -1.0733 | PDE Loss:  -1.7494 | Function Loss:  -2.1761\n",
      "Total loss:  -1.0734 | PDE Loss:  -1.7495 | Function Loss:  -2.1762\n",
      "Total loss:  -1.0734 | PDE Loss:  -1.7495 | Function Loss:  -2.1762\n",
      "Total loss:  -1.0734 | PDE Loss:  -1.7495 | Function Loss:  -2.1762\n",
      "Total loss:  -1.0734 | PDE Loss:  -1.7495 | Function Loss:  -2.1762\n",
      "Total loss:  -1.0734 | PDE Loss:  -1.7495 | Function Loss:  -2.1763\n",
      "Total loss:  -1.0735 | PDE Loss:  -1.7495 | Function Loss:  -2.1763\n",
      "Total loss:  -1.0735 | PDE Loss:  -1.7496 | Function Loss:  -2.1763\n",
      "Total loss:  -1.0735 | PDE Loss:  -1.7495 | Function Loss:  -2.1764\n",
      "Total loss:  -1.0734 | PDE Loss:  -1.7497 | Function Loss:  -2.1762\n",
      "Total loss:  -1.0735 | PDE Loss:  -1.7496 | Function Loss:  -2.1764\n",
      "Total loss:  -1.0736 | PDE Loss:  -1.7496 | Function Loss:  -2.1764\n",
      "Total loss:  -1.0736 | PDE Loss:  -1.7496 | Function Loss:  -2.1764\n",
      "Total loss:  -1.0736 | PDE Loss:  -1.7496 | Function Loss:  -2.1765\n",
      "Total loss:  -1.0736 | PDE Loss:  -1.7497 | Function Loss:  -2.1765\n",
      "Total loss:  -1.0737 | PDE Loss:  -1.7498 | Function Loss:  -2.1765\n",
      "Total loss:  -1.0737 | PDE Loss:  -1.75 | Function Loss:  -2.1765\n",
      "Total loss:  -1.0737 | PDE Loss:  -1.7502 | Function Loss:  -2.1764\n",
      "Total loss:  -1.0738 | PDE Loss:  -1.7503 | Function Loss:  -2.1765\n",
      "Total loss:  -1.0738 | PDE Loss:  -1.7505 | Function Loss:  -2.1765\n",
      "Total loss:  -1.0739 | PDE Loss:  -1.7506 | Function Loss:  -2.1765\n",
      "Total loss:  -1.0739 | PDE Loss:  -1.7508 | Function Loss:  -2.1765\n",
      "Total loss:  -1.074 | PDE Loss:  -1.7508 | Function Loss:  -2.1766\n",
      "Total loss:  -1.074 | PDE Loss:  -1.7508 | Function Loss:  -2.1766\n",
      "Total loss:  -1.074 | PDE Loss:  -1.7508 | Function Loss:  -2.1766\n",
      "Total loss:  -1.074 | PDE Loss:  -1.7509 | Function Loss:  -2.1767\n",
      "Total loss:  -1.074 | PDE Loss:  -1.7508 | Function Loss:  -2.1767\n",
      "Total loss:  -1.0741 | PDE Loss:  -1.7509 | Function Loss:  -2.1767\n",
      "Total loss:  -1.0741 | PDE Loss:  -1.751 | Function Loss:  -2.1767\n",
      "Total loss:  -1.0741 | PDE Loss:  -1.7512 | Function Loss:  -2.1767\n",
      "Total loss:  -1.0742 | PDE Loss:  -1.7514 | Function Loss:  -2.1767\n",
      "Total loss:  -1.0742 | PDE Loss:  -1.7516 | Function Loss:  -2.1766\n",
      "Total loss:  -1.0742 | PDE Loss:  -1.7518 | Function Loss:  -2.1766\n",
      "Total loss:  -1.0742 | PDE Loss:  -1.7519 | Function Loss:  -2.1766\n",
      "Total loss:  -1.0742 | PDE Loss:  -1.752 | Function Loss:  -2.1766\n",
      "Total loss:  -1.0743 | PDE Loss:  -1.7522 | Function Loss:  -2.1766\n",
      "Total loss:  -1.0743 | PDE Loss:  -1.7523 | Function Loss:  -2.1766\n",
      "Total loss:  -1.0743 | PDE Loss:  -1.7524 | Function Loss:  -2.1766\n",
      "Total loss:  -1.0743 | PDE Loss:  -1.7524 | Function Loss:  -2.1766\n",
      "Total loss:  -1.0743 | PDE Loss:  -1.7524 | Function Loss:  -2.1766\n",
      "Total loss:  -1.0743 | PDE Loss:  -1.7524 | Function Loss:  -2.1766\n",
      "Total loss:  -1.0744 | PDE Loss:  -1.7524 | Function Loss:  -2.1767\n",
      "Total loss:  -1.0744 | PDE Loss:  -1.7524 | Function Loss:  -2.1767\n",
      "Total loss:  -1.0744 | PDE Loss:  -1.7525 | Function Loss:  -2.1767\n",
      "Total loss:  -1.0744 | PDE Loss:  -1.7526 | Function Loss:  -2.1767\n",
      "Total loss:  -1.0745 | PDE Loss:  -1.7528 | Function Loss:  -2.1767\n",
      "Total loss:  -1.0744 | PDE Loss:  -1.753 | Function Loss:  -2.1766\n",
      "Total loss:  -1.0745 | PDE Loss:  -1.753 | Function Loss:  -2.1767\n",
      "Total loss:  -1.0745 | PDE Loss:  -1.7531 | Function Loss:  -2.1767\n",
      "Total loss:  -1.0746 | PDE Loss:  -1.7533 | Function Loss:  -2.1767\n",
      "Total loss:  -1.0746 | PDE Loss:  -1.7527 | Function Loss:  -2.1768\n",
      "Total loss:  -1.0746 | PDE Loss:  -1.7531 | Function Loss:  -2.1768\n",
      "Total loss:  -1.0746 | PDE Loss:  -1.7531 | Function Loss:  -2.1768\n",
      "Total loss:  -1.0747 | PDE Loss:  -1.7531 | Function Loss:  -2.1769\n",
      "Total loss:  -1.0747 | PDE Loss:  -1.753 | Function Loss:  -2.177\n",
      "Total loss:  -1.0748 | PDE Loss:  -1.7527 | Function Loss:  -2.1771\n",
      "Total loss:  -1.0748 | PDE Loss:  -1.7526 | Function Loss:  -2.1772\n",
      "Total loss:  -1.0748 | PDE Loss:  -1.7525 | Function Loss:  -2.1772\n",
      "Total loss:  -1.0749 | PDE Loss:  -1.7525 | Function Loss:  -2.1773\n",
      "Total loss:  -1.0749 | PDE Loss:  -1.7525 | Function Loss:  -2.1773\n",
      "Total loss:  -1.0749 | PDE Loss:  -1.7526 | Function Loss:  -2.1773\n",
      "Total loss:  -1.075 | PDE Loss:  -1.7527 | Function Loss:  -2.1774\n",
      "Total loss:  -1.075 | PDE Loss:  -1.7529 | Function Loss:  -2.1773\n",
      "Total loss:  -1.0751 | PDE Loss:  -1.753 | Function Loss:  -2.1774\n",
      "Total loss:  -1.0751 | PDE Loss:  -1.7536 | Function Loss:  -2.1773\n",
      "Total loss:  -1.0752 | PDE Loss:  -1.7541 | Function Loss:  -2.1772\n",
      "Total loss:  -1.0753 | PDE Loss:  -1.7547 | Function Loss:  -2.1772\n",
      "Total loss:  -1.0753 | PDE Loss:  -1.755 | Function Loss:  -2.1772\n",
      "Total loss:  -1.0754 | PDE Loss:  -1.7552 | Function Loss:  -2.1772\n",
      "Total loss:  -1.0752 | PDE Loss:  -1.7543 | Function Loss:  -2.1772\n",
      "Total loss:  -1.0754 | PDE Loss:  -1.7551 | Function Loss:  -2.1773\n",
      "Total loss:  -1.0754 | PDE Loss:  -1.7551 | Function Loss:  -2.1773\n",
      "Total loss:  -1.0754 | PDE Loss:  -1.7551 | Function Loss:  -2.1773\n",
      "Total loss:  -1.0755 | PDE Loss:  -1.755 | Function Loss:  -2.1774\n",
      "Total loss:  -1.0755 | PDE Loss:  -1.755 | Function Loss:  -2.1774\n",
      "Total loss:  -1.0755 | PDE Loss:  -1.755 | Function Loss:  -2.1774\n",
      "Total loss:  -1.0755 | PDE Loss:  -1.755 | Function Loss:  -2.1775\n",
      "Total loss:  -1.0756 | PDE Loss:  -1.7551 | Function Loss:  -2.1775\n",
      "Total loss:  -1.0756 | PDE Loss:  -1.7551 | Function Loss:  -2.1775\n",
      "Total loss:  -1.0756 | PDE Loss:  -1.7551 | Function Loss:  -2.1775\n",
      "Total loss:  -1.0757 | PDE Loss:  -1.7553 | Function Loss:  -2.1775\n",
      "Total loss:  -1.0757 | PDE Loss:  -1.7554 | Function Loss:  -2.1775\n",
      "Total loss:  -1.0757 | PDE Loss:  -1.7556 | Function Loss:  -2.1775\n",
      "Total loss:  -1.0758 | PDE Loss:  -1.7558 | Function Loss:  -2.1775\n",
      "Total loss:  -1.0758 | PDE Loss:  -1.7561 | Function Loss:  -2.1775\n",
      "Total loss:  -1.0758 | PDE Loss:  -1.7559 | Function Loss:  -2.1776\n",
      "Total loss:  -1.0758 | PDE Loss:  -1.7563 | Function Loss:  -2.1775\n",
      "Total loss:  -1.0759 | PDE Loss:  -1.7563 | Function Loss:  -2.1776\n",
      "Total loss:  -1.0759 | PDE Loss:  -1.7562 | Function Loss:  -2.1776\n",
      "Total loss:  -1.0759 | PDE Loss:  -1.7562 | Function Loss:  -2.1777\n",
      "Total loss:  -1.076 | PDE Loss:  -1.7561 | Function Loss:  -2.1777\n",
      "Total loss:  -1.076 | PDE Loss:  -1.7561 | Function Loss:  -2.1777\n",
      "Total loss:  -1.076 | PDE Loss:  -1.7561 | Function Loss:  -2.1778\n",
      "Total loss:  -1.076 | PDE Loss:  -1.7562 | Function Loss:  -2.1778\n",
      "Total loss:  -1.076 | PDE Loss:  -1.7563 | Function Loss:  -2.1777\n",
      "Total loss:  -1.0761 | PDE Loss:  -1.7564 | Function Loss:  -2.1778\n",
      "Total loss:  -1.0761 | PDE Loss:  -1.7565 | Function Loss:  -2.1778\n",
      "Total loss:  -1.0761 | PDE Loss:  -1.7567 | Function Loss:  -2.1778\n",
      "Total loss:  -1.0762 | PDE Loss:  -1.7571 | Function Loss:  -2.1777\n",
      "Total loss:  -1.0762 | PDE Loss:  -1.7573 | Function Loss:  -2.1777\n",
      "Total loss:  -1.0763 | PDE Loss:  -1.7578 | Function Loss:  -2.1776\n",
      "Total loss:  -1.0763 | PDE Loss:  -1.758 | Function Loss:  -2.1776\n",
      "Total loss:  -1.0763 | PDE Loss:  -1.7581 | Function Loss:  -2.1777\n",
      "Total loss:  -1.0764 | PDE Loss:  -1.7582 | Function Loss:  -2.1777\n",
      "Total loss:  -1.0765 | PDE Loss:  -1.7583 | Function Loss:  -2.1778\n",
      "Total loss:  -1.0765 | PDE Loss:  -1.7583 | Function Loss:  -2.1778\n",
      "Total loss:  -1.0765 | PDE Loss:  -1.7582 | Function Loss:  -2.1779\n",
      "Total loss:  -1.0766 | PDE Loss:  -1.7582 | Function Loss:  -2.1779\n",
      "Total loss:  -1.0766 | PDE Loss:  -1.758 | Function Loss:  -2.178\n",
      "Total loss:  -1.0766 | PDE Loss:  -1.7582 | Function Loss:  -2.178\n",
      "Total loss:  -1.0767 | PDE Loss:  -1.7584 | Function Loss:  -2.178\n",
      "Total loss:  -1.0767 | PDE Loss:  -1.7586 | Function Loss:  -2.178\n",
      "Total loss:  -1.0767 | PDE Loss:  -1.7588 | Function Loss:  -2.178\n",
      "Total loss:  -1.0768 | PDE Loss:  -1.7589 | Function Loss:  -2.178\n",
      "Total loss:  -1.0768 | PDE Loss:  -1.7591 | Function Loss:  -2.178\n",
      "Total loss:  -1.0768 | PDE Loss:  -1.7591 | Function Loss:  -2.178\n",
      "Total loss:  -1.0768 | PDE Loss:  -1.7592 | Function Loss:  -2.178\n",
      "Total loss:  -1.0769 | PDE Loss:  -1.7593 | Function Loss:  -2.178\n",
      "Total loss:  -1.0769 | PDE Loss:  -1.7593 | Function Loss:  -2.178\n",
      "Total loss:  -1.0769 | PDE Loss:  -1.7593 | Function Loss:  -2.1781\n",
      "Total loss:  -1.077 | PDE Loss:  -1.7593 | Function Loss:  -2.1781\n",
      "Total loss:  -1.077 | PDE Loss:  -1.7592 | Function Loss:  -2.1782\n",
      "Total loss:  -1.077 | PDE Loss:  -1.7592 | Function Loss:  -2.1783\n",
      "Total loss:  -1.0771 | PDE Loss:  -1.7592 | Function Loss:  -2.1783\n",
      "Total loss:  -1.0771 | PDE Loss:  -1.7593 | Function Loss:  -2.1783\n",
      "Total loss:  -1.0771 | PDE Loss:  -1.7594 | Function Loss:  -2.1783\n",
      "Total loss:  -1.0771 | PDE Loss:  -1.7595 | Function Loss:  -2.1783\n",
      "Total loss:  -1.0771 | PDE Loss:  -1.7596 | Function Loss:  -2.1783\n",
      "Total loss:  -1.0772 | PDE Loss:  -1.7598 | Function Loss:  -2.1782\n",
      "Total loss:  -1.0772 | PDE Loss:  -1.7599 | Function Loss:  -2.1782\n",
      "Total loss:  -1.0772 | PDE Loss:  -1.7601 | Function Loss:  -2.1782\n",
      "Total loss:  -1.0772 | PDE Loss:  -1.7602 | Function Loss:  -2.1782\n",
      "Total loss:  -1.0772 | PDE Loss:  -1.7603 | Function Loss:  -2.1782\n",
      "Total loss:  -1.0773 | PDE Loss:  -1.7603 | Function Loss:  -2.1782\n",
      "Total loss:  -1.0773 | PDE Loss:  -1.7603 | Function Loss:  -2.1783\n",
      "Total loss:  -1.0773 | PDE Loss:  -1.7603 | Function Loss:  -2.1783\n",
      "Total loss:  -1.0773 | PDE Loss:  -1.7603 | Function Loss:  -2.1783\n",
      "Total loss:  -1.0773 | PDE Loss:  -1.7603 | Function Loss:  -2.1783\n",
      "Total loss:  -1.0773 | PDE Loss:  -1.7603 | Function Loss:  -2.1784\n",
      "Total loss:  -1.0774 | PDE Loss:  -1.7602 | Function Loss:  -2.1784\n",
      "Total loss:  -1.0774 | PDE Loss:  -1.7602 | Function Loss:  -2.1784\n",
      "Total loss:  -1.0774 | PDE Loss:  -1.7603 | Function Loss:  -2.1784\n",
      "Total loss:  -1.0775 | PDE Loss:  -1.7603 | Function Loss:  -2.1785\n",
      "Total loss:  -1.0775 | PDE Loss:  -1.7603 | Function Loss:  -2.1785\n",
      "Total loss:  -1.0775 | PDE Loss:  -1.7604 | Function Loss:  -2.1785\n",
      "Total loss:  -1.0775 | PDE Loss:  -1.7605 | Function Loss:  -2.1786\n",
      "Total loss:  -1.0776 | PDE Loss:  -1.7607 | Function Loss:  -2.1785\n",
      "Total loss:  -1.0776 | PDE Loss:  -1.7607 | Function Loss:  -2.1785\n",
      "Total loss:  -1.0776 | PDE Loss:  -1.7609 | Function Loss:  -2.1785\n",
      "Total loss:  -1.0776 | PDE Loss:  -1.761 | Function Loss:  -2.1785\n",
      "Total loss:  -1.0776 | PDE Loss:  -1.7609 | Function Loss:  -2.1786\n",
      "Total loss:  -1.0777 | PDE Loss:  -1.7609 | Function Loss:  -2.1786\n",
      "Total loss:  -1.0777 | PDE Loss:  -1.7609 | Function Loss:  -2.1786\n",
      "Total loss:  -1.0777 | PDE Loss:  -1.7608 | Function Loss:  -2.1786\n",
      "Total loss:  -1.0777 | PDE Loss:  -1.7608 | Function Loss:  -2.1787\n",
      "Total loss:  -1.0777 | PDE Loss:  -1.7607 | Function Loss:  -2.1787\n",
      "Total loss:  -1.0777 | PDE Loss:  -1.7608 | Function Loss:  -2.1787\n",
      "Total loss:  -1.0777 | PDE Loss:  -1.7602 | Function Loss:  -2.1788\n",
      "Total loss:  -1.0777 | PDE Loss:  -1.7606 | Function Loss:  -2.1788\n",
      "Total loss:  -1.0777 | PDE Loss:  -1.7607 | Function Loss:  -2.1788\n",
      "Total loss:  -1.0778 | PDE Loss:  -1.7607 | Function Loss:  -2.1788\n",
      "Total loss:  -1.0778 | PDE Loss:  -1.7607 | Function Loss:  -2.1788\n",
      "Total loss:  -1.0779 | PDE Loss:  -1.7606 | Function Loss:  -2.1789\n",
      "Total loss:  -1.0779 | PDE Loss:  -1.7604 | Function Loss:  -2.179\n",
      "Total loss:  -1.0779 | PDE Loss:  -1.7603 | Function Loss:  -2.1791\n",
      "Total loss:  -1.078 | PDE Loss:  -1.7601 | Function Loss:  -2.1792\n",
      "Total loss:  -1.078 | PDE Loss:  -1.76 | Function Loss:  -2.1793\n",
      "Total loss:  -1.078 | PDE Loss:  -1.76 | Function Loss:  -2.1793\n",
      "Total loss:  -1.0781 | PDE Loss:  -1.76 | Function Loss:  -2.1793\n",
      "Total loss:  -1.0781 | PDE Loss:  -1.7601 | Function Loss:  -2.1793\n",
      "Total loss:  -1.0781 | PDE Loss:  -1.7601 | Function Loss:  -2.1793\n",
      "Total loss:  -1.0781 | PDE Loss:  -1.7602 | Function Loss:  -2.1793\n",
      "Total loss:  -1.0781 | PDE Loss:  -1.7603 | Function Loss:  -2.1793\n",
      "Total loss:  -1.0782 | PDE Loss:  -1.7604 | Function Loss:  -2.1793\n",
      "Total loss:  -1.0782 | PDE Loss:  -1.7606 | Function Loss:  -2.1793\n",
      "Total loss:  -1.0782 | PDE Loss:  -1.7608 | Function Loss:  -2.1793\n",
      "Total loss:  -1.0782 | PDE Loss:  -1.761 | Function Loss:  -2.1793\n",
      "Total loss:  -1.0783 | PDE Loss:  -1.7613 | Function Loss:  -2.1793\n",
      "Total loss:  -1.0783 | PDE Loss:  -1.7614 | Function Loss:  -2.1793\n",
      "Total loss:  -1.0783 | PDE Loss:  -1.7614 | Function Loss:  -2.1793\n",
      "Total loss:  -1.0783 | PDE Loss:  -1.7616 | Function Loss:  -2.1793\n",
      "Total loss:  -1.0784 | PDE Loss:  -1.7617 | Function Loss:  -2.1793\n",
      "Total loss:  -1.0784 | PDE Loss:  -1.7619 | Function Loss:  -2.1793\n",
      "Total loss:  -1.0784 | PDE Loss:  -1.7622 | Function Loss:  -2.1793\n",
      "Total loss:  -1.0785 | PDE Loss:  -1.7625 | Function Loss:  -2.1792\n",
      "Total loss:  -1.0786 | PDE Loss:  -1.7628 | Function Loss:  -2.1792\n",
      "Total loss:  -1.0786 | PDE Loss:  -1.7633 | Function Loss:  -2.1791\n",
      "Total loss:  -1.0787 | PDE Loss:  -1.7635 | Function Loss:  -2.1792\n",
      "Total loss:  -1.0787 | PDE Loss:  -1.7635 | Function Loss:  -2.1793\n",
      "Total loss:  -1.0788 | PDE Loss:  -1.7635 | Function Loss:  -2.1793\n",
      "Total loss:  -1.0788 | PDE Loss:  -1.7634 | Function Loss:  -2.1794\n",
      "Total loss:  -1.0789 | PDE Loss:  -1.7632 | Function Loss:  -2.1795\n",
      "Total loss:  -1.0789 | PDE Loss:  -1.7632 | Function Loss:  -2.1796\n",
      "Total loss:  -1.0789 | PDE Loss:  -1.7622 | Function Loss:  -2.1798\n",
      "Total loss:  -1.0789 | PDE Loss:  -1.7628 | Function Loss:  -2.1797\n",
      "Total loss:  -1.0789 | PDE Loss:  -1.7628 | Function Loss:  -2.1797\n",
      "Total loss:  -1.079 | PDE Loss:  -1.7627 | Function Loss:  -2.1798\n",
      "Total loss:  -1.079 | PDE Loss:  -1.7626 | Function Loss:  -2.1798\n",
      "Total loss:  -1.079 | PDE Loss:  -1.7625 | Function Loss:  -2.1799\n",
      "Total loss:  -1.0791 | PDE Loss:  -1.7624 | Function Loss:  -2.18\n",
      "Total loss:  -1.0791 | PDE Loss:  -1.7623 | Function Loss:  -2.18\n",
      "Total loss:  -1.0791 | PDE Loss:  -1.7623 | Function Loss:  -2.1801\n",
      "Total loss:  -1.0792 | PDE Loss:  -1.7623 | Function Loss:  -2.1802\n",
      "Total loss:  -1.0791 | PDE Loss:  -1.7616 | Function Loss:  -2.1802\n",
      "Total loss:  -1.0792 | PDE Loss:  -1.7622 | Function Loss:  -2.1802\n",
      "Total loss:  -1.0793 | PDE Loss:  -1.7623 | Function Loss:  -2.1803\n",
      "Total loss:  -1.0794 | PDE Loss:  -1.7625 | Function Loss:  -2.1803\n",
      "Total loss:  -1.0794 | PDE Loss:  -1.7626 | Function Loss:  -2.1804\n",
      "Total loss:  -1.0795 | PDE Loss:  -1.7628 | Function Loss:  -2.1804\n",
      "Total loss:  -1.0795 | PDE Loss:  -1.7631 | Function Loss:  -2.1803\n",
      "Total loss:  -1.0795 | PDE Loss:  -1.7631 | Function Loss:  -2.1803\n",
      "Total loss:  -1.0795 | PDE Loss:  -1.7632 | Function Loss:  -2.1803\n",
      "Total loss:  -1.0795 | PDE Loss:  -1.7633 | Function Loss:  -2.1803\n",
      "Total loss:  -1.0796 | PDE Loss:  -1.7635 | Function Loss:  -2.1803\n",
      "Total loss:  -1.0796 | PDE Loss:  -1.7636 | Function Loss:  -2.1803\n",
      "Total loss:  -1.0796 | PDE Loss:  -1.7638 | Function Loss:  -2.1803\n",
      "Total loss:  -1.0796 | PDE Loss:  -1.764 | Function Loss:  -2.1803\n",
      "Total loss:  -1.0797 | PDE Loss:  -1.7641 | Function Loss:  -2.1803\n",
      "Total loss:  -1.0797 | PDE Loss:  -1.7643 | Function Loss:  -2.1803\n",
      "Total loss:  -1.0797 | PDE Loss:  -1.7645 | Function Loss:  -2.1803\n",
      "Total loss:  -1.0798 | PDE Loss:  -1.7647 | Function Loss:  -2.1803\n",
      "Total loss:  -1.0798 | PDE Loss:  -1.7649 | Function Loss:  -2.1803\n",
      "Total loss:  -1.0798 | PDE Loss:  -1.7649 | Function Loss:  -2.1803\n",
      "Total loss:  -1.0799 | PDE Loss:  -1.7651 | Function Loss:  -2.1803\n",
      "Total loss:  -1.0799 | PDE Loss:  -1.7652 | Function Loss:  -2.1803\n",
      "Total loss:  -1.0799 | PDE Loss:  -1.7653 | Function Loss:  -2.1803\n",
      "Total loss:  -1.08 | PDE Loss:  -1.7656 | Function Loss:  -2.1803\n",
      "Total loss:  -1.08 | PDE Loss:  -1.7658 | Function Loss:  -2.1803\n",
      "Total loss:  -1.08 | PDE Loss:  -1.7659 | Function Loss:  -2.1802\n",
      "Total loss:  -1.08 | PDE Loss:  -1.7662 | Function Loss:  -2.1802\n",
      "Total loss:  -1.0801 | PDE Loss:  -1.7664 | Function Loss:  -2.1802\n",
      "Total loss:  -1.0801 | PDE Loss:  -1.7668 | Function Loss:  -2.1802\n",
      "Total loss:  -1.0802 | PDE Loss:  -1.7672 | Function Loss:  -2.1801\n",
      "Total loss:  -1.0802 | PDE Loss:  -1.7677 | Function Loss:  -2.18\n",
      "Total loss:  -1.0802 | PDE Loss:  -1.7679 | Function Loss:  -2.18\n",
      "Total loss:  -1.0803 | PDE Loss:  -1.7681 | Function Loss:  -2.18\n",
      "Total loss:  -1.0803 | PDE Loss:  -1.7682 | Function Loss:  -2.18\n",
      "Total loss:  -1.0803 | PDE Loss:  -1.7683 | Function Loss:  -2.18\n",
      "Total loss:  -1.0803 | PDE Loss:  -1.7684 | Function Loss:  -2.18\n",
      "Total loss:  -1.0804 | PDE Loss:  -1.7685 | Function Loss:  -2.18\n",
      "Total loss:  -1.0804 | PDE Loss:  -1.7686 | Function Loss:  -2.18\n",
      "Total loss:  -1.0804 | PDE Loss:  -1.7689 | Function Loss:  -2.18\n",
      "Total loss:  -1.0804 | PDE Loss:  -1.7689 | Function Loss:  -2.18\n",
      "Total loss:  -1.0805 | PDE Loss:  -1.7689 | Function Loss:  -2.1801\n",
      "Total loss:  -1.0805 | PDE Loss:  -1.7691 | Function Loss:  -2.1801\n",
      "Total loss:  -1.0806 | PDE Loss:  -1.7691 | Function Loss:  -2.1801\n",
      "Total loss:  -1.0806 | PDE Loss:  -1.7694 | Function Loss:  -2.1801\n",
      "Total loss:  -1.0806 | PDE Loss:  -1.7693 | Function Loss:  -2.1801\n",
      "Total loss:  -1.0806 | PDE Loss:  -1.7694 | Function Loss:  -2.1802\n",
      "Total loss:  -1.0807 | PDE Loss:  -1.7695 | Function Loss:  -2.1802\n",
      "Total loss:  -1.0807 | PDE Loss:  -1.7697 | Function Loss:  -2.1802\n",
      "Total loss:  -1.0808 | PDE Loss:  -1.7699 | Function Loss:  -2.1802\n",
      "Total loss:  -1.0809 | PDE Loss:  -1.77 | Function Loss:  -2.1803\n",
      "Total loss:  -1.0809 | PDE Loss:  -1.7701 | Function Loss:  -2.1803\n",
      "Total loss:  -1.081 | PDE Loss:  -1.7703 | Function Loss:  -2.1803\n",
      "Total loss:  -1.081 | PDE Loss:  -1.7704 | Function Loss:  -2.1804\n",
      "Total loss:  -1.0809 | PDE Loss:  -1.77 | Function Loss:  -2.1804\n",
      "Total loss:  -1.0811 | PDE Loss:  -1.7704 | Function Loss:  -2.1804\n",
      "Total loss:  -1.0811 | PDE Loss:  -1.7706 | Function Loss:  -2.1804\n",
      "Total loss:  -1.0812 | PDE Loss:  -1.7707 | Function Loss:  -2.1805\n",
      "Total loss:  -1.0812 | PDE Loss:  -1.7709 | Function Loss:  -2.1805\n",
      "Total loss:  -1.0813 | PDE Loss:  -1.7711 | Function Loss:  -2.1805\n",
      "Total loss:  -1.0813 | PDE Loss:  -1.7713 | Function Loss:  -2.1805\n",
      "Total loss:  -1.0814 | PDE Loss:  -1.7716 | Function Loss:  -2.1805\n",
      "Total loss:  -1.0814 | PDE Loss:  -1.772 | Function Loss:  -2.1804\n",
      "Total loss:  -1.0814 | PDE Loss:  -1.7721 | Function Loss:  -2.1804\n",
      "Total loss:  -1.0815 | PDE Loss:  -1.7722 | Function Loss:  -2.1805\n",
      "Total loss:  -1.0815 | PDE Loss:  -1.7722 | Function Loss:  -2.1805\n",
      "Total loss:  -1.0815 | PDE Loss:  -1.7722 | Function Loss:  -2.1805\n",
      "Total loss:  -1.0816 | PDE Loss:  -1.7723 | Function Loss:  -2.1806\n",
      "Total loss:  -1.0816 | PDE Loss:  -1.7722 | Function Loss:  -2.1806\n",
      "Total loss:  -1.0816 | PDE Loss:  -1.7722 | Function Loss:  -2.1806\n",
      "Total loss:  -1.0816 | PDE Loss:  -1.7721 | Function Loss:  -2.1807\n",
      "Total loss:  -1.0817 | PDE Loss:  -1.7719 | Function Loss:  -2.1808\n",
      "Total loss:  -1.0817 | PDE Loss:  -1.7719 | Function Loss:  -2.1808\n",
      "Total loss:  -1.0817 | PDE Loss:  -1.7718 | Function Loss:  -2.1809\n",
      "Total loss:  -1.0818 | PDE Loss:  -1.7719 | Function Loss:  -2.1809\n",
      "Total loss:  -1.0818 | PDE Loss:  -1.7719 | Function Loss:  -2.1809\n",
      "Total loss:  -1.0818 | PDE Loss:  -1.772 | Function Loss:  -2.181\n",
      "Total loss:  -1.0818 | PDE Loss:  -1.772 | Function Loss:  -2.181\n",
      "Total loss:  -1.0819 | PDE Loss:  -1.7721 | Function Loss:  -2.181\n",
      "Total loss:  -1.0819 | PDE Loss:  -1.7718 | Function Loss:  -2.1811\n",
      "Total loss:  -1.0819 | PDE Loss:  -1.7719 | Function Loss:  -2.1811\n",
      "Total loss:  -1.082 | PDE Loss:  -1.772 | Function Loss:  -2.1812\n",
      "Total loss:  -1.082 | PDE Loss:  -1.7719 | Function Loss:  -2.1812\n",
      "Total loss:  -1.082 | PDE Loss:  -1.7719 | Function Loss:  -2.1813\n",
      "Total loss:  -1.0821 | PDE Loss:  -1.7718 | Function Loss:  -2.1813\n",
      "Total loss:  -1.0821 | PDE Loss:  -1.7716 | Function Loss:  -2.1814\n",
      "Total loss:  -1.0821 | PDE Loss:  -1.7715 | Function Loss:  -2.1815\n",
      "Total loss:  -1.0821 | PDE Loss:  -1.7709 | Function Loss:  -2.1816\n",
      "Total loss:  -1.0821 | PDE Loss:  -1.7714 | Function Loss:  -2.1815\n",
      "Total loss:  -1.0822 | PDE Loss:  -1.7712 | Function Loss:  -2.1816\n",
      "Total loss:  -1.0822 | PDE Loss:  -1.7712 | Function Loss:  -2.1817\n",
      "Total loss:  -1.0823 | PDE Loss:  -1.7709 | Function Loss:  -2.1818\n",
      "Total loss:  -1.0823 | PDE Loss:  -1.7711 | Function Loss:  -2.1818\n",
      "Total loss:  -1.0823 | PDE Loss:  -1.7713 | Function Loss:  -2.1818\n",
      "Total loss:  -1.0824 | PDE Loss:  -1.7716 | Function Loss:  -2.1817\n",
      "Total loss:  -1.0824 | PDE Loss:  -1.7719 | Function Loss:  -2.1817\n",
      "Total loss:  -1.0824 | PDE Loss:  -1.7723 | Function Loss:  -2.1816\n",
      "Total loss:  -1.0825 | PDE Loss:  -1.7727 | Function Loss:  -2.1816\n",
      "Total loss:  -1.0825 | PDE Loss:  -1.7731 | Function Loss:  -2.1816\n",
      "Total loss:  -1.0826 | PDE Loss:  -1.7737 | Function Loss:  -2.1815\n",
      "Total loss:  -1.0826 | PDE Loss:  -1.7737 | Function Loss:  -2.1815\n",
      "Total loss:  -1.0827 | PDE Loss:  -1.7734 | Function Loss:  -2.1817\n",
      "Total loss:  -1.0827 | PDE Loss:  -1.7731 | Function Loss:  -2.1818\n",
      "Total loss:  -1.0828 | PDE Loss:  -1.7729 | Function Loss:  -2.1819\n",
      "Total loss:  -1.0828 | PDE Loss:  -1.7726 | Function Loss:  -2.182\n",
      "Total loss:  -1.0828 | PDE Loss:  -1.7721 | Function Loss:  -2.1822\n",
      "Total loss:  -1.0828 | PDE Loss:  -1.772 | Function Loss:  -2.1822\n",
      "Total loss:  -1.0829 | PDE Loss:  -1.772 | Function Loss:  -2.1823\n",
      "Total loss:  -1.0829 | PDE Loss:  -1.772 | Function Loss:  -2.1823\n",
      "Total loss:  -1.0829 | PDE Loss:  -1.7719 | Function Loss:  -2.1824\n",
      "Total loss:  -1.0829 | PDE Loss:  -1.7719 | Function Loss:  -2.1824\n",
      "Total loss:  -1.083 | PDE Loss:  -1.7718 | Function Loss:  -2.1824\n",
      "Total loss:  -1.083 | PDE Loss:  -1.7717 | Function Loss:  -2.1825\n",
      "Total loss:  -1.0829 | PDE Loss:  -1.7719 | Function Loss:  -2.1823\n",
      "Total loss:  -1.083 | PDE Loss:  -1.7718 | Function Loss:  -2.1825\n",
      "Total loss:  -1.0831 | PDE Loss:  -1.7717 | Function Loss:  -2.1826\n",
      "Total loss:  -1.0831 | PDE Loss:  -1.7716 | Function Loss:  -2.1827\n",
      "Total loss:  -1.0832 | PDE Loss:  -1.7715 | Function Loss:  -2.1828\n",
      "Total loss:  -1.0833 | PDE Loss:  -1.7713 | Function Loss:  -2.183\n",
      "Total loss:  -1.0834 | PDE Loss:  -1.7707 | Function Loss:  -2.1833\n",
      "Total loss:  -1.0835 | PDE Loss:  -1.7706 | Function Loss:  -2.1834\n",
      "Total loss:  -1.0836 | PDE Loss:  -1.7705 | Function Loss:  -2.1836\n",
      "Total loss:  -1.0837 | PDE Loss:  -1.7705 | Function Loss:  -2.1837\n",
      "Total loss:  -1.0837 | PDE Loss:  -1.7705 | Function Loss:  -2.1838\n",
      "Total loss:  -1.0838 | PDE Loss:  -1.7704 | Function Loss:  -2.1839\n",
      "Total loss:  -1.0838 | PDE Loss:  -1.7701 | Function Loss:  -2.1839\n",
      "Total loss:  -1.0838 | PDE Loss:  -1.7704 | Function Loss:  -2.1839\n",
      "Total loss:  -1.0839 | PDE Loss:  -1.7703 | Function Loss:  -2.184\n",
      "Total loss:  -1.084 | PDE Loss:  -1.7703 | Function Loss:  -2.1841\n",
      "Total loss:  -1.0841 | PDE Loss:  -1.7704 | Function Loss:  -2.1842\n",
      "Total loss:  -1.0842 | PDE Loss:  -1.7707 | Function Loss:  -2.1843\n",
      "Total loss:  -1.0843 | PDE Loss:  -1.7708 | Function Loss:  -2.1843\n",
      "Total loss:  -1.0843 | PDE Loss:  -1.7711 | Function Loss:  -2.1844\n",
      "Total loss:  -1.0844 | PDE Loss:  -1.7713 | Function Loss:  -2.1844\n",
      "Total loss:  -1.0845 | PDE Loss:  -1.7716 | Function Loss:  -2.1844\n",
      "Total loss:  -1.0845 | PDE Loss:  -1.7717 | Function Loss:  -2.1845\n",
      "Total loss:  -1.0846 | PDE Loss:  -1.7719 | Function Loss:  -2.1845\n",
      "Total loss:  -1.0846 | PDE Loss:  -1.7719 | Function Loss:  -2.1845\n",
      "Total loss:  -1.0847 | PDE Loss:  -1.7718 | Function Loss:  -2.1846\n",
      "Total loss:  -1.0847 | PDE Loss:  -1.7716 | Function Loss:  -2.1847\n",
      "Total loss:  -1.0847 | PDE Loss:  -1.7715 | Function Loss:  -2.1848\n",
      "Total loss:  -1.0848 | PDE Loss:  -1.7713 | Function Loss:  -2.1848\n",
      "Total loss:  -1.0848 | PDE Loss:  -1.7712 | Function Loss:  -2.1849\n",
      "Total loss:  -1.0848 | PDE Loss:  -1.7711 | Function Loss:  -2.185\n",
      "Total loss:  -1.0849 | PDE Loss:  -1.771 | Function Loss:  -2.1851\n",
      "Total loss:  -1.0849 | PDE Loss:  -1.7706 | Function Loss:  -2.1852\n",
      "Total loss:  -1.085 | PDE Loss:  -1.7707 | Function Loss:  -2.1853\n",
      "Total loss:  -1.085 | PDE Loss:  -1.7709 | Function Loss:  -2.1853\n",
      "Total loss:  -1.0851 | PDE Loss:  -1.7711 | Function Loss:  -2.1853\n",
      "Total loss:  -1.0851 | PDE Loss:  -1.7712 | Function Loss:  -2.1853\n",
      "Total loss:  -1.0851 | PDE Loss:  -1.7719 | Function Loss:  -2.1851\n",
      "Total loss:  -1.0851 | PDE Loss:  -1.7715 | Function Loss:  -2.1852\n",
      "Total loss:  -1.0852 | PDE Loss:  -1.7717 | Function Loss:  -2.1852\n",
      "Total loss:  -1.0852 | PDE Loss:  -1.7717 | Function Loss:  -2.1852\n",
      "Total loss:  -1.0852 | PDE Loss:  -1.7718 | Function Loss:  -2.1853\n",
      "Total loss:  -1.0852 | PDE Loss:  -1.7718 | Function Loss:  -2.1853\n",
      "Total loss:  -1.0853 | PDE Loss:  -1.772 | Function Loss:  -2.1853\n",
      "Total loss:  -1.0851 | PDE Loss:  -1.7699 | Function Loss:  -2.1856\n",
      "Total loss:  -1.0853 | PDE Loss:  -1.7717 | Function Loss:  -2.1854\n",
      "Total loss:  -1.0853 | PDE Loss:  -1.7719 | Function Loss:  -2.1854\n",
      "Total loss:  -1.0854 | PDE Loss:  -1.7721 | Function Loss:  -2.1854\n",
      "Total loss:  -1.0855 | PDE Loss:  -1.7724 | Function Loss:  -2.1855\n",
      "Total loss:  -1.0856 | PDE Loss:  -1.7726 | Function Loss:  -2.1855\n",
      "Total loss:  -1.0856 | PDE Loss:  -1.7727 | Function Loss:  -2.1855\n",
      "Total loss:  -1.0857 | PDE Loss:  -1.7727 | Function Loss:  -2.1856\n",
      "Total loss:  -1.0858 | PDE Loss:  -1.7727 | Function Loss:  -2.1857\n",
      "Total loss:  -1.0858 | PDE Loss:  -1.7722 | Function Loss:  -2.1859\n",
      "Total loss:  -1.0859 | PDE Loss:  -1.7724 | Function Loss:  -2.1859\n",
      "Total loss:  -1.0859 | PDE Loss:  -1.7726 | Function Loss:  -2.1859\n",
      "Total loss:  -1.086 | PDE Loss:  -1.7727 | Function Loss:  -2.186\n",
      "Total loss:  -1.086 | PDE Loss:  -1.7728 | Function Loss:  -2.186\n",
      "Total loss:  -1.0861 | PDE Loss:  -1.7729 | Function Loss:  -2.1861\n",
      "Total loss:  -1.0861 | PDE Loss:  -1.7725 | Function Loss:  -2.1862\n",
      "Total loss:  -1.0861 | PDE Loss:  -1.7728 | Function Loss:  -2.1861\n",
      "Total loss:  -1.0861 | PDE Loss:  -1.7729 | Function Loss:  -2.1861\n",
      "Total loss:  -1.0862 | PDE Loss:  -1.7729 | Function Loss:  -2.1862\n",
      "Total loss:  -1.0862 | PDE Loss:  -1.773 | Function Loss:  -2.1862\n",
      "Total loss:  -1.0862 | PDE Loss:  -1.773 | Function Loss:  -2.1862\n",
      "Total loss:  -1.0862 | PDE Loss:  -1.773 | Function Loss:  -2.1863\n",
      "Total loss:  -1.0863 | PDE Loss:  -1.773 | Function Loss:  -2.1863\n",
      "Total loss:  -1.0863 | PDE Loss:  -1.7729 | Function Loss:  -2.1863\n",
      "Total loss:  -1.0861 | PDE Loss:  -1.7727 | Function Loss:  -2.1862\n",
      "Total loss:  -1.0863 | PDE Loss:  -1.773 | Function Loss:  -2.1863\n",
      "Total loss:  -1.0863 | PDE Loss:  -1.773 | Function Loss:  -2.1864\n",
      "Total loss:  -1.0864 | PDE Loss:  -1.7729 | Function Loss:  -2.1864\n",
      "Total loss:  -1.0864 | PDE Loss:  -1.7728 | Function Loss:  -2.1865\n",
      "Total loss:  -1.0864 | PDE Loss:  -1.7728 | Function Loss:  -2.1866\n",
      "Total loss:  -1.0865 | PDE Loss:  -1.7727 | Function Loss:  -2.1866\n",
      "Total loss:  -1.0865 | PDE Loss:  -1.7727 | Function Loss:  -2.1866\n",
      "Total loss:  -1.0865 | PDE Loss:  -1.7726 | Function Loss:  -2.1867\n",
      "Total loss:  -1.0865 | PDE Loss:  -1.7727 | Function Loss:  -2.1867\n",
      "Total loss:  -1.0866 | PDE Loss:  -1.7727 | Function Loss:  -2.1867\n",
      "Total loss:  -1.0866 | PDE Loss:  -1.7729 | Function Loss:  -2.1867\n",
      "Total loss:  -1.0866 | PDE Loss:  -1.773 | Function Loss:  -2.1867\n",
      "Total loss:  -1.0866 | PDE Loss:  -1.7731 | Function Loss:  -2.1867\n",
      "Total loss:  -1.0866 | PDE Loss:  -1.7732 | Function Loss:  -2.1867\n",
      "Total loss:  -1.0866 | PDE Loss:  -1.7733 | Function Loss:  -2.1867\n",
      "Total loss:  -1.0852 | PDE Loss:  -1.7709 | Function Loss:  -2.1855\n",
      "Total loss:  -1.0866 | PDE Loss:  -1.7734 | Function Loss:  -2.1867\n",
      "Total loss:  -1.0867 | PDE Loss:  -1.7735 | Function Loss:  -2.1867\n",
      "Total loss:  -1.0867 | PDE Loss:  -1.7735 | Function Loss:  -2.1867\n",
      "Total loss:  -1.0867 | PDE Loss:  -1.7736 | Function Loss:  -2.1867\n",
      "Total loss:  -1.0867 | PDE Loss:  -1.7737 | Function Loss:  -2.1867\n",
      "Total loss:  -1.0867 | PDE Loss:  -1.7737 | Function Loss:  -2.1867\n",
      "Total loss:  -1.0868 | PDE Loss:  -1.7736 | Function Loss:  -2.1868\n",
      "Total loss:  -1.0868 | PDE Loss:  -1.7736 | Function Loss:  -2.1868\n",
      "Total loss:  -1.0868 | PDE Loss:  -1.7735 | Function Loss:  -2.1869\n",
      "Total loss:  -1.0869 | PDE Loss:  -1.7734 | Function Loss:  -2.1869\n",
      "Total loss:  -1.0869 | PDE Loss:  -1.7729 | Function Loss:  -2.1871\n",
      "Total loss:  -1.0869 | PDE Loss:  -1.7729 | Function Loss:  -2.1871\n",
      "Total loss:  -1.0869 | PDE Loss:  -1.7731 | Function Loss:  -2.1871\n",
      "Total loss:  -1.087 | PDE Loss:  -1.773 | Function Loss:  -2.1872\n",
      "Total loss:  -1.087 | PDE Loss:  -1.7731 | Function Loss:  -2.1872\n",
      "Total loss:  -1.087 | PDE Loss:  -1.773 | Function Loss:  -2.1872\n",
      "Total loss:  -1.0871 | PDE Loss:  -1.7731 | Function Loss:  -2.1873\n",
      "Total loss:  -1.0872 | PDE Loss:  -1.773 | Function Loss:  -2.1874\n",
      "Total loss:  -1.0872 | PDE Loss:  -1.7732 | Function Loss:  -2.1875\n",
      "Total loss:  -1.0873 | PDE Loss:  -1.7732 | Function Loss:  -2.1875\n",
      "Total loss:  -1.0874 | PDE Loss:  -1.7733 | Function Loss:  -2.1876\n",
      "Total loss:  -1.0874 | PDE Loss:  -1.7735 | Function Loss:  -2.1876\n",
      "Total loss:  -1.0874 | PDE Loss:  -1.7737 | Function Loss:  -2.1876\n",
      "Total loss:  -1.0875 | PDE Loss:  -1.7739 | Function Loss:  -2.1876\n",
      "Total loss:  -1.0875 | PDE Loss:  -1.7742 | Function Loss:  -2.1875\n",
      "Total loss:  -1.0875 | PDE Loss:  -1.7744 | Function Loss:  -2.1875\n",
      "Total loss:  -1.0876 | PDE Loss:  -1.775 | Function Loss:  -2.1874\n",
      "Total loss:  -1.0876 | PDE Loss:  -1.7751 | Function Loss:  -2.1874\n",
      "Total loss:  -1.0877 | PDE Loss:  -1.7752 | Function Loss:  -2.1875\n",
      "Total loss:  -1.0877 | PDE Loss:  -1.7753 | Function Loss:  -2.1875\n",
      "Total loss:  -1.0877 | PDE Loss:  -1.7754 | Function Loss:  -2.1875\n",
      "Total loss:  -1.0877 | PDE Loss:  -1.7754 | Function Loss:  -2.1875\n",
      "Total loss:  -1.0878 | PDE Loss:  -1.7754 | Function Loss:  -2.1876\n",
      "Total loss:  -1.0878 | PDE Loss:  -1.775 | Function Loss:  -2.1877\n",
      "Total loss:  -1.0878 | PDE Loss:  -1.7751 | Function Loss:  -2.1877\n",
      "Total loss:  -1.0879 | PDE Loss:  -1.7751 | Function Loss:  -2.1877\n",
      "Total loss:  -1.0879 | PDE Loss:  -1.7751 | Function Loss:  -2.1878\n",
      "Total loss:  -1.0879 | PDE Loss:  -1.775 | Function Loss:  -2.1878\n",
      "Total loss:  -1.0879 | PDE Loss:  -1.7749 | Function Loss:  -2.1879\n",
      "Total loss:  -1.088 | PDE Loss:  -1.7748 | Function Loss:  -2.188\n",
      "Total loss:  -1.088 | PDE Loss:  -1.7746 | Function Loss:  -2.1881\n",
      "Total loss:  -1.0881 | PDE Loss:  -1.7744 | Function Loss:  -2.1882\n",
      "Total loss:  -1.0881 | PDE Loss:  -1.7741 | Function Loss:  -2.1883\n",
      "Total loss:  -1.0881 | PDE Loss:  -1.7741 | Function Loss:  -2.1884\n",
      "Total loss:  -1.0882 | PDE Loss:  -1.7741 | Function Loss:  -2.1884\n",
      "Total loss:  -1.0882 | PDE Loss:  -1.7741 | Function Loss:  -2.1884\n",
      "Total loss:  -1.0882 | PDE Loss:  -1.7741 | Function Loss:  -2.1884\n",
      "Total loss:  -1.0882 | PDE Loss:  -1.7741 | Function Loss:  -2.1885\n",
      "Total loss:  -1.0882 | PDE Loss:  -1.7741 | Function Loss:  -2.1885\n",
      "Total loss:  -1.0883 | PDE Loss:  -1.7741 | Function Loss:  -2.1885\n",
      "Total loss:  -1.0883 | PDE Loss:  -1.7741 | Function Loss:  -2.1886\n",
      "Total loss:  -1.0883 | PDE Loss:  -1.7743 | Function Loss:  -2.1885\n",
      "Total loss:  -1.0884 | PDE Loss:  -1.7743 | Function Loss:  -2.1886\n",
      "Total loss:  -1.0884 | PDE Loss:  -1.7742 | Function Loss:  -2.1887\n",
      "Total loss:  -1.0884 | PDE Loss:  -1.7743 | Function Loss:  -2.1887\n",
      "Total loss:  -1.0885 | PDE Loss:  -1.7744 | Function Loss:  -2.1887\n",
      "Total loss:  -1.0885 | PDE Loss:  -1.7744 | Function Loss:  -2.1887\n",
      "Total loss:  -1.0885 | PDE Loss:  -1.7744 | Function Loss:  -2.1887\n",
      "Total loss:  -1.0885 | PDE Loss:  -1.7745 | Function Loss:  -2.1887\n",
      "Total loss:  -1.0886 | PDE Loss:  -1.7747 | Function Loss:  -2.1888\n",
      "Total loss:  -1.0886 | PDE Loss:  -1.7749 | Function Loss:  -2.1888\n",
      "Total loss:  -1.0887 | PDE Loss:  -1.7749 | Function Loss:  -2.1888\n",
      "Total loss:  -1.0887 | PDE Loss:  -1.7751 | Function Loss:  -2.1888\n",
      "Total loss:  -1.0888 | PDE Loss:  -1.7754 | Function Loss:  -2.1888\n",
      "Total loss:  -1.0888 | PDE Loss:  -1.7756 | Function Loss:  -2.1888\n",
      "Total loss:  -1.0888 | PDE Loss:  -1.7757 | Function Loss:  -2.1888\n",
      "Total loss:  -1.0888 | PDE Loss:  -1.7762 | Function Loss:  -2.1887\n",
      "Total loss:  -1.0888 | PDE Loss:  -1.7759 | Function Loss:  -2.1888\n",
      "Total loss:  -1.0889 | PDE Loss:  -1.776 | Function Loss:  -2.1888\n",
      "Total loss:  -1.0889 | PDE Loss:  -1.776 | Function Loss:  -2.1888\n",
      "Total loss:  -1.0889 | PDE Loss:  -1.776 | Function Loss:  -2.1888\n",
      "Total loss:  -1.0889 | PDE Loss:  -1.776 | Function Loss:  -2.1888\n",
      "Total loss:  -1.0889 | PDE Loss:  -1.776 | Function Loss:  -2.1889\n",
      "Total loss:  -1.089 | PDE Loss:  -1.7762 | Function Loss:  -2.1889\n",
      "Total loss:  -1.089 | PDE Loss:  -1.776 | Function Loss:  -2.1889\n",
      "Total loss:  -1.089 | PDE Loss:  -1.7761 | Function Loss:  -2.1889\n",
      "Total loss:  -1.089 | PDE Loss:  -1.7762 | Function Loss:  -2.189\n",
      "Total loss:  -1.0891 | PDE Loss:  -1.7763 | Function Loss:  -2.189\n",
      "Total loss:  -1.0891 | PDE Loss:  -1.7762 | Function Loss:  -2.189\n",
      "Total loss:  -1.0892 | PDE Loss:  -1.7761 | Function Loss:  -2.1891\n",
      "Total loss:  -1.0892 | PDE Loss:  -1.776 | Function Loss:  -2.1892\n",
      "Total loss:  -1.0893 | PDE Loss:  -1.7756 | Function Loss:  -2.1894\n",
      "Total loss:  -1.0893 | PDE Loss:  -1.7755 | Function Loss:  -2.1895\n",
      "Total loss:  -1.0894 | PDE Loss:  -1.7755 | Function Loss:  -2.1895\n",
      "Total loss:  -1.0894 | PDE Loss:  -1.7755 | Function Loss:  -2.1896\n",
      "Total loss:  -1.0894 | PDE Loss:  -1.7755 | Function Loss:  -2.1896\n",
      "Total loss:  -1.0894 | PDE Loss:  -1.7755 | Function Loss:  -2.1896\n",
      "Total loss:  -1.0895 | PDE Loss:  -1.7756 | Function Loss:  -2.1896\n",
      "Total loss:  -1.0895 | PDE Loss:  -1.7757 | Function Loss:  -2.1896\n",
      "Total loss:  -1.0895 | PDE Loss:  -1.7759 | Function Loss:  -2.1896\n",
      "Total loss:  -1.0895 | PDE Loss:  -1.776 | Function Loss:  -2.1896\n",
      "Total loss:  -1.0896 | PDE Loss:  -1.7764 | Function Loss:  -2.1895\n",
      "Total loss:  -1.0896 | PDE Loss:  -1.7765 | Function Loss:  -2.1895\n",
      "Total loss:  -1.0896 | PDE Loss:  -1.7765 | Function Loss:  -2.1896\n",
      "Total loss:  -1.0896 | PDE Loss:  -1.7765 | Function Loss:  -2.1896\n",
      "Total loss:  -1.0896 | PDE Loss:  -1.7766 | Function Loss:  -2.1896\n",
      "Total loss:  -1.0897 | PDE Loss:  -1.7765 | Function Loss:  -2.1897\n",
      "Total loss:  -1.0897 | PDE Loss:  -1.7765 | Function Loss:  -2.1897\n",
      "Total loss:  -1.0898 | PDE Loss:  -1.7766 | Function Loss:  -2.1898\n",
      "Total loss:  -1.0898 | PDE Loss:  -1.7766 | Function Loss:  -2.1898\n",
      "Total loss:  -1.0899 | PDE Loss:  -1.7768 | Function Loss:  -2.1898\n",
      "Total loss:  -1.0899 | PDE Loss:  -1.7772 | Function Loss:  -2.1898\n",
      "Total loss:  -1.09 | PDE Loss:  -1.7777 | Function Loss:  -2.1898\n",
      "Total loss:  -1.0901 | PDE Loss:  -1.7783 | Function Loss:  -2.1897\n",
      "Total loss:  -1.0901 | PDE Loss:  -1.7788 | Function Loss:  -2.1897\n",
      "Total loss:  -1.0902 | PDE Loss:  -1.7792 | Function Loss:  -2.1896\n",
      "Total loss:  -1.0902 | PDE Loss:  -1.7797 | Function Loss:  -2.1896\n",
      "Total loss:  -1.0903 | PDE Loss:  -1.7799 | Function Loss:  -2.1896\n",
      "Total loss:  -1.0903 | PDE Loss:  -1.7801 | Function Loss:  -2.1895\n",
      "Total loss:  -1.0904 | PDE Loss:  -1.7803 | Function Loss:  -2.1896\n",
      "Total loss:  -1.0904 | PDE Loss:  -1.7804 | Function Loss:  -2.1896\n",
      "Total loss:  -1.0904 | PDE Loss:  -1.7802 | Function Loss:  -2.1897\n",
      "Total loss:  -1.0905 | PDE Loss:  -1.7804 | Function Loss:  -2.1897\n",
      "Total loss:  -1.0905 | PDE Loss:  -1.7805 | Function Loss:  -2.1897\n",
      "Total loss:  -1.0905 | PDE Loss:  -1.7806 | Function Loss:  -2.1897\n",
      "Total loss:  -1.0906 | PDE Loss:  -1.7807 | Function Loss:  -2.1897\n",
      "Total loss:  -1.0906 | PDE Loss:  -1.781 | Function Loss:  -2.1897\n",
      "Total loss:  -1.0907 | PDE Loss:  -1.781 | Function Loss:  -2.1898\n",
      "Total loss:  -1.0908 | PDE Loss:  -1.7814 | Function Loss:  -2.1899\n",
      "Total loss:  -1.0909 | PDE Loss:  -1.781 | Function Loss:  -2.1901\n",
      "Total loss:  -1.091 | PDE Loss:  -1.7812 | Function Loss:  -2.1901\n",
      "Total loss:  -1.091 | PDE Loss:  -1.7813 | Function Loss:  -2.1902\n",
      "Total loss:  -1.0911 | PDE Loss:  -1.7812 | Function Loss:  -2.1902\n",
      "Total loss:  -1.0911 | PDE Loss:  -1.7811 | Function Loss:  -2.1903\n",
      "Total loss:  -1.0911 | PDE Loss:  -1.7811 | Function Loss:  -2.1903\n",
      "Total loss:  -1.0912 | PDE Loss:  -1.7809 | Function Loss:  -2.1904\n",
      "Total loss:  -1.0912 | PDE Loss:  -1.7809 | Function Loss:  -2.1905\n",
      "Total loss:  -1.0912 | PDE Loss:  -1.7809 | Function Loss:  -2.1905\n",
      "Total loss:  -1.0913 | PDE Loss:  -1.781 | Function Loss:  -2.1906\n",
      "Total loss:  -1.0914 | PDE Loss:  -1.7812 | Function Loss:  -2.1906\n",
      "Total loss:  -1.0914 | PDE Loss:  -1.7815 | Function Loss:  -2.1906\n",
      "Total loss:  -1.0915 | PDE Loss:  -1.7817 | Function Loss:  -2.1906\n",
      "Total loss:  -1.0916 | PDE Loss:  -1.782 | Function Loss:  -2.1906\n",
      "Total loss:  -1.0916 | PDE Loss:  -1.7823 | Function Loss:  -2.1906\n",
      "Total loss:  -1.0916 | PDE Loss:  -1.7825 | Function Loss:  -2.1906\n",
      "Total loss:  -1.0917 | PDE Loss:  -1.7827 | Function Loss:  -2.1906\n",
      "Total loss:  -1.0917 | PDE Loss:  -1.783 | Function Loss:  -2.1906\n",
      "Total loss:  -1.0918 | PDE Loss:  -1.7832 | Function Loss:  -2.1906\n",
      "Total loss:  -1.0916 | PDE Loss:  -1.7831 | Function Loss:  -2.1904\n",
      "Total loss:  -1.0918 | PDE Loss:  -1.7833 | Function Loss:  -2.1906\n",
      "Total loss:  -1.0918 | PDE Loss:  -1.7834 | Function Loss:  -2.1906\n",
      "Total loss:  -1.0919 | PDE Loss:  -1.7834 | Function Loss:  -2.1907\n",
      "Total loss:  -1.0919 | PDE Loss:  -1.7832 | Function Loss:  -2.1908\n",
      "Total loss:  -1.092 | PDE Loss:  -1.7831 | Function Loss:  -2.1909\n",
      "Total loss:  -1.092 | PDE Loss:  -1.7828 | Function Loss:  -2.191\n",
      "Total loss:  -1.092 | PDE Loss:  -1.7825 | Function Loss:  -2.191\n",
      "Total loss:  -1.092 | PDE Loss:  -1.7827 | Function Loss:  -2.191\n",
      "Total loss:  -1.0921 | PDE Loss:  -1.7825 | Function Loss:  -2.1911\n",
      "Total loss:  -1.0921 | PDE Loss:  -1.7824 | Function Loss:  -2.1912\n",
      "Total loss:  -1.0921 | PDE Loss:  -1.7826 | Function Loss:  -2.1912\n",
      "Total loss:  -1.0922 | PDE Loss:  -1.7826 | Function Loss:  -2.1913\n",
      "Total loss:  -1.0922 | PDE Loss:  -1.7828 | Function Loss:  -2.1913\n",
      "Total loss:  -1.0922 | PDE Loss:  -1.783 | Function Loss:  -2.1912\n",
      "Total loss:  -1.0923 | PDE Loss:  -1.7831 | Function Loss:  -2.1912\n",
      "Total loss:  -1.0922 | PDE Loss:  -1.7829 | Function Loss:  -2.1913\n",
      "Total loss:  -1.0923 | PDE Loss:  -1.7831 | Function Loss:  -2.1913\n",
      "Total loss:  -1.0923 | PDE Loss:  -1.7833 | Function Loss:  -2.1912\n",
      "Total loss:  -1.0924 | PDE Loss:  -1.7835 | Function Loss:  -2.1912\n",
      "Total loss:  -1.0924 | PDE Loss:  -1.7837 | Function Loss:  -2.1913\n",
      "Total loss:  -1.0924 | PDE Loss:  -1.7838 | Function Loss:  -2.1913\n",
      "Total loss:  -1.0925 | PDE Loss:  -1.7838 | Function Loss:  -2.1913\n",
      "Total loss:  -1.0925 | PDE Loss:  -1.7838 | Function Loss:  -2.1914\n",
      "Total loss:  -1.0925 | PDE Loss:  -1.7838 | Function Loss:  -2.1914\n",
      "Total loss:  -1.0926 | PDE Loss:  -1.7837 | Function Loss:  -2.1915\n",
      "Total loss:  -1.0926 | PDE Loss:  -1.7837 | Function Loss:  -2.1915\n",
      "Total loss:  -1.0926 | PDE Loss:  -1.7836 | Function Loss:  -2.1916\n",
      "Total loss:  -1.0927 | PDE Loss:  -1.7835 | Function Loss:  -2.1916\n",
      "Total loss:  -1.0927 | PDE Loss:  -1.7835 | Function Loss:  -2.1917\n",
      "Total loss:  -1.0927 | PDE Loss:  -1.7836 | Function Loss:  -2.1917\n",
      "Total loss:  -1.0928 | PDE Loss:  -1.7836 | Function Loss:  -2.1918\n",
      "Total loss:  -1.0929 | PDE Loss:  -1.784 | Function Loss:  -2.1918\n",
      "Total loss:  -1.0929 | PDE Loss:  -1.784 | Function Loss:  -2.1918\n",
      "Total loss:  -1.093 | PDE Loss:  -1.784 | Function Loss:  -2.1919\n",
      "Total loss:  -1.093 | PDE Loss:  -1.7841 | Function Loss:  -2.1919\n",
      "Total loss:  -1.0931 | PDE Loss:  -1.7842 | Function Loss:  -2.192\n",
      "Total loss:  -1.0931 | PDE Loss:  -1.7842 | Function Loss:  -2.192\n",
      "Total loss:  -1.0932 | PDE Loss:  -1.7842 | Function Loss:  -2.1921\n",
      "Total loss:  -1.0932 | PDE Loss:  -1.7842 | Function Loss:  -2.1922\n",
      "Total loss:  -1.0933 | PDE Loss:  -1.7842 | Function Loss:  -2.1922\n",
      "Total loss:  -1.0933 | PDE Loss:  -1.7843 | Function Loss:  -2.1923\n",
      "Total loss:  -1.0934 | PDE Loss:  -1.7843 | Function Loss:  -2.1923\n",
      "Total loss:  -1.0934 | PDE Loss:  -1.7843 | Function Loss:  -2.1924\n",
      "Total loss:  -1.0934 | PDE Loss:  -1.7844 | Function Loss:  -2.1924\n",
      "Total loss:  -1.0935 | PDE Loss:  -1.7846 | Function Loss:  -2.1924\n",
      "Total loss:  -1.0936 | PDE Loss:  -1.7847 | Function Loss:  -2.1925\n",
      "Total loss:  -1.0936 | PDE Loss:  -1.7849 | Function Loss:  -2.1924\n",
      "Total loss:  -1.0936 | PDE Loss:  -1.7851 | Function Loss:  -2.1924\n",
      "Total loss:  -1.0937 | PDE Loss:  -1.7853 | Function Loss:  -2.1924\n",
      "Total loss:  -1.0937 | PDE Loss:  -1.7854 | Function Loss:  -2.1925\n",
      "Total loss:  -1.0937 | PDE Loss:  -1.7855 | Function Loss:  -2.1925\n",
      "Total loss:  -1.0938 | PDE Loss:  -1.7856 | Function Loss:  -2.1925\n",
      "Total loss:  -1.0938 | PDE Loss:  -1.7857 | Function Loss:  -2.1925\n",
      "Total loss:  -1.0939 | PDE Loss:  -1.7857 | Function Loss:  -2.1926\n",
      "Total loss:  -1.0939 | PDE Loss:  -1.7857 | Function Loss:  -2.1927\n",
      "Total loss:  -1.094 | PDE Loss:  -1.7856 | Function Loss:  -2.1928\n",
      "Total loss:  -1.0941 | PDE Loss:  -1.7856 | Function Loss:  -2.1929\n",
      "Total loss:  -1.0941 | PDE Loss:  -1.7855 | Function Loss:  -2.193\n",
      "Total loss:  -1.0942 | PDE Loss:  -1.7855 | Function Loss:  -2.193\n",
      "Total loss:  -1.0942 | PDE Loss:  -1.7854 | Function Loss:  -2.1931\n",
      "Total loss:  -1.0942 | PDE Loss:  -1.7855 | Function Loss:  -2.1931\n",
      "Total loss:  -1.0942 | PDE Loss:  -1.7855 | Function Loss:  -2.1931\n",
      "Total loss:  -1.0943 | PDE Loss:  -1.7854 | Function Loss:  -2.1932\n",
      "Total loss:  -1.0943 | PDE Loss:  -1.7855 | Function Loss:  -2.1932\n",
      "Total loss:  -1.0944 | PDE Loss:  -1.7855 | Function Loss:  -2.1932\n",
      "Total loss:  -1.0944 | PDE Loss:  -1.7856 | Function Loss:  -2.1933\n",
      "Total loss:  -1.0944 | PDE Loss:  -1.7856 | Function Loss:  -2.1933\n",
      "Total loss:  -1.0945 | PDE Loss:  -1.7856 | Function Loss:  -2.1934\n",
      "Total loss:  -1.0944 | PDE Loss:  -1.7849 | Function Loss:  -2.1934\n",
      "Total loss:  -1.0945 | PDE Loss:  -1.7855 | Function Loss:  -2.1934\n",
      "Total loss:  -1.0945 | PDE Loss:  -1.7855 | Function Loss:  -2.1934\n",
      "Total loss:  -1.0945 | PDE Loss:  -1.7855 | Function Loss:  -2.1935\n",
      "Total loss:  -1.0946 | PDE Loss:  -1.7855 | Function Loss:  -2.1935\n",
      "Total loss:  -1.0946 | PDE Loss:  -1.7854 | Function Loss:  -2.1935\n",
      "Total loss:  -1.0946 | PDE Loss:  -1.7854 | Function Loss:  -2.1936\n",
      "Total loss:  -1.0946 | PDE Loss:  -1.7854 | Function Loss:  -2.1936\n",
      "Total loss:  -1.0946 | PDE Loss:  -1.7854 | Function Loss:  -2.1936\n",
      "Total loss:  -1.0946 | PDE Loss:  -1.7853 | Function Loss:  -2.1936\n",
      "Total loss:  -1.0946 | PDE Loss:  -1.7853 | Function Loss:  -2.1936\n",
      "Total loss:  -1.0947 | PDE Loss:  -1.7852 | Function Loss:  -2.1937\n",
      "Total loss:  -1.0947 | PDE Loss:  -1.7853 | Function Loss:  -2.1937\n",
      "Total loss:  -1.0947 | PDE Loss:  -1.7854 | Function Loss:  -2.1937\n",
      "Total loss:  -1.0947 | PDE Loss:  -1.7855 | Function Loss:  -2.1937\n",
      "Total loss:  -1.0948 | PDE Loss:  -1.7857 | Function Loss:  -2.1937\n",
      "Total loss:  -1.0948 | PDE Loss:  -1.7858 | Function Loss:  -2.1937\n",
      "Total loss:  -1.0948 | PDE Loss:  -1.7861 | Function Loss:  -2.1937\n",
      "Total loss:  -1.0948 | PDE Loss:  -1.7862 | Function Loss:  -2.1937\n",
      "Total loss:  -1.0949 | PDE Loss:  -1.7862 | Function Loss:  -2.1937\n",
      "Total loss:  -1.0949 | PDE Loss:  -1.7863 | Function Loss:  -2.1937\n",
      "Total loss:  -1.095 | PDE Loss:  -1.7864 | Function Loss:  -2.1938\n",
      "Total loss:  -1.095 | PDE Loss:  -1.7866 | Function Loss:  -2.1938\n",
      "Total loss:  -1.0951 | PDE Loss:  -1.787 | Function Loss:  -2.1938\n",
      "Total loss:  -1.0949 | PDE Loss:  -1.787 | Function Loss:  -2.1935\n",
      "Total loss:  -1.0951 | PDE Loss:  -1.7872 | Function Loss:  -2.1937\n",
      "Total loss:  -1.0952 | PDE Loss:  -1.7875 | Function Loss:  -2.1937\n",
      "Total loss:  -1.0952 | PDE Loss:  -1.788 | Function Loss:  -2.1937\n",
      "Total loss:  -1.0952 | PDE Loss:  -1.7884 | Function Loss:  -2.1936\n",
      "Total loss:  -1.0953 | PDE Loss:  -1.7887 | Function Loss:  -2.1936\n",
      "Total loss:  -1.0953 | PDE Loss:  -1.789 | Function Loss:  -2.1936\n",
      "Total loss:  -1.0953 | PDE Loss:  -1.7892 | Function Loss:  -2.1935\n",
      "Total loss:  -1.0954 | PDE Loss:  -1.7893 | Function Loss:  -2.1935\n",
      "Total loss:  -1.0954 | PDE Loss:  -1.7894 | Function Loss:  -2.1936\n",
      "Total loss:  -1.0955 | PDE Loss:  -1.7893 | Function Loss:  -2.1937\n",
      "Total loss:  -1.0955 | PDE Loss:  -1.7892 | Function Loss:  -2.1938\n",
      "Total loss:  -1.0956 | PDE Loss:  -1.7891 | Function Loss:  -2.1938\n",
      "Total loss:  -1.0956 | PDE Loss:  -1.7887 | Function Loss:  -2.194\n",
      "Total loss:  -1.0956 | PDE Loss:  -1.7887 | Function Loss:  -2.194\n",
      "Total loss:  -1.0957 | PDE Loss:  -1.7887 | Function Loss:  -2.1941\n",
      "Total loss:  -1.0957 | PDE Loss:  -1.7887 | Function Loss:  -2.1941\n",
      "Total loss:  -1.0957 | PDE Loss:  -1.7887 | Function Loss:  -2.1941\n",
      "Total loss:  -1.0957 | PDE Loss:  -1.7888 | Function Loss:  -2.1941\n",
      "Total loss:  -1.0957 | PDE Loss:  -1.7889 | Function Loss:  -2.1941\n",
      "Total loss:  -1.0957 | PDE Loss:  -1.7889 | Function Loss:  -2.1941\n",
      "Total loss:  -1.0958 | PDE Loss:  -1.7889 | Function Loss:  -2.1941\n",
      "Total loss:  -1.0958 | PDE Loss:  -1.789 | Function Loss:  -2.1942\n",
      "Total loss:  -1.0958 | PDE Loss:  -1.7889 | Function Loss:  -2.1942\n",
      "Total loss:  -1.0958 | PDE Loss:  -1.7889 | Function Loss:  -2.1942\n",
      "Total loss:  -1.0959 | PDE Loss:  -1.7889 | Function Loss:  -2.1943\n",
      "Total loss:  -1.0959 | PDE Loss:  -1.7888 | Function Loss:  -2.1943\n",
      "Total loss:  -1.0959 | PDE Loss:  -1.7888 | Function Loss:  -2.1944\n",
      "Total loss:  -1.0959 | PDE Loss:  -1.7887 | Function Loss:  -2.1944\n",
      "Total loss:  -1.096 | PDE Loss:  -1.7887 | Function Loss:  -2.1945\n",
      "Total loss:  -1.096 | PDE Loss:  -1.7886 | Function Loss:  -2.1946\n",
      "Total loss:  -1.0961 | PDE Loss:  -1.7886 | Function Loss:  -2.1946\n",
      "Total loss:  -1.0961 | PDE Loss:  -1.7886 | Function Loss:  -2.1946\n",
      "Total loss:  -1.0961 | PDE Loss:  -1.7886 | Function Loss:  -2.1947\n",
      "Total loss:  -1.0962 | PDE Loss:  -1.7886 | Function Loss:  -2.1948\n",
      "Total loss:  -1.0962 | PDE Loss:  -1.7886 | Function Loss:  -2.1948\n",
      "Total loss:  -1.0963 | PDE Loss:  -1.7884 | Function Loss:  -2.1949\n",
      "Total loss:  -1.0963 | PDE Loss:  -1.7884 | Function Loss:  -2.195\n",
      "Total loss:  -1.0964 | PDE Loss:  -1.7881 | Function Loss:  -2.1951\n",
      "Total loss:  -1.0965 | PDE Loss:  -1.7879 | Function Loss:  -2.1953\n",
      "Total loss:  -1.0965 | PDE Loss:  -1.7875 | Function Loss:  -2.1955\n",
      "Total loss:  -1.0966 | PDE Loss:  -1.7872 | Function Loss:  -2.1956\n",
      "Total loss:  -1.0966 | PDE Loss:  -1.787 | Function Loss:  -2.1957\n",
      "Total loss:  -1.0967 | PDE Loss:  -1.7868 | Function Loss:  -2.1959\n",
      "Total loss:  -1.0968 | PDE Loss:  -1.7868 | Function Loss:  -2.1959\n",
      "Total loss:  -1.0968 | PDE Loss:  -1.7867 | Function Loss:  -2.196\n",
      "Total loss:  -1.0968 | PDE Loss:  -1.7869 | Function Loss:  -2.196\n",
      "Total loss:  -1.0969 | PDE Loss:  -1.787 | Function Loss:  -2.196\n",
      "Total loss:  -1.0969 | PDE Loss:  -1.7872 | Function Loss:  -2.196\n",
      "Total loss:  -1.0969 | PDE Loss:  -1.7874 | Function Loss:  -2.196\n",
      "Total loss:  -1.097 | PDE Loss:  -1.7876 | Function Loss:  -2.196\n",
      "Total loss:  -1.097 | PDE Loss:  -1.7877 | Function Loss:  -2.196\n",
      "Total loss:  -1.097 | PDE Loss:  -1.7878 | Function Loss:  -2.196\n",
      "Total loss:  -1.097 | PDE Loss:  -1.7879 | Function Loss:  -2.196\n",
      "Total loss:  -1.0971 | PDE Loss:  -1.7879 | Function Loss:  -2.1961\n",
      "Total loss:  -1.0971 | PDE Loss:  -1.7878 | Function Loss:  -2.1961\n",
      "Total loss:  -1.0972 | PDE Loss:  -1.7876 | Function Loss:  -2.1962\n",
      "Total loss:  -1.0972 | PDE Loss:  -1.7872 | Function Loss:  -2.1964\n",
      "Total loss:  -1.0973 | PDE Loss:  -1.7871 | Function Loss:  -2.1965\n",
      "Total loss:  -1.0973 | PDE Loss:  -1.7871 | Function Loss:  -2.1965\n",
      "Total loss:  -1.0973 | PDE Loss:  -1.7871 | Function Loss:  -2.1966\n",
      "Total loss:  -1.0973 | PDE Loss:  -1.7871 | Function Loss:  -2.1966\n",
      "Total loss:  -1.0974 | PDE Loss:  -1.7871 | Function Loss:  -2.1966\n",
      "Total loss:  -1.0974 | PDE Loss:  -1.7872 | Function Loss:  -2.1966\n",
      "Total loss:  -1.0974 | PDE Loss:  -1.7873 | Function Loss:  -2.1966\n",
      "Total loss:  -1.0974 | PDE Loss:  -1.7876 | Function Loss:  -2.1965\n",
      "Total loss:  -1.0974 | PDE Loss:  -1.7874 | Function Loss:  -2.1966\n",
      "Total loss:  -1.0974 | PDE Loss:  -1.7875 | Function Loss:  -2.1966\n",
      "Total loss:  -1.0974 | PDE Loss:  -1.7875 | Function Loss:  -2.1966\n",
      "Total loss:  -1.0975 | PDE Loss:  -1.7875 | Function Loss:  -2.1966\n",
      "Total loss:  -1.0975 | PDE Loss:  -1.7875 | Function Loss:  -2.1967\n",
      "Total loss:  -1.0975 | PDE Loss:  -1.7874 | Function Loss:  -2.1967\n",
      "Total loss:  -1.0975 | PDE Loss:  -1.7873 | Function Loss:  -2.1968\n",
      "Total loss:  -1.0975 | PDE Loss:  -1.7871 | Function Loss:  -2.1968\n",
      "Total loss:  -1.0976 | PDE Loss:  -1.7869 | Function Loss:  -2.1969\n",
      "Total loss:  -1.0976 | PDE Loss:  -1.7867 | Function Loss:  -2.197\n",
      "Total loss:  -1.0976 | PDE Loss:  -1.7863 | Function Loss:  -2.1971\n",
      "Total loss:  -1.0976 | PDE Loss:  -1.7862 | Function Loss:  -2.1972\n",
      "Total loss:  -1.0977 | PDE Loss:  -1.7862 | Function Loss:  -2.1972\n",
      "Total loss:  -1.0977 | PDE Loss:  -1.7863 | Function Loss:  -2.1972\n",
      "Total loss:  -1.0977 | PDE Loss:  -1.7863 | Function Loss:  -2.1972\n",
      "Total loss:  -1.0977 | PDE Loss:  -1.7866 | Function Loss:  -2.1971\n",
      "Total loss:  -1.0977 | PDE Loss:  -1.7864 | Function Loss:  -2.1972\n",
      "Total loss:  -1.0977 | PDE Loss:  -1.7865 | Function Loss:  -2.1972\n",
      "Total loss:  -1.0977 | PDE Loss:  -1.7865 | Function Loss:  -2.1972\n",
      "Total loss:  -1.0978 | PDE Loss:  -1.7866 | Function Loss:  -2.1972\n",
      "Total loss:  -1.0978 | PDE Loss:  -1.7866 | Function Loss:  -2.1973\n",
      "Total loss:  -1.0978 | PDE Loss:  -1.7867 | Function Loss:  -2.1973\n",
      "Total loss:  -1.0978 | PDE Loss:  -1.7867 | Function Loss:  -2.1973\n",
      "Total loss:  -1.0978 | PDE Loss:  -1.7869 | Function Loss:  -2.1973\n",
      "Total loss:  -1.0979 | PDE Loss:  -1.7869 | Function Loss:  -2.1973\n",
      "Total loss:  -1.0979 | PDE Loss:  -1.7869 | Function Loss:  -2.1973\n",
      "Total loss:  -1.098 | PDE Loss:  -1.7871 | Function Loss:  -2.1974\n",
      "Total loss:  -1.098 | PDE Loss:  -1.7872 | Function Loss:  -2.1974\n",
      "Total loss:  -1.098 | PDE Loss:  -1.7873 | Function Loss:  -2.1974\n",
      "Total loss:  -1.0981 | PDE Loss:  -1.7875 | Function Loss:  -2.1974\n",
      "Total loss:  -1.0981 | PDE Loss:  -1.7876 | Function Loss:  -2.1974\n",
      "Total loss:  -1.0981 | PDE Loss:  -1.788 | Function Loss:  -2.1974\n",
      "Total loss:  -1.0982 | PDE Loss:  -1.7881 | Function Loss:  -2.1974\n",
      "Total loss:  -1.0982 | PDE Loss:  -1.7885 | Function Loss:  -2.1973\n",
      "Total loss:  -1.0983 | PDE Loss:  -1.7888 | Function Loss:  -2.1973\n",
      "Total loss:  -1.0983 | PDE Loss:  -1.789 | Function Loss:  -2.1973\n",
      "Total loss:  -1.0983 | PDE Loss:  -1.7892 | Function Loss:  -2.1973\n",
      "Total loss:  -1.0984 | PDE Loss:  -1.7895 | Function Loss:  -2.1973\n",
      "Total loss:  -1.0984 | PDE Loss:  -1.7896 | Function Loss:  -2.1973\n",
      "Total loss:  -1.0984 | PDE Loss:  -1.7898 | Function Loss:  -2.1973\n",
      "Total loss:  -1.0985 | PDE Loss:  -1.79 | Function Loss:  -2.1973\n",
      "Total loss:  -1.0985 | PDE Loss:  -1.79 | Function Loss:  -2.1973\n",
      "Total loss:  -1.0986 | PDE Loss:  -1.79 | Function Loss:  -2.1974\n",
      "Total loss:  -1.0987 | PDE Loss:  -1.79 | Function Loss:  -2.1975\n",
      "Total loss:  -1.0987 | PDE Loss:  -1.7899 | Function Loss:  -2.1976\n",
      "Total loss:  -1.0988 | PDE Loss:  -1.7899 | Function Loss:  -2.1977\n",
      "Total loss:  -1.0988 | PDE Loss:  -1.7898 | Function Loss:  -2.1978\n",
      "Total loss:  -1.0989 | PDE Loss:  -1.7897 | Function Loss:  -2.1978\n",
      "Total loss:  -1.0989 | PDE Loss:  -1.7898 | Function Loss:  -2.1979\n",
      "Total loss:  -1.099 | PDE Loss:  -1.7898 | Function Loss:  -2.1979\n",
      "Total loss:  -1.099 | PDE Loss:  -1.7899 | Function Loss:  -2.198\n",
      "Total loss:  -1.0991 | PDE Loss:  -1.7901 | Function Loss:  -2.198\n",
      "Total loss:  -1.0991 | PDE Loss:  -1.7902 | Function Loss:  -2.198\n",
      "Total loss:  -1.0992 | PDE Loss:  -1.7906 | Function Loss:  -2.1981\n",
      "Total loss:  -1.0993 | PDE Loss:  -1.7909 | Function Loss:  -2.198\n",
      "Total loss:  -1.0993 | PDE Loss:  -1.791 | Function Loss:  -2.198\n",
      "Total loss:  -1.0993 | PDE Loss:  -1.7911 | Function Loss:  -2.198\n",
      "Total loss:  -1.0993 | PDE Loss:  -1.7912 | Function Loss:  -2.1981\n",
      "Total loss:  -1.0992 | PDE Loss:  -1.7905 | Function Loss:  -2.198\n",
      "Total loss:  -1.0994 | PDE Loss:  -1.7912 | Function Loss:  -2.1981\n",
      "Total loss:  -1.0994 | PDE Loss:  -1.7913 | Function Loss:  -2.1981\n",
      "Total loss:  -1.0994 | PDE Loss:  -1.7913 | Function Loss:  -2.1981\n",
      "Total loss:  -1.0995 | PDE Loss:  -1.7913 | Function Loss:  -2.1981\n",
      "Total loss:  -1.0995 | PDE Loss:  -1.7913 | Function Loss:  -2.1982\n",
      "Total loss:  -1.0995 | PDE Loss:  -1.7913 | Function Loss:  -2.1982\n",
      "Total loss:  -1.0995 | PDE Loss:  -1.7912 | Function Loss:  -2.1982\n",
      "Total loss:  -1.0995 | PDE Loss:  -1.7911 | Function Loss:  -2.1983\n",
      "Total loss:  -1.0995 | PDE Loss:  -1.7911 | Function Loss:  -2.1983\n",
      "Total loss:  -1.0996 | PDE Loss:  -1.7912 | Function Loss:  -2.1983\n",
      "Total loss:  -1.0996 | PDE Loss:  -1.7913 | Function Loss:  -2.1983\n",
      "Total loss:  -1.0994 | PDE Loss:  -1.7909 | Function Loss:  -2.1982\n",
      "Total loss:  -1.0996 | PDE Loss:  -1.7913 | Function Loss:  -2.1983\n",
      "Total loss:  -1.0996 | PDE Loss:  -1.7915 | Function Loss:  -2.1983\n",
      "Total loss:  -1.0996 | PDE Loss:  -1.7917 | Function Loss:  -2.1983\n",
      "Total loss:  -1.0996 | PDE Loss:  -1.7919 | Function Loss:  -2.1982\n",
      "Total loss:  -1.0997 | PDE Loss:  -1.7922 | Function Loss:  -2.1982\n",
      "Total loss:  -1.0997 | PDE Loss:  -1.7925 | Function Loss:  -2.1982\n",
      "Total loss:  -1.0997 | PDE Loss:  -1.7929 | Function Loss:  -2.1981\n",
      "Total loss:  -1.0998 | PDE Loss:  -1.7932 | Function Loss:  -2.198\n",
      "Total loss:  -1.0998 | PDE Loss:  -1.7936 | Function Loss:  -2.198\n",
      "Total loss:  -1.0998 | PDE Loss:  -1.7938 | Function Loss:  -2.198\n",
      "Total loss:  -1.0998 | PDE Loss:  -1.794 | Function Loss:  -2.198\n",
      "Total loss:  -1.0999 | PDE Loss:  -1.7941 | Function Loss:  -2.198\n",
      "Total loss:  -1.0999 | PDE Loss:  -1.7943 | Function Loss:  -2.1979\n",
      "Total loss:  -1.0999 | PDE Loss:  -1.7942 | Function Loss:  -2.198\n",
      "Total loss:  -1.0999 | PDE Loss:  -1.7943 | Function Loss:  -2.198\n",
      "Total loss:  -1.0999 | PDE Loss:  -1.7944 | Function Loss:  -2.198\n",
      "Total loss:  -1.1 | PDE Loss:  -1.7946 | Function Loss:  -2.198\n",
      "Total loss:  -1.1 | PDE Loss:  -1.7949 | Function Loss:  -2.198\n",
      "Total loss:  -1.1001 | PDE Loss:  -1.7954 | Function Loss:  -2.1979\n",
      "Total loss:  -1.1001 | PDE Loss:  -1.7959 | Function Loss:  -2.1979\n",
      "Total loss:  -1.1002 | PDE Loss:  -1.7965 | Function Loss:  -2.1978\n",
      "Total loss:  -1.1002 | PDE Loss:  -1.7967 | Function Loss:  -2.1977\n",
      "Total loss:  -1.1003 | PDE Loss:  -1.7971 | Function Loss:  -2.1977\n",
      "Total loss:  -1.1003 | PDE Loss:  -1.7973 | Function Loss:  -2.1977\n",
      "Total loss:  -1.1003 | PDE Loss:  -1.7975 | Function Loss:  -2.1977\n",
      "Total loss:  -1.1004 | PDE Loss:  -1.7976 | Function Loss:  -2.1977\n",
      "Total loss:  -1.1004 | PDE Loss:  -1.7977 | Function Loss:  -2.1977\n",
      "Total loss:  -1.1005 | PDE Loss:  -1.7984 | Function Loss:  -2.1976\n",
      "Total loss:  -1.1006 | PDE Loss:  -1.7984 | Function Loss:  -2.1978\n",
      "Total loss:  -1.1006 | PDE Loss:  -1.7984 | Function Loss:  -2.1978\n",
      "Total loss:  -1.1007 | PDE Loss:  -1.7984 | Function Loss:  -2.198\n",
      "Total loss:  -1.1008 | PDE Loss:  -1.7986 | Function Loss:  -2.1981\n",
      "Total loss:  -1.1009 | PDE Loss:  -1.7987 | Function Loss:  -2.1981\n",
      "Total loss:  -1.101 | PDE Loss:  -1.7989 | Function Loss:  -2.1982\n",
      "Total loss:  -1.101 | PDE Loss:  -1.799 | Function Loss:  -2.1982\n",
      "Total loss:  -1.1011 | PDE Loss:  -1.7991 | Function Loss:  -2.1982\n",
      "Total loss:  -1.1011 | PDE Loss:  -1.7993 | Function Loss:  -2.1983\n",
      "Total loss:  -1.1012 | PDE Loss:  -1.7994 | Function Loss:  -2.1983\n",
      "Total loss:  -1.1012 | PDE Loss:  -1.7995 | Function Loss:  -2.1983\n",
      "Total loss:  -1.1012 | PDE Loss:  -1.7996 | Function Loss:  -2.1983\n",
      "Total loss:  -1.1012 | PDE Loss:  -1.7996 | Function Loss:  -2.1983\n",
      "Total loss:  -1.1013 | PDE Loss:  -1.7997 | Function Loss:  -2.1983\n",
      "Total loss:  -1.1013 | PDE Loss:  -1.7996 | Function Loss:  -2.1984\n",
      "Total loss:  -1.1014 | PDE Loss:  -1.7996 | Function Loss:  -2.1985\n",
      "Total loss:  -1.1014 | PDE Loss:  -1.7996 | Function Loss:  -2.1985\n",
      "Total loss:  -1.1015 | PDE Loss:  -1.7994 | Function Loss:  -2.1986\n",
      "Total loss:  -1.1015 | PDE Loss:  -1.7995 | Function Loss:  -2.1987\n",
      "Total loss:  -1.1016 | PDE Loss:  -1.7995 | Function Loss:  -2.1987\n",
      "Total loss:  -1.1016 | PDE Loss:  -1.7996 | Function Loss:  -2.1988\n",
      "Total loss:  -1.1017 | PDE Loss:  -1.7998 | Function Loss:  -2.1988\n",
      "Total loss:  -1.1017 | PDE Loss:  -1.8 | Function Loss:  -2.1988\n",
      "Total loss:  -1.1018 | PDE Loss:  -1.8001 | Function Loss:  -2.1988\n",
      "Total loss:  -1.1018 | PDE Loss:  -1.8003 | Function Loss:  -2.1988\n",
      "Total loss:  -1.1018 | PDE Loss:  -1.8003 | Function Loss:  -2.1988\n",
      "Total loss:  -1.1019 | PDE Loss:  -1.8004 | Function Loss:  -2.1989\n",
      "Total loss:  -1.1019 | PDE Loss:  -1.8005 | Function Loss:  -2.1989\n",
      "Total loss:  -1.102 | PDE Loss:  -1.8006 | Function Loss:  -2.1989\n",
      "Total loss:  -1.102 | PDE Loss:  -1.8006 | Function Loss:  -2.199\n",
      "Total loss:  -1.102 | PDE Loss:  -1.801 | Function Loss:  -2.1989\n",
      "Total loss:  -1.1021 | PDE Loss:  -1.8009 | Function Loss:  -2.199\n",
      "Total loss:  -1.1021 | PDE Loss:  -1.8009 | Function Loss:  -2.1991\n",
      "Total loss:  -1.1022 | PDE Loss:  -1.8009 | Function Loss:  -2.1991\n",
      "Total loss:  -1.1022 | PDE Loss:  -1.801 | Function Loss:  -2.1991\n",
      "Total loss:  -1.1022 | PDE Loss:  -1.8012 | Function Loss:  -2.1991\n",
      "Total loss:  -1.1023 | PDE Loss:  -1.8015 | Function Loss:  -2.1991\n",
      "Total loss:  -1.1023 | PDE Loss:  -1.8017 | Function Loss:  -2.1991\n",
      "Total loss:  -1.1023 | PDE Loss:  -1.8021 | Function Loss:  -2.199\n",
      "Total loss:  -1.1024 | PDE Loss:  -1.8023 | Function Loss:  -2.199\n",
      "Total loss:  -1.1024 | PDE Loss:  -1.8024 | Function Loss:  -2.199\n",
      "Total loss:  -1.1024 | PDE Loss:  -1.8025 | Function Loss:  -2.1991\n",
      "Total loss:  -1.1025 | PDE Loss:  -1.8025 | Function Loss:  -2.1991\n",
      "Total loss:  -1.1025 | PDE Loss:  -1.8025 | Function Loss:  -2.1991\n",
      "Total loss:  -1.1025 | PDE Loss:  -1.8024 | Function Loss:  -2.1992\n",
      "Total loss:  -1.1026 | PDE Loss:  -1.8023 | Function Loss:  -2.1993\n",
      "Total loss:  -1.1027 | PDE Loss:  -1.8023 | Function Loss:  -2.1994\n",
      "Total loss:  -1.1028 | PDE Loss:  -1.8018 | Function Loss:  -2.1996\n",
      "Total loss:  -1.1027 | PDE Loss:  -1.8014 | Function Loss:  -2.1997\n",
      "Total loss:  -1.1028 | PDE Loss:  -1.8018 | Function Loss:  -2.1997\n",
      "Total loss:  -1.1028 | PDE Loss:  -1.8018 | Function Loss:  -2.1997\n",
      "Total loss:  -1.1029 | PDE Loss:  -1.802 | Function Loss:  -2.1998\n",
      "Total loss:  -1.103 | PDE Loss:  -1.8021 | Function Loss:  -2.1999\n",
      "Total loss:  -1.103 | PDE Loss:  -1.8023 | Function Loss:  -2.1999\n",
      "Total loss:  -1.1031 | PDE Loss:  -1.8025 | Function Loss:  -2.1999\n",
      "Total loss:  -1.1032 | PDE Loss:  -1.8028 | Function Loss:  -2.1999\n",
      "Total loss:  -1.1033 | PDE Loss:  -1.8031 | Function Loss:  -2.2\n",
      "Total loss:  -1.1034 | PDE Loss:  -1.8038 | Function Loss:  -2.2\n",
      "Total loss:  -1.1035 | PDE Loss:  -1.8039 | Function Loss:  -2.2\n",
      "Total loss:  -1.1035 | PDE Loss:  -1.8042 | Function Loss:  -2.2\n",
      "Total loss:  -1.1036 | PDE Loss:  -1.8043 | Function Loss:  -2.2001\n",
      "Total loss:  -1.1036 | PDE Loss:  -1.8047 | Function Loss:  -2.2\n",
      "Total loss:  -1.1037 | PDE Loss:  -1.8047 | Function Loss:  -2.2001\n",
      "Total loss:  -1.1037 | PDE Loss:  -1.8049 | Function Loss:  -2.2001\n",
      "Total loss:  -1.1038 | PDE Loss:  -1.8052 | Function Loss:  -2.2001\n",
      "Total loss:  -1.1039 | PDE Loss:  -1.8055 | Function Loss:  -2.2001\n",
      "Total loss:  -1.1039 | PDE Loss:  -1.806 | Function Loss:  -2.2001\n",
      "Total loss:  -1.104 | PDE Loss:  -1.8062 | Function Loss:  -2.2001\n",
      "Total loss:  -1.104 | PDE Loss:  -1.8064 | Function Loss:  -2.2001\n",
      "Total loss:  -1.1041 | PDE Loss:  -1.8066 | Function Loss:  -2.2001\n",
      "Total loss:  -1.1042 | PDE Loss:  -1.8066 | Function Loss:  -2.2002\n",
      "Total loss:  -1.1043 | PDE Loss:  -1.8066 | Function Loss:  -2.2003\n",
      "Total loss:  -1.1044 | PDE Loss:  -1.8066 | Function Loss:  -2.2004\n",
      "Total loss:  -1.1044 | PDE Loss:  -1.8065 | Function Loss:  -2.2005\n",
      "Total loss:  -1.1044 | PDE Loss:  -1.8064 | Function Loss:  -2.2006\n",
      "Total loss:  -1.1045 | PDE Loss:  -1.8063 | Function Loss:  -2.2007\n",
      "Total loss:  -1.1045 | PDE Loss:  -1.8064 | Function Loss:  -2.2007\n",
      "Total loss:  -1.1046 | PDE Loss:  -1.8065 | Function Loss:  -2.2007\n",
      "Total loss:  -1.1046 | PDE Loss:  -1.8068 | Function Loss:  -2.2007\n",
      "Total loss:  -1.1047 | PDE Loss:  -1.8074 | Function Loss:  -2.2006\n",
      "Total loss:  -1.1047 | PDE Loss:  -1.8075 | Function Loss:  -2.2007\n",
      "Total loss:  -1.1048 | PDE Loss:  -1.8078 | Function Loss:  -2.2007\n",
      "Total loss:  -1.1048 | PDE Loss:  -1.808 | Function Loss:  -2.2007\n",
      "Total loss:  -1.1049 | PDE Loss:  -1.8082 | Function Loss:  -2.2007\n",
      "Total loss:  -1.1049 | PDE Loss:  -1.8083 | Function Loss:  -2.2007\n",
      "Total loss:  -1.1049 | PDE Loss:  -1.8083 | Function Loss:  -2.2008\n",
      "Total loss:  -1.105 | PDE Loss:  -1.8082 | Function Loss:  -2.2009\n",
      "Total loss:  -1.1051 | PDE Loss:  -1.8078 | Function Loss:  -2.201\n",
      "Total loss:  -1.1051 | PDE Loss:  -1.8076 | Function Loss:  -2.2011\n",
      "Total loss:  -1.1051 | PDE Loss:  -1.8077 | Function Loss:  -2.2012\n",
      "Total loss:  -1.1052 | PDE Loss:  -1.8075 | Function Loss:  -2.2013\n",
      "Total loss:  -1.1052 | PDE Loss:  -1.8075 | Function Loss:  -2.2013\n",
      "Total loss:  -1.1053 | PDE Loss:  -1.8074 | Function Loss:  -2.2014\n",
      "Total loss:  -1.1053 | PDE Loss:  -1.8075 | Function Loss:  -2.2014\n",
      "Total loss:  -1.1054 | PDE Loss:  -1.8074 | Function Loss:  -2.2015\n",
      "Total loss:  -1.1055 | PDE Loss:  -1.8078 | Function Loss:  -2.2016\n",
      "Total loss:  -1.1055 | PDE Loss:  -1.8079 | Function Loss:  -2.2016\n",
      "Total loss:  -1.1056 | PDE Loss:  -1.8083 | Function Loss:  -2.2016\n",
      "Total loss:  -1.1056 | PDE Loss:  -1.8086 | Function Loss:  -2.2015\n",
      "Total loss:  -1.1056 | PDE Loss:  -1.8088 | Function Loss:  -2.2015\n",
      "Total loss:  -1.1057 | PDE Loss:  -1.8091 | Function Loss:  -2.2015\n",
      "Total loss:  -1.1057 | PDE Loss:  -1.8093 | Function Loss:  -2.2014\n",
      "Total loss:  -1.1057 | PDE Loss:  -1.8096 | Function Loss:  -2.2014\n",
      "Total loss:  -1.1058 | PDE Loss:  -1.8099 | Function Loss:  -2.2014\n",
      "Total loss:  -1.1058 | PDE Loss:  -1.8102 | Function Loss:  -2.2014\n",
      "Total loss:  -1.1059 | PDE Loss:  -1.8105 | Function Loss:  -2.2014\n",
      "Total loss:  -1.106 | PDE Loss:  -1.8105 | Function Loss:  -2.2015\n",
      "Total loss:  -1.106 | PDE Loss:  -1.8105 | Function Loss:  -2.2016\n",
      "Total loss:  -1.1061 | PDE Loss:  -1.8103 | Function Loss:  -2.2017\n",
      "Total loss:  -1.1061 | PDE Loss:  -1.8102 | Function Loss:  -2.2018\n",
      "Total loss:  -1.1062 | PDE Loss:  -1.81 | Function Loss:  -2.2019\n",
      "Total loss:  -1.1062 | PDE Loss:  -1.8101 | Function Loss:  -2.2019\n",
      "Total loss:  -1.1063 | PDE Loss:  -1.8099 | Function Loss:  -2.2021\n",
      "Total loss:  -1.1063 | PDE Loss:  -1.8102 | Function Loss:  -2.2021\n",
      "Total loss:  -1.1064 | PDE Loss:  -1.8102 | Function Loss:  -2.2021\n",
      "Total loss:  -1.1065 | PDE Loss:  -1.8105 | Function Loss:  -2.2021\n",
      "Total loss:  -1.1065 | PDE Loss:  -1.8108 | Function Loss:  -2.2021\n",
      "Total loss:  -1.1066 | PDE Loss:  -1.8112 | Function Loss:  -2.202\n",
      "Total loss:  -1.1066 | PDE Loss:  -1.8116 | Function Loss:  -2.202\n",
      "Total loss:  -1.1066 | PDE Loss:  -1.8119 | Function Loss:  -2.202\n",
      "Total loss:  -1.1067 | PDE Loss:  -1.8123 | Function Loss:  -2.202\n",
      "Total loss:  -1.1067 | PDE Loss:  -1.8127 | Function Loss:  -2.2019\n",
      "Total loss:  -1.1068 | PDE Loss:  -1.8127 | Function Loss:  -2.2019\n",
      "Total loss:  -1.1068 | PDE Loss:  -1.8127 | Function Loss:  -2.202\n",
      "Total loss:  -1.1068 | PDE Loss:  -1.8125 | Function Loss:  -2.2021\n",
      "Total loss:  -1.1069 | PDE Loss:  -1.8123 | Function Loss:  -2.2022\n",
      "Total loss:  -1.1069 | PDE Loss:  -1.8121 | Function Loss:  -2.2022\n",
      "Total loss:  -1.1069 | PDE Loss:  -1.812 | Function Loss:  -2.2023\n",
      "Total loss:  -1.1069 | PDE Loss:  -1.8119 | Function Loss:  -2.2024\n",
      "Total loss:  -1.107 | PDE Loss:  -1.8118 | Function Loss:  -2.2024\n",
      "Total loss:  -1.107 | PDE Loss:  -1.8119 | Function Loss:  -2.2024\n",
      "Total loss:  -1.107 | PDE Loss:  -1.812 | Function Loss:  -2.2025\n",
      "Total loss:  -1.1071 | PDE Loss:  -1.8119 | Function Loss:  -2.2025\n",
      "Total loss:  -1.1071 | PDE Loss:  -1.8122 | Function Loss:  -2.2025\n",
      "Total loss:  -1.1072 | PDE Loss:  -1.8125 | Function Loss:  -2.2025\n",
      "Total loss:  -1.1072 | PDE Loss:  -1.8127 | Function Loss:  -2.2025\n",
      "Total loss:  -1.1073 | PDE Loss:  -1.8129 | Function Loss:  -2.2026\n",
      "Total loss:  -1.1074 | PDE Loss:  -1.8131 | Function Loss:  -2.2026\n",
      "Total loss:  -1.1072 | PDE Loss:  -1.8112 | Function Loss:  -2.2029\n",
      "Total loss:  -1.1074 | PDE Loss:  -1.8128 | Function Loss:  -2.2027\n",
      "Total loss:  -1.1074 | PDE Loss:  -1.8129 | Function Loss:  -2.2027\n",
      "Total loss:  -1.1075 | PDE Loss:  -1.813 | Function Loss:  -2.2028\n",
      "Total loss:  -1.1076 | PDE Loss:  -1.813 | Function Loss:  -2.2029\n",
      "Total loss:  -1.1077 | PDE Loss:  -1.8129 | Function Loss:  -2.203\n",
      "Total loss:  -1.1077 | PDE Loss:  -1.8129 | Function Loss:  -2.2031\n",
      "Total loss:  -1.1078 | PDE Loss:  -1.8127 | Function Loss:  -2.2032\n",
      "Total loss:  -1.1079 | PDE Loss:  -1.8124 | Function Loss:  -2.2034\n",
      "Total loss:  -1.1079 | PDE Loss:  -1.8123 | Function Loss:  -2.2035\n",
      "Total loss:  -1.108 | PDE Loss:  -1.8124 | Function Loss:  -2.2035\n",
      "Total loss:  -1.1081 | PDE Loss:  -1.8125 | Function Loss:  -2.2036\n",
      "Total loss:  -1.1082 | PDE Loss:  -1.8125 | Function Loss:  -2.2038\n",
      "Total loss:  -1.1083 | PDE Loss:  -1.8123 | Function Loss:  -2.2039\n",
      "Total loss:  -1.1084 | PDE Loss:  -1.8126 | Function Loss:  -2.204\n",
      "Total loss:  -1.1084 | PDE Loss:  -1.8127 | Function Loss:  -2.204\n",
      "Total loss:  -1.1085 | PDE Loss:  -1.813 | Function Loss:  -2.204\n",
      "Total loss:  -1.1086 | PDE Loss:  -1.8131 | Function Loss:  -2.2041\n",
      "Total loss:  -1.1086 | PDE Loss:  -1.8133 | Function Loss:  -2.2041\n",
      "Total loss:  -1.1087 | PDE Loss:  -1.8136 | Function Loss:  -2.2042\n",
      "Total loss:  -1.1088 | PDE Loss:  -1.8142 | Function Loss:  -2.2042\n",
      "Total loss:  -1.109 | PDE Loss:  -1.8147 | Function Loss:  -2.2042\n",
      "Total loss:  -1.1091 | PDE Loss:  -1.8154 | Function Loss:  -2.2042\n",
      "Total loss:  -1.1092 | PDE Loss:  -1.8163 | Function Loss:  -2.2041\n",
      "Total loss:  -1.1093 | PDE Loss:  -1.8173 | Function Loss:  -2.2039\n",
      "Total loss:  -1.1093 | PDE Loss:  -1.8176 | Function Loss:  -2.2039\n",
      "Total loss:  -1.1094 | PDE Loss:  -1.8181 | Function Loss:  -2.2039\n",
      "Total loss:  -1.1095 | PDE Loss:  -1.8185 | Function Loss:  -2.2039\n",
      "Total loss:  -1.1095 | PDE Loss:  -1.8189 | Function Loss:  -2.2039\n",
      "Total loss:  -1.1096 | PDE Loss:  -1.8191 | Function Loss:  -2.2039\n",
      "Total loss:  -1.1096 | PDE Loss:  -1.8192 | Function Loss:  -2.2039\n",
      "Total loss:  -1.1097 | PDE Loss:  -1.8192 | Function Loss:  -2.204\n",
      "Total loss:  -1.1097 | PDE Loss:  -1.8193 | Function Loss:  -2.2041\n",
      "Total loss:  -1.1098 | PDE Loss:  -1.8192 | Function Loss:  -2.2041\n",
      "Total loss:  -1.1098 | PDE Loss:  -1.8193 | Function Loss:  -2.2041\n",
      "Total loss:  -1.1099 | PDE Loss:  -1.8193 | Function Loss:  -2.2042\n",
      "Total loss:  -1.11 | PDE Loss:  -1.8196 | Function Loss:  -2.2042\n",
      "Total loss:  -1.11 | PDE Loss:  -1.8196 | Function Loss:  -2.2043\n",
      "Total loss:  -1.1101 | PDE Loss:  -1.8201 | Function Loss:  -2.2043\n",
      "Total loss:  -1.1101 | PDE Loss:  -1.8202 | Function Loss:  -2.2043\n",
      "Total loss:  -1.1102 | PDE Loss:  -1.8203 | Function Loss:  -2.2044\n",
      "Total loss:  -1.1102 | PDE Loss:  -1.8205 | Function Loss:  -2.2044\n",
      "Total loss:  -1.1103 | PDE Loss:  -1.8206 | Function Loss:  -2.2044\n",
      "Total loss:  -1.1103 | PDE Loss:  -1.8208 | Function Loss:  -2.2044\n",
      "Total loss:  -1.1103 | PDE Loss:  -1.8209 | Function Loss:  -2.2044\n",
      "Total loss:  -1.1104 | PDE Loss:  -1.8212 | Function Loss:  -2.2044\n",
      "Total loss:  -1.1103 | PDE Loss:  -1.8213 | Function Loss:  -2.2043\n",
      "Total loss:  -1.1104 | PDE Loss:  -1.8214 | Function Loss:  -2.2044\n",
      "Total loss:  -1.1105 | PDE Loss:  -1.8217 | Function Loss:  -2.2044\n",
      "Total loss:  -1.1105 | PDE Loss:  -1.822 | Function Loss:  -2.2044\n",
      "Total loss:  -1.1106 | PDE Loss:  -1.8223 | Function Loss:  -2.2044\n",
      "Total loss:  -1.1106 | PDE Loss:  -1.8225 | Function Loss:  -2.2044\n",
      "Total loss:  -1.1107 | PDE Loss:  -1.8227 | Function Loss:  -2.2044\n",
      "Total loss:  -1.1107 | PDE Loss:  -1.8227 | Function Loss:  -2.2044\n",
      "Total loss:  -1.1107 | PDE Loss:  -1.8228 | Function Loss:  -2.2044\n",
      "Total loss:  -1.1108 | PDE Loss:  -1.8228 | Function Loss:  -2.2045\n",
      "Total loss:  -1.1108 | PDE Loss:  -1.8228 | Function Loss:  -2.2045\n",
      "Total loss:  -1.1108 | PDE Loss:  -1.8228 | Function Loss:  -2.2045\n",
      "Total loss:  -1.1108 | PDE Loss:  -1.8228 | Function Loss:  -2.2046\n",
      "Total loss:  -1.1109 | PDE Loss:  -1.8228 | Function Loss:  -2.2046\n",
      "Total loss:  -1.1109 | PDE Loss:  -1.8229 | Function Loss:  -2.2046\n",
      "Total loss:  -1.1109 | PDE Loss:  -1.823 | Function Loss:  -2.2046\n",
      "Total loss:  -1.1109 | PDE Loss:  -1.823 | Function Loss:  -2.2046\n",
      "Total loss:  -1.1109 | PDE Loss:  -1.8231 | Function Loss:  -2.2046\n",
      "Total loss:  -1.111 | PDE Loss:  -1.8231 | Function Loss:  -2.2046\n",
      "Total loss:  -1.111 | PDE Loss:  -1.8233 | Function Loss:  -2.2047\n",
      "Total loss:  -1.111 | PDE Loss:  -1.8234 | Function Loss:  -2.2047\n",
      "Total loss:  -1.1111 | PDE Loss:  -1.8235 | Function Loss:  -2.2047\n",
      "Total loss:  -1.1111 | PDE Loss:  -1.8236 | Function Loss:  -2.2047\n",
      "Total loss:  -1.1111 | PDE Loss:  -1.8237 | Function Loss:  -2.2047\n",
      "Total loss:  -1.1112 | PDE Loss:  -1.8238 | Function Loss:  -2.2048\n",
      "Total loss:  -1.1112 | PDE Loss:  -1.8238 | Function Loss:  -2.2048\n",
      "Total loss:  -1.1112 | PDE Loss:  -1.8238 | Function Loss:  -2.2048\n",
      "Total loss:  -1.1113 | PDE Loss:  -1.8238 | Function Loss:  -2.2049\n",
      "Total loss:  -1.1113 | PDE Loss:  -1.8238 | Function Loss:  -2.2049\n",
      "Total loss:  -1.1113 | PDE Loss:  -1.8238 | Function Loss:  -2.2049\n",
      "Total loss:  -1.1113 | PDE Loss:  -1.8238 | Function Loss:  -2.2049\n",
      "Total loss:  -1.1114 | PDE Loss:  -1.8239 | Function Loss:  -2.205\n",
      "Total loss:  -1.1112 | PDE Loss:  -1.8229 | Function Loss:  -2.205\n",
      "Total loss:  -1.1114 | PDE Loss:  -1.8237 | Function Loss:  -2.205\n",
      "Total loss:  -1.1114 | PDE Loss:  -1.8239 | Function Loss:  -2.205\n",
      "Total loss:  -1.1114 | PDE Loss:  -1.824 | Function Loss:  -2.205\n",
      "Total loss:  -1.1115 | PDE Loss:  -1.8241 | Function Loss:  -2.205\n",
      "Total loss:  -1.1115 | PDE Loss:  -1.8242 | Function Loss:  -2.205\n",
      "Total loss:  -1.1115 | PDE Loss:  -1.8243 | Function Loss:  -2.205\n",
      "Total loss:  -1.1115 | PDE Loss:  -1.8243 | Function Loss:  -2.205\n",
      "Total loss:  -1.1115 | PDE Loss:  -1.8244 | Function Loss:  -2.205\n",
      "Total loss:  -1.1116 | PDE Loss:  -1.8244 | Function Loss:  -2.2051\n",
      "Total loss:  -1.1116 | PDE Loss:  -1.8245 | Function Loss:  -2.2051\n",
      "Total loss:  -1.1116 | PDE Loss:  -1.8243 | Function Loss:  -2.2052\n",
      "Total loss:  -1.1117 | PDE Loss:  -1.8244 | Function Loss:  -2.2052\n",
      "Total loss:  -1.1117 | PDE Loss:  -1.8245 | Function Loss:  -2.2053\n",
      "Total loss:  -1.1118 | PDE Loss:  -1.8246 | Function Loss:  -2.2053\n",
      "Total loss:  -1.1118 | PDE Loss:  -1.8247 | Function Loss:  -2.2053\n",
      "Total loss:  -1.1118 | PDE Loss:  -1.8247 | Function Loss:  -2.2054\n",
      "Total loss:  -1.1119 | PDE Loss:  -1.8248 | Function Loss:  -2.2054\n",
      "Total loss:  -1.1117 | PDE Loss:  -1.8239 | Function Loss:  -2.2054\n",
      "Total loss:  -1.1119 | PDE Loss:  -1.8247 | Function Loss:  -2.2054\n",
      "Total loss:  -1.1119 | PDE Loss:  -1.8247 | Function Loss:  -2.2055\n",
      "Total loss:  -1.112 | PDE Loss:  -1.8247 | Function Loss:  -2.2055\n",
      "Total loss:  -1.112 | PDE Loss:  -1.8246 | Function Loss:  -2.2056\n",
      "Total loss:  -1.1121 | PDE Loss:  -1.8245 | Function Loss:  -2.2057\n",
      "Total loss:  -1.1121 | PDE Loss:  -1.8244 | Function Loss:  -2.2057\n",
      "Total loss:  -1.1121 | PDE Loss:  -1.8241 | Function Loss:  -2.2059\n",
      "Total loss:  -1.1122 | PDE Loss:  -1.8237 | Function Loss:  -2.206\n",
      "Total loss:  -1.1122 | PDE Loss:  -1.8237 | Function Loss:  -2.206\n",
      "Total loss:  -1.1122 | PDE Loss:  -1.8238 | Function Loss:  -2.2061\n",
      "Total loss:  -1.1123 | PDE Loss:  -1.8239 | Function Loss:  -2.2061\n",
      "Total loss:  -1.1123 | PDE Loss:  -1.8239 | Function Loss:  -2.2061\n",
      "Total loss:  -1.1123 | PDE Loss:  -1.8239 | Function Loss:  -2.2061\n",
      "Total loss:  -1.1123 | PDE Loss:  -1.8238 | Function Loss:  -2.2062\n",
      "Total loss:  -1.1124 | PDE Loss:  -1.8238 | Function Loss:  -2.2062\n",
      "Total loss:  -1.1124 | PDE Loss:  -1.8237 | Function Loss:  -2.2063\n",
      "Total loss:  -1.1124 | PDE Loss:  -1.8236 | Function Loss:  -2.2063\n",
      "Total loss:  -1.1125 | PDE Loss:  -1.8235 | Function Loss:  -2.2064\n",
      "Total loss:  -1.1125 | PDE Loss:  -1.8235 | Function Loss:  -2.2064\n",
      "Total loss:  -1.1125 | PDE Loss:  -1.8233 | Function Loss:  -2.2065\n",
      "Total loss:  -1.1125 | PDE Loss:  -1.8233 | Function Loss:  -2.2065\n",
      "Total loss:  -1.1125 | PDE Loss:  -1.8234 | Function Loss:  -2.2065\n",
      "Total loss:  -1.1126 | PDE Loss:  -1.8234 | Function Loss:  -2.2066\n",
      "Total loss:  -1.1126 | PDE Loss:  -1.8234 | Function Loss:  -2.2066\n",
      "Total loss:  -1.1127 | PDE Loss:  -1.8234 | Function Loss:  -2.2067\n",
      "Total loss:  -1.1127 | PDE Loss:  -1.8233 | Function Loss:  -2.2067\n",
      "Total loss:  -1.1127 | PDE Loss:  -1.8232 | Function Loss:  -2.2068\n",
      "Total loss:  -1.1128 | PDE Loss:  -1.8232 | Function Loss:  -2.2069\n",
      "Total loss:  -1.1129 | PDE Loss:  -1.8232 | Function Loss:  -2.207\n",
      "Total loss:  -1.1129 | PDE Loss:  -1.8231 | Function Loss:  -2.2071\n",
      "Total loss:  -1.113 | PDE Loss:  -1.8231 | Function Loss:  -2.2071\n",
      "Total loss:  -1.1131 | PDE Loss:  -1.8233 | Function Loss:  -2.2072\n",
      "Total loss:  -1.1131 | PDE Loss:  -1.8235 | Function Loss:  -2.2072\n",
      "Total loss:  -1.1132 | PDE Loss:  -1.8237 | Function Loss:  -2.2073\n",
      "Total loss:  -1.1133 | PDE Loss:  -1.824 | Function Loss:  -2.2073\n",
      "Total loss:  -1.1133 | PDE Loss:  -1.825 | Function Loss:  -2.2071\n",
      "Total loss:  -1.1134 | PDE Loss:  -1.825 | Function Loss:  -2.2072\n",
      "Total loss:  -1.1134 | PDE Loss:  -1.825 | Function Loss:  -2.2072\n",
      "Total loss:  -1.1135 | PDE Loss:  -1.8249 | Function Loss:  -2.2073\n",
      "Total loss:  -1.1135 | PDE Loss:  -1.8249 | Function Loss:  -2.2074\n",
      "Total loss:  -1.1135 | PDE Loss:  -1.8249 | Function Loss:  -2.2074\n",
      "Total loss:  -1.1136 | PDE Loss:  -1.8248 | Function Loss:  -2.2075\n",
      "Total loss:  -1.1136 | PDE Loss:  -1.8245 | Function Loss:  -2.2076\n",
      "Total loss:  -1.1136 | PDE Loss:  -1.8247 | Function Loss:  -2.2076\n",
      "Total loss:  -1.1136 | PDE Loss:  -1.8247 | Function Loss:  -2.2076\n",
      "Total loss:  -1.1137 | PDE Loss:  -1.8248 | Function Loss:  -2.2076\n",
      "Total loss:  -1.1137 | PDE Loss:  -1.8249 | Function Loss:  -2.2076\n",
      "Total loss:  -1.1137 | PDE Loss:  -1.825 | Function Loss:  -2.2076\n",
      "Total loss:  -1.1138 | PDE Loss:  -1.8252 | Function Loss:  -2.2076\n",
      "Total loss:  -1.1138 | PDE Loss:  -1.8251 | Function Loss:  -2.2076\n",
      "Total loss:  -1.1138 | PDE Loss:  -1.8252 | Function Loss:  -2.2077\n",
      "Total loss:  -1.1138 | PDE Loss:  -1.8254 | Function Loss:  -2.2077\n",
      "Total loss:  -1.1139 | PDE Loss:  -1.8256 | Function Loss:  -2.2077\n",
      "Total loss:  -1.1139 | PDE Loss:  -1.8259 | Function Loss:  -2.2077\n",
      "Total loss:  -1.114 | PDE Loss:  -1.8263 | Function Loss:  -2.2076\n",
      "Total loss:  -1.114 | PDE Loss:  -1.8265 | Function Loss:  -2.2076\n",
      "Total loss:  -1.1141 | PDE Loss:  -1.8269 | Function Loss:  -2.2076\n",
      "Total loss:  -1.1141 | PDE Loss:  -1.827 | Function Loss:  -2.2076\n",
      "Total loss:  -1.1142 | PDE Loss:  -1.8273 | Function Loss:  -2.2076\n",
      "Total loss:  -1.1142 | PDE Loss:  -1.8275 | Function Loss:  -2.2077\n",
      "Total loss:  -1.1143 | PDE Loss:  -1.8278 | Function Loss:  -2.2077\n",
      "Total loss:  -1.1142 | PDE Loss:  -1.8286 | Function Loss:  -2.2073\n",
      "Total loss:  -1.1143 | PDE Loss:  -1.8281 | Function Loss:  -2.2076\n",
      "Total loss:  -1.1144 | PDE Loss:  -1.8282 | Function Loss:  -2.2076\n",
      "Total loss:  -1.1144 | PDE Loss:  -1.8284 | Function Loss:  -2.2076\n",
      "Total loss:  -1.1145 | PDE Loss:  -1.8287 | Function Loss:  -2.2076\n",
      "Total loss:  -1.1145 | PDE Loss:  -1.8289 | Function Loss:  -2.2076\n",
      "Total loss:  -1.1146 | PDE Loss:  -1.8292 | Function Loss:  -2.2076\n",
      "Total loss:  -1.1146 | PDE Loss:  -1.8296 | Function Loss:  -2.2076\n",
      "Total loss:  -1.1123 | PDE Loss:  -1.8238 | Function Loss:  -2.2062\n",
      "Total loss:  -1.1146 | PDE Loss:  -1.8296 | Function Loss:  -2.2076\n",
      "Total loss:  -1.1146 | PDE Loss:  -1.8299 | Function Loss:  -2.2076\n",
      "Total loss:  -1.1147 | PDE Loss:  -1.8302 | Function Loss:  -2.2076\n",
      "Total loss:  -1.1148 | PDE Loss:  -1.831 | Function Loss:  -2.2076\n",
      "Total loss:  -1.1149 | PDE Loss:  -1.8313 | Function Loss:  -2.2076\n",
      "Total loss:  -1.115 | PDE Loss:  -1.8317 | Function Loss:  -2.2076\n",
      "Total loss:  -1.1151 | PDE Loss:  -1.8316 | Function Loss:  -2.2077\n",
      "Total loss:  -1.1152 | PDE Loss:  -1.8317 | Function Loss:  -2.2078\n",
      "Total loss:  -1.1152 | PDE Loss:  -1.8318 | Function Loss:  -2.2078\n",
      "Total loss:  -1.1152 | PDE Loss:  -1.8317 | Function Loss:  -2.2079\n",
      "Total loss:  -1.1153 | PDE Loss:  -1.8317 | Function Loss:  -2.2079\n",
      "Total loss:  -1.1153 | PDE Loss:  -1.8317 | Function Loss:  -2.2079\n",
      "Total loss:  -1.1153 | PDE Loss:  -1.8318 | Function Loss:  -2.2079\n",
      "Total loss:  -1.1153 | PDE Loss:  -1.832 | Function Loss:  -2.2079\n",
      "Total loss:  -1.1154 | PDE Loss:  -1.8323 | Function Loss:  -2.2079\n",
      "Total loss:  -1.1154 | PDE Loss:  -1.8327 | Function Loss:  -2.2079\n",
      "Total loss:  -1.1154 | PDE Loss:  -1.8336 | Function Loss:  -2.2076\n",
      "Total loss:  -1.1154 | PDE Loss:  -1.8331 | Function Loss:  -2.2078\n",
      "Total loss:  -1.1155 | PDE Loss:  -1.8335 | Function Loss:  -2.2077\n",
      "Total loss:  -1.1155 | PDE Loss:  -1.834 | Function Loss:  -2.2077\n",
      "Total loss:  -1.1156 | PDE Loss:  -1.8345 | Function Loss:  -2.2076\n",
      "Total loss:  -1.1156 | PDE Loss:  -1.8349 | Function Loss:  -2.2076\n",
      "Total loss:  -1.1156 | PDE Loss:  -1.8352 | Function Loss:  -2.2076\n",
      "Total loss:  -1.1157 | PDE Loss:  -1.8354 | Function Loss:  -2.2075\n",
      "Total loss:  -1.1157 | PDE Loss:  -1.8356 | Function Loss:  -2.2075\n",
      "Total loss:  -1.1157 | PDE Loss:  -1.8357 | Function Loss:  -2.2075\n",
      "Total loss:  -1.1158 | PDE Loss:  -1.8359 | Function Loss:  -2.2076\n",
      "Total loss:  -1.1158 | PDE Loss:  -1.8359 | Function Loss:  -2.2076\n",
      "Total loss:  -1.1158 | PDE Loss:  -1.8359 | Function Loss:  -2.2076\n",
      "Total loss:  -1.1159 | PDE Loss:  -1.836 | Function Loss:  -2.2076\n",
      "Total loss:  -1.1159 | PDE Loss:  -1.836 | Function Loss:  -2.2076\n",
      "Total loss:  -1.1159 | PDE Loss:  -1.8361 | Function Loss:  -2.2076\n",
      "Total loss:  -1.1159 | PDE Loss:  -1.8362 | Function Loss:  -2.2076\n",
      "Total loss:  -1.1159 | PDE Loss:  -1.8363 | Function Loss:  -2.2076\n",
      "Total loss:  -1.1159 | PDE Loss:  -1.8362 | Function Loss:  -2.2077\n",
      "Total loss:  -1.1159 | PDE Loss:  -1.8363 | Function Loss:  -2.2077\n",
      "Total loss:  -1.116 | PDE Loss:  -1.8365 | Function Loss:  -2.2076\n",
      "Total loss:  -1.116 | PDE Loss:  -1.8367 | Function Loss:  -2.2076\n",
      "Total loss:  -1.116 | PDE Loss:  -1.8369 | Function Loss:  -2.2076\n",
      "Total loss:  -1.116 | PDE Loss:  -1.8371 | Function Loss:  -2.2076\n",
      "Total loss:  -1.1161 | PDE Loss:  -1.8372 | Function Loss:  -2.2076\n",
      "Total loss:  -1.1161 | PDE Loss:  -1.8372 | Function Loss:  -2.2077\n",
      "Total loss:  -1.1161 | PDE Loss:  -1.8371 | Function Loss:  -2.2077\n",
      "Total loss:  -1.1162 | PDE Loss:  -1.8371 | Function Loss:  -2.2077\n",
      "Total loss:  -1.1161 | PDE Loss:  -1.8368 | Function Loss:  -2.2078\n",
      "Total loss:  -1.1162 | PDE Loss:  -1.837 | Function Loss:  -2.2078\n",
      "Total loss:  -1.1162 | PDE Loss:  -1.8369 | Function Loss:  -2.2079\n",
      "Total loss:  -1.1163 | PDE Loss:  -1.8368 | Function Loss:  -2.208\n",
      "Total loss:  -1.1163 | PDE Loss:  -1.8366 | Function Loss:  -2.2081\n",
      "Total loss:  -1.1164 | PDE Loss:  -1.8365 | Function Loss:  -2.2082\n",
      "Total loss:  -1.1164 | PDE Loss:  -1.8365 | Function Loss:  -2.2082\n",
      "Total loss:  -1.1165 | PDE Loss:  -1.8365 | Function Loss:  -2.2083\n",
      "Total loss:  -1.1165 | PDE Loss:  -1.8366 | Function Loss:  -2.2083\n",
      "Total loss:  -1.1166 | PDE Loss:  -1.8367 | Function Loss:  -2.2084\n",
      "Total loss:  -1.1166 | PDE Loss:  -1.8368 | Function Loss:  -2.2084\n",
      "Total loss:  -1.1167 | PDE Loss:  -1.8373 | Function Loss:  -2.2084\n",
      "Total loss:  -1.1168 | PDE Loss:  -1.8379 | Function Loss:  -2.2084\n",
      "Total loss:  -1.1169 | PDE Loss:  -1.8382 | Function Loss:  -2.2084\n",
      "Total loss:  -1.117 | PDE Loss:  -1.8383 | Function Loss:  -2.2085\n",
      "Total loss:  -1.1171 | PDE Loss:  -1.8385 | Function Loss:  -2.2085\n",
      "Total loss:  -1.1171 | PDE Loss:  -1.8386 | Function Loss:  -2.2086\n",
      "Total loss:  -1.1172 | PDE Loss:  -1.8385 | Function Loss:  -2.2087\n",
      "Total loss:  -1.1173 | PDE Loss:  -1.8387 | Function Loss:  -2.2088\n",
      "Total loss:  -1.1173 | PDE Loss:  -1.8386 | Function Loss:  -2.2089\n",
      "Total loss:  -1.1174 | PDE Loss:  -1.8387 | Function Loss:  -2.2089\n",
      "Total loss:  -1.1175 | PDE Loss:  -1.8386 | Function Loss:  -2.209\n",
      "Total loss:  -1.1176 | PDE Loss:  -1.8387 | Function Loss:  -2.2091\n",
      "Total loss:  -1.1176 | PDE Loss:  -1.8389 | Function Loss:  -2.2092\n",
      "Total loss:  -1.1178 | PDE Loss:  -1.839 | Function Loss:  -2.2093\n",
      "Total loss:  -1.1179 | PDE Loss:  -1.8392 | Function Loss:  -2.2094\n",
      "Total loss:  -1.1179 | PDE Loss:  -1.8388 | Function Loss:  -2.2095\n",
      "Total loss:  -1.118 | PDE Loss:  -1.8389 | Function Loss:  -2.2096\n",
      "Total loss:  -1.1181 | PDE Loss:  -1.8391 | Function Loss:  -2.2096\n",
      "Total loss:  -1.1181 | PDE Loss:  -1.8391 | Function Loss:  -2.2097\n",
      "Total loss:  -1.1182 | PDE Loss:  -1.8392 | Function Loss:  -2.2098\n",
      "Total loss:  -1.1182 | PDE Loss:  -1.8387 | Function Loss:  -2.2099\n",
      "Total loss:  -1.1183 | PDE Loss:  -1.8388 | Function Loss:  -2.21\n",
      "Total loss:  -1.1184 | PDE Loss:  -1.839 | Function Loss:  -2.2101\n",
      "Total loss:  -1.1185 | PDE Loss:  -1.8392 | Function Loss:  -2.2102\n",
      "Total loss:  -1.1186 | PDE Loss:  -1.8393 | Function Loss:  -2.2102\n",
      "Total loss:  -1.1187 | PDE Loss:  -1.8394 | Function Loss:  -2.2103\n",
      "Total loss:  -1.1188 | PDE Loss:  -1.8395 | Function Loss:  -2.2104\n",
      "Total loss:  -1.1179 | PDE Loss:  -1.8353 | Function Loss:  -2.2104\n",
      "Total loss:  -1.1188 | PDE Loss:  -1.8393 | Function Loss:  -2.2105\n",
      "Total loss:  -1.1189 | PDE Loss:  -1.8393 | Function Loss:  -2.2106\n",
      "Total loss:  -1.119 | PDE Loss:  -1.8393 | Function Loss:  -2.2107\n",
      "Total loss:  -1.119 | PDE Loss:  -1.8393 | Function Loss:  -2.2108\n",
      "Total loss:  -1.1191 | PDE Loss:  -1.8393 | Function Loss:  -2.2108\n",
      "Total loss:  -1.1191 | PDE Loss:  -1.8393 | Function Loss:  -2.2109\n",
      "Total loss:  -1.1191 | PDE Loss:  -1.8393 | Function Loss:  -2.2109\n",
      "Total loss:  -1.1192 | PDE Loss:  -1.8393 | Function Loss:  -2.2109\n",
      "Total loss:  -1.1192 | PDE Loss:  -1.8394 | Function Loss:  -2.211\n",
      "Total loss:  -1.1193 | PDE Loss:  -1.8394 | Function Loss:  -2.211\n",
      "Total loss:  -1.1193 | PDE Loss:  -1.8395 | Function Loss:  -2.2111\n",
      "Total loss:  -1.1194 | PDE Loss:  -1.8398 | Function Loss:  -2.2111\n",
      "Total loss:  -1.1194 | PDE Loss:  -1.8399 | Function Loss:  -2.2112\n",
      "Total loss:  -1.1195 | PDE Loss:  -1.84 | Function Loss:  -2.2112\n",
      "Total loss:  -1.1196 | PDE Loss:  -1.8401 | Function Loss:  -2.2113\n",
      "Total loss:  -1.1196 | PDE Loss:  -1.8402 | Function Loss:  -2.2113\n",
      "Total loss:  -1.1197 | PDE Loss:  -1.8401 | Function Loss:  -2.2114\n",
      "Total loss:  -1.1197 | PDE Loss:  -1.8406 | Function Loss:  -2.2113\n",
      "Total loss:  -1.1198 | PDE Loss:  -1.8404 | Function Loss:  -2.2115\n",
      "Total loss:  -1.1199 | PDE Loss:  -1.8402 | Function Loss:  -2.2116\n",
      "Total loss:  -1.1199 | PDE Loss:  -1.8401 | Function Loss:  -2.2117\n",
      "Total loss:  -1.12 | PDE Loss:  -1.84 | Function Loss:  -2.2118\n",
      "Total loss:  -1.12 | PDE Loss:  -1.84 | Function Loss:  -2.2118\n",
      "Total loss:  -1.1201 | PDE Loss:  -1.84 | Function Loss:  -2.2119\n",
      "Total loss:  -1.1201 | PDE Loss:  -1.84 | Function Loss:  -2.2119\n",
      "Total loss:  -1.1202 | PDE Loss:  -1.84 | Function Loss:  -2.212\n",
      "Total loss:  -1.1202 | PDE Loss:  -1.8398 | Function Loss:  -2.2121\n",
      "Total loss:  -1.1203 | PDE Loss:  -1.8399 | Function Loss:  -2.2122\n",
      "Total loss:  -1.1203 | PDE Loss:  -1.84 | Function Loss:  -2.2122\n",
      "Total loss:  -1.1204 | PDE Loss:  -1.8402 | Function Loss:  -2.2123\n",
      "Total loss:  -1.1204 | PDE Loss:  -1.8404 | Function Loss:  -2.2123\n",
      "Total loss:  -1.1205 | PDE Loss:  -1.8405 | Function Loss:  -2.2123\n",
      "Total loss:  -1.1205 | PDE Loss:  -1.8407 | Function Loss:  -2.2123\n",
      "Total loss:  -1.1205 | PDE Loss:  -1.8409 | Function Loss:  -2.2123\n",
      "Total loss:  -1.1206 | PDE Loss:  -1.8413 | Function Loss:  -2.2122\n",
      "Total loss:  -1.1206 | PDE Loss:  -1.8417 | Function Loss:  -2.2122\n",
      "Total loss:  -1.1207 | PDE Loss:  -1.8422 | Function Loss:  -2.2121\n",
      "Total loss:  -1.1207 | PDE Loss:  -1.8428 | Function Loss:  -2.212\n",
      "Total loss:  -1.1207 | PDE Loss:  -1.8431 | Function Loss:  -2.212\n",
      "Total loss:  -1.1208 | PDE Loss:  -1.8435 | Function Loss:  -2.212\n",
      "Total loss:  -1.1208 | PDE Loss:  -1.8437 | Function Loss:  -2.2119\n",
      "Total loss:  -1.1208 | PDE Loss:  -1.8439 | Function Loss:  -2.2119\n",
      "Total loss:  -1.1209 | PDE Loss:  -1.8441 | Function Loss:  -2.2119\n",
      "Total loss:  -1.1209 | PDE Loss:  -1.8442 | Function Loss:  -2.212\n",
      "Total loss:  -1.121 | PDE Loss:  -1.8446 | Function Loss:  -2.2119\n",
      "Total loss:  -1.1211 | PDE Loss:  -1.8446 | Function Loss:  -2.212\n",
      "Total loss:  -1.1211 | PDE Loss:  -1.8446 | Function Loss:  -2.2121\n",
      "Total loss:  -1.1212 | PDE Loss:  -1.8445 | Function Loss:  -2.2122\n",
      "Total loss:  -1.1212 | PDE Loss:  -1.8444 | Function Loss:  -2.2122\n",
      "Total loss:  -1.1212 | PDE Loss:  -1.8443 | Function Loss:  -2.2123\n",
      "Total loss:  -1.1212 | PDE Loss:  -1.8443 | Function Loss:  -2.2123\n",
      "Total loss:  -1.1213 | PDE Loss:  -1.8442 | Function Loss:  -2.2124\n",
      "Total loss:  -1.1213 | PDE Loss:  -1.8439 | Function Loss:  -2.2125\n",
      "Total loss:  -1.1213 | PDE Loss:  -1.8439 | Function Loss:  -2.2126\n",
      "Total loss:  -1.1214 | PDE Loss:  -1.8437 | Function Loss:  -2.2126\n",
      "Total loss:  -1.1214 | PDE Loss:  -1.8439 | Function Loss:  -2.2127\n",
      "Total loss:  -1.1215 | PDE Loss:  -1.844 | Function Loss:  -2.2127\n",
      "Total loss:  -1.1215 | PDE Loss:  -1.8441 | Function Loss:  -2.2127\n",
      "Total loss:  -1.1215 | PDE Loss:  -1.8442 | Function Loss:  -2.2127\n",
      "Total loss:  -1.1216 | PDE Loss:  -1.8444 | Function Loss:  -2.2127\n",
      "Total loss:  -1.1216 | PDE Loss:  -1.8444 | Function Loss:  -2.2127\n",
      "Total loss:  -1.1216 | PDE Loss:  -1.8447 | Function Loss:  -2.2127\n",
      "Total loss:  -1.1216 | PDE Loss:  -1.8447 | Function Loss:  -2.2127\n",
      "Total loss:  -1.1217 | PDE Loss:  -1.8446 | Function Loss:  -2.2128\n",
      "Total loss:  -1.1217 | PDE Loss:  -1.8445 | Function Loss:  -2.2129\n",
      "Total loss:  -1.1217 | PDE Loss:  -1.8445 | Function Loss:  -2.2129\n",
      "Total loss:  -1.1217 | PDE Loss:  -1.8444 | Function Loss:  -2.2129\n",
      "Total loss:  -1.1218 | PDE Loss:  -1.8444 | Function Loss:  -2.2129\n",
      "Total loss:  -1.1218 | PDE Loss:  -1.8446 | Function Loss:  -2.2129\n",
      "Total loss:  -1.1218 | PDE Loss:  -1.8447 | Function Loss:  -2.2129\n",
      "Total loss:  -1.1218 | PDE Loss:  -1.8449 | Function Loss:  -2.2129\n",
      "Total loss:  -1.1219 | PDE Loss:  -1.845 | Function Loss:  -2.2129\n",
      "Total loss:  -1.1219 | PDE Loss:  -1.8451 | Function Loss:  -2.2129\n",
      "Total loss:  -1.1219 | PDE Loss:  -1.8449 | Function Loss:  -2.213\n",
      "Total loss:  -1.1219 | PDE Loss:  -1.845 | Function Loss:  -2.213\n",
      "Total loss:  -1.1219 | PDE Loss:  -1.8449 | Function Loss:  -2.213\n",
      "Total loss:  -1.122 | PDE Loss:  -1.8448 | Function Loss:  -2.2131\n",
      "Total loss:  -1.122 | PDE Loss:  -1.8448 | Function Loss:  -2.2131\n",
      "Total loss:  -1.122 | PDE Loss:  -1.8447 | Function Loss:  -2.2132\n",
      "Total loss:  -1.1221 | PDE Loss:  -1.8446 | Function Loss:  -2.2133\n",
      "Total loss:  -1.1221 | PDE Loss:  -1.8446 | Function Loss:  -2.2134\n",
      "Total loss:  -1.1222 | PDE Loss:  -1.8447 | Function Loss:  -2.2134\n",
      "Total loss:  -1.1223 | PDE Loss:  -1.8448 | Function Loss:  -2.2135\n",
      "Total loss:  -1.1223 | PDE Loss:  -1.8451 | Function Loss:  -2.2135\n",
      "Total loss:  -1.1224 | PDE Loss:  -1.8455 | Function Loss:  -2.2135\n",
      "Total loss:  -1.1224 | PDE Loss:  -1.846 | Function Loss:  -2.2134\n",
      "Total loss:  -1.1225 | PDE Loss:  -1.8464 | Function Loss:  -2.2134\n",
      "Total loss:  -1.1225 | PDE Loss:  -1.8469 | Function Loss:  -2.2133\n",
      "Total loss:  -1.1226 | PDE Loss:  -1.8474 | Function Loss:  -2.2133\n",
      "Total loss:  -1.1226 | PDE Loss:  -1.8475 | Function Loss:  -2.2133\n",
      "Total loss:  -1.1226 | PDE Loss:  -1.8475 | Function Loss:  -2.2133\n",
      "Total loss:  -1.1227 | PDE Loss:  -1.8474 | Function Loss:  -2.2134\n",
      "Total loss:  -1.1227 | PDE Loss:  -1.8474 | Function Loss:  -2.2134\n",
      "Total loss:  -1.1228 | PDE Loss:  -1.8474 | Function Loss:  -2.2135\n",
      "Total loss:  -1.1228 | PDE Loss:  -1.8474 | Function Loss:  -2.2136\n",
      "Total loss:  -1.1229 | PDE Loss:  -1.8475 | Function Loss:  -2.2136\n",
      "Total loss:  -1.1229 | PDE Loss:  -1.8474 | Function Loss:  -2.2137\n",
      "Total loss:  -1.123 | PDE Loss:  -1.8477 | Function Loss:  -2.2137\n",
      "Total loss:  -1.1231 | PDE Loss:  -1.848 | Function Loss:  -2.2137\n",
      "Total loss:  -1.1231 | PDE Loss:  -1.8484 | Function Loss:  -2.2137\n",
      "Total loss:  -1.1232 | PDE Loss:  -1.8487 | Function Loss:  -2.2137\n",
      "Total loss:  -1.1232 | PDE Loss:  -1.8489 | Function Loss:  -2.2137\n",
      "Total loss:  -1.1232 | PDE Loss:  -1.8492 | Function Loss:  -2.2136\n",
      "Total loss:  -1.1233 | PDE Loss:  -1.8494 | Function Loss:  -2.2137\n",
      "Total loss:  -1.1233 | PDE Loss:  -1.8498 | Function Loss:  -2.2136\n",
      "Total loss:  -1.1233 | PDE Loss:  -1.8495 | Function Loss:  -2.2136\n",
      "Total loss:  -1.1234 | PDE Loss:  -1.8499 | Function Loss:  -2.2137\n",
      "Total loss:  -1.1234 | PDE Loss:  -1.8499 | Function Loss:  -2.2137\n",
      "Total loss:  -1.1235 | PDE Loss:  -1.85 | Function Loss:  -2.2138\n",
      "Total loss:  -1.1236 | PDE Loss:  -1.8502 | Function Loss:  -2.2139\n",
      "Total loss:  -1.1237 | PDE Loss:  -1.85 | Function Loss:  -2.2141\n",
      "Total loss:  -1.1238 | PDE Loss:  -1.8499 | Function Loss:  -2.2141\n",
      "Total loss:  -1.1238 | PDE Loss:  -1.8497 | Function Loss:  -2.2143\n",
      "Total loss:  -1.1239 | PDE Loss:  -1.8495 | Function Loss:  -2.2144\n",
      "Total loss:  -1.1239 | PDE Loss:  -1.8496 | Function Loss:  -2.2144\n",
      "Total loss:  -1.124 | PDE Loss:  -1.8499 | Function Loss:  -2.2144\n",
      "Total loss:  -1.1241 | PDE Loss:  -1.8502 | Function Loss:  -2.2145\n",
      "Total loss:  -1.1241 | PDE Loss:  -1.8504 | Function Loss:  -2.2145\n",
      "Total loss:  -1.1242 | PDE Loss:  -1.8506 | Function Loss:  -2.2145\n",
      "Total loss:  -1.1242 | PDE Loss:  -1.8507 | Function Loss:  -2.2145\n",
      "Total loss:  -1.1243 | PDE Loss:  -1.8509 | Function Loss:  -2.2145\n",
      "Total loss:  -1.1243 | PDE Loss:  -1.8511 | Function Loss:  -2.2146\n",
      "Total loss:  -1.1244 | PDE Loss:  -1.851 | Function Loss:  -2.2146\n",
      "Total loss:  -1.1244 | PDE Loss:  -1.8512 | Function Loss:  -2.2147\n",
      "Total loss:  -1.1245 | PDE Loss:  -1.8512 | Function Loss:  -2.2147\n",
      "Total loss:  -1.1245 | PDE Loss:  -1.8512 | Function Loss:  -2.2147\n",
      "Total loss:  -1.1245 | PDE Loss:  -1.8513 | Function Loss:  -2.2147\n",
      "Total loss:  -1.1245 | PDE Loss:  -1.8513 | Function Loss:  -2.2147\n",
      "Total loss:  -1.1245 | PDE Loss:  -1.8514 | Function Loss:  -2.2147\n",
      "Total loss:  -1.1246 | PDE Loss:  -1.8515 | Function Loss:  -2.2148\n",
      "Total loss:  -1.1246 | PDE Loss:  -1.8517 | Function Loss:  -2.2147\n",
      "Total loss:  -1.1246 | PDE Loss:  -1.8519 | Function Loss:  -2.2148\n",
      "Total loss:  -1.1247 | PDE Loss:  -1.852 | Function Loss:  -2.2148\n",
      "Total loss:  -1.1247 | PDE Loss:  -1.8522 | Function Loss:  -2.2148\n",
      "Total loss:  -1.1247 | PDE Loss:  -1.8523 | Function Loss:  -2.2148\n",
      "Total loss:  -1.1248 | PDE Loss:  -1.8523 | Function Loss:  -2.2148\n",
      "Total loss:  -1.1248 | PDE Loss:  -1.8523 | Function Loss:  -2.2149\n",
      "Total loss:  -1.1249 | PDE Loss:  -1.8522 | Function Loss:  -2.215\n",
      "Total loss:  -1.1249 | PDE Loss:  -1.8521 | Function Loss:  -2.215\n",
      "Total loss:  -1.1249 | PDE Loss:  -1.8521 | Function Loss:  -2.2151\n",
      "Total loss:  -1.125 | PDE Loss:  -1.8522 | Function Loss:  -2.2151\n",
      "Total loss:  -1.125 | PDE Loss:  -1.852 | Function Loss:  -2.2152\n",
      "Total loss:  -1.1249 | PDE Loss:  -1.8524 | Function Loss:  -2.215\n",
      "Total loss:  -1.1251 | PDE Loss:  -1.8522 | Function Loss:  -2.2152\n",
      "Total loss:  -1.1251 | PDE Loss:  -1.8522 | Function Loss:  -2.2153\n",
      "Total loss:  -1.1251 | PDE Loss:  -1.8522 | Function Loss:  -2.2153\n",
      "Total loss:  -1.1252 | PDE Loss:  -1.8525 | Function Loss:  -2.2153\n",
      "Total loss:  -1.1253 | PDE Loss:  -1.8525 | Function Loss:  -2.2154\n",
      "Total loss:  -1.1253 | PDE Loss:  -1.8525 | Function Loss:  -2.2154\n",
      "Total loss:  -1.1254 | PDE Loss:  -1.8526 | Function Loss:  -2.2155\n",
      "Total loss:  -1.1254 | PDE Loss:  -1.8525 | Function Loss:  -2.2156\n",
      "Total loss:  -1.1254 | PDE Loss:  -1.8526 | Function Loss:  -2.2156\n",
      "Total loss:  -1.1255 | PDE Loss:  -1.8526 | Function Loss:  -2.2156\n",
      "Total loss:  -1.1255 | PDE Loss:  -1.8529 | Function Loss:  -2.2156\n",
      "Total loss:  -1.1256 | PDE Loss:  -1.8528 | Function Loss:  -2.2157\n",
      "Total loss:  -1.1256 | PDE Loss:  -1.8527 | Function Loss:  -2.2158\n",
      "Total loss:  -1.1257 | PDE Loss:  -1.8527 | Function Loss:  -2.2158\n",
      "Total loss:  -1.1257 | PDE Loss:  -1.8528 | Function Loss:  -2.2159\n",
      "Total loss:  -1.1257 | PDE Loss:  -1.8529 | Function Loss:  -2.2159\n",
      "Total loss:  -1.1258 | PDE Loss:  -1.8531 | Function Loss:  -2.2159\n",
      "Total loss:  -1.1258 | PDE Loss:  -1.8533 | Function Loss:  -2.2159\n",
      "Total loss:  -1.1259 | PDE Loss:  -1.8535 | Function Loss:  -2.2159\n",
      "Total loss:  -1.1259 | PDE Loss:  -1.8538 | Function Loss:  -2.2159\n",
      "Total loss:  -1.126 | PDE Loss:  -1.8539 | Function Loss:  -2.216\n",
      "Total loss:  -1.1261 | PDE Loss:  -1.854 | Function Loss:  -2.216\n",
      "Total loss:  -1.1261 | PDE Loss:  -1.8541 | Function Loss:  -2.2161\n",
      "Total loss:  -1.1262 | PDE Loss:  -1.8541 | Function Loss:  -2.2161\n",
      "Total loss:  -1.1262 | PDE Loss:  -1.8539 | Function Loss:  -2.2162\n",
      "Total loss:  -1.1263 | PDE Loss:  -1.8539 | Function Loss:  -2.2163\n",
      "Total loss:  -1.1263 | PDE Loss:  -1.8538 | Function Loss:  -2.2164\n",
      "Total loss:  -1.1264 | PDE Loss:  -1.8537 | Function Loss:  -2.2165\n",
      "Total loss:  -1.1265 | PDE Loss:  -1.8537 | Function Loss:  -2.2166\n",
      "Total loss:  -1.1265 | PDE Loss:  -1.8537 | Function Loss:  -2.2167\n",
      "Total loss:  -1.1266 | PDE Loss:  -1.8538 | Function Loss:  -2.2167\n",
      "Total loss:  -1.1267 | PDE Loss:  -1.8538 | Function Loss:  -2.2168\n",
      "Total loss:  -1.1267 | PDE Loss:  -1.8541 | Function Loss:  -2.2168\n",
      "Total loss:  -1.1268 | PDE Loss:  -1.8546 | Function Loss:  -2.2169\n",
      "Total loss:  -1.1269 | PDE Loss:  -1.855 | Function Loss:  -2.2169\n",
      "Total loss:  -1.127 | PDE Loss:  -1.8554 | Function Loss:  -2.2169\n",
      "Total loss:  -1.1271 | PDE Loss:  -1.8556 | Function Loss:  -2.2169\n",
      "Total loss:  -1.1271 | PDE Loss:  -1.8557 | Function Loss:  -2.2169\n",
      "Total loss:  -1.1271 | PDE Loss:  -1.8558 | Function Loss:  -2.2169\n",
      "Total loss:  -1.1272 | PDE Loss:  -1.8558 | Function Loss:  -2.2169\n",
      "Total loss:  -1.1272 | PDE Loss:  -1.8559 | Function Loss:  -2.217\n",
      "Total loss:  -1.1272 | PDE Loss:  -1.8558 | Function Loss:  -2.217\n",
      "Total loss:  -1.1272 | PDE Loss:  -1.8558 | Function Loss:  -2.2171\n",
      "Total loss:  -1.1273 | PDE Loss:  -1.8558 | Function Loss:  -2.2171\n",
      "Total loss:  -1.1273 | PDE Loss:  -1.8556 | Function Loss:  -2.2172\n",
      "Total loss:  -1.1273 | PDE Loss:  -1.8556 | Function Loss:  -2.2172\n",
      "Total loss:  -1.1274 | PDE Loss:  -1.8556 | Function Loss:  -2.2173\n",
      "Total loss:  -1.1274 | PDE Loss:  -1.8556 | Function Loss:  -2.2173\n",
      "Total loss:  -1.1274 | PDE Loss:  -1.8558 | Function Loss:  -2.2173\n",
      "Total loss:  -1.1275 | PDE Loss:  -1.8558 | Function Loss:  -2.2173\n",
      "Total loss:  -1.1275 | PDE Loss:  -1.8558 | Function Loss:  -2.2174\n",
      "Total loss:  -1.1275 | PDE Loss:  -1.8559 | Function Loss:  -2.2174\n",
      "Total loss:  -1.1276 | PDE Loss:  -1.856 | Function Loss:  -2.2174\n",
      "Total loss:  -1.1276 | PDE Loss:  -1.8563 | Function Loss:  -2.2174\n",
      "Total loss:  -1.1276 | PDE Loss:  -1.8561 | Function Loss:  -2.2174\n",
      "Total loss:  -1.1276 | PDE Loss:  -1.8563 | Function Loss:  -2.2174\n",
      "Total loss:  -1.1277 | PDE Loss:  -1.8564 | Function Loss:  -2.2174\n",
      "Total loss:  -1.1277 | PDE Loss:  -1.8566 | Function Loss:  -2.2174\n",
      "Total loss:  -1.1277 | PDE Loss:  -1.8568 | Function Loss:  -2.2174\n",
      "Total loss:  -1.1277 | PDE Loss:  -1.8569 | Function Loss:  -2.2174\n",
      "Total loss:  -1.1278 | PDE Loss:  -1.8572 | Function Loss:  -2.2174\n",
      "Total loss:  -1.1278 | PDE Loss:  -1.8573 | Function Loss:  -2.2174\n",
      "Total loss:  -1.1278 | PDE Loss:  -1.8575 | Function Loss:  -2.2174\n",
      "Total loss:  -1.1279 | PDE Loss:  -1.8576 | Function Loss:  -2.2174\n",
      "Total loss:  -1.1279 | PDE Loss:  -1.8577 | Function Loss:  -2.2175\n",
      "Total loss:  -1.128 | PDE Loss:  -1.8577 | Function Loss:  -2.2175\n",
      "Total loss:  -1.128 | PDE Loss:  -1.8576 | Function Loss:  -2.2176\n",
      "Total loss:  -1.128 | PDE Loss:  -1.8576 | Function Loss:  -2.2176\n",
      "Total loss:  -1.128 | PDE Loss:  -1.8574 | Function Loss:  -2.2177\n",
      "Total loss:  -1.1281 | PDE Loss:  -1.8573 | Function Loss:  -2.2177\n",
      "Total loss:  -1.1281 | PDE Loss:  -1.8572 | Function Loss:  -2.2178\n",
      "Total loss:  -1.1281 | PDE Loss:  -1.8571 | Function Loss:  -2.2179\n",
      "Total loss:  -1.1282 | PDE Loss:  -1.8571 | Function Loss:  -2.2179\n",
      "Total loss:  -1.1282 | PDE Loss:  -1.8568 | Function Loss:  -2.218\n",
      "Total loss:  -1.1282 | PDE Loss:  -1.8569 | Function Loss:  -2.218\n",
      "Total loss:  -1.1283 | PDE Loss:  -1.8572 | Function Loss:  -2.218\n",
      "Total loss:  -1.1283 | PDE Loss:  -1.8574 | Function Loss:  -2.218\n",
      "Total loss:  -1.1283 | PDE Loss:  -1.8577 | Function Loss:  -2.2179\n",
      "Total loss:  -1.1283 | PDE Loss:  -1.8579 | Function Loss:  -2.2179\n",
      "Total loss:  -1.1283 | PDE Loss:  -1.8581 | Function Loss:  -2.2179\n",
      "Total loss:  -1.1284 | PDE Loss:  -1.8583 | Function Loss:  -2.2179\n",
      "Total loss:  -1.1284 | PDE Loss:  -1.8584 | Function Loss:  -2.2179\n",
      "Total loss:  -1.1284 | PDE Loss:  -1.8584 | Function Loss:  -2.2179\n",
      "Total loss:  -1.1285 | PDE Loss:  -1.8584 | Function Loss:  -2.218\n",
      "Total loss:  -1.1285 | PDE Loss:  -1.8583 | Function Loss:  -2.218\n",
      "Total loss:  -1.1284 | PDE Loss:  -1.8574 | Function Loss:  -2.2181\n",
      "Total loss:  -1.1285 | PDE Loss:  -1.8582 | Function Loss:  -2.2181\n",
      "Total loss:  -1.1285 | PDE Loss:  -1.8582 | Function Loss:  -2.2181\n",
      "Total loss:  -1.1286 | PDE Loss:  -1.858 | Function Loss:  -2.2182\n",
      "Total loss:  -1.1286 | PDE Loss:  -1.8579 | Function Loss:  -2.2183\n",
      "Total loss:  -1.1287 | PDE Loss:  -1.8579 | Function Loss:  -2.2183\n",
      "Total loss:  -1.1287 | PDE Loss:  -1.8579 | Function Loss:  -2.2184\n",
      "Total loss:  -1.1288 | PDE Loss:  -1.858 | Function Loss:  -2.2184\n",
      "Total loss:  -1.1288 | PDE Loss:  -1.8581 | Function Loss:  -2.2185\n",
      "Total loss:  -1.1289 | PDE Loss:  -1.8584 | Function Loss:  -2.2185\n",
      "Total loss:  -1.1289 | PDE Loss:  -1.8585 | Function Loss:  -2.2185\n",
      "Total loss:  -1.129 | PDE Loss:  -1.8587 | Function Loss:  -2.2185\n",
      "Total loss:  -1.129 | PDE Loss:  -1.8587 | Function Loss:  -2.2186\n",
      "Total loss:  -1.1291 | PDE Loss:  -1.8589 | Function Loss:  -2.2186\n",
      "Total loss:  -1.1291 | PDE Loss:  -1.8589 | Function Loss:  -2.2186\n",
      "Total loss:  -1.1291 | PDE Loss:  -1.859 | Function Loss:  -2.2186\n",
      "Total loss:  -1.1292 | PDE Loss:  -1.859 | Function Loss:  -2.2187\n",
      "Total loss:  -1.1292 | PDE Loss:  -1.8588 | Function Loss:  -2.2188\n",
      "Total loss:  -1.1292 | PDE Loss:  -1.8589 | Function Loss:  -2.2188\n",
      "Total loss:  -1.1293 | PDE Loss:  -1.859 | Function Loss:  -2.2188\n",
      "Total loss:  -1.1293 | PDE Loss:  -1.8591 | Function Loss:  -2.2189\n",
      "Total loss:  -1.1294 | PDE Loss:  -1.8593 | Function Loss:  -2.2189\n",
      "Total loss:  -1.1295 | PDE Loss:  -1.8593 | Function Loss:  -2.219\n",
      "Total loss:  -1.1295 | PDE Loss:  -1.8594 | Function Loss:  -2.219\n",
      "Total loss:  -1.1296 | PDE Loss:  -1.8597 | Function Loss:  -2.2191\n",
      "Total loss:  -1.1297 | PDE Loss:  -1.8598 | Function Loss:  -2.2192\n",
      "Total loss:  -1.1298 | PDE Loss:  -1.8603 | Function Loss:  -2.2192\n",
      "Total loss:  -1.1299 | PDE Loss:  -1.8606 | Function Loss:  -2.2192\n",
      "Total loss:  -1.13 | PDE Loss:  -1.861 | Function Loss:  -2.2193\n",
      "Total loss:  -1.1301 | PDE Loss:  -1.8616 | Function Loss:  -2.2193\n",
      "Total loss:  -1.1302 | PDE Loss:  -1.8619 | Function Loss:  -2.2193\n",
      "Total loss:  -1.1303 | PDE Loss:  -1.8622 | Function Loss:  -2.2193\n",
      "Total loss:  -1.1303 | PDE Loss:  -1.8625 | Function Loss:  -2.2193\n",
      "Total loss:  -1.1304 | PDE Loss:  -1.8624 | Function Loss:  -2.2194\n",
      "Total loss:  -1.1304 | PDE Loss:  -1.8626 | Function Loss:  -2.2194\n",
      "Total loss:  -1.1304 | PDE Loss:  -1.8626 | Function Loss:  -2.2194\n",
      "Total loss:  -1.1304 | PDE Loss:  -1.8627 | Function Loss:  -2.2194\n",
      "Total loss:  -1.1305 | PDE Loss:  -1.8627 | Function Loss:  -2.2194\n",
      "Total loss:  -1.1305 | PDE Loss:  -1.8627 | Function Loss:  -2.2195\n",
      "Total loss:  -1.1305 | PDE Loss:  -1.8627 | Function Loss:  -2.2195\n",
      "Total loss:  -1.1306 | PDE Loss:  -1.8626 | Function Loss:  -2.2196\n",
      "Total loss:  -1.1306 | PDE Loss:  -1.8625 | Function Loss:  -2.2197\n",
      "Total loss:  -1.1306 | PDE Loss:  -1.8624 | Function Loss:  -2.2197\n",
      "Total loss:  -1.1307 | PDE Loss:  -1.8623 | Function Loss:  -2.2198\n",
      "Total loss:  -1.1307 | PDE Loss:  -1.8624 | Function Loss:  -2.2198\n",
      "Total loss:  -1.1307 | PDE Loss:  -1.8624 | Function Loss:  -2.2198\n",
      "Total loss:  -1.1307 | PDE Loss:  -1.8625 | Function Loss:  -2.2198\n",
      "Total loss:  -1.1308 | PDE Loss:  -1.8627 | Function Loss:  -2.2198\n",
      "Total loss:  -1.1308 | PDE Loss:  -1.8629 | Function Loss:  -2.2198\n",
      "Total loss:  -1.1308 | PDE Loss:  -1.8643 | Function Loss:  -2.2196\n",
      "Total loss:  -1.1309 | PDE Loss:  -1.8642 | Function Loss:  -2.2197\n",
      "Total loss:  -1.131 | PDE Loss:  -1.8641 | Function Loss:  -2.2198\n",
      "Total loss:  -1.1311 | PDE Loss:  -1.8641 | Function Loss:  -2.2198\n",
      "Total loss:  -1.1311 | PDE Loss:  -1.8641 | Function Loss:  -2.2199\n",
      "Total loss:  -1.1311 | PDE Loss:  -1.8642 | Function Loss:  -2.2199\n",
      "Total loss:  -1.1312 | PDE Loss:  -1.8643 | Function Loss:  -2.22\n",
      "Total loss:  -1.1312 | PDE Loss:  -1.8645 | Function Loss:  -2.22\n",
      "Total loss:  -1.1313 | PDE Loss:  -1.8647 | Function Loss:  -2.22\n",
      "Total loss:  -1.1313 | PDE Loss:  -1.8649 | Function Loss:  -2.22\n",
      "Total loss:  -1.1314 | PDE Loss:  -1.8651 | Function Loss:  -2.22\n",
      "Total loss:  -1.1314 | PDE Loss:  -1.8651 | Function Loss:  -2.2201\n",
      "Total loss:  -1.1314 | PDE Loss:  -1.8653 | Function Loss:  -2.2201\n",
      "Total loss:  -1.1315 | PDE Loss:  -1.8652 | Function Loss:  -2.2201\n",
      "Total loss:  -1.1315 | PDE Loss:  -1.8653 | Function Loss:  -2.2201\n",
      "Total loss:  -1.1315 | PDE Loss:  -1.8652 | Function Loss:  -2.2202\n",
      "Total loss:  -1.1316 | PDE Loss:  -1.8653 | Function Loss:  -2.2202\n",
      "Total loss:  -1.1316 | PDE Loss:  -1.8653 | Function Loss:  -2.2202\n",
      "Total loss:  -1.1316 | PDE Loss:  -1.8654 | Function Loss:  -2.2203\n",
      "Total loss:  -1.1317 | PDE Loss:  -1.8655 | Function Loss:  -2.2203\n",
      "Total loss:  -1.1317 | PDE Loss:  -1.8657 | Function Loss:  -2.2203\n",
      "Total loss:  -1.1318 | PDE Loss:  -1.8659 | Function Loss:  -2.2203\n",
      "Total loss:  -1.1318 | PDE Loss:  -1.8661 | Function Loss:  -2.2203\n",
      "Total loss:  -1.1318 | PDE Loss:  -1.8664 | Function Loss:  -2.2203\n",
      "Total loss:  -1.1318 | PDE Loss:  -1.8665 | Function Loss:  -2.2203\n",
      "Total loss:  -1.1319 | PDE Loss:  -1.8667 | Function Loss:  -2.2202\n",
      "Total loss:  -1.1319 | PDE Loss:  -1.8667 | Function Loss:  -2.2202\n",
      "Total loss:  -1.1319 | PDE Loss:  -1.8668 | Function Loss:  -2.2202\n",
      "Total loss:  -1.1319 | PDE Loss:  -1.8669 | Function Loss:  -2.2202\n",
      "Total loss:  -1.1319 | PDE Loss:  -1.8669 | Function Loss:  -2.2202\n",
      "Total loss:  -1.1319 | PDE Loss:  -1.867 | Function Loss:  -2.2202\n",
      "Total loss:  -1.1318 | PDE Loss:  -1.8663 | Function Loss:  -2.2202\n",
      "Total loss:  -1.1319 | PDE Loss:  -1.8669 | Function Loss:  -2.2203\n",
      "Total loss:  -1.1319 | PDE Loss:  -1.867 | Function Loss:  -2.2203\n",
      "Total loss:  -1.1319 | PDE Loss:  -1.8671 | Function Loss:  -2.2203\n",
      "Total loss:  -1.132 | PDE Loss:  -1.8673 | Function Loss:  -2.2202\n",
      "Total loss:  -1.132 | PDE Loss:  -1.8674 | Function Loss:  -2.2203\n",
      "Total loss:  -1.132 | PDE Loss:  -1.8675 | Function Loss:  -2.2203\n",
      "Total loss:  -1.132 | PDE Loss:  -1.8676 | Function Loss:  -2.2203\n",
      "Total loss:  -1.1321 | PDE Loss:  -1.8677 | Function Loss:  -2.2203\n",
      "Total loss:  -1.1321 | PDE Loss:  -1.8677 | Function Loss:  -2.2203\n",
      "Total loss:  -1.1321 | PDE Loss:  -1.8678 | Function Loss:  -2.2203\n",
      "Total loss:  -1.1321 | PDE Loss:  -1.868 | Function Loss:  -2.2203\n",
      "Total loss:  -1.1321 | PDE Loss:  -1.8682 | Function Loss:  -2.2203\n",
      "Total loss:  -1.1322 | PDE Loss:  -1.8685 | Function Loss:  -2.2202\n",
      "Total loss:  -1.1322 | PDE Loss:  -1.8688 | Function Loss:  -2.2202\n",
      "Total loss:  -1.1322 | PDE Loss:  -1.8691 | Function Loss:  -2.2201\n",
      "Total loss:  -1.1322 | PDE Loss:  -1.8695 | Function Loss:  -2.2201\n",
      "Total loss:  -1.1323 | PDE Loss:  -1.8699 | Function Loss:  -2.2201\n",
      "Total loss:  -1.1323 | PDE Loss:  -1.8704 | Function Loss:  -2.22\n",
      "Total loss:  -1.1323 | PDE Loss:  -1.8705 | Function Loss:  -2.22\n",
      "Total loss:  -1.1324 | PDE Loss:  -1.8707 | Function Loss:  -2.22\n",
      "Total loss:  -1.1324 | PDE Loss:  -1.8707 | Function Loss:  -2.22\n",
      "Total loss:  -1.1324 | PDE Loss:  -1.8707 | Function Loss:  -2.22\n",
      "Total loss:  -1.1324 | PDE Loss:  -1.8707 | Function Loss:  -2.22\n",
      "Total loss:  -1.1324 | PDE Loss:  -1.8707 | Function Loss:  -2.2201\n",
      "Total loss:  -1.1325 | PDE Loss:  -1.8707 | Function Loss:  -2.2201\n",
      "Total loss:  -1.1325 | PDE Loss:  -1.8706 | Function Loss:  -2.2201\n",
      "Total loss:  -1.1325 | PDE Loss:  -1.8705 | Function Loss:  -2.2202\n",
      "Total loss:  -1.1325 | PDE Loss:  -1.8705 | Function Loss:  -2.2202\n",
      "Total loss:  -1.1326 | PDE Loss:  -1.8704 | Function Loss:  -2.2203\n",
      "Total loss:  -1.1326 | PDE Loss:  -1.8704 | Function Loss:  -2.2203\n",
      "Total loss:  -1.1326 | PDE Loss:  -1.8701 | Function Loss:  -2.2204\n",
      "Total loss:  -1.1327 | PDE Loss:  -1.8702 | Function Loss:  -2.2205\n",
      "Total loss:  -1.1327 | PDE Loss:  -1.8703 | Function Loss:  -2.2205\n",
      "Total loss:  -1.1328 | PDE Loss:  -1.8704 | Function Loss:  -2.2206\n",
      "Total loss:  -1.1328 | PDE Loss:  -1.8705 | Function Loss:  -2.2206\n",
      "Total loss:  -1.1329 | PDE Loss:  -1.8706 | Function Loss:  -2.2206\n",
      "Total loss:  -1.1329 | PDE Loss:  -1.8707 | Function Loss:  -2.2206\n",
      "Total loss:  -1.1329 | PDE Loss:  -1.8706 | Function Loss:  -2.2207\n",
      "Total loss:  -1.133 | PDE Loss:  -1.8705 | Function Loss:  -2.2208\n",
      "Total loss:  -1.1331 | PDE Loss:  -1.8703 | Function Loss:  -2.2209\n",
      "Total loss:  -1.1331 | PDE Loss:  -1.8701 | Function Loss:  -2.221\n",
      "Total loss:  -1.1332 | PDE Loss:  -1.8697 | Function Loss:  -2.2212\n",
      "Total loss:  -1.1333 | PDE Loss:  -1.8696 | Function Loss:  -2.2213\n",
      "Total loss:  -1.1333 | PDE Loss:  -1.8691 | Function Loss:  -2.2215\n",
      "Total loss:  -1.1334 | PDE Loss:  -1.8694 | Function Loss:  -2.2215\n",
      "Total loss:  -1.1334 | PDE Loss:  -1.8697 | Function Loss:  -2.2215\n",
      "Total loss:  -1.1335 | PDE Loss:  -1.8702 | Function Loss:  -2.2215\n",
      "Total loss:  -1.1335 | PDE Loss:  -1.8706 | Function Loss:  -2.2214\n",
      "Total loss:  -1.1336 | PDE Loss:  -1.8712 | Function Loss:  -2.2213\n",
      "Total loss:  -1.1336 | PDE Loss:  -1.8716 | Function Loss:  -2.2213\n",
      "Total loss:  -1.1337 | PDE Loss:  -1.8719 | Function Loss:  -2.2213\n",
      "Total loss:  -1.1337 | PDE Loss:  -1.8723 | Function Loss:  -2.2213\n",
      "Total loss:  -1.1337 | PDE Loss:  -1.8724 | Function Loss:  -2.2213\n",
      "Total loss:  -1.1338 | PDE Loss:  -1.8724 | Function Loss:  -2.2213\n",
      "Total loss:  -1.1338 | PDE Loss:  -1.8725 | Function Loss:  -2.2213\n",
      "Total loss:  -1.1338 | PDE Loss:  -1.8724 | Function Loss:  -2.2214\n",
      "Total loss:  -1.1338 | PDE Loss:  -1.8723 | Function Loss:  -2.2214\n",
      "Total loss:  -1.1339 | PDE Loss:  -1.8723 | Function Loss:  -2.2215\n",
      "Total loss:  -1.1339 | PDE Loss:  -1.8722 | Function Loss:  -2.2215\n",
      "Total loss:  -1.134 | PDE Loss:  -1.8723 | Function Loss:  -2.2216\n",
      "Total loss:  -1.134 | PDE Loss:  -1.8725 | Function Loss:  -2.2216\n",
      "Total loss:  -1.1341 | PDE Loss:  -1.8726 | Function Loss:  -2.2217\n",
      "Total loss:  -1.1341 | PDE Loss:  -1.8727 | Function Loss:  -2.2217\n",
      "Total loss:  -1.1342 | PDE Loss:  -1.8729 | Function Loss:  -2.2217\n",
      "Total loss:  -1.1342 | PDE Loss:  -1.8732 | Function Loss:  -2.2217\n",
      "Total loss:  -1.1343 | PDE Loss:  -1.8732 | Function Loss:  -2.2218\n",
      "Total loss:  -1.1344 | PDE Loss:  -1.8735 | Function Loss:  -2.2218\n",
      "Total loss:  -1.1344 | PDE Loss:  -1.8735 | Function Loss:  -2.2218\n",
      "Total loss:  -1.1344 | PDE Loss:  -1.8737 | Function Loss:  -2.2219\n",
      "Total loss:  -1.1345 | PDE Loss:  -1.8736 | Function Loss:  -2.2219\n",
      "Total loss:  -1.1345 | PDE Loss:  -1.8735 | Function Loss:  -2.222\n",
      "Total loss:  -1.1346 | PDE Loss:  -1.8736 | Function Loss:  -2.222\n",
      "Total loss:  -1.1346 | PDE Loss:  -1.8733 | Function Loss:  -2.2221\n",
      "Total loss:  -1.1346 | PDE Loss:  -1.8733 | Function Loss:  -2.2221\n",
      "Total loss:  -1.1346 | PDE Loss:  -1.8733 | Function Loss:  -2.2221\n",
      "Total loss:  -1.1346 | PDE Loss:  -1.8733 | Function Loss:  -2.2222\n",
      "Total loss:  -1.1347 | PDE Loss:  -1.8734 | Function Loss:  -2.2222\n",
      "Total loss:  -1.1347 | PDE Loss:  -1.8734 | Function Loss:  -2.2222\n",
      "Total loss:  -1.1347 | PDE Loss:  -1.8736 | Function Loss:  -2.2222\n",
      "Total loss:  -1.1347 | PDE Loss:  -1.8737 | Function Loss:  -2.2221\n",
      "Total loss:  -1.1347 | PDE Loss:  -1.8739 | Function Loss:  -2.2221\n",
      "Total loss:  -1.1347 | PDE Loss:  -1.874 | Function Loss:  -2.2221\n",
      "Total loss:  -1.1347 | PDE Loss:  -1.8741 | Function Loss:  -2.2221\n",
      "Total loss:  -1.1347 | PDE Loss:  -1.8742 | Function Loss:  -2.2221\n",
      "Total loss:  -1.1347 | PDE Loss:  -1.8742 | Function Loss:  -2.2221\n",
      "Total loss:  -1.1348 | PDE Loss:  -1.8741 | Function Loss:  -2.2221\n",
      "Total loss:  -1.1348 | PDE Loss:  -1.8742 | Function Loss:  -2.2221\n",
      "Total loss:  -1.1348 | PDE Loss:  -1.8742 | Function Loss:  -2.2222\n",
      "Total loss:  -1.1348 | PDE Loss:  -1.8741 | Function Loss:  -2.2222\n",
      "Total loss:  -1.1348 | PDE Loss:  -1.8741 | Function Loss:  -2.2222\n",
      "Total loss:  -1.1348 | PDE Loss:  -1.8741 | Function Loss:  -2.2222\n",
      "Total loss:  -1.1349 | PDE Loss:  -1.8742 | Function Loss:  -2.2222\n",
      "Total loss:  -1.1349 | PDE Loss:  -1.8742 | Function Loss:  -2.2223\n",
      "Total loss:  -1.1349 | PDE Loss:  -1.8743 | Function Loss:  -2.2223\n",
      "Total loss:  -1.135 | PDE Loss:  -1.8743 | Function Loss:  -2.2223\n",
      "Total loss:  -1.135 | PDE Loss:  -1.8745 | Function Loss:  -2.2223\n",
      "Total loss:  -1.135 | PDE Loss:  -1.8746 | Function Loss:  -2.2223\n",
      "Total loss:  -1.1351 | PDE Loss:  -1.8748 | Function Loss:  -2.2223\n",
      "Total loss:  -1.1351 | PDE Loss:  -1.875 | Function Loss:  -2.2223\n",
      "Total loss:  -1.1351 | PDE Loss:  -1.8754 | Function Loss:  -2.2223\n",
      "Total loss:  -1.1352 | PDE Loss:  -1.8757 | Function Loss:  -2.2223\n",
      "Total loss:  -1.1352 | PDE Loss:  -1.876 | Function Loss:  -2.2223\n",
      "Total loss:  -1.1353 | PDE Loss:  -1.8763 | Function Loss:  -2.2223\n",
      "Total loss:  -1.1353 | PDE Loss:  -1.8766 | Function Loss:  -2.2223\n",
      "Total loss:  -1.1353 | PDE Loss:  -1.8769 | Function Loss:  -2.2222\n",
      "Total loss:  -1.1354 | PDE Loss:  -1.8772 | Function Loss:  -2.2222\n",
      "Total loss:  -1.1354 | PDE Loss:  -1.8775 | Function Loss:  -2.2222\n",
      "Total loss:  -1.1355 | PDE Loss:  -1.8777 | Function Loss:  -2.2222\n",
      "Total loss:  -1.1355 | PDE Loss:  -1.8779 | Function Loss:  -2.2222\n",
      "Total loss:  -1.1355 | PDE Loss:  -1.878 | Function Loss:  -2.2222\n",
      "Total loss:  -1.1355 | PDE Loss:  -1.8781 | Function Loss:  -2.2222\n",
      "Total loss:  -1.1356 | PDE Loss:  -1.8781 | Function Loss:  -2.2222\n",
      "Total loss:  -1.1356 | PDE Loss:  -1.878 | Function Loss:  -2.2223\n",
      "Total loss:  -1.1356 | PDE Loss:  -1.8779 | Function Loss:  -2.2224\n",
      "Total loss:  -1.1357 | PDE Loss:  -1.8777 | Function Loss:  -2.2225\n",
      "Total loss:  -1.1357 | PDE Loss:  -1.8775 | Function Loss:  -2.2226\n",
      "Total loss:  -1.1358 | PDE Loss:  -1.8773 | Function Loss:  -2.2227\n",
      "Total loss:  -1.1358 | PDE Loss:  -1.877 | Function Loss:  -2.2228\n",
      "Total loss:  -1.1358 | PDE Loss:  -1.877 | Function Loss:  -2.2228\n",
      "Total loss:  -1.1359 | PDE Loss:  -1.877 | Function Loss:  -2.2229\n",
      "Total loss:  -1.1359 | PDE Loss:  -1.877 | Function Loss:  -2.2229\n",
      "Total loss:  -1.136 | PDE Loss:  -1.8771 | Function Loss:  -2.2229\n",
      "Total loss:  -1.136 | PDE Loss:  -1.877 | Function Loss:  -2.223\n",
      "Total loss:  -1.136 | PDE Loss:  -1.8775 | Function Loss:  -2.2229\n",
      "Total loss:  -1.136 | PDE Loss:  -1.8774 | Function Loss:  -2.223\n",
      "Total loss:  -1.1361 | PDE Loss:  -1.8775 | Function Loss:  -2.223\n",
      "Total loss:  -1.1361 | PDE Loss:  -1.8776 | Function Loss:  -2.223\n",
      "Total loss:  -1.1361 | PDE Loss:  -1.8777 | Function Loss:  -2.223\n",
      "Total loss:  -1.1361 | PDE Loss:  -1.8778 | Function Loss:  -2.223\n",
      "Total loss:  -1.1361 | PDE Loss:  -1.8779 | Function Loss:  -2.223\n",
      "Total loss:  -1.1361 | PDE Loss:  -1.8781 | Function Loss:  -2.2229\n",
      "Total loss:  -1.1362 | PDE Loss:  -1.8783 | Function Loss:  -2.2229\n",
      "Total loss:  -1.1362 | PDE Loss:  -1.8785 | Function Loss:  -2.2229\n",
      "Total loss:  -1.1362 | PDE Loss:  -1.8787 | Function Loss:  -2.2229\n",
      "Total loss:  -1.1363 | PDE Loss:  -1.8788 | Function Loss:  -2.2229\n",
      "Total loss:  -1.1363 | PDE Loss:  -1.879 | Function Loss:  -2.2229\n",
      "Total loss:  -1.1363 | PDE Loss:  -1.879 | Function Loss:  -2.223\n",
      "Total loss:  -1.1364 | PDE Loss:  -1.8791 | Function Loss:  -2.223\n",
      "Total loss:  -1.1364 | PDE Loss:  -1.8791 | Function Loss:  -2.223\n",
      "Total loss:  -1.1364 | PDE Loss:  -1.879 | Function Loss:  -2.2231\n",
      "Total loss:  -1.1364 | PDE Loss:  -1.879 | Function Loss:  -2.2231\n",
      "Total loss:  -1.1365 | PDE Loss:  -1.8789 | Function Loss:  -2.2232\n",
      "Total loss:  -1.1365 | PDE Loss:  -1.8788 | Function Loss:  -2.2232\n",
      "Total loss:  -1.1365 | PDE Loss:  -1.8784 | Function Loss:  -2.2234\n",
      "Total loss:  -1.1366 | PDE Loss:  -1.8785 | Function Loss:  -2.2234\n",
      "Total loss:  -1.1366 | PDE Loss:  -1.8786 | Function Loss:  -2.2234\n",
      "Total loss:  -1.1366 | PDE Loss:  -1.8787 | Function Loss:  -2.2234\n",
      "Total loss:  -1.1366 | PDE Loss:  -1.8787 | Function Loss:  -2.2234\n",
      "Total loss:  -1.1366 | PDE Loss:  -1.8788 | Function Loss:  -2.2234\n",
      "Total loss:  -1.1367 | PDE Loss:  -1.8787 | Function Loss:  -2.2234\n",
      "Total loss:  -1.1367 | PDE Loss:  -1.8787 | Function Loss:  -2.2235\n",
      "Total loss:  -1.1367 | PDE Loss:  -1.8787 | Function Loss:  -2.2235\n",
      "Total loss:  -1.1367 | PDE Loss:  -1.8786 | Function Loss:  -2.2235\n",
      "Total loss:  -1.1367 | PDE Loss:  -1.8786 | Function Loss:  -2.2236\n",
      "Total loss:  -1.1368 | PDE Loss:  -1.8786 | Function Loss:  -2.2236\n",
      "Total loss:  -1.1368 | PDE Loss:  -1.8787 | Function Loss:  -2.2236\n",
      "Total loss:  -1.1368 | PDE Loss:  -1.8787 | Function Loss:  -2.2236\n",
      "Total loss:  -1.1368 | PDE Loss:  -1.8789 | Function Loss:  -2.2236\n",
      "Total loss:  -1.1368 | PDE Loss:  -1.8791 | Function Loss:  -2.2236\n",
      "Total loss:  -1.1369 | PDE Loss:  -1.8793 | Function Loss:  -2.2236\n",
      "Total loss:  -1.1369 | PDE Loss:  -1.8795 | Function Loss:  -2.2236\n",
      "Total loss:  -1.1369 | PDE Loss:  -1.8796 | Function Loss:  -2.2235\n",
      "Total loss:  -1.1369 | PDE Loss:  -1.8797 | Function Loss:  -2.2235\n",
      "Total loss:  -1.1369 | PDE Loss:  -1.8798 | Function Loss:  -2.2235\n",
      "Total loss:  -1.137 | PDE Loss:  -1.8798 | Function Loss:  -2.2236\n",
      "Total loss:  -1.137 | PDE Loss:  -1.8799 | Function Loss:  -2.2236\n",
      "Total loss:  -1.1369 | PDE Loss:  -1.8791 | Function Loss:  -2.2237\n",
      "Total loss:  -1.137 | PDE Loss:  -1.8798 | Function Loss:  -2.2237\n",
      "Total loss:  -1.1371 | PDE Loss:  -1.8799 | Function Loss:  -2.2237\n",
      "Total loss:  -1.1371 | PDE Loss:  -1.8799 | Function Loss:  -2.2238\n",
      "Total loss:  -1.1372 | PDE Loss:  -1.8799 | Function Loss:  -2.2238\n",
      "Total loss:  -1.1372 | PDE Loss:  -1.8799 | Function Loss:  -2.2238\n",
      "Total loss:  -1.1372 | PDE Loss:  -1.88 | Function Loss:  -2.2239\n",
      "Total loss:  -1.1373 | PDE Loss:  -1.88 | Function Loss:  -2.2239\n",
      "Total loss:  -1.1372 | PDE Loss:  -1.8798 | Function Loss:  -2.2238\n",
      "Total loss:  -1.1373 | PDE Loss:  -1.8801 | Function Loss:  -2.2239\n",
      "Total loss:  -1.1373 | PDE Loss:  -1.8802 | Function Loss:  -2.2239\n",
      "Total loss:  -1.1374 | PDE Loss:  -1.8804 | Function Loss:  -2.2239\n",
      "Total loss:  -1.1374 | PDE Loss:  -1.8806 | Function Loss:  -2.2239\n",
      "Total loss:  -1.1374 | PDE Loss:  -1.8809 | Function Loss:  -2.2239\n",
      "Total loss:  -1.1375 | PDE Loss:  -1.8811 | Function Loss:  -2.2239\n",
      "Total loss:  -1.1375 | PDE Loss:  -1.8812 | Function Loss:  -2.2239\n",
      "Total loss:  -1.1375 | PDE Loss:  -1.8814 | Function Loss:  -2.2239\n",
      "Total loss:  -1.1376 | PDE Loss:  -1.8814 | Function Loss:  -2.2239\n",
      "Total loss:  -1.1376 | PDE Loss:  -1.8813 | Function Loss:  -2.224\n",
      "Total loss:  -1.1376 | PDE Loss:  -1.8814 | Function Loss:  -2.224\n",
      "Total loss:  -1.1377 | PDE Loss:  -1.8813 | Function Loss:  -2.2241\n",
      "Total loss:  -1.1377 | PDE Loss:  -1.8813 | Function Loss:  -2.2241\n",
      "Total loss:  -1.1377 | PDE Loss:  -1.8811 | Function Loss:  -2.2242\n",
      "Total loss:  -1.1377 | PDE Loss:  -1.8811 | Function Loss:  -2.2242\n",
      "Total loss:  -1.1377 | PDE Loss:  -1.8809 | Function Loss:  -2.2243\n",
      "Total loss:  -1.1378 | PDE Loss:  -1.8807 | Function Loss:  -2.2244\n",
      "Total loss:  -1.1378 | PDE Loss:  -1.8809 | Function Loss:  -2.2243\n",
      "Total loss:  -1.1378 | PDE Loss:  -1.8807 | Function Loss:  -2.2244\n",
      "Total loss:  -1.1379 | PDE Loss:  -1.8807 | Function Loss:  -2.2245\n",
      "Total loss:  -1.1379 | PDE Loss:  -1.8808 | Function Loss:  -2.2245\n",
      "Total loss:  -1.1379 | PDE Loss:  -1.8809 | Function Loss:  -2.2245\n",
      "Total loss:  -1.1379 | PDE Loss:  -1.881 | Function Loss:  -2.2245\n",
      "Total loss:  -1.138 | PDE Loss:  -1.8812 | Function Loss:  -2.2245\n",
      "Total loss:  -1.138 | PDE Loss:  -1.8813 | Function Loss:  -2.2245\n",
      "Total loss:  -1.138 | PDE Loss:  -1.8817 | Function Loss:  -2.2244\n",
      "Total loss:  -1.138 | PDE Loss:  -1.8818 | Function Loss:  -2.2244\n",
      "Total loss:  -1.1381 | PDE Loss:  -1.882 | Function Loss:  -2.2245\n",
      "Total loss:  -1.1381 | PDE Loss:  -1.8821 | Function Loss:  -2.2245\n",
      "Total loss:  -1.1382 | PDE Loss:  -1.8822 | Function Loss:  -2.2245\n",
      "Total loss:  -1.1382 | PDE Loss:  -1.8822 | Function Loss:  -2.2245\n",
      "Total loss:  -1.1382 | PDE Loss:  -1.8823 | Function Loss:  -2.2245\n",
      "Total loss:  -1.1382 | PDE Loss:  -1.8824 | Function Loss:  -2.2245\n",
      "Total loss:  -1.1382 | PDE Loss:  -1.8826 | Function Loss:  -2.2245\n",
      "Total loss:  -1.1383 | PDE Loss:  -1.8828 | Function Loss:  -2.2245\n",
      "Total loss:  -1.1383 | PDE Loss:  -1.883 | Function Loss:  -2.2245\n",
      "Total loss:  -1.1383 | PDE Loss:  -1.8832 | Function Loss:  -2.2245\n",
      "Total loss:  -1.1384 | PDE Loss:  -1.8834 | Function Loss:  -2.2245\n",
      "Total loss:  -1.1384 | PDE Loss:  -1.8836 | Function Loss:  -2.2244\n",
      "Total loss:  -1.1384 | PDE Loss:  -1.8838 | Function Loss:  -2.2244\n",
      "Total loss:  -1.1384 | PDE Loss:  -1.8841 | Function Loss:  -2.2244\n",
      "Total loss:  -1.1385 | PDE Loss:  -1.8841 | Function Loss:  -2.2245\n",
      "Total loss:  -1.1385 | PDE Loss:  -1.8841 | Function Loss:  -2.2245\n",
      "Total loss:  -1.1385 | PDE Loss:  -1.8839 | Function Loss:  -2.2246\n",
      "Total loss:  -1.1386 | PDE Loss:  -1.8839 | Function Loss:  -2.2246\n",
      "Total loss:  -1.1386 | PDE Loss:  -1.8838 | Function Loss:  -2.2247\n",
      "Total loss:  -1.1386 | PDE Loss:  -1.8838 | Function Loss:  -2.2247\n",
      "Total loss:  -1.1387 | PDE Loss:  -1.8839 | Function Loss:  -2.2247\n",
      "Total loss:  -1.1387 | PDE Loss:  -1.884 | Function Loss:  -2.2247\n",
      "Total loss:  -1.1387 | PDE Loss:  -1.8841 | Function Loss:  -2.2248\n",
      "Total loss:  -1.1388 | PDE Loss:  -1.8844 | Function Loss:  -2.2247\n",
      "Total loss:  -1.1388 | PDE Loss:  -1.8846 | Function Loss:  -2.2247\n",
      "Total loss:  -1.1388 | PDE Loss:  -1.8848 | Function Loss:  -2.2247\n",
      "Total loss:  -1.1388 | PDE Loss:  -1.8849 | Function Loss:  -2.2247\n",
      "Total loss:  -1.1388 | PDE Loss:  -1.8851 | Function Loss:  -2.2247\n",
      "Total loss:  -1.1389 | PDE Loss:  -1.8852 | Function Loss:  -2.2247\n",
      "Total loss:  -1.1389 | PDE Loss:  -1.8852 | Function Loss:  -2.2247\n",
      "Total loss:  -1.1389 | PDE Loss:  -1.8852 | Function Loss:  -2.2247\n",
      "Total loss:  -1.1389 | PDE Loss:  -1.8853 | Function Loss:  -2.2247\n",
      "Total loss:  -1.1389 | PDE Loss:  -1.885 | Function Loss:  -2.2248\n",
      "Total loss:  -1.1389 | PDE Loss:  -1.885 | Function Loss:  -2.2248\n",
      "Total loss:  -1.139 | PDE Loss:  -1.8851 | Function Loss:  -2.2248\n",
      "Total loss:  -1.139 | PDE Loss:  -1.8851 | Function Loss:  -2.2249\n",
      "Total loss:  -1.139 | PDE Loss:  -1.8851 | Function Loss:  -2.2249\n",
      "Total loss:  -1.139 | PDE Loss:  -1.885 | Function Loss:  -2.2249\n",
      "Total loss:  -1.139 | PDE Loss:  -1.885 | Function Loss:  -2.2249\n",
      "Total loss:  -1.139 | PDE Loss:  -1.885 | Function Loss:  -2.2249\n",
      "Total loss:  -1.139 | PDE Loss:  -1.8846 | Function Loss:  -2.225\n",
      "Total loss:  -1.139 | PDE Loss:  -1.8848 | Function Loss:  -2.225\n",
      "Total loss:  -1.139 | PDE Loss:  -1.8848 | Function Loss:  -2.225\n",
      "Total loss:  -1.1391 | PDE Loss:  -1.8849 | Function Loss:  -2.225\n",
      "Total loss:  -1.1391 | PDE Loss:  -1.8849 | Function Loss:  -2.225\n",
      "Total loss:  -1.1391 | PDE Loss:  -1.885 | Function Loss:  -2.225\n",
      "Total loss:  -1.1391 | PDE Loss:  -1.885 | Function Loss:  -2.2251\n",
      "Total loss:  -1.1391 | PDE Loss:  -1.885 | Function Loss:  -2.2251\n",
      "Total loss:  -1.1392 | PDE Loss:  -1.8849 | Function Loss:  -2.2251\n",
      "Total loss:  -1.1392 | PDE Loss:  -1.8848 | Function Loss:  -2.2252\n",
      "Total loss:  -1.1392 | PDE Loss:  -1.8847 | Function Loss:  -2.2252\n",
      "Total loss:  -1.1393 | PDE Loss:  -1.8847 | Function Loss:  -2.2253\n",
      "Total loss:  -1.1393 | PDE Loss:  -1.8847 | Function Loss:  -2.2254\n",
      "Total loss:  -1.1394 | PDE Loss:  -1.8847 | Function Loss:  -2.2254\n",
      "Total loss:  -1.1394 | PDE Loss:  -1.8848 | Function Loss:  -2.2254\n",
      "Total loss:  -1.1394 | PDE Loss:  -1.8848 | Function Loss:  -2.2255\n",
      "Total loss:  -1.1395 | PDE Loss:  -1.8849 | Function Loss:  -2.2255\n",
      "Total loss:  -1.1395 | PDE Loss:  -1.8851 | Function Loss:  -2.2255\n",
      "Total loss:  -1.1396 | PDE Loss:  -1.8855 | Function Loss:  -2.2255\n",
      "Total loss:  -1.1396 | PDE Loss:  -1.8856 | Function Loss:  -2.2255\n",
      "Total loss:  -1.1397 | PDE Loss:  -1.8858 | Function Loss:  -2.2255\n",
      "Total loss:  -1.1397 | PDE Loss:  -1.886 | Function Loss:  -2.2255\n",
      "Total loss:  -1.1398 | PDE Loss:  -1.886 | Function Loss:  -2.2256\n",
      "Total loss:  -1.1398 | PDE Loss:  -1.8861 | Function Loss:  -2.2257\n",
      "Total loss:  -1.1399 | PDE Loss:  -1.8861 | Function Loss:  -2.2257\n",
      "Total loss:  -1.1399 | PDE Loss:  -1.886 | Function Loss:  -2.2258\n",
      "Total loss:  -1.14 | PDE Loss:  -1.886 | Function Loss:  -2.2259\n",
      "Total loss:  -1.14 | PDE Loss:  -1.8859 | Function Loss:  -2.2259\n",
      "Total loss:  -1.1401 | PDE Loss:  -1.8859 | Function Loss:  -2.226\n",
      "Total loss:  -1.1401 | PDE Loss:  -1.886 | Function Loss:  -2.226\n",
      "Total loss:  -1.1401 | PDE Loss:  -1.8858 | Function Loss:  -2.2261\n",
      "Total loss:  -1.1402 | PDE Loss:  -1.8859 | Function Loss:  -2.2261\n",
      "Total loss:  -1.1402 | PDE Loss:  -1.8861 | Function Loss:  -2.2261\n",
      "Total loss:  -1.1402 | PDE Loss:  -1.8862 | Function Loss:  -2.2261\n",
      "Total loss:  -1.1403 | PDE Loss:  -1.8864 | Function Loss:  -2.2261\n",
      "Total loss:  -1.1403 | PDE Loss:  -1.8866 | Function Loss:  -2.2261\n",
      "Total loss:  -1.1403 | PDE Loss:  -1.8867 | Function Loss:  -2.2261\n",
      "Total loss:  -1.1404 | PDE Loss:  -1.8872 | Function Loss:  -2.2261\n",
      "Total loss:  -1.1404 | PDE Loss:  -1.8873 | Function Loss:  -2.2261\n",
      "Total loss:  -1.1404 | PDE Loss:  -1.8873 | Function Loss:  -2.2262\n",
      "Total loss:  -1.1405 | PDE Loss:  -1.8873 | Function Loss:  -2.2262\n",
      "Total loss:  -1.1405 | PDE Loss:  -1.8873 | Function Loss:  -2.2262\n",
      "Total loss:  -1.1405 | PDE Loss:  -1.8873 | Function Loss:  -2.2262\n",
      "Total loss:  -1.1405 | PDE Loss:  -1.8873 | Function Loss:  -2.2263\n",
      "Total loss:  -1.1406 | PDE Loss:  -1.8873 | Function Loss:  -2.2263\n",
      "Total loss:  -1.1406 | PDE Loss:  -1.8874 | Function Loss:  -2.2264\n",
      "Total loss:  -1.1407 | PDE Loss:  -1.8873 | Function Loss:  -2.2265\n",
      "Total loss:  -1.1406 | PDE Loss:  -1.8877 | Function Loss:  -2.2262\n",
      "Total loss:  -1.1407 | PDE Loss:  -1.8875 | Function Loss:  -2.2264\n",
      "Total loss:  -1.1407 | PDE Loss:  -1.8875 | Function Loss:  -2.2265\n",
      "Total loss:  -1.1408 | PDE Loss:  -1.8875 | Function Loss:  -2.2265\n",
      "Total loss:  -1.1408 | PDE Loss:  -1.8876 | Function Loss:  -2.2266\n",
      "Total loss:  -1.1409 | PDE Loss:  -1.8877 | Function Loss:  -2.2266\n",
      "Total loss:  -1.1409 | PDE Loss:  -1.8879 | Function Loss:  -2.2266\n",
      "Total loss:  -1.141 | PDE Loss:  -1.8882 | Function Loss:  -2.2266\n",
      "Total loss:  -1.141 | PDE Loss:  -1.8884 | Function Loss:  -2.2266\n",
      "Total loss:  -1.1411 | PDE Loss:  -1.8886 | Function Loss:  -2.2266\n",
      "Total loss:  -1.1411 | PDE Loss:  -1.8887 | Function Loss:  -2.2266\n",
      "Total loss:  -1.1411 | PDE Loss:  -1.8887 | Function Loss:  -2.2267\n",
      "Total loss:  -1.1411 | PDE Loss:  -1.8886 | Function Loss:  -2.2267\n",
      "Total loss:  -1.1412 | PDE Loss:  -1.8884 | Function Loss:  -2.2268\n",
      "Total loss:  -1.1412 | PDE Loss:  -1.8879 | Function Loss:  -2.2269\n",
      "Total loss:  -1.1412 | PDE Loss:  -1.8878 | Function Loss:  -2.227\n",
      "Total loss:  -1.1412 | PDE Loss:  -1.8876 | Function Loss:  -2.2271\n",
      "Total loss:  -1.1413 | PDE Loss:  -1.8874 | Function Loss:  -2.2271\n",
      "Total loss:  -1.1413 | PDE Loss:  -1.8872 | Function Loss:  -2.2272\n",
      "Total loss:  -1.1413 | PDE Loss:  -1.8868 | Function Loss:  -2.2273\n",
      "Total loss:  -1.1413 | PDE Loss:  -1.8869 | Function Loss:  -2.2273\n",
      "Total loss:  -1.1413 | PDE Loss:  -1.8871 | Function Loss:  -2.2273\n",
      "Total loss:  -1.1413 | PDE Loss:  -1.8872 | Function Loss:  -2.2273\n",
      "Total loss:  -1.1414 | PDE Loss:  -1.8872 | Function Loss:  -2.2273\n",
      "Total loss:  -1.1414 | PDE Loss:  -1.8875 | Function Loss:  -2.2273\n",
      "Total loss:  -1.1414 | PDE Loss:  -1.8878 | Function Loss:  -2.2273\n",
      "Total loss:  -1.1415 | PDE Loss:  -1.8881 | Function Loss:  -2.2272\n",
      "Total loss:  -1.1415 | PDE Loss:  -1.8883 | Function Loss:  -2.2272\n",
      "Total loss:  -1.1395 | PDE Loss:  -1.8871 | Function Loss:  -2.2251\n",
      "Total loss:  -1.1415 | PDE Loss:  -1.8885 | Function Loss:  -2.2272\n",
      "Total loss:  -1.1415 | PDE Loss:  -1.8886 | Function Loss:  -2.2272\n",
      "Total loss:  -1.1416 | PDE Loss:  -1.8889 | Function Loss:  -2.2272\n",
      "Total loss:  -1.1416 | PDE Loss:  -1.8892 | Function Loss:  -2.2272\n",
      "Total loss:  -1.1417 | PDE Loss:  -1.8898 | Function Loss:  -2.2271\n",
      "Total loss:  -1.1418 | PDE Loss:  -1.8902 | Function Loss:  -2.2271\n",
      "Total loss:  -1.1418 | PDE Loss:  -1.8906 | Function Loss:  -2.2271\n",
      "Total loss:  -1.1418 | PDE Loss:  -1.8913 | Function Loss:  -2.227\n",
      "Total loss:  -1.1419 | PDE Loss:  -1.8911 | Function Loss:  -2.2271\n",
      "Total loss:  -1.1419 | PDE Loss:  -1.8914 | Function Loss:  -2.2271\n",
      "Total loss:  -1.142 | PDE Loss:  -1.8917 | Function Loss:  -2.2271\n",
      "Total loss:  -1.142 | PDE Loss:  -1.892 | Function Loss:  -2.227\n",
      "Total loss:  -1.142 | PDE Loss:  -1.8923 | Function Loss:  -2.227\n",
      "Total loss:  -1.1421 | PDE Loss:  -1.8926 | Function Loss:  -2.227\n",
      "Total loss:  -1.1421 | PDE Loss:  -1.893 | Function Loss:  -2.2269\n",
      "Total loss:  -1.1421 | PDE Loss:  -1.8933 | Function Loss:  -2.2269\n",
      "Total loss:  -1.1421 | PDE Loss:  -1.8935 | Function Loss:  -2.2269\n",
      "Total loss:  -1.1422 | PDE Loss:  -1.8939 | Function Loss:  -2.2268\n",
      "Total loss:  -1.1422 | PDE Loss:  -1.8942 | Function Loss:  -2.2268\n",
      "Total loss:  -1.1422 | PDE Loss:  -1.8945 | Function Loss:  -2.2268\n",
      "Total loss:  -1.1423 | PDE Loss:  -1.8947 | Function Loss:  -2.2268\n",
      "Total loss:  -1.1423 | PDE Loss:  -1.8949 | Function Loss:  -2.2268\n",
      "Total loss:  -1.1423 | PDE Loss:  -1.8951 | Function Loss:  -2.2267\n",
      "Total loss:  -1.1423 | PDE Loss:  -1.8951 | Function Loss:  -2.2268\n",
      "Total loss:  -1.1424 | PDE Loss:  -1.8951 | Function Loss:  -2.2268\n",
      "Total loss:  -1.1424 | PDE Loss:  -1.895 | Function Loss:  -2.2269\n",
      "Total loss:  -1.1424 | PDE Loss:  -1.8948 | Function Loss:  -2.2269\n",
      "Total loss:  -1.1424 | PDE Loss:  -1.8947 | Function Loss:  -2.227\n",
      "Total loss:  -1.1425 | PDE Loss:  -1.8946 | Function Loss:  -2.227\n",
      "Total loss:  -1.1425 | PDE Loss:  -1.8945 | Function Loss:  -2.2271\n",
      "Total loss:  -1.1425 | PDE Loss:  -1.8945 | Function Loss:  -2.2271\n",
      "Total loss:  -1.1425 | PDE Loss:  -1.8946 | Function Loss:  -2.2271\n",
      "Total loss:  -1.1426 | PDE Loss:  -1.8946 | Function Loss:  -2.2271\n",
      "Total loss:  -1.1426 | PDE Loss:  -1.8947 | Function Loss:  -2.2271\n",
      "Total loss:  -1.1426 | PDE Loss:  -1.8948 | Function Loss:  -2.2271\n",
      "Total loss:  -1.1426 | PDE Loss:  -1.8949 | Function Loss:  -2.2272\n",
      "Total loss:  -1.1426 | PDE Loss:  -1.895 | Function Loss:  -2.2272\n",
      "Total loss:  -1.1426 | PDE Loss:  -1.8947 | Function Loss:  -2.2272\n",
      "Total loss:  -1.1427 | PDE Loss:  -1.8948 | Function Loss:  -2.2272\n",
      "Total loss:  -1.1427 | PDE Loss:  -1.8948 | Function Loss:  -2.2273\n",
      "Total loss:  -1.1427 | PDE Loss:  -1.8948 | Function Loss:  -2.2273\n",
      "Total loss:  -1.1427 | PDE Loss:  -1.8947 | Function Loss:  -2.2273\n",
      "Total loss:  -1.1427 | PDE Loss:  -1.8946 | Function Loss:  -2.2274\n",
      "Total loss:  -1.1427 | PDE Loss:  -1.8945 | Function Loss:  -2.2274\n",
      "Total loss:  -1.1428 | PDE Loss:  -1.8944 | Function Loss:  -2.2274\n",
      "Total loss:  -1.1428 | PDE Loss:  -1.8944 | Function Loss:  -2.2274\n",
      "Total loss:  -1.1428 | PDE Loss:  -1.8944 | Function Loss:  -2.2275\n",
      "Total loss:  -1.1428 | PDE Loss:  -1.8943 | Function Loss:  -2.2275\n",
      "Total loss:  -1.1428 | PDE Loss:  -1.8943 | Function Loss:  -2.2275\n",
      "Total loss:  -1.1428 | PDE Loss:  -1.8944 | Function Loss:  -2.2276\n",
      "Total loss:  -1.1429 | PDE Loss:  -1.8944 | Function Loss:  -2.2276\n",
      "Total loss:  -1.1429 | PDE Loss:  -1.8944 | Function Loss:  -2.2276\n",
      "Total loss:  -1.1429 | PDE Loss:  -1.8945 | Function Loss:  -2.2276\n",
      "Total loss:  -1.1429 | PDE Loss:  -1.8945 | Function Loss:  -2.2276\n",
      "Total loss:  -1.1429 | PDE Loss:  -1.8945 | Function Loss:  -2.2276\n",
      "Total loss:  -1.1429 | PDE Loss:  -1.8945 | Function Loss:  -2.2276\n",
      "Total loss:  -1.143 | PDE Loss:  -1.8945 | Function Loss:  -2.2277\n",
      "Total loss:  -1.143 | PDE Loss:  -1.8945 | Function Loss:  -2.2277\n",
      "Total loss:  -1.143 | PDE Loss:  -1.8944 | Function Loss:  -2.2277\n",
      "Total loss:  -1.143 | PDE Loss:  -1.8944 | Function Loss:  -2.2278\n",
      "Total loss:  -1.143 | PDE Loss:  -1.8944 | Function Loss:  -2.2278\n",
      "Total loss:  -1.1431 | PDE Loss:  -1.8944 | Function Loss:  -2.2278\n",
      "Total loss:  -1.1431 | PDE Loss:  -1.8943 | Function Loss:  -2.2279\n",
      "Total loss:  -1.1431 | PDE Loss:  -1.894 | Function Loss:  -2.228\n",
      "Total loss:  -1.1431 | PDE Loss:  -1.8941 | Function Loss:  -2.228\n",
      "Total loss:  -1.1432 | PDE Loss:  -1.8942 | Function Loss:  -2.228\n",
      "Total loss:  -1.1432 | PDE Loss:  -1.8943 | Function Loss:  -2.228\n",
      "Total loss:  -1.1432 | PDE Loss:  -1.8944 | Function Loss:  -2.228\n",
      "Total loss:  -1.1432 | PDE Loss:  -1.8944 | Function Loss:  -2.228\n",
      "Total loss:  -1.1432 | PDE Loss:  -1.8944 | Function Loss:  -2.228\n",
      "Total loss:  -1.1433 | PDE Loss:  -1.8946 | Function Loss:  -2.228\n",
      "Total loss:  -1.143 | PDE Loss:  -1.8925 | Function Loss:  -2.2282\n",
      "Total loss:  -1.1433 | PDE Loss:  -1.8944 | Function Loss:  -2.2281\n",
      "Total loss:  -1.1433 | PDE Loss:  -1.8945 | Function Loss:  -2.2281\n",
      "Total loss:  -1.1433 | PDE Loss:  -1.8946 | Function Loss:  -2.2281\n",
      "Total loss:  -1.1434 | PDE Loss:  -1.8946 | Function Loss:  -2.2281\n",
      "Total loss:  -1.1434 | PDE Loss:  -1.8947 | Function Loss:  -2.2281\n",
      "Total loss:  -1.1434 | PDE Loss:  -1.8947 | Function Loss:  -2.2282\n",
      "Total loss:  -1.1434 | PDE Loss:  -1.8948 | Function Loss:  -2.2282\n",
      "Total loss:  -1.1435 | PDE Loss:  -1.8948 | Function Loss:  -2.2282\n",
      "Total loss:  -1.1435 | PDE Loss:  -1.8949 | Function Loss:  -2.2282\n",
      "Total loss:  -1.1435 | PDE Loss:  -1.895 | Function Loss:  -2.2282\n",
      "Total loss:  -1.1436 | PDE Loss:  -1.8951 | Function Loss:  -2.2282\n",
      "Total loss:  -1.1436 | PDE Loss:  -1.8952 | Function Loss:  -2.2283\n",
      "Total loss:  -1.1436 | PDE Loss:  -1.8953 | Function Loss:  -2.2283\n",
      "Total loss:  -1.1436 | PDE Loss:  -1.8953 | Function Loss:  -2.2283\n",
      "Total loss:  -1.1436 | PDE Loss:  -1.8952 | Function Loss:  -2.2283\n",
      "Total loss:  -1.1436 | PDE Loss:  -1.8952 | Function Loss:  -2.2283\n",
      "Total loss:  -1.1436 | PDE Loss:  -1.8952 | Function Loss:  -2.2283\n",
      "Total loss:  -1.1437 | PDE Loss:  -1.8952 | Function Loss:  -2.2284\n",
      "Total loss:  -1.1437 | PDE Loss:  -1.8952 | Function Loss:  -2.2284\n",
      "Total loss:  -1.1437 | PDE Loss:  -1.8953 | Function Loss:  -2.2284\n",
      "Total loss:  -1.1437 | PDE Loss:  -1.8953 | Function Loss:  -2.2284\n",
      "Total loss:  -1.1437 | PDE Loss:  -1.8954 | Function Loss:  -2.2284\n",
      "Total loss:  -1.1437 | PDE Loss:  -1.8954 | Function Loss:  -2.2284\n",
      "Total loss:  -1.1437 | PDE Loss:  -1.8956 | Function Loss:  -2.2284\n",
      "Total loss:  -1.1438 | PDE Loss:  -1.8958 | Function Loss:  -2.2284\n",
      "Total loss:  -1.1438 | PDE Loss:  -1.896 | Function Loss:  -2.2283\n",
      "Total loss:  -1.1438 | PDE Loss:  -1.8963 | Function Loss:  -2.2283\n",
      "Total loss:  -1.1438 | PDE Loss:  -1.8966 | Function Loss:  -2.2283\n",
      "Total loss:  -1.1439 | PDE Loss:  -1.8967 | Function Loss:  -2.2283\n",
      "Total loss:  -1.1439 | PDE Loss:  -1.8969 | Function Loss:  -2.2283\n",
      "Total loss:  -1.1439 | PDE Loss:  -1.897 | Function Loss:  -2.2283\n",
      "Total loss:  -1.1439 | PDE Loss:  -1.897 | Function Loss:  -2.2283\n",
      "Total loss:  -1.1439 | PDE Loss:  -1.8971 | Function Loss:  -2.2283\n",
      "Total loss:  -1.144 | PDE Loss:  -1.8973 | Function Loss:  -2.2283\n",
      "Total loss:  -1.144 | PDE Loss:  -1.8977 | Function Loss:  -2.2282\n",
      "Total loss:  -1.144 | PDE Loss:  -1.8978 | Function Loss:  -2.2283\n",
      "Total loss:  -1.1441 | PDE Loss:  -1.898 | Function Loss:  -2.2283\n",
      "Total loss:  -1.1441 | PDE Loss:  -1.8983 | Function Loss:  -2.2283\n",
      "Total loss:  -1.1441 | PDE Loss:  -1.8985 | Function Loss:  -2.2282\n",
      "Total loss:  -1.1442 | PDE Loss:  -1.8989 | Function Loss:  -2.2282\n",
      "Total loss:  -1.1442 | PDE Loss:  -1.8991 | Function Loss:  -2.2282\n",
      "Total loss:  -1.1443 | PDE Loss:  -1.8994 | Function Loss:  -2.2282\n",
      "Total loss:  -1.1443 | PDE Loss:  -1.8997 | Function Loss:  -2.2282\n",
      "Total loss:  -1.1443 | PDE Loss:  -1.8999 | Function Loss:  -2.2282\n",
      "Total loss:  -1.1443 | PDE Loss:  -1.9001 | Function Loss:  -2.2281\n",
      "Total loss:  -1.1444 | PDE Loss:  -1.9003 | Function Loss:  -2.2281\n",
      "Total loss:  -1.1444 | PDE Loss:  -1.9004 | Function Loss:  -2.2281\n",
      "Total loss:  -1.1444 | PDE Loss:  -1.9004 | Function Loss:  -2.2281\n",
      "Total loss:  -1.1444 | PDE Loss:  -1.9004 | Function Loss:  -2.2281\n",
      "Total loss:  -1.1444 | PDE Loss:  -1.9004 | Function Loss:  -2.2282\n",
      "Total loss:  -1.1444 | PDE Loss:  -1.9005 | Function Loss:  -2.2282\n",
      "Total loss:  -1.1444 | PDE Loss:  -1.9005 | Function Loss:  -2.2282\n",
      "Total loss:  -1.1445 | PDE Loss:  -1.9007 | Function Loss:  -2.2282\n",
      "Total loss:  -1.1445 | PDE Loss:  -1.9009 | Function Loss:  -2.2282\n",
      "Total loss:  -1.1445 | PDE Loss:  -1.9015 | Function Loss:  -2.2281\n",
      "Total loss:  -1.1445 | PDE Loss:  -1.9015 | Function Loss:  -2.228\n",
      "Total loss:  -1.1445 | PDE Loss:  -1.9016 | Function Loss:  -2.2281\n",
      "Total loss:  -1.1446 | PDE Loss:  -1.9017 | Function Loss:  -2.2281\n",
      "Total loss:  -1.1446 | PDE Loss:  -1.9021 | Function Loss:  -2.2281\n",
      "Total loss:  -1.1447 | PDE Loss:  -1.9025 | Function Loss:  -2.228\n",
      "Total loss:  -1.1447 | PDE Loss:  -1.9027 | Function Loss:  -2.228\n",
      "Total loss:  -1.1447 | PDE Loss:  -1.9029 | Function Loss:  -2.228\n",
      "Total loss:  -1.1448 | PDE Loss:  -1.9031 | Function Loss:  -2.228\n",
      "Total loss:  -1.1448 | PDE Loss:  -1.9032 | Function Loss:  -2.228\n",
      "Total loss:  -1.1448 | PDE Loss:  -1.9034 | Function Loss:  -2.228\n",
      "Total loss:  -1.1449 | PDE Loss:  -1.9034 | Function Loss:  -2.2281\n",
      "Total loss:  -1.1449 | PDE Loss:  -1.9034 | Function Loss:  -2.2281\n",
      "Total loss:  -1.1449 | PDE Loss:  -1.9035 | Function Loss:  -2.2281\n",
      "Total loss:  -1.1449 | PDE Loss:  -1.9034 | Function Loss:  -2.2282\n",
      "Total loss:  -1.145 | PDE Loss:  -1.9033 | Function Loss:  -2.2282\n",
      "Total loss:  -1.145 | PDE Loss:  -1.9035 | Function Loss:  -2.2282\n",
      "Total loss:  -1.145 | PDE Loss:  -1.9034 | Function Loss:  -2.2283\n",
      "Total loss:  -1.1451 | PDE Loss:  -1.9034 | Function Loss:  -2.2283\n",
      "Total loss:  -1.1451 | PDE Loss:  -1.9035 | Function Loss:  -2.2283\n",
      "Total loss:  -1.1451 | PDE Loss:  -1.9037 | Function Loss:  -2.2283\n",
      "Total loss:  -1.1451 | PDE Loss:  -1.9037 | Function Loss:  -2.2283\n",
      "Total loss:  -1.1452 | PDE Loss:  -1.9038 | Function Loss:  -2.2283\n",
      "Total loss:  -1.1452 | PDE Loss:  -1.9039 | Function Loss:  -2.2283\n",
      "Total loss:  -1.1452 | PDE Loss:  -1.904 | Function Loss:  -2.2283\n",
      "Total loss:  -1.1452 | PDE Loss:  -1.9042 | Function Loss:  -2.2283\n",
      "Total loss:  -1.1452 | PDE Loss:  -1.9042 | Function Loss:  -2.2283\n",
      "Total loss:  -1.1452 | PDE Loss:  -1.9043 | Function Loss:  -2.2283\n",
      "Total loss:  -1.1453 | PDE Loss:  -1.9044 | Function Loss:  -2.2283\n",
      "Total loss:  -1.1453 | PDE Loss:  -1.9044 | Function Loss:  -2.2284\n",
      "Total loss:  -1.1453 | PDE Loss:  -1.9045 | Function Loss:  -2.2283\n",
      "Total loss:  -1.1453 | PDE Loss:  -1.9046 | Function Loss:  -2.2283\n",
      "Total loss:  -1.1453 | PDE Loss:  -1.9054 | Function Loss:  -2.2282\n",
      "Total loss:  -1.1453 | PDE Loss:  -1.9048 | Function Loss:  -2.2283\n",
      "Total loss:  -1.1453 | PDE Loss:  -1.9049 | Function Loss:  -2.2283\n",
      "Total loss:  -1.1453 | PDE Loss:  -1.905 | Function Loss:  -2.2283\n",
      "Total loss:  -1.1453 | PDE Loss:  -1.9051 | Function Loss:  -2.2283\n",
      "Total loss:  -1.1454 | PDE Loss:  -1.9053 | Function Loss:  -2.2283\n",
      "Total loss:  -1.1454 | PDE Loss:  -1.9054 | Function Loss:  -2.2283\n",
      "Total loss:  -1.1454 | PDE Loss:  -1.9056 | Function Loss:  -2.2283\n",
      "Total loss:  -1.1454 | PDE Loss:  -1.9058 | Function Loss:  -2.2283\n",
      "Total loss:  -1.1455 | PDE Loss:  -1.906 | Function Loss:  -2.2282\n",
      "Total loss:  -1.1455 | PDE Loss:  -1.9064 | Function Loss:  -2.2282\n",
      "Total loss:  -1.1455 | PDE Loss:  -1.9064 | Function Loss:  -2.2282\n",
      "Total loss:  -1.1455 | PDE Loss:  -1.9065 | Function Loss:  -2.2282\n",
      "Total loss:  -1.1455 | PDE Loss:  -1.9065 | Function Loss:  -2.2282\n",
      "Total loss:  -1.1456 | PDE Loss:  -1.9065 | Function Loss:  -2.2283\n",
      "Total loss:  -1.1456 | PDE Loss:  -1.9066 | Function Loss:  -2.2283\n",
      "Total loss:  -1.1456 | PDE Loss:  -1.9066 | Function Loss:  -2.2283\n",
      "Total loss:  -1.1456 | PDE Loss:  -1.9066 | Function Loss:  -2.2283\n",
      "Total loss:  -1.1456 | PDE Loss:  -1.9069 | Function Loss:  -2.2283\n",
      "Total loss:  -1.1457 | PDE Loss:  -1.9067 | Function Loss:  -2.2284\n",
      "Total loss:  -1.1457 | PDE Loss:  -1.9067 | Function Loss:  -2.2284\n",
      "Total loss:  -1.1457 | PDE Loss:  -1.9067 | Function Loss:  -2.2284\n",
      "Total loss:  -1.1457 | PDE Loss:  -1.9067 | Function Loss:  -2.2284\n",
      "Total loss:  -1.1458 | PDE Loss:  -1.9067 | Function Loss:  -2.2285\n",
      "Total loss:  -1.1458 | PDE Loss:  -1.9066 | Function Loss:  -2.2285\n",
      "Total loss:  -1.1458 | PDE Loss:  -1.9067 | Function Loss:  -2.2285\n",
      "Total loss:  -1.1458 | PDE Loss:  -1.9068 | Function Loss:  -2.2286\n",
      "Total loss:  -1.1459 | PDE Loss:  -1.9069 | Function Loss:  -2.2286\n",
      "Total loss:  -1.1459 | PDE Loss:  -1.9071 | Function Loss:  -2.2286\n",
      "Total loss:  -1.1459 | PDE Loss:  -1.9074 | Function Loss:  -2.2285\n",
      "Total loss:  -1.146 | PDE Loss:  -1.9078 | Function Loss:  -2.2285\n",
      "Total loss:  -1.146 | PDE Loss:  -1.9083 | Function Loss:  -2.2285\n",
      "Total loss:  -1.1461 | PDE Loss:  -1.9087 | Function Loss:  -2.2284\n",
      "Total loss:  -1.1461 | PDE Loss:  -1.909 | Function Loss:  -2.2284\n",
      "Total loss:  -1.1462 | PDE Loss:  -1.9093 | Function Loss:  -2.2284\n",
      "Total loss:  -1.1462 | PDE Loss:  -1.9095 | Function Loss:  -2.2285\n",
      "Total loss:  -1.1463 | PDE Loss:  -1.9096 | Function Loss:  -2.2285\n",
      "Total loss:  -1.1463 | PDE Loss:  -1.9095 | Function Loss:  -2.2286\n",
      "Total loss:  -1.1463 | PDE Loss:  -1.9095 | Function Loss:  -2.2286\n",
      "Total loss:  -1.1464 | PDE Loss:  -1.9093 | Function Loss:  -2.2286\n",
      "Total loss:  -1.1464 | PDE Loss:  -1.9092 | Function Loss:  -2.2287\n",
      "Total loss:  -1.1464 | PDE Loss:  -1.9091 | Function Loss:  -2.2287\n",
      "Total loss:  -1.1464 | PDE Loss:  -1.909 | Function Loss:  -2.2288\n",
      "Total loss:  -1.1464 | PDE Loss:  -1.9089 | Function Loss:  -2.2288\n",
      "Total loss:  -1.1464 | PDE Loss:  -1.9088 | Function Loss:  -2.2289\n",
      "Total loss:  -1.1465 | PDE Loss:  -1.9087 | Function Loss:  -2.2289\n",
      "Total loss:  -1.1465 | PDE Loss:  -1.9087 | Function Loss:  -2.2289\n",
      "Total loss:  -1.1465 | PDE Loss:  -1.9088 | Function Loss:  -2.2289\n",
      "Total loss:  -1.1465 | PDE Loss:  -1.9089 | Function Loss:  -2.2289\n",
      "Total loss:  -1.1465 | PDE Loss:  -1.9091 | Function Loss:  -2.2289\n",
      "Total loss:  -1.1465 | PDE Loss:  -1.9092 | Function Loss:  -2.2289\n",
      "Total loss:  -1.1466 | PDE Loss:  -1.9094 | Function Loss:  -2.2289\n",
      "Total loss:  -1.1466 | PDE Loss:  -1.9095 | Function Loss:  -2.2289\n",
      "Total loss:  -1.1466 | PDE Loss:  -1.9096 | Function Loss:  -2.2289\n",
      "Total loss:  -1.1466 | PDE Loss:  -1.9096 | Function Loss:  -2.2289\n",
      "Total loss:  -1.1466 | PDE Loss:  -1.9097 | Function Loss:  -2.2289\n",
      "Total loss:  -1.1466 | PDE Loss:  -1.9097 | Function Loss:  -2.2289\n",
      "Total loss:  -1.1466 | PDE Loss:  -1.9098 | Function Loss:  -2.2289\n",
      "Total loss:  -1.1467 | PDE Loss:  -1.9099 | Function Loss:  -2.2289\n",
      "Total loss:  -1.1467 | PDE Loss:  -1.91 | Function Loss:  -2.2289\n",
      "Total loss:  -1.1467 | PDE Loss:  -1.9101 | Function Loss:  -2.2289\n",
      "Total loss:  -1.1467 | PDE Loss:  -1.9102 | Function Loss:  -2.2289\n",
      "Total loss:  -1.1467 | PDE Loss:  -1.9102 | Function Loss:  -2.2289\n",
      "Total loss:  -1.1468 | PDE Loss:  -1.9103 | Function Loss:  -2.2289\n",
      "Total loss:  -1.1468 | PDE Loss:  -1.9103 | Function Loss:  -2.2289\n",
      "Total loss:  -1.1468 | PDE Loss:  -1.9104 | Function Loss:  -2.2289\n",
      "Total loss:  -1.1468 | PDE Loss:  -1.9103 | Function Loss:  -2.229\n",
      "Total loss:  -1.1468 | PDE Loss:  -1.9103 | Function Loss:  -2.229\n",
      "Total loss:  -1.1468 | PDE Loss:  -1.9103 | Function Loss:  -2.229\n",
      "Total loss:  -1.1468 | PDE Loss:  -1.9103 | Function Loss:  -2.229\n",
      "Total loss:  -1.1469 | PDE Loss:  -1.9103 | Function Loss:  -2.229\n",
      "Total loss:  -1.1469 | PDE Loss:  -1.9102 | Function Loss:  -2.2291\n",
      "Total loss:  -1.1468 | PDE Loss:  -1.9096 | Function Loss:  -2.2291\n",
      "Total loss:  -1.1469 | PDE Loss:  -1.9102 | Function Loss:  -2.2291\n",
      "Total loss:  -1.1469 | PDE Loss:  -1.9102 | Function Loss:  -2.2291\n",
      "Total loss:  -1.1469 | PDE Loss:  -1.9102 | Function Loss:  -2.2291\n",
      "Total loss:  -1.1468 | PDE Loss:  -1.91 | Function Loss:  -2.2291\n",
      "Total loss:  -1.1469 | PDE Loss:  -1.9103 | Function Loss:  -2.2291\n",
      "Total loss:  -1.147 | PDE Loss:  -1.9103 | Function Loss:  -2.2292\n",
      "Total loss:  -1.147 | PDE Loss:  -1.9103 | Function Loss:  -2.2292\n",
      "Total loss:  -1.147 | PDE Loss:  -1.9105 | Function Loss:  -2.2292\n",
      "Total loss:  -1.147 | PDE Loss:  -1.9107 | Function Loss:  -2.2292\n",
      "Total loss:  -1.147 | PDE Loss:  -1.9109 | Function Loss:  -2.2291\n",
      "Total loss:  -1.1471 | PDE Loss:  -1.9111 | Function Loss:  -2.2291\n",
      "Total loss:  -1.1471 | PDE Loss:  -1.9114 | Function Loss:  -2.2291\n",
      "Total loss:  -1.1471 | PDE Loss:  -1.9118 | Function Loss:  -2.229\n",
      "Total loss:  -1.1471 | PDE Loss:  -1.9122 | Function Loss:  -2.229\n",
      "Total loss:  -1.1472 | PDE Loss:  -1.9124 | Function Loss:  -2.229\n",
      "Total loss:  -1.1472 | PDE Loss:  -1.9126 | Function Loss:  -2.229\n",
      "Total loss:  -1.1473 | PDE Loss:  -1.9128 | Function Loss:  -2.229\n",
      "Total loss:  -1.1473 | PDE Loss:  -1.9128 | Function Loss:  -2.229\n",
      "Total loss:  -1.1473 | PDE Loss:  -1.9129 | Function Loss:  -2.229\n",
      "Total loss:  -1.1473 | PDE Loss:  -1.9129 | Function Loss:  -2.2291\n",
      "Total loss:  -1.1474 | PDE Loss:  -1.913 | Function Loss:  -2.2291\n",
      "Total loss:  -1.1474 | PDE Loss:  -1.9133 | Function Loss:  -2.2291\n",
      "Total loss:  -1.1474 | PDE Loss:  -1.9133 | Function Loss:  -2.2291\n",
      "Total loss:  -1.1475 | PDE Loss:  -1.9133 | Function Loss:  -2.2292\n",
      "Total loss:  -1.1475 | PDE Loss:  -1.9133 | Function Loss:  -2.2292\n",
      "Total loss:  -1.1475 | PDE Loss:  -1.9132 | Function Loss:  -2.2292\n",
      "Total loss:  -1.1475 | PDE Loss:  -1.9132 | Function Loss:  -2.2293\n",
      "Total loss:  -1.1475 | PDE Loss:  -1.9131 | Function Loss:  -2.2293\n",
      "Total loss:  -1.1476 | PDE Loss:  -1.9129 | Function Loss:  -2.2294\n",
      "Total loss:  -1.1476 | PDE Loss:  -1.9127 | Function Loss:  -2.2294\n",
      "Total loss:  -1.1476 | PDE Loss:  -1.9126 | Function Loss:  -2.2295\n",
      "Total loss:  -1.1476 | PDE Loss:  -1.9125 | Function Loss:  -2.2295\n",
      "Total loss:  -1.1476 | PDE Loss:  -1.9124 | Function Loss:  -2.2295\n",
      "Total loss:  -1.1476 | PDE Loss:  -1.9123 | Function Loss:  -2.2296\n",
      "Total loss:  -1.1476 | PDE Loss:  -1.9122 | Function Loss:  -2.2296\n",
      "Total loss:  -1.1477 | PDE Loss:  -1.9121 | Function Loss:  -2.2296\n",
      "Total loss:  -1.1477 | PDE Loss:  -1.912 | Function Loss:  -2.2297\n",
      "Total loss:  -1.1477 | PDE Loss:  -1.912 | Function Loss:  -2.2297\n",
      "Total loss:  -1.1477 | PDE Loss:  -1.912 | Function Loss:  -2.2297\n",
      "Total loss:  -1.1478 | PDE Loss:  -1.9122 | Function Loss:  -2.2297\n",
      "Total loss:  -1.1478 | PDE Loss:  -1.9123 | Function Loss:  -2.2297\n",
      "Total loss:  -1.1478 | PDE Loss:  -1.9125 | Function Loss:  -2.2297\n",
      "Total loss:  -1.1478 | PDE Loss:  -1.9128 | Function Loss:  -2.2297\n",
      "Total loss:  -1.1479 | PDE Loss:  -1.9129 | Function Loss:  -2.2297\n",
      "Total loss:  -1.1479 | PDE Loss:  -1.9132 | Function Loss:  -2.2297\n",
      "Total loss:  -1.1479 | PDE Loss:  -1.9135 | Function Loss:  -2.2297\n",
      "Total loss:  -1.148 | PDE Loss:  -1.9137 | Function Loss:  -2.2297\n",
      "Total loss:  -1.148 | PDE Loss:  -1.9138 | Function Loss:  -2.2297\n",
      "Total loss:  -1.148 | PDE Loss:  -1.9141 | Function Loss:  -2.2297\n",
      "Total loss:  -1.148 | PDE Loss:  -1.9143 | Function Loss:  -2.2296\n",
      "Total loss:  -1.1481 | PDE Loss:  -1.9145 | Function Loss:  -2.2296\n",
      "Total loss:  -1.1481 | PDE Loss:  -1.9148 | Function Loss:  -2.2296\n",
      "Total loss:  -1.1481 | PDE Loss:  -1.9151 | Function Loss:  -2.2296\n",
      "Total loss:  -1.1481 | PDE Loss:  -1.9154 | Function Loss:  -2.2295\n",
      "Total loss:  -1.1481 | PDE Loss:  -1.9155 | Function Loss:  -2.2295\n",
      "Total loss:  -1.1482 | PDE Loss:  -1.9156 | Function Loss:  -2.2295\n",
      "Total loss:  -1.1482 | PDE Loss:  -1.9158 | Function Loss:  -2.2295\n",
      "Total loss:  -1.1482 | PDE Loss:  -1.9159 | Function Loss:  -2.2295\n",
      "Total loss:  -1.1482 | PDE Loss:  -1.9159 | Function Loss:  -2.2295\n",
      "Total loss:  -1.1483 | PDE Loss:  -1.916 | Function Loss:  -2.2296\n",
      "Total loss:  -1.1483 | PDE Loss:  -1.9161 | Function Loss:  -2.2296\n",
      "Total loss:  -1.1483 | PDE Loss:  -1.9161 | Function Loss:  -2.2296\n",
      "Total loss:  -1.1484 | PDE Loss:  -1.9161 | Function Loss:  -2.2297\n",
      "Total loss:  -1.1484 | PDE Loss:  -1.9162 | Function Loss:  -2.2297\n",
      "Total loss:  -1.1484 | PDE Loss:  -1.9163 | Function Loss:  -2.2297\n",
      "Total loss:  -1.1485 | PDE Loss:  -1.9164 | Function Loss:  -2.2297\n",
      "Total loss:  -1.1485 | PDE Loss:  -1.9165 | Function Loss:  -2.2297\n",
      "Total loss:  -1.1484 | PDE Loss:  -1.9166 | Function Loss:  -2.2296\n",
      "Total loss:  -1.1485 | PDE Loss:  -1.9166 | Function Loss:  -2.2297\n",
      "Total loss:  -1.1485 | PDE Loss:  -1.9166 | Function Loss:  -2.2297\n",
      "Total loss:  -1.1485 | PDE Loss:  -1.9167 | Function Loss:  -2.2297\n",
      "Total loss:  -1.1486 | PDE Loss:  -1.9168 | Function Loss:  -2.2298\n",
      "Total loss:  -1.1486 | PDE Loss:  -1.9168 | Function Loss:  -2.2298\n",
      "Total loss:  -1.1486 | PDE Loss:  -1.9169 | Function Loss:  -2.2298\n",
      "Total loss:  -1.1486 | PDE Loss:  -1.9169 | Function Loss:  -2.2298\n",
      "Total loss:  -1.1486 | PDE Loss:  -1.9169 | Function Loss:  -2.2298\n",
      "Total loss:  -1.1487 | PDE Loss:  -1.917 | Function Loss:  -2.2298\n",
      "Total loss:  -1.1487 | PDE Loss:  -1.917 | Function Loss:  -2.2298\n",
      "Total loss:  -1.1487 | PDE Loss:  -1.9171 | Function Loss:  -2.2299\n",
      "Total loss:  -1.1487 | PDE Loss:  -1.917 | Function Loss:  -2.2299\n",
      "Total loss:  -1.1488 | PDE Loss:  -1.917 | Function Loss:  -2.2299\n",
      "Total loss:  -1.1488 | PDE Loss:  -1.9171 | Function Loss:  -2.23\n",
      "Total loss:  -1.1488 | PDE Loss:  -1.917 | Function Loss:  -2.23\n",
      "Total loss:  -1.1488 | PDE Loss:  -1.917 | Function Loss:  -2.23\n",
      "Total loss:  -1.1489 | PDE Loss:  -1.917 | Function Loss:  -2.2301\n",
      "Total loss:  -1.1489 | PDE Loss:  -1.9171 | Function Loss:  -2.2301\n",
      "Total loss:  -1.1489 | PDE Loss:  -1.9165 | Function Loss:  -2.2302\n",
      "Total loss:  -1.1489 | PDE Loss:  -1.9167 | Function Loss:  -2.2302\n",
      "Total loss:  -1.1489 | PDE Loss:  -1.917 | Function Loss:  -2.2302\n",
      "Total loss:  -1.149 | PDE Loss:  -1.9172 | Function Loss:  -2.2302\n",
      "Total loss:  -1.149 | PDE Loss:  -1.9173 | Function Loss:  -2.2302\n",
      "Total loss:  -1.149 | PDE Loss:  -1.9174 | Function Loss:  -2.2302\n",
      "Total loss:  -1.149 | PDE Loss:  -1.9174 | Function Loss:  -2.2302\n",
      "Total loss:  -1.149 | PDE Loss:  -1.9175 | Function Loss:  -2.2302\n",
      "Total loss:  -1.149 | PDE Loss:  -1.9175 | Function Loss:  -2.2302\n",
      "Total loss:  -1.149 | PDE Loss:  -1.9176 | Function Loss:  -2.2302\n",
      "Total loss:  -1.1491 | PDE Loss:  -1.9176 | Function Loss:  -2.2302\n",
      "Total loss:  -1.1491 | PDE Loss:  -1.9176 | Function Loss:  -2.2302\n",
      "Total loss:  -1.1491 | PDE Loss:  -1.9177 | Function Loss:  -2.2302\n",
      "Total loss:  -1.1491 | PDE Loss:  -1.9177 | Function Loss:  -2.2302\n",
      "Total loss:  -1.1491 | PDE Loss:  -1.9178 | Function Loss:  -2.2302\n",
      "Total loss:  -1.1492 | PDE Loss:  -1.9178 | Function Loss:  -2.2303\n",
      "Total loss:  -1.1492 | PDE Loss:  -1.918 | Function Loss:  -2.2303\n",
      "Total loss:  -1.1492 | PDE Loss:  -1.918 | Function Loss:  -2.2303\n",
      "Total loss:  -1.1493 | PDE Loss:  -1.9181 | Function Loss:  -2.2303\n",
      "Total loss:  -1.1493 | PDE Loss:  -1.9182 | Function Loss:  -2.2304\n",
      "Total loss:  -1.1493 | PDE Loss:  -1.9183 | Function Loss:  -2.2304\n",
      "Total loss:  -1.1493 | PDE Loss:  -1.9183 | Function Loss:  -2.2304\n",
      "Total loss:  -1.1494 | PDE Loss:  -1.9183 | Function Loss:  -2.2304\n",
      "Total loss:  -1.1493 | PDE Loss:  -1.9187 | Function Loss:  -2.2303\n",
      "Total loss:  -1.1494 | PDE Loss:  -1.9185 | Function Loss:  -2.2304\n",
      "Total loss:  -1.1494 | PDE Loss:  -1.9185 | Function Loss:  -2.2304\n",
      "Total loss:  -1.1494 | PDE Loss:  -1.9185 | Function Loss:  -2.2305\n",
      "Total loss:  -1.1495 | PDE Loss:  -1.9185 | Function Loss:  -2.2305\n",
      "Total loss:  -1.1495 | PDE Loss:  -1.9186 | Function Loss:  -2.2305\n",
      "Total loss:  -1.1495 | PDE Loss:  -1.9186 | Function Loss:  -2.2306\n",
      "Total loss:  -1.1495 | PDE Loss:  -1.9187 | Function Loss:  -2.2306\n",
      "Total loss:  -1.1496 | PDE Loss:  -1.9188 | Function Loss:  -2.2306\n",
      "Total loss:  -1.1496 | PDE Loss:  -1.919 | Function Loss:  -2.2306\n",
      "Total loss:  -1.1496 | PDE Loss:  -1.9191 | Function Loss:  -2.2306\n",
      "Total loss:  -1.1497 | PDE Loss:  -1.9193 | Function Loss:  -2.2306\n",
      "Total loss:  -1.1497 | PDE Loss:  -1.9193 | Function Loss:  -2.2306\n",
      "Total loss:  -1.1497 | PDE Loss:  -1.9193 | Function Loss:  -2.2306\n",
      "Total loss:  -1.1498 | PDE Loss:  -1.9192 | Function Loss:  -2.2307\n",
      "Total loss:  -1.1498 | PDE Loss:  -1.9191 | Function Loss:  -2.2307\n",
      "Total loss:  -1.1498 | PDE Loss:  -1.919 | Function Loss:  -2.2308\n",
      "Total loss:  -1.1499 | PDE Loss:  -1.9189 | Function Loss:  -2.2309\n",
      "Total loss:  -1.1499 | PDE Loss:  -1.9187 | Function Loss:  -2.231\n",
      "Total loss:  -1.15 | PDE Loss:  -1.9185 | Function Loss:  -2.2311\n",
      "Total loss:  -1.15 | PDE Loss:  -1.9185 | Function Loss:  -2.2311\n",
      "Total loss:  -1.15 | PDE Loss:  -1.9187 | Function Loss:  -2.2311\n",
      "Total loss:  -1.15 | PDE Loss:  -1.9188 | Function Loss:  -2.2311\n",
      "Total loss:  -1.15 | PDE Loss:  -1.919 | Function Loss:  -2.2311\n",
      "Total loss:  -1.15 | PDE Loss:  -1.9191 | Function Loss:  -2.2311\n",
      "Total loss:  -1.1501 | PDE Loss:  -1.9191 | Function Loss:  -2.2311\n",
      "Total loss:  -1.1501 | PDE Loss:  -1.9193 | Function Loss:  -2.2311\n",
      "Total loss:  -1.1501 | PDE Loss:  -1.9196 | Function Loss:  -2.2311\n",
      "Total loss:  -1.1502 | PDE Loss:  -1.9198 | Function Loss:  -2.2311\n",
      "Total loss:  -1.1502 | PDE Loss:  -1.9199 | Function Loss:  -2.2311\n",
      "Total loss:  -1.1502 | PDE Loss:  -1.92 | Function Loss:  -2.2311\n",
      "Total loss:  -1.1502 | PDE Loss:  -1.92 | Function Loss:  -2.2311\n",
      "Total loss:  -1.1503 | PDE Loss:  -1.9201 | Function Loss:  -2.2311\n",
      "Total loss:  -1.1503 | PDE Loss:  -1.9202 | Function Loss:  -2.2311\n",
      "Total loss:  -1.1503 | PDE Loss:  -1.9203 | Function Loss:  -2.2311\n",
      "Total loss:  -1.1503 | PDE Loss:  -1.9204 | Function Loss:  -2.2311\n",
      "Total loss:  -1.1503 | PDE Loss:  -1.9206 | Function Loss:  -2.2311\n",
      "Total loss:  -1.1504 | PDE Loss:  -1.9208 | Function Loss:  -2.2311\n",
      "Total loss:  -1.1504 | PDE Loss:  -1.9209 | Function Loss:  -2.2311\n",
      "Total loss:  -1.1504 | PDE Loss:  -1.9211 | Function Loss:  -2.2311\n",
      "Total loss:  -1.1504 | PDE Loss:  -1.9212 | Function Loss:  -2.2311\n",
      "Total loss:  -1.1504 | PDE Loss:  -1.9214 | Function Loss:  -2.2311\n",
      "Total loss:  -1.1504 | PDE Loss:  -1.9215 | Function Loss:  -2.231\n",
      "Total loss:  -1.1505 | PDE Loss:  -1.9217 | Function Loss:  -2.231\n",
      "Total loss:  -1.1505 | PDE Loss:  -1.9219 | Function Loss:  -2.231\n",
      "Total loss:  -1.1505 | PDE Loss:  -1.922 | Function Loss:  -2.231\n",
      "Total loss:  -1.1505 | PDE Loss:  -1.9221 | Function Loss:  -2.231\n",
      "Total loss:  -1.1506 | PDE Loss:  -1.9222 | Function Loss:  -2.2311\n",
      "Total loss:  -1.1506 | PDE Loss:  -1.9222 | Function Loss:  -2.2311\n",
      "Total loss:  -1.1489 | PDE Loss:  -1.9173 | Function Loss:  -2.2301\n",
      "Total loss:  -1.1506 | PDE Loss:  -1.9222 | Function Loss:  -2.2311\n",
      "Total loss:  -1.1506 | PDE Loss:  -1.9221 | Function Loss:  -2.2311\n",
      "Total loss:  -1.1506 | PDE Loss:  -1.9221 | Function Loss:  -2.2312\n",
      "Total loss:  -1.1507 | PDE Loss:  -1.9219 | Function Loss:  -2.2312\n",
      "Total loss:  -1.1507 | PDE Loss:  -1.9218 | Function Loss:  -2.2313\n",
      "Total loss:  -1.1507 | PDE Loss:  -1.9215 | Function Loss:  -2.2314\n",
      "Total loss:  -1.1507 | PDE Loss:  -1.9215 | Function Loss:  -2.2314\n",
      "Total loss:  -1.1507 | PDE Loss:  -1.9216 | Function Loss:  -2.2314\n",
      "Total loss:  -1.1508 | PDE Loss:  -1.9217 | Function Loss:  -2.2314\n",
      "Total loss:  -1.1508 | PDE Loss:  -1.9218 | Function Loss:  -2.2314\n",
      "Total loss:  -1.1508 | PDE Loss:  -1.9219 | Function Loss:  -2.2314\n",
      "Total loss:  -1.1509 | PDE Loss:  -1.922 | Function Loss:  -2.2315\n",
      "Total loss:  -1.1509 | PDE Loss:  -1.9221 | Function Loss:  -2.2315\n",
      "Total loss:  -1.1509 | PDE Loss:  -1.9222 | Function Loss:  -2.2315\n",
      "Total loss:  -1.1509 | PDE Loss:  -1.9222 | Function Loss:  -2.2315\n",
      "Total loss:  -1.151 | PDE Loss:  -1.9224 | Function Loss:  -2.2315\n",
      "Total loss:  -1.151 | PDE Loss:  -1.9224 | Function Loss:  -2.2316\n",
      "Total loss:  -1.1511 | PDE Loss:  -1.9226 | Function Loss:  -2.2316\n",
      "Total loss:  -1.1511 | PDE Loss:  -1.9226 | Function Loss:  -2.2316\n",
      "Total loss:  -1.1511 | PDE Loss:  -1.9227 | Function Loss:  -2.2316\n",
      "Total loss:  -1.1512 | PDE Loss:  -1.9227 | Function Loss:  -2.2317\n",
      "Total loss:  -1.1512 | PDE Loss:  -1.9227 | Function Loss:  -2.2317\n",
      "Total loss:  -1.1512 | PDE Loss:  -1.9227 | Function Loss:  -2.2317\n",
      "Total loss:  -1.1512 | PDE Loss:  -1.9231 | Function Loss:  -2.2317\n",
      "Total loss:  -1.1512 | PDE Loss:  -1.9229 | Function Loss:  -2.2317\n",
      "Total loss:  -1.1513 | PDE Loss:  -1.9227 | Function Loss:  -2.2318\n",
      "Total loss:  -1.1513 | PDE Loss:  -1.9225 | Function Loss:  -2.2319\n",
      "Total loss:  -1.1513 | PDE Loss:  -1.9224 | Function Loss:  -2.2319\n",
      "Total loss:  -1.1513 | PDE Loss:  -1.9225 | Function Loss:  -2.2319\n",
      "Total loss:  -1.1514 | PDE Loss:  -1.9224 | Function Loss:  -2.232\n",
      "Total loss:  -1.1514 | PDE Loss:  -1.9226 | Function Loss:  -2.232\n",
      "Total loss:  -1.1514 | PDE Loss:  -1.9228 | Function Loss:  -2.2319\n",
      "Total loss:  -1.1514 | PDE Loss:  -1.9232 | Function Loss:  -2.2319\n",
      "Total loss:  -1.1514 | PDE Loss:  -1.9236 | Function Loss:  -2.2318\n",
      "Total loss:  -1.1515 | PDE Loss:  -1.9238 | Function Loss:  -2.2318\n",
      "Total loss:  -1.1515 | PDE Loss:  -1.924 | Function Loss:  -2.2318\n",
      "Total loss:  -1.1515 | PDE Loss:  -1.9242 | Function Loss:  -2.2318\n",
      "Total loss:  -1.1515 | PDE Loss:  -1.9243 | Function Loss:  -2.2318\n",
      "Total loss:  -1.1515 | PDE Loss:  -1.9244 | Function Loss:  -2.2318\n",
      "Total loss:  -1.1515 | PDE Loss:  -1.9244 | Function Loss:  -2.2318\n",
      "Total loss:  -1.1516 | PDE Loss:  -1.9245 | Function Loss:  -2.2318\n",
      "Total loss:  -1.1516 | PDE Loss:  -1.9247 | Function Loss:  -2.2318\n",
      "Total loss:  -1.1516 | PDE Loss:  -1.9247 | Function Loss:  -2.2318\n",
      "Total loss:  -1.1516 | PDE Loss:  -1.9248 | Function Loss:  -2.2318\n",
      "Total loss:  -1.1516 | PDE Loss:  -1.925 | Function Loss:  -2.2317\n",
      "Total loss:  -1.1516 | PDE Loss:  -1.9249 | Function Loss:  -2.2318\n",
      "Total loss:  -1.1516 | PDE Loss:  -1.925 | Function Loss:  -2.2318\n",
      "Total loss:  -1.1517 | PDE Loss:  -1.9251 | Function Loss:  -2.2318\n",
      "Total loss:  -1.1517 | PDE Loss:  -1.9252 | Function Loss:  -2.2318\n",
      "Total loss:  -1.1517 | PDE Loss:  -1.9255 | Function Loss:  -2.2318\n",
      "Total loss:  -1.1517 | PDE Loss:  -1.9256 | Function Loss:  -2.2318\n",
      "Total loss:  -1.1518 | PDE Loss:  -1.9259 | Function Loss:  -2.2318\n",
      "Total loss:  -1.1518 | PDE Loss:  -1.9258 | Function Loss:  -2.2318\n",
      "Total loss:  -1.1518 | PDE Loss:  -1.9259 | Function Loss:  -2.2318\n",
      "Total loss:  -1.1518 | PDE Loss:  -1.926 | Function Loss:  -2.2318\n",
      "Total loss:  -1.1518 | PDE Loss:  -1.9261 | Function Loss:  -2.2318\n",
      "Total loss:  -1.1519 | PDE Loss:  -1.9261 | Function Loss:  -2.2318\n",
      "Total loss:  -1.1519 | PDE Loss:  -1.926 | Function Loss:  -2.2319\n",
      "Total loss:  -1.1519 | PDE Loss:  -1.9258 | Function Loss:  -2.232\n",
      "Total loss:  -1.152 | PDE Loss:  -1.9256 | Function Loss:  -2.232\n",
      "Total loss:  -1.152 | PDE Loss:  -1.9255 | Function Loss:  -2.2321\n",
      "Total loss:  -1.152 | PDE Loss:  -1.9254 | Function Loss:  -2.2321\n",
      "Total loss:  -1.152 | PDE Loss:  -1.9254 | Function Loss:  -2.2322\n",
      "Total loss:  -1.152 | PDE Loss:  -1.9253 | Function Loss:  -2.2322\n",
      "Total loss:  -1.1521 | PDE Loss:  -1.9254 | Function Loss:  -2.2322\n",
      "Total loss:  -1.1521 | PDE Loss:  -1.9256 | Function Loss:  -2.2322\n",
      "Total loss:  -1.1521 | PDE Loss:  -1.9258 | Function Loss:  -2.2322\n",
      "Total loss:  -1.1521 | PDE Loss:  -1.9261 | Function Loss:  -2.2321\n",
      "Total loss:  -1.1521 | PDE Loss:  -1.9263 | Function Loss:  -2.2321\n",
      "Total loss:  -1.1522 | PDE Loss:  -1.9264 | Function Loss:  -2.2321\n",
      "Total loss:  -1.1522 | PDE Loss:  -1.9268 | Function Loss:  -2.2321\n",
      "Total loss:  -1.1522 | PDE Loss:  -1.9268 | Function Loss:  -2.232\n",
      "Total loss:  -1.1522 | PDE Loss:  -1.9269 | Function Loss:  -2.2321\n",
      "Total loss:  -1.1522 | PDE Loss:  -1.927 | Function Loss:  -2.2321\n",
      "Total loss:  -1.1523 | PDE Loss:  -1.9271 | Function Loss:  -2.2321\n",
      "Total loss:  -1.1523 | PDE Loss:  -1.9271 | Function Loss:  -2.2321\n",
      "Total loss:  -1.1523 | PDE Loss:  -1.9272 | Function Loss:  -2.2321\n",
      "Total loss:  -1.1523 | PDE Loss:  -1.9272 | Function Loss:  -2.2321\n",
      "Total loss:  -1.1523 | PDE Loss:  -1.9274 | Function Loss:  -2.2321\n",
      "Total loss:  -1.1523 | PDE Loss:  -1.9273 | Function Loss:  -2.2322\n",
      "Total loss:  -1.1524 | PDE Loss:  -1.9273 | Function Loss:  -2.2322\n",
      "Total loss:  -1.1524 | PDE Loss:  -1.9273 | Function Loss:  -2.2322\n",
      "Total loss:  -1.1524 | PDE Loss:  -1.9273 | Function Loss:  -2.2322\n",
      "Total loss:  -1.1524 | PDE Loss:  -1.9274 | Function Loss:  -2.2322\n",
      "Total loss:  -1.1524 | PDE Loss:  -1.9276 | Function Loss:  -2.2322\n",
      "Total loss:  -1.1524 | PDE Loss:  -1.9277 | Function Loss:  -2.2322\n",
      "Total loss:  -1.1524 | PDE Loss:  -1.9281 | Function Loss:  -2.2321\n",
      "Total loss:  -1.1525 | PDE Loss:  -1.9282 | Function Loss:  -2.2321\n",
      "Total loss:  -1.1525 | PDE Loss:  -1.9282 | Function Loss:  -2.2321\n",
      "Total loss:  -1.1525 | PDE Loss:  -1.9283 | Function Loss:  -2.2322\n",
      "Total loss:  -1.1525 | PDE Loss:  -1.9282 | Function Loss:  -2.2322\n",
      "Total loss:  -1.1525 | PDE Loss:  -1.9282 | Function Loss:  -2.2322\n",
      "Total loss:  -1.1526 | PDE Loss:  -1.9281 | Function Loss:  -2.2323\n",
      "Total loss:  -1.1525 | PDE Loss:  -1.9277 | Function Loss:  -2.2323\n",
      "Total loss:  -1.1526 | PDE Loss:  -1.9281 | Function Loss:  -2.2323\n",
      "Total loss:  -1.1526 | PDE Loss:  -1.9279 | Function Loss:  -2.2323\n",
      "Total loss:  -1.1526 | PDE Loss:  -1.9278 | Function Loss:  -2.2324\n",
      "Total loss:  -1.1526 | PDE Loss:  -1.9276 | Function Loss:  -2.2325\n",
      "Total loss:  -1.1527 | PDE Loss:  -1.9275 | Function Loss:  -2.2325\n",
      "Total loss:  -1.1527 | PDE Loss:  -1.9275 | Function Loss:  -2.2325\n",
      "Total loss:  -1.1527 | PDE Loss:  -1.9275 | Function Loss:  -2.2326\n",
      "Total loss:  -1.1527 | PDE Loss:  -1.9275 | Function Loss:  -2.2326\n",
      "Total loss:  -1.1527 | PDE Loss:  -1.9275 | Function Loss:  -2.2326\n",
      "Total loss:  -1.1528 | PDE Loss:  -1.9276 | Function Loss:  -2.2326\n",
      "Total loss:  -1.1528 | PDE Loss:  -1.9276 | Function Loss:  -2.2326\n",
      "Total loss:  -1.1528 | PDE Loss:  -1.9276 | Function Loss:  -2.2327\n",
      "Total loss:  -1.1528 | PDE Loss:  -1.9277 | Function Loss:  -2.2327\n",
      "Total loss:  -1.1529 | PDE Loss:  -1.9277 | Function Loss:  -2.2327\n",
      "Total loss:  -1.1529 | PDE Loss:  -1.9276 | Function Loss:  -2.2328\n",
      "Total loss:  -1.1529 | PDE Loss:  -1.9277 | Function Loss:  -2.2328\n",
      "Total loss:  -1.153 | PDE Loss:  -1.9275 | Function Loss:  -2.2329\n",
      "Total loss:  -1.153 | PDE Loss:  -1.9274 | Function Loss:  -2.2329\n",
      "Total loss:  -1.153 | PDE Loss:  -1.9273 | Function Loss:  -2.233\n",
      "Total loss:  -1.1531 | PDE Loss:  -1.9272 | Function Loss:  -2.233\n",
      "Total loss:  -1.1531 | PDE Loss:  -1.9272 | Function Loss:  -2.2331\n",
      "Total loss:  -1.1531 | PDE Loss:  -1.9271 | Function Loss:  -2.2331\n",
      "Total loss:  -1.1531 | PDE Loss:  -1.927 | Function Loss:  -2.2331\n",
      "Total loss:  -1.1531 | PDE Loss:  -1.927 | Function Loss:  -2.2332\n",
      "Total loss:  -1.1532 | PDE Loss:  -1.9268 | Function Loss:  -2.2333\n",
      "Total loss:  -1.1532 | PDE Loss:  -1.9268 | Function Loss:  -2.2333\n",
      "Total loss:  -1.1532 | PDE Loss:  -1.9269 | Function Loss:  -2.2333\n",
      "Total loss:  -1.1532 | PDE Loss:  -1.927 | Function Loss:  -2.2333\n",
      "Total loss:  -1.1532 | PDE Loss:  -1.927 | Function Loss:  -2.2333\n",
      "Total loss:  -1.1532 | PDE Loss:  -1.9271 | Function Loss:  -2.2333\n",
      "Total loss:  -1.1533 | PDE Loss:  -1.9272 | Function Loss:  -2.2333\n",
      "Total loss:  -1.1533 | PDE Loss:  -1.9273 | Function Loss:  -2.2333\n",
      "Total loss:  -1.1533 | PDE Loss:  -1.9274 | Function Loss:  -2.2333\n",
      "Total loss:  -1.1533 | PDE Loss:  -1.9274 | Function Loss:  -2.2333\n",
      "Total loss:  -1.1534 | PDE Loss:  -1.9273 | Function Loss:  -2.2334\n",
      "Total loss:  -1.1534 | PDE Loss:  -1.9273 | Function Loss:  -2.2334\n",
      "Total loss:  -1.1534 | PDE Loss:  -1.9273 | Function Loss:  -2.2334\n",
      "Total loss:  -1.1534 | PDE Loss:  -1.9273 | Function Loss:  -2.2334\n",
      "Total loss:  -1.1534 | PDE Loss:  -1.9273 | Function Loss:  -2.2335\n",
      "Total loss:  -1.1534 | PDE Loss:  -1.9273 | Function Loss:  -2.2335\n",
      "Total loss:  -1.1535 | PDE Loss:  -1.9272 | Function Loss:  -2.2335\n",
      "Total loss:  -1.1535 | PDE Loss:  -1.9273 | Function Loss:  -2.2336\n",
      "Total loss:  -1.1535 | PDE Loss:  -1.9273 | Function Loss:  -2.2336\n",
      "Total loss:  -1.1536 | PDE Loss:  -1.9274 | Function Loss:  -2.2336\n",
      "Total loss:  -1.1534 | PDE Loss:  -1.9274 | Function Loss:  -2.2334\n",
      "Total loss:  -1.1536 | PDE Loss:  -1.9275 | Function Loss:  -2.2336\n",
      "Total loss:  -1.1536 | PDE Loss:  -1.9275 | Function Loss:  -2.2337\n",
      "Total loss:  -1.1537 | PDE Loss:  -1.9276 | Function Loss:  -2.2337\n",
      "Total loss:  -1.1537 | PDE Loss:  -1.9276 | Function Loss:  -2.2337\n",
      "Total loss:  -1.1537 | PDE Loss:  -1.9276 | Function Loss:  -2.2338\n",
      "Total loss:  -1.1537 | PDE Loss:  -1.9274 | Function Loss:  -2.2338\n",
      "Total loss:  -1.1538 | PDE Loss:  -1.9275 | Function Loss:  -2.2339\n",
      "Total loss:  -1.1538 | PDE Loss:  -1.9275 | Function Loss:  -2.2339\n",
      "Total loss:  -1.1538 | PDE Loss:  -1.9277 | Function Loss:  -2.2339\n",
      "Total loss:  -1.1539 | PDE Loss:  -1.9277 | Function Loss:  -2.2339\n",
      "Total loss:  -1.1539 | PDE Loss:  -1.9278 | Function Loss:  -2.2339\n",
      "Total loss:  -1.1539 | PDE Loss:  -1.9278 | Function Loss:  -2.2339\n",
      "Total loss:  -1.1539 | PDE Loss:  -1.9279 | Function Loss:  -2.2339\n",
      "Total loss:  -1.1539 | PDE Loss:  -1.9279 | Function Loss:  -2.234\n",
      "Total loss:  -1.154 | PDE Loss:  -1.928 | Function Loss:  -2.234\n",
      "Total loss:  -1.154 | PDE Loss:  -1.9282 | Function Loss:  -2.234\n",
      "Total loss:  -1.1541 | PDE Loss:  -1.9284 | Function Loss:  -2.234\n",
      "Total loss:  -1.1541 | PDE Loss:  -1.9287 | Function Loss:  -2.234\n",
      "Total loss:  -1.1542 | PDE Loss:  -1.929 | Function Loss:  -2.234\n",
      "Total loss:  -1.1542 | PDE Loss:  -1.9291 | Function Loss:  -2.234\n",
      "Total loss:  -1.1542 | PDE Loss:  -1.9292 | Function Loss:  -2.234\n",
      "Total loss:  -1.1542 | PDE Loss:  -1.9293 | Function Loss:  -2.234\n",
      "Total loss:  -1.1543 | PDE Loss:  -1.9294 | Function Loss:  -2.234\n",
      "Total loss:  -1.1543 | PDE Loss:  -1.9294 | Function Loss:  -2.2341\n",
      "Total loss:  -1.1543 | PDE Loss:  -1.9298 | Function Loss:  -2.234\n",
      "Total loss:  -1.1544 | PDE Loss:  -1.9298 | Function Loss:  -2.2341\n",
      "Total loss:  -1.1544 | PDE Loss:  -1.9297 | Function Loss:  -2.2342\n",
      "Total loss:  -1.1545 | PDE Loss:  -1.9296 | Function Loss:  -2.2343\n",
      "Total loss:  -1.1546 | PDE Loss:  -1.9291 | Function Loss:  -2.2345\n",
      "Total loss:  -1.1546 | PDE Loss:  -1.9292 | Function Loss:  -2.2345\n",
      "Total loss:  -1.1546 | PDE Loss:  -1.9294 | Function Loss:  -2.2345\n",
      "Total loss:  -1.1547 | PDE Loss:  -1.9295 | Function Loss:  -2.2345\n",
      "Total loss:  -1.1547 | PDE Loss:  -1.9296 | Function Loss:  -2.2345\n",
      "Total loss:  -1.1548 | PDE Loss:  -1.9299 | Function Loss:  -2.2346\n",
      "Total loss:  -1.1548 | PDE Loss:  -1.9302 | Function Loss:  -2.2346\n",
      "Total loss:  -1.1548 | PDE Loss:  -1.9301 | Function Loss:  -2.2346\n",
      "Total loss:  -1.1548 | PDE Loss:  -1.9302 | Function Loss:  -2.2346\n",
      "Total loss:  -1.1549 | PDE Loss:  -1.9305 | Function Loss:  -2.2346\n",
      "Total loss:  -1.1549 | PDE Loss:  -1.9307 | Function Loss:  -2.2346\n",
      "Total loss:  -1.155 | PDE Loss:  -1.931 | Function Loss:  -2.2346\n",
      "Total loss:  -1.155 | PDE Loss:  -1.9311 | Function Loss:  -2.2346\n",
      "Total loss:  -1.155 | PDE Loss:  -1.9312 | Function Loss:  -2.2346\n",
      "Total loss:  -1.155 | PDE Loss:  -1.9313 | Function Loss:  -2.2346\n",
      "Total loss:  -1.1551 | PDE Loss:  -1.9314 | Function Loss:  -2.2346\n",
      "Total loss:  -1.1551 | PDE Loss:  -1.9315 | Function Loss:  -2.2346\n",
      "Total loss:  -1.1551 | PDE Loss:  -1.9311 | Function Loss:  -2.2348\n",
      "Total loss:  -1.1552 | PDE Loss:  -1.9312 | Function Loss:  -2.2348\n",
      "Total loss:  -1.1552 | PDE Loss:  -1.9313 | Function Loss:  -2.2348\n",
      "Total loss:  -1.1552 | PDE Loss:  -1.9314 | Function Loss:  -2.2348\n",
      "Total loss:  -1.1553 | PDE Loss:  -1.9314 | Function Loss:  -2.2348\n",
      "Total loss:  -1.1553 | PDE Loss:  -1.9313 | Function Loss:  -2.2349\n",
      "Total loss:  -1.1553 | PDE Loss:  -1.9313 | Function Loss:  -2.2349\n",
      "Total loss:  -1.1553 | PDE Loss:  -1.9312 | Function Loss:  -2.2349\n",
      "Total loss:  -1.1553 | PDE Loss:  -1.9312 | Function Loss:  -2.235\n",
      "Total loss:  -1.1553 | PDE Loss:  -1.9312 | Function Loss:  -2.235\n",
      "Total loss:  -1.1554 | PDE Loss:  -1.9312 | Function Loss:  -2.235\n",
      "Total loss:  -1.1554 | PDE Loss:  -1.9312 | Function Loss:  -2.235\n",
      "Total loss:  -1.1554 | PDE Loss:  -1.9312 | Function Loss:  -2.2351\n",
      "Total loss:  -1.1554 | PDE Loss:  -1.9313 | Function Loss:  -2.2351\n",
      "Total loss:  -1.1555 | PDE Loss:  -1.9314 | Function Loss:  -2.2351\n",
      "Total loss:  -1.1555 | PDE Loss:  -1.9313 | Function Loss:  -2.2352\n",
      "Total loss:  -1.1555 | PDE Loss:  -1.9314 | Function Loss:  -2.2352\n",
      "Total loss:  -1.1555 | PDE Loss:  -1.9314 | Function Loss:  -2.2352\n",
      "Total loss:  -1.1556 | PDE Loss:  -1.9316 | Function Loss:  -2.2352\n",
      "Total loss:  -1.1556 | PDE Loss:  -1.9316 | Function Loss:  -2.2352\n",
      "Total loss:  -1.1556 | PDE Loss:  -1.9317 | Function Loss:  -2.2352\n",
      "Total loss:  -1.1556 | PDE Loss:  -1.9319 | Function Loss:  -2.2352\n",
      "Total loss:  -1.1556 | PDE Loss:  -1.9321 | Function Loss:  -2.2352\n",
      "Total loss:  -1.1557 | PDE Loss:  -1.9322 | Function Loss:  -2.2352\n",
      "Total loss:  -1.1557 | PDE Loss:  -1.9324 | Function Loss:  -2.2352\n",
      "Total loss:  -1.1557 | PDE Loss:  -1.9324 | Function Loss:  -2.2352\n",
      "Total loss:  -1.1557 | PDE Loss:  -1.9325 | Function Loss:  -2.2352\n",
      "Total loss:  -1.1557 | PDE Loss:  -1.9325 | Function Loss:  -2.2352\n",
      "Total loss:  -1.1557 | PDE Loss:  -1.9326 | Function Loss:  -2.2352\n",
      "Total loss:  -1.1558 | PDE Loss:  -1.9326 | Function Loss:  -2.2352\n",
      "Total loss:  -1.1558 | PDE Loss:  -1.9327 | Function Loss:  -2.2352\n",
      "Total loss:  -1.1558 | PDE Loss:  -1.9328 | Function Loss:  -2.2352\n",
      "Total loss:  -1.1558 | PDE Loss:  -1.9329 | Function Loss:  -2.2352\n",
      "Total loss:  -1.1559 | PDE Loss:  -1.9332 | Function Loss:  -2.2352\n",
      "Total loss:  -1.1559 | PDE Loss:  -1.9333 | Function Loss:  -2.2352\n",
      "Total loss:  -1.1559 | PDE Loss:  -1.9334 | Function Loss:  -2.2352\n",
      "Total loss:  -1.1559 | PDE Loss:  -1.9335 | Function Loss:  -2.2352\n",
      "Total loss:  -1.156 | PDE Loss:  -1.9335 | Function Loss:  -2.2352\n",
      "Total loss:  -1.156 | PDE Loss:  -1.9336 | Function Loss:  -2.2353\n",
      "Total loss:  -1.156 | PDE Loss:  -1.9338 | Function Loss:  -2.2353\n",
      "Total loss:  -1.156 | PDE Loss:  -1.9338 | Function Loss:  -2.2353\n",
      "Total loss:  -1.1561 | PDE Loss:  -1.9339 | Function Loss:  -2.2353\n",
      "Total loss:  -1.1561 | PDE Loss:  -1.934 | Function Loss:  -2.2353\n",
      "Total loss:  -1.1562 | PDE Loss:  -1.9341 | Function Loss:  -2.2354\n",
      "Total loss:  -1.1562 | PDE Loss:  -1.9342 | Function Loss:  -2.2354\n",
      "Total loss:  -1.1562 | PDE Loss:  -1.9343 | Function Loss:  -2.2354\n",
      "Total loss:  -1.1562 | PDE Loss:  -1.9344 | Function Loss:  -2.2354\n",
      "Total loss:  -1.1562 | PDE Loss:  -1.9345 | Function Loss:  -2.2354\n",
      "Total loss:  -1.1563 | PDE Loss:  -1.9346 | Function Loss:  -2.2354\n",
      "Total loss:  -1.1563 | PDE Loss:  -1.9348 | Function Loss:  -2.2354\n",
      "Total loss:  -1.1563 | PDE Loss:  -1.9349 | Function Loss:  -2.2354\n",
      "Total loss:  -1.1563 | PDE Loss:  -1.9352 | Function Loss:  -2.2354\n",
      "Total loss:  -1.1564 | PDE Loss:  -1.9352 | Function Loss:  -2.2354\n",
      "Total loss:  -1.1564 | PDE Loss:  -1.9354 | Function Loss:  -2.2354\n",
      "Total loss:  -1.1564 | PDE Loss:  -1.9353 | Function Loss:  -2.2354\n",
      "Total loss:  -1.1564 | PDE Loss:  -1.9354 | Function Loss:  -2.2354\n",
      "Total loss:  -1.1565 | PDE Loss:  -1.9354 | Function Loss:  -2.2355\n",
      "Total loss:  -1.1565 | PDE Loss:  -1.9353 | Function Loss:  -2.2356\n",
      "Total loss:  -1.1565 | PDE Loss:  -1.9353 | Function Loss:  -2.2356\n",
      "Total loss:  -1.1565 | PDE Loss:  -1.9343 | Function Loss:  -2.2358\n",
      "Total loss:  -1.1566 | PDE Loss:  -1.9351 | Function Loss:  -2.2357\n",
      "Total loss:  -1.1566 | PDE Loss:  -1.9352 | Function Loss:  -2.2357\n",
      "Total loss:  -1.1566 | PDE Loss:  -1.9353 | Function Loss:  -2.2357\n",
      "Total loss:  -1.1566 | PDE Loss:  -1.9354 | Function Loss:  -2.2357\n",
      "Total loss:  -1.1566 | PDE Loss:  -1.9355 | Function Loss:  -2.2356\n",
      "Total loss:  -1.1566 | PDE Loss:  -1.9357 | Function Loss:  -2.2356\n",
      "Total loss:  -1.1566 | PDE Loss:  -1.9359 | Function Loss:  -2.2356\n",
      "Total loss:  -1.155 | PDE Loss:  -1.9313 | Function Loss:  -2.2346\n",
      "Total loss:  -1.1566 | PDE Loss:  -1.9359 | Function Loss:  -2.2356\n",
      "Total loss:  -1.1567 | PDE Loss:  -1.9361 | Function Loss:  -2.2356\n",
      "Total loss:  -1.1567 | PDE Loss:  -1.9365 | Function Loss:  -2.2355\n",
      "Total loss:  -1.1567 | PDE Loss:  -1.9367 | Function Loss:  -2.2355\n",
      "Total loss:  -1.1567 | PDE Loss:  -1.9369 | Function Loss:  -2.2355\n",
      "Total loss:  -1.1568 | PDE Loss:  -1.937 | Function Loss:  -2.2355\n",
      "Total loss:  -1.1568 | PDE Loss:  -1.937 | Function Loss:  -2.2356\n",
      "Total loss:  -1.1568 | PDE Loss:  -1.9369 | Function Loss:  -2.2356\n",
      "Total loss:  -1.1569 | PDE Loss:  -1.937 | Function Loss:  -2.2357\n",
      "Total loss:  -1.1569 | PDE Loss:  -1.9369 | Function Loss:  -2.2357\n",
      "Total loss:  -1.157 | PDE Loss:  -1.9371 | Function Loss:  -2.2357\n",
      "Total loss:  -1.157 | PDE Loss:  -1.937 | Function Loss:  -2.2358\n",
      "Total loss:  -1.1571 | PDE Loss:  -1.9369 | Function Loss:  -2.2359\n",
      "Total loss:  -1.1571 | PDE Loss:  -1.9369 | Function Loss:  -2.236\n",
      "Total loss:  -1.1571 | PDE Loss:  -1.937 | Function Loss:  -2.236\n",
      "Total loss:  -1.1572 | PDE Loss:  -1.9372 | Function Loss:  -2.236\n",
      "Total loss:  -1.1572 | PDE Loss:  -1.9373 | Function Loss:  -2.236\n",
      "Total loss:  -1.1572 | PDE Loss:  -1.9376 | Function Loss:  -2.236\n",
      "Total loss:  -1.1573 | PDE Loss:  -1.9374 | Function Loss:  -2.2361\n",
      "Total loss:  -1.1573 | PDE Loss:  -1.938 | Function Loss:  -2.236\n",
      "Total loss:  -1.1573 | PDE Loss:  -1.9382 | Function Loss:  -2.236\n",
      "Total loss:  -1.1574 | PDE Loss:  -1.9383 | Function Loss:  -2.236\n",
      "Total loss:  -1.1574 | PDE Loss:  -1.9384 | Function Loss:  -2.236\n",
      "Total loss:  -1.1574 | PDE Loss:  -1.9385 | Function Loss:  -2.236\n",
      "Total loss:  -1.1575 | PDE Loss:  -1.9384 | Function Loss:  -2.2361\n",
      "Total loss:  -1.1575 | PDE Loss:  -1.9384 | Function Loss:  -2.2361\n",
      "Total loss:  -1.1575 | PDE Loss:  -1.9385 | Function Loss:  -2.2361\n",
      "Total loss:  -1.1575 | PDE Loss:  -1.9386 | Function Loss:  -2.2361\n",
      "Total loss:  -1.1576 | PDE Loss:  -1.9388 | Function Loss:  -2.2361\n",
      "Total loss:  -1.1576 | PDE Loss:  -1.9389 | Function Loss:  -2.2362\n",
      "Total loss:  -1.1576 | PDE Loss:  -1.939 | Function Loss:  -2.2361\n",
      "Total loss:  -1.1576 | PDE Loss:  -1.9391 | Function Loss:  -2.2361\n",
      "Total loss:  -1.1576 | PDE Loss:  -1.9392 | Function Loss:  -2.2361\n",
      "Total loss:  -1.1576 | PDE Loss:  -1.9393 | Function Loss:  -2.2361\n",
      "Total loss:  -1.1577 | PDE Loss:  -1.9393 | Function Loss:  -2.2361\n",
      "Total loss:  -1.1577 | PDE Loss:  -1.9394 | Function Loss:  -2.2361\n",
      "Total loss:  -1.1577 | PDE Loss:  -1.9394 | Function Loss:  -2.2361\n",
      "Total loss:  -1.1577 | PDE Loss:  -1.9394 | Function Loss:  -2.2362\n",
      "Total loss:  -1.1577 | PDE Loss:  -1.9395 | Function Loss:  -2.2362\n",
      "Total loss:  -1.1577 | PDE Loss:  -1.9394 | Function Loss:  -2.2362\n",
      "Total loss:  -1.1577 | PDE Loss:  -1.9395 | Function Loss:  -2.2362\n",
      "Total loss:  -1.1578 | PDE Loss:  -1.9397 | Function Loss:  -2.2363\n",
      "Total loss:  -1.1579 | PDE Loss:  -1.9397 | Function Loss:  -2.2363\n",
      "Total loss:  -1.1579 | PDE Loss:  -1.9399 | Function Loss:  -2.2363\n",
      "Total loss:  -1.1579 | PDE Loss:  -1.9399 | Function Loss:  -2.2363\n",
      "Total loss:  -1.1579 | PDE Loss:  -1.94 | Function Loss:  -2.2363\n",
      "Total loss:  -1.1579 | PDE Loss:  -1.94 | Function Loss:  -2.2363\n",
      "Total loss:  -1.158 | PDE Loss:  -1.9401 | Function Loss:  -2.2363\n",
      "Total loss:  -1.158 | PDE Loss:  -1.9402 | Function Loss:  -2.2364\n",
      "Total loss:  -1.158 | PDE Loss:  -1.9403 | Function Loss:  -2.2364\n",
      "Total loss:  -1.158 | PDE Loss:  -1.9402 | Function Loss:  -2.2364\n",
      "Total loss:  -1.158 | PDE Loss:  -1.9403 | Function Loss:  -2.2364\n",
      "Total loss:  -1.1581 | PDE Loss:  -1.9404 | Function Loss:  -2.2364\n",
      "Total loss:  -1.1581 | PDE Loss:  -1.9406 | Function Loss:  -2.2365\n",
      "Total loss:  -1.1582 | PDE Loss:  -1.9407 | Function Loss:  -2.2365\n",
      "Total loss:  -1.1582 | PDE Loss:  -1.9408 | Function Loss:  -2.2365\n",
      "Total loss:  -1.1582 | PDE Loss:  -1.9409 | Function Loss:  -2.2365\n",
      "Total loss:  -1.1582 | PDE Loss:  -1.941 | Function Loss:  -2.2365\n",
      "Total loss:  -1.1582 | PDE Loss:  -1.9411 | Function Loss:  -2.2365\n",
      "Total loss:  -1.1582 | PDE Loss:  -1.9412 | Function Loss:  -2.2365\n",
      "Total loss:  -1.1583 | PDE Loss:  -1.9413 | Function Loss:  -2.2365\n",
      "Total loss:  -1.1583 | PDE Loss:  -1.9413 | Function Loss:  -2.2365\n",
      "Total loss:  -1.1583 | PDE Loss:  -1.9413 | Function Loss:  -2.2365\n",
      "Total loss:  -1.1583 | PDE Loss:  -1.941 | Function Loss:  -2.2365\n",
      "Total loss:  -1.1583 | PDE Loss:  -1.9413 | Function Loss:  -2.2365\n",
      "Total loss:  -1.1583 | PDE Loss:  -1.9412 | Function Loss:  -2.2366\n",
      "Total loss:  -1.1583 | PDE Loss:  -1.9412 | Function Loss:  -2.2366\n",
      "Total loss:  -1.1584 | PDE Loss:  -1.9411 | Function Loss:  -2.2366\n",
      "Total loss:  -1.1584 | PDE Loss:  -1.9411 | Function Loss:  -2.2367\n",
      "Total loss:  -1.1584 | PDE Loss:  -1.941 | Function Loss:  -2.2367\n",
      "Total loss:  -1.1584 | PDE Loss:  -1.941 | Function Loss:  -2.2367\n",
      "Total loss:  -1.1584 | PDE Loss:  -1.941 | Function Loss:  -2.2367\n",
      "Total loss:  -1.1585 | PDE Loss:  -1.9411 | Function Loss:  -2.2367\n",
      "Total loss:  -1.1585 | PDE Loss:  -1.9412 | Function Loss:  -2.2368\n",
      "Total loss:  -1.1585 | PDE Loss:  -1.9417 | Function Loss:  -2.2366\n",
      "Total loss:  -1.1585 | PDE Loss:  -1.9415 | Function Loss:  -2.2367\n",
      "Total loss:  -1.1585 | PDE Loss:  -1.9415 | Function Loss:  -2.2367\n",
      "Total loss:  -1.1585 | PDE Loss:  -1.9415 | Function Loss:  -2.2368\n",
      "Total loss:  -1.1586 | PDE Loss:  -1.9416 | Function Loss:  -2.2368\n",
      "Total loss:  -1.1586 | PDE Loss:  -1.9416 | Function Loss:  -2.2368\n",
      "Total loss:  -1.1586 | PDE Loss:  -1.9416 | Function Loss:  -2.2368\n",
      "Total loss:  -1.1586 | PDE Loss:  -1.9416 | Function Loss:  -2.2368\n",
      "Total loss:  -1.1586 | PDE Loss:  -1.9415 | Function Loss:  -2.2369\n",
      "Total loss:  -1.1587 | PDE Loss:  -1.9415 | Function Loss:  -2.2369\n",
      "Total loss:  -1.1587 | PDE Loss:  -1.9413 | Function Loss:  -2.237\n",
      "Total loss:  -1.1587 | PDE Loss:  -1.9413 | Function Loss:  -2.237\n",
      "Total loss:  -1.1587 | PDE Loss:  -1.9413 | Function Loss:  -2.237\n",
      "Total loss:  -1.1587 | PDE Loss:  -1.9414 | Function Loss:  -2.237\n",
      "Total loss:  -1.1588 | PDE Loss:  -1.9414 | Function Loss:  -2.2371\n",
      "Total loss:  -1.1588 | PDE Loss:  -1.9413 | Function Loss:  -2.2371\n",
      "Total loss:  -1.1588 | PDE Loss:  -1.9412 | Function Loss:  -2.2371\n",
      "Total loss:  -1.1588 | PDE Loss:  -1.9412 | Function Loss:  -2.2372\n",
      "Total loss:  -1.1588 | PDE Loss:  -1.9412 | Function Loss:  -2.2372\n",
      "Total loss:  -1.1589 | PDE Loss:  -1.9411 | Function Loss:  -2.2372\n",
      "Total loss:  -1.1589 | PDE Loss:  -1.941 | Function Loss:  -2.2372\n",
      "Total loss:  -1.1589 | PDE Loss:  -1.941 | Function Loss:  -2.2373\n",
      "Total loss:  -1.1589 | PDE Loss:  -1.941 | Function Loss:  -2.2373\n",
      "Total loss:  -1.1589 | PDE Loss:  -1.941 | Function Loss:  -2.2373\n",
      "Total loss:  -1.159 | PDE Loss:  -1.9408 | Function Loss:  -2.2374\n",
      "Total loss:  -1.159 | PDE Loss:  -1.9409 | Function Loss:  -2.2374\n",
      "Total loss:  -1.159 | PDE Loss:  -1.941 | Function Loss:  -2.2374\n",
      "Total loss:  -1.159 | PDE Loss:  -1.9411 | Function Loss:  -2.2374\n",
      "Total loss:  -1.159 | PDE Loss:  -1.9412 | Function Loss:  -2.2374\n",
      "Total loss:  -1.1591 | PDE Loss:  -1.9414 | Function Loss:  -2.2374\n",
      "Total loss:  -1.1591 | PDE Loss:  -1.9416 | Function Loss:  -2.2374\n",
      "Total loss:  -1.1591 | PDE Loss:  -1.9417 | Function Loss:  -2.2374\n",
      "Total loss:  -1.1591 | PDE Loss:  -1.9419 | Function Loss:  -2.2374\n",
      "Total loss:  -1.1592 | PDE Loss:  -1.9421 | Function Loss:  -2.2374\n",
      "Total loss:  -1.1592 | PDE Loss:  -1.9424 | Function Loss:  -2.2374\n",
      "Total loss:  -1.1592 | PDE Loss:  -1.9425 | Function Loss:  -2.2374\n",
      "Total loss:  -1.1592 | PDE Loss:  -1.9425 | Function Loss:  -2.2374\n",
      "Total loss:  -1.1593 | PDE Loss:  -1.9425 | Function Loss:  -2.2374\n",
      "Total loss:  -1.1593 | PDE Loss:  -1.9424 | Function Loss:  -2.2375\n",
      "Total loss:  -1.1592 | PDE Loss:  -1.9426 | Function Loss:  -2.2373\n",
      "Total loss:  -1.1593 | PDE Loss:  -1.9426 | Function Loss:  -2.2375\n",
      "Total loss:  -1.1593 | PDE Loss:  -1.9424 | Function Loss:  -2.2376\n",
      "Total loss:  -1.1594 | PDE Loss:  -1.9422 | Function Loss:  -2.2376\n",
      "Total loss:  -1.1594 | PDE Loss:  -1.942 | Function Loss:  -2.2377\n",
      "Total loss:  -1.1595 | PDE Loss:  -1.9418 | Function Loss:  -2.2378\n",
      "Total loss:  -1.1595 | PDE Loss:  -1.9417 | Function Loss:  -2.2379\n",
      "Total loss:  -1.1595 | PDE Loss:  -1.9417 | Function Loss:  -2.2379\n",
      "Total loss:  -1.1596 | PDE Loss:  -1.9417 | Function Loss:  -2.238\n",
      "Total loss:  -1.1596 | PDE Loss:  -1.9418 | Function Loss:  -2.238\n",
      "Total loss:  -1.1597 | PDE Loss:  -1.9419 | Function Loss:  -2.238\n",
      "Total loss:  -1.1597 | PDE Loss:  -1.942 | Function Loss:  -2.238\n",
      "Total loss:  -1.1597 | PDE Loss:  -1.942 | Function Loss:  -2.2381\n",
      "Total loss:  -1.1598 | PDE Loss:  -1.9422 | Function Loss:  -2.2381\n",
      "Total loss:  -1.1598 | PDE Loss:  -1.9424 | Function Loss:  -2.2381\n",
      "Total loss:  -1.1598 | PDE Loss:  -1.9425 | Function Loss:  -2.2381\n",
      "Total loss:  -1.1599 | PDE Loss:  -1.9425 | Function Loss:  -2.2381\n",
      "Total loss:  -1.1599 | PDE Loss:  -1.9425 | Function Loss:  -2.2382\n",
      "Total loss:  -1.1599 | PDE Loss:  -1.9424 | Function Loss:  -2.2383\n",
      "Total loss:  -1.16 | PDE Loss:  -1.9421 | Function Loss:  -2.2384\n",
      "Total loss:  -1.16 | PDE Loss:  -1.942 | Function Loss:  -2.2385\n",
      "Total loss:  -1.1601 | PDE Loss:  -1.942 | Function Loss:  -2.2385\n",
      "Total loss:  -1.1601 | PDE Loss:  -1.942 | Function Loss:  -2.2385\n",
      "Total loss:  -1.1601 | PDE Loss:  -1.942 | Function Loss:  -2.2385\n",
      "Total loss:  -1.1601 | PDE Loss:  -1.942 | Function Loss:  -2.2386\n",
      "Total loss:  -1.1601 | PDE Loss:  -1.9421 | Function Loss:  -2.2386\n",
      "Total loss:  -1.1601 | PDE Loss:  -1.9423 | Function Loss:  -2.2385\n",
      "Total loss:  -1.1602 | PDE Loss:  -1.9423 | Function Loss:  -2.2386\n",
      "Total loss:  -1.1602 | PDE Loss:  -1.9424 | Function Loss:  -2.2386\n",
      "Total loss:  -1.1602 | PDE Loss:  -1.9424 | Function Loss:  -2.2386\n",
      "Total loss:  -1.1602 | PDE Loss:  -1.9424 | Function Loss:  -2.2386\n",
      "Total loss:  -1.1602 | PDE Loss:  -1.9424 | Function Loss:  -2.2386\n",
      "Total loss:  -1.1602 | PDE Loss:  -1.9424 | Function Loss:  -2.2386\n",
      "Total loss:  -1.1602 | PDE Loss:  -1.9424 | Function Loss:  -2.2386\n",
      "Total loss:  -1.1603 | PDE Loss:  -1.9424 | Function Loss:  -2.2386\n",
      "Total loss:  -1.1603 | PDE Loss:  -1.9423 | Function Loss:  -2.2387\n",
      "Total loss:  -1.1603 | PDE Loss:  -1.9427 | Function Loss:  -2.2386\n",
      "Total loss:  -1.1603 | PDE Loss:  -1.9425 | Function Loss:  -2.2387\n",
      "Total loss:  -1.1603 | PDE Loss:  -1.9424 | Function Loss:  -2.2387\n",
      "Total loss:  -1.1604 | PDE Loss:  -1.9423 | Function Loss:  -2.2388\n",
      "Total loss:  -1.1604 | PDE Loss:  -1.942 | Function Loss:  -2.2389\n",
      "Total loss:  -1.1604 | PDE Loss:  -1.9421 | Function Loss:  -2.2389\n",
      "Total loss:  -1.1604 | PDE Loss:  -1.9422 | Function Loss:  -2.2389\n",
      "Total loss:  -1.1605 | PDE Loss:  -1.9424 | Function Loss:  -2.2389\n",
      "Total loss:  -1.1605 | PDE Loss:  -1.9425 | Function Loss:  -2.2389\n",
      "Total loss:  -1.1605 | PDE Loss:  -1.9426 | Function Loss:  -2.2389\n",
      "Total loss:  -1.1605 | PDE Loss:  -1.9427 | Function Loss:  -2.2389\n",
      "Total loss:  -1.1605 | PDE Loss:  -1.9428 | Function Loss:  -2.2389\n",
      "Total loss:  -1.1606 | PDE Loss:  -1.9428 | Function Loss:  -2.2389\n",
      "Total loss:  -1.1606 | PDE Loss:  -1.9428 | Function Loss:  -2.2389\n",
      "Total loss:  -1.1606 | PDE Loss:  -1.9428 | Function Loss:  -2.239\n",
      "Total loss:  -1.1606 | PDE Loss:  -1.9429 | Function Loss:  -2.239\n",
      "Total loss:  -1.1607 | PDE Loss:  -1.9429 | Function Loss:  -2.239\n",
      "Total loss:  -1.1607 | PDE Loss:  -1.943 | Function Loss:  -2.239\n",
      "Total loss:  -1.1607 | PDE Loss:  -1.943 | Function Loss:  -2.2391\n",
      "Total loss:  -1.1607 | PDE Loss:  -1.9431 | Function Loss:  -2.2391\n",
      "Total loss:  -1.1607 | PDE Loss:  -1.9432 | Function Loss:  -2.2391\n",
      "Total loss:  -1.1608 | PDE Loss:  -1.9433 | Function Loss:  -2.2391\n",
      "Total loss:  -1.1608 | PDE Loss:  -1.9433 | Function Loss:  -2.2391\n",
      "Total loss:  -1.1608 | PDE Loss:  -1.9434 | Function Loss:  -2.2391\n",
      "Total loss:  -1.1608 | PDE Loss:  -1.9433 | Function Loss:  -2.2392\n",
      "Total loss:  -1.1609 | PDE Loss:  -1.9433 | Function Loss:  -2.2392\n",
      "Total loss:  -1.1609 | PDE Loss:  -1.9433 | Function Loss:  -2.2392\n",
      "Total loss:  -1.1609 | PDE Loss:  -1.9434 | Function Loss:  -2.2392\n",
      "Total loss:  -1.1609 | PDE Loss:  -1.9434 | Function Loss:  -2.2392\n",
      "Total loss:  -1.1609 | PDE Loss:  -1.9434 | Function Loss:  -2.2392\n",
      "Total loss:  -1.1609 | PDE Loss:  -1.9435 | Function Loss:  -2.2393\n",
      "Total loss:  -1.161 | PDE Loss:  -1.9436 | Function Loss:  -2.2392\n",
      "Total loss:  -1.161 | PDE Loss:  -1.9437 | Function Loss:  -2.2392\n",
      "Total loss:  -1.161 | PDE Loss:  -1.9438 | Function Loss:  -2.2392\n",
      "Total loss:  -1.161 | PDE Loss:  -1.9441 | Function Loss:  -2.2392\n",
      "Total loss:  -1.161 | PDE Loss:  -1.9443 | Function Loss:  -2.2392\n",
      "Total loss:  -1.1611 | PDE Loss:  -1.9445 | Function Loss:  -2.2392\n",
      "Total loss:  -1.1611 | PDE Loss:  -1.9447 | Function Loss:  -2.2392\n",
      "Total loss:  -1.1611 | PDE Loss:  -1.9449 | Function Loss:  -2.2392\n",
      "Total loss:  -1.1611 | PDE Loss:  -1.9451 | Function Loss:  -2.2391\n",
      "Total loss:  -1.1611 | PDE Loss:  -1.9455 | Function Loss:  -2.2391\n",
      "Total loss:  -1.1611 | PDE Loss:  -1.9453 | Function Loss:  -2.2391\n",
      "Total loss:  -1.1612 | PDE Loss:  -1.9454 | Function Loss:  -2.2391\n",
      "Total loss:  -1.1612 | PDE Loss:  -1.9456 | Function Loss:  -2.2391\n",
      "Total loss:  -1.1612 | PDE Loss:  -1.9457 | Function Loss:  -2.2391\n",
      "Total loss:  -1.1612 | PDE Loss:  -1.9457 | Function Loss:  -2.2392\n",
      "Total loss:  -1.1612 | PDE Loss:  -1.9457 | Function Loss:  -2.2392\n",
      "Total loss:  -1.1613 | PDE Loss:  -1.9458 | Function Loss:  -2.2392\n",
      "Total loss:  -1.1613 | PDE Loss:  -1.9459 | Function Loss:  -2.2392\n",
      "Total loss:  -1.1613 | PDE Loss:  -1.9459 | Function Loss:  -2.2392\n",
      "Total loss:  -1.1613 | PDE Loss:  -1.9461 | Function Loss:  -2.2392\n",
      "Total loss:  -1.1614 | PDE Loss:  -1.9462 | Function Loss:  -2.2392\n",
      "Total loss:  -1.1614 | PDE Loss:  -1.9464 | Function Loss:  -2.2392\n",
      "Total loss:  -1.1614 | PDE Loss:  -1.9465 | Function Loss:  -2.2392\n",
      "Total loss:  -1.1615 | PDE Loss:  -1.9467 | Function Loss:  -2.2392\n",
      "Total loss:  -1.1615 | PDE Loss:  -1.9468 | Function Loss:  -2.2393\n",
      "Total loss:  -1.1615 | PDE Loss:  -1.9468 | Function Loss:  -2.2393\n",
      "Total loss:  -1.1615 | PDE Loss:  -1.9469 | Function Loss:  -2.2393\n",
      "Total loss:  -1.1616 | PDE Loss:  -1.9469 | Function Loss:  -2.2393\n",
      "Total loss:  -1.1615 | PDE Loss:  -1.9467 | Function Loss:  -2.2393\n",
      "Total loss:  -1.1616 | PDE Loss:  -1.9469 | Function Loss:  -2.2393\n",
      "Total loss:  -1.1616 | PDE Loss:  -1.9469 | Function Loss:  -2.2394\n",
      "Total loss:  -1.1616 | PDE Loss:  -1.9468 | Function Loss:  -2.2394\n",
      "Total loss:  -1.1616 | PDE Loss:  -1.9469 | Function Loss:  -2.2394\n",
      "Total loss:  -1.1616 | PDE Loss:  -1.9469 | Function Loss:  -2.2394\n",
      "Total loss:  -1.1617 | PDE Loss:  -1.9469 | Function Loss:  -2.2395\n",
      "Total loss:  -1.1617 | PDE Loss:  -1.9469 | Function Loss:  -2.2395\n",
      "Total loss:  -1.1617 | PDE Loss:  -1.9468 | Function Loss:  -2.2395\n",
      "Total loss:  -1.1618 | PDE Loss:  -1.9469 | Function Loss:  -2.2396\n",
      "Total loss:  -1.1618 | PDE Loss:  -1.947 | Function Loss:  -2.2396\n",
      "Total loss:  -1.1618 | PDE Loss:  -1.9471 | Function Loss:  -2.2396\n",
      "Total loss:  -1.1618 | PDE Loss:  -1.9471 | Function Loss:  -2.2396\n",
      "Total loss:  -1.1618 | PDE Loss:  -1.9472 | Function Loss:  -2.2396\n",
      "Total loss:  -1.1618 | PDE Loss:  -1.9471 | Function Loss:  -2.2396\n",
      "Total loss:  -1.1618 | PDE Loss:  -1.9476 | Function Loss:  -2.2395\n",
      "Total loss:  -1.1618 | PDE Loss:  -1.9474 | Function Loss:  -2.2396\n",
      "Total loss:  -1.1619 | PDE Loss:  -1.9474 | Function Loss:  -2.2396\n",
      "Total loss:  -1.1619 | PDE Loss:  -1.9474 | Function Loss:  -2.2396\n",
      "Total loss:  -1.1619 | PDE Loss:  -1.9475 | Function Loss:  -2.2396\n",
      "Total loss:  -1.1619 | PDE Loss:  -1.9475 | Function Loss:  -2.2397\n",
      "Total loss:  -1.162 | PDE Loss:  -1.9476 | Function Loss:  -2.2397\n",
      "Total loss:  -1.162 | PDE Loss:  -1.9478 | Function Loss:  -2.2397\n",
      "Total loss:  -1.162 | PDE Loss:  -1.9473 | Function Loss:  -2.2398\n",
      "Total loss:  -1.162 | PDE Loss:  -1.9476 | Function Loss:  -2.2397\n",
      "Total loss:  -1.1621 | PDE Loss:  -1.9479 | Function Loss:  -2.2397\n",
      "Total loss:  -1.1621 | PDE Loss:  -1.9482 | Function Loss:  -2.2397\n",
      "Total loss:  -1.1621 | PDE Loss:  -1.9485 | Function Loss:  -2.2397\n",
      "Total loss:  -1.1622 | PDE Loss:  -1.9488 | Function Loss:  -2.2397\n",
      "Total loss:  -1.1622 | PDE Loss:  -1.9489 | Function Loss:  -2.2397\n",
      "Total loss:  -1.1622 | PDE Loss:  -1.949 | Function Loss:  -2.2397\n",
      "Total loss:  -1.1622 | PDE Loss:  -1.9491 | Function Loss:  -2.2397\n",
      "Total loss:  -1.1623 | PDE Loss:  -1.9491 | Function Loss:  -2.2397\n",
      "Total loss:  -1.1623 | PDE Loss:  -1.9493 | Function Loss:  -2.2397\n",
      "Total loss:  -1.1623 | PDE Loss:  -1.9492 | Function Loss:  -2.2398\n",
      "Total loss:  -1.1623 | PDE Loss:  -1.9492 | Function Loss:  -2.2398\n",
      "Total loss:  -1.1624 | PDE Loss:  -1.9492 | Function Loss:  -2.2399\n",
      "Total loss:  -1.1624 | PDE Loss:  -1.9493 | Function Loss:  -2.2399\n",
      "Total loss:  -1.1624 | PDE Loss:  -1.9494 | Function Loss:  -2.2399\n",
      "Total loss:  -1.1625 | PDE Loss:  -1.95 | Function Loss:  -2.2398\n",
      "Total loss:  -1.1625 | PDE Loss:  -1.95 | Function Loss:  -2.2398\n",
      "Total loss:  -1.1625 | PDE Loss:  -1.95 | Function Loss:  -2.2399\n",
      "Total loss:  -1.1625 | PDE Loss:  -1.9501 | Function Loss:  -2.2399\n",
      "Total loss:  -1.1626 | PDE Loss:  -1.9502 | Function Loss:  -2.2399\n",
      "Total loss:  -1.1626 | PDE Loss:  -1.9503 | Function Loss:  -2.2399\n",
      "Total loss:  -1.1626 | PDE Loss:  -1.9504 | Function Loss:  -2.2399\n",
      "Total loss:  -1.1626 | PDE Loss:  -1.9504 | Function Loss:  -2.2399\n",
      "Total loss:  -1.1626 | PDE Loss:  -1.9505 | Function Loss:  -2.2399\n",
      "Total loss:  -1.1626 | PDE Loss:  -1.9505 | Function Loss:  -2.2399\n",
      "Total loss:  -1.1626 | PDE Loss:  -1.9506 | Function Loss:  -2.2399\n",
      "Total loss:  -1.1627 | PDE Loss:  -1.9506 | Function Loss:  -2.2399\n",
      "Total loss:  -1.1627 | PDE Loss:  -1.9507 | Function Loss:  -2.2399\n",
      "Total loss:  -1.1627 | PDE Loss:  -1.9506 | Function Loss:  -2.2399\n",
      "Total loss:  -1.1627 | PDE Loss:  -1.9506 | Function Loss:  -2.24\n",
      "Total loss:  -1.1626 | PDE Loss:  -1.9497 | Function Loss:  -2.2401\n",
      "Total loss:  -1.1627 | PDE Loss:  -1.9505 | Function Loss:  -2.24\n",
      "Total loss:  -1.1628 | PDE Loss:  -1.9504 | Function Loss:  -2.2401\n",
      "Total loss:  -1.1628 | PDE Loss:  -1.9504 | Function Loss:  -2.2401\n",
      "Total loss:  -1.1629 | PDE Loss:  -1.9505 | Function Loss:  -2.2402\n",
      "Total loss:  -1.1629 | PDE Loss:  -1.9504 | Function Loss:  -2.2403\n",
      "Total loss:  -1.1629 | PDE Loss:  -1.9504 | Function Loss:  -2.2403\n",
      "Total loss:  -1.163 | PDE Loss:  -1.9505 | Function Loss:  -2.2403\n",
      "Total loss:  -1.163 | PDE Loss:  -1.9506 | Function Loss:  -2.2403\n",
      "Total loss:  -1.1631 | PDE Loss:  -1.9509 | Function Loss:  -2.2403\n",
      "Total loss:  -1.1631 | PDE Loss:  -1.951 | Function Loss:  -2.2404\n",
      "Total loss:  -1.1631 | PDE Loss:  -1.9511 | Function Loss:  -2.2404\n",
      "Total loss:  -1.1631 | PDE Loss:  -1.9512 | Function Loss:  -2.2404\n",
      "Total loss:  -1.1632 | PDE Loss:  -1.9513 | Function Loss:  -2.2404\n",
      "Total loss:  -1.1632 | PDE Loss:  -1.9513 | Function Loss:  -2.2404\n",
      "Total loss:  -1.1632 | PDE Loss:  -1.9513 | Function Loss:  -2.2404\n",
      "Total loss:  -1.1632 | PDE Loss:  -1.9514 | Function Loss:  -2.2404\n",
      "Total loss:  -1.1632 | PDE Loss:  -1.9513 | Function Loss:  -2.2404\n",
      "Total loss:  -1.1632 | PDE Loss:  -1.9513 | Function Loss:  -2.2404\n",
      "Total loss:  -1.1632 | PDE Loss:  -1.9512 | Function Loss:  -2.2405\n",
      "Total loss:  -1.1632 | PDE Loss:  -1.9512 | Function Loss:  -2.2405\n",
      "Total loss:  -1.1632 | PDE Loss:  -1.9511 | Function Loss:  -2.2405\n",
      "Total loss:  -1.1632 | PDE Loss:  -1.9511 | Function Loss:  -2.2405\n",
      "Total loss:  -1.1633 | PDE Loss:  -1.9511 | Function Loss:  -2.2405\n",
      "Total loss:  -1.1632 | PDE Loss:  -1.9509 | Function Loss:  -2.2406\n",
      "Total loss:  -1.1633 | PDE Loss:  -1.951 | Function Loss:  -2.2405\n",
      "Total loss:  -1.1633 | PDE Loss:  -1.951 | Function Loss:  -2.2406\n",
      "Total loss:  -1.1633 | PDE Loss:  -1.951 | Function Loss:  -2.2406\n",
      "Total loss:  -1.1633 | PDE Loss:  -1.951 | Function Loss:  -2.2406\n",
      "Total loss:  -1.1633 | PDE Loss:  -1.951 | Function Loss:  -2.2406\n",
      "Total loss:  -1.1633 | PDE Loss:  -1.951 | Function Loss:  -2.2406\n",
      "Total loss:  -1.1633 | PDE Loss:  -1.9509 | Function Loss:  -2.2407\n",
      "Total loss:  -1.1634 | PDE Loss:  -1.9509 | Function Loss:  -2.2407\n",
      "Total loss:  -1.1634 | PDE Loss:  -1.951 | Function Loss:  -2.2407\n",
      "Total loss:  -1.1634 | PDE Loss:  -1.951 | Function Loss:  -2.2407\n",
      "Total loss:  -1.1634 | PDE Loss:  -1.951 | Function Loss:  -2.2407\n",
      "Total loss:  -1.1635 | PDE Loss:  -1.9511 | Function Loss:  -2.2408\n",
      "Total loss:  -1.1635 | PDE Loss:  -1.9512 | Function Loss:  -2.2408\n",
      "Total loss:  -1.1635 | PDE Loss:  -1.9513 | Function Loss:  -2.2408\n",
      "Total loss:  -1.1635 | PDE Loss:  -1.9514 | Function Loss:  -2.2408\n",
      "Total loss:  -1.1635 | PDE Loss:  -1.9515 | Function Loss:  -2.2408\n",
      "Total loss:  -1.1635 | PDE Loss:  -1.9516 | Function Loss:  -2.2408\n",
      "Total loss:  -1.1636 | PDE Loss:  -1.9517 | Function Loss:  -2.2408\n",
      "Total loss:  -1.1636 | PDE Loss:  -1.9517 | Function Loss:  -2.2408\n",
      "Total loss:  -1.1636 | PDE Loss:  -1.9518 | Function Loss:  -2.2408\n",
      "Total loss:  -1.1636 | PDE Loss:  -1.9518 | Function Loss:  -2.2408\n",
      "Total loss:  -1.1636 | PDE Loss:  -1.9518 | Function Loss:  -2.2408\n",
      "Total loss:  -1.1636 | PDE Loss:  -1.9518 | Function Loss:  -2.2408\n",
      "Total loss:  -1.1636 | PDE Loss:  -1.952 | Function Loss:  -2.2408\n",
      "Total loss:  -1.1636 | PDE Loss:  -1.9519 | Function Loss:  -2.2408\n",
      "Total loss:  -1.1636 | PDE Loss:  -1.9518 | Function Loss:  -2.2408\n",
      "Total loss:  -1.1636 | PDE Loss:  -1.9518 | Function Loss:  -2.2408\n",
      "Total loss:  -1.1637 | PDE Loss:  -1.9518 | Function Loss:  -2.2409\n",
      "Total loss:  -1.1637 | PDE Loss:  -1.9519 | Function Loss:  -2.2409\n",
      "Total loss:  -1.1637 | PDE Loss:  -1.9519 | Function Loss:  -2.2409\n",
      "Total loss:  -1.1637 | PDE Loss:  -1.952 | Function Loss:  -2.2409\n",
      "Total loss:  -1.1637 | PDE Loss:  -1.952 | Function Loss:  -2.2409\n",
      "Total loss:  -1.1637 | PDE Loss:  -1.9522 | Function Loss:  -2.2409\n",
      "Total loss:  -1.1637 | PDE Loss:  -1.9522 | Function Loss:  -2.2409\n",
      "Total loss:  -1.1637 | PDE Loss:  -1.9521 | Function Loss:  -2.2409\n",
      "Total loss:  -1.1638 | PDE Loss:  -1.9521 | Function Loss:  -2.2409\n",
      "Total loss:  -1.1638 | PDE Loss:  -1.9521 | Function Loss:  -2.241\n",
      "Total loss:  -1.1638 | PDE Loss:  -1.952 | Function Loss:  -2.241\n",
      "Total loss:  -1.1638 | PDE Loss:  -1.9519 | Function Loss:  -2.241\n",
      "Total loss:  -1.1638 | PDE Loss:  -1.9518 | Function Loss:  -2.2411\n",
      "Total loss:  -1.1638 | PDE Loss:  -1.9515 | Function Loss:  -2.2411\n",
      "Total loss:  -1.1638 | PDE Loss:  -1.9517 | Function Loss:  -2.2411\n",
      "Total loss:  -1.1638 | PDE Loss:  -1.9517 | Function Loss:  -2.2411\n",
      "Total loss:  -1.1639 | PDE Loss:  -1.9517 | Function Loss:  -2.2411\n",
      "Total loss:  -1.1639 | PDE Loss:  -1.9516 | Function Loss:  -2.2412\n",
      "Total loss:  -1.1639 | PDE Loss:  -1.9516 | Function Loss:  -2.2412\n",
      "Total loss:  -1.1639 | PDE Loss:  -1.9515 | Function Loss:  -2.2412\n",
      "Total loss:  -1.1639 | PDE Loss:  -1.9515 | Function Loss:  -2.2413\n",
      "Total loss:  -1.164 | PDE Loss:  -1.9515 | Function Loss:  -2.2413\n",
      "Total loss:  -1.164 | PDE Loss:  -1.9514 | Function Loss:  -2.2413\n",
      "Total loss:  -1.164 | PDE Loss:  -1.9514 | Function Loss:  -2.2413\n",
      "Total loss:  -1.164 | PDE Loss:  -1.9513 | Function Loss:  -2.2414\n",
      "Total loss:  -1.164 | PDE Loss:  -1.9512 | Function Loss:  -2.2414\n",
      "Total loss:  -1.164 | PDE Loss:  -1.9512 | Function Loss:  -2.2414\n",
      "Total loss:  -1.164 | PDE Loss:  -1.9511 | Function Loss:  -2.2415\n",
      "Total loss:  -1.1641 | PDE Loss:  -1.9511 | Function Loss:  -2.2415\n",
      "Total loss:  -1.1641 | PDE Loss:  -1.9511 | Function Loss:  -2.2415\n",
      "Total loss:  -1.1641 | PDE Loss:  -1.9511 | Function Loss:  -2.2415\n",
      "Total loss:  -1.1641 | PDE Loss:  -1.9511 | Function Loss:  -2.2415\n",
      "Total loss:  -1.1641 | PDE Loss:  -1.9512 | Function Loss:  -2.2415\n",
      "Total loss:  -1.1641 | PDE Loss:  -1.9512 | Function Loss:  -2.2415\n",
      "Total loss:  -1.1641 | PDE Loss:  -1.9514 | Function Loss:  -2.2415\n",
      "Total loss:  -1.1642 | PDE Loss:  -1.9515 | Function Loss:  -2.2415\n",
      "Total loss:  -1.1642 | PDE Loss:  -1.9516 | Function Loss:  -2.2415\n",
      "Total loss:  -1.1642 | PDE Loss:  -1.9517 | Function Loss:  -2.2415\n",
      "Total loss:  -1.1642 | PDE Loss:  -1.9518 | Function Loss:  -2.2415\n",
      "Total loss:  -1.1642 | PDE Loss:  -1.9519 | Function Loss:  -2.2415\n",
      "Total loss:  -1.1642 | PDE Loss:  -1.9519 | Function Loss:  -2.2415\n",
      "Total loss:  -1.1642 | PDE Loss:  -1.952 | Function Loss:  -2.2415\n",
      "Total loss:  -1.1642 | PDE Loss:  -1.952 | Function Loss:  -2.2415\n",
      "Total loss:  -1.1643 | PDE Loss:  -1.9521 | Function Loss:  -2.2415\n",
      "Total loss:  -1.1643 | PDE Loss:  -1.9521 | Function Loss:  -2.2416\n",
      "Total loss:  -1.1643 | PDE Loss:  -1.9521 | Function Loss:  -2.2416\n",
      "Total loss:  -1.1643 | PDE Loss:  -1.9522 | Function Loss:  -2.2416\n",
      "Total loss:  -1.1644 | PDE Loss:  -1.9522 | Function Loss:  -2.2416\n",
      "Total loss:  -1.1644 | PDE Loss:  -1.9523 | Function Loss:  -2.2416\n",
      "Total loss:  -1.1644 | PDE Loss:  -1.9523 | Function Loss:  -2.2417\n",
      "Total loss:  -1.1644 | PDE Loss:  -1.9523 | Function Loss:  -2.2417\n",
      "Total loss:  -1.1644 | PDE Loss:  -1.9524 | Function Loss:  -2.2417\n",
      "Total loss:  -1.1644 | PDE Loss:  -1.9525 | Function Loss:  -2.2417\n",
      "Total loss:  -1.1645 | PDE Loss:  -1.9526 | Function Loss:  -2.2417\n",
      "Total loss:  -1.1645 | PDE Loss:  -1.9527 | Function Loss:  -2.2417\n",
      "Total loss:  -1.1645 | PDE Loss:  -1.9527 | Function Loss:  -2.2417\n",
      "Total loss:  -1.1645 | PDE Loss:  -1.9528 | Function Loss:  -2.2417\n",
      "Total loss:  -1.1645 | PDE Loss:  -1.9528 | Function Loss:  -2.2417\n",
      "Total loss:  -1.1645 | PDE Loss:  -1.9527 | Function Loss:  -2.2417\n",
      "Total loss:  -1.1645 | PDE Loss:  -1.9528 | Function Loss:  -2.2417\n",
      "Total loss:  -1.1646 | PDE Loss:  -1.9528 | Function Loss:  -2.2418\n",
      "Total loss:  -1.1646 | PDE Loss:  -1.9528 | Function Loss:  -2.2418\n",
      "Total loss:  -1.1646 | PDE Loss:  -1.9528 | Function Loss:  -2.2418\n",
      "Total loss:  -1.1646 | PDE Loss:  -1.9529 | Function Loss:  -2.2418\n",
      "Total loss:  -1.1646 | PDE Loss:  -1.953 | Function Loss:  -2.2418\n",
      "Total loss:  -1.1647 | PDE Loss:  -1.9531 | Function Loss:  -2.2418\n",
      "Total loss:  -1.1647 | PDE Loss:  -1.9532 | Function Loss:  -2.2418\n",
      "Total loss:  -1.1647 | PDE Loss:  -1.9535 | Function Loss:  -2.2418\n",
      "Total loss:  -1.1647 | PDE Loss:  -1.9536 | Function Loss:  -2.2418\n",
      "Total loss:  -1.1647 | PDE Loss:  -1.9536 | Function Loss:  -2.2418\n",
      "Total loss:  -1.1647 | PDE Loss:  -1.9537 | Function Loss:  -2.2418\n",
      "Total loss:  -1.1647 | PDE Loss:  -1.9537 | Function Loss:  -2.2418\n",
      "Total loss:  -1.1648 | PDE Loss:  -1.9537 | Function Loss:  -2.2418\n",
      "Total loss:  -1.1648 | PDE Loss:  -1.9537 | Function Loss:  -2.2418\n",
      "Total loss:  -1.1648 | PDE Loss:  -1.9537 | Function Loss:  -2.2419\n",
      "Total loss:  -1.1648 | PDE Loss:  -1.9538 | Function Loss:  -2.2419\n",
      "Total loss:  -1.1649 | PDE Loss:  -1.9536 | Function Loss:  -2.242\n",
      "Total loss:  -1.1649 | PDE Loss:  -1.9536 | Function Loss:  -2.242\n",
      "Total loss:  -1.1649 | PDE Loss:  -1.9537 | Function Loss:  -2.242\n",
      "Total loss:  -1.165 | PDE Loss:  -1.9537 | Function Loss:  -2.242\n",
      "Total loss:  -1.165 | PDE Loss:  -1.9539 | Function Loss:  -2.242\n",
      "Total loss:  -1.165 | PDE Loss:  -1.9538 | Function Loss:  -2.2421\n",
      "Total loss:  -1.165 | PDE Loss:  -1.9538 | Function Loss:  -2.2421\n",
      "Total loss:  -1.165 | PDE Loss:  -1.9538 | Function Loss:  -2.2421\n",
      "Total loss:  -1.165 | PDE Loss:  -1.9537 | Function Loss:  -2.2421\n",
      "Total loss:  -1.165 | PDE Loss:  -1.9537 | Function Loss:  -2.2422\n",
      "Total loss:  -1.1651 | PDE Loss:  -1.9536 | Function Loss:  -2.2422\n",
      "Total loss:  -1.1651 | PDE Loss:  -1.9535 | Function Loss:  -2.2422\n",
      "Total loss:  -1.1651 | PDE Loss:  -1.9532 | Function Loss:  -2.2423\n",
      "Total loss:  -1.1651 | PDE Loss:  -1.9531 | Function Loss:  -2.2424\n",
      "Total loss:  -1.1652 | PDE Loss:  -1.9529 | Function Loss:  -2.2424\n",
      "Total loss:  -1.1652 | PDE Loss:  -1.9528 | Function Loss:  -2.2425\n",
      "Total loss:  -1.1652 | PDE Loss:  -1.9528 | Function Loss:  -2.2425\n",
      "Total loss:  -1.1652 | PDE Loss:  -1.9527 | Function Loss:  -2.2425\n",
      "Total loss:  -1.1652 | PDE Loss:  -1.9527 | Function Loss:  -2.2425\n",
      "Total loss:  -1.1652 | PDE Loss:  -1.9527 | Function Loss:  -2.2426\n",
      "Total loss:  -1.1653 | PDE Loss:  -1.9527 | Function Loss:  -2.2426\n",
      "Total loss:  -1.1653 | PDE Loss:  -1.9527 | Function Loss:  -2.2426\n",
      "Total loss:  -1.1653 | PDE Loss:  -1.9527 | Function Loss:  -2.2427\n",
      "Total loss:  -1.1653 | PDE Loss:  -1.9527 | Function Loss:  -2.2427\n",
      "Total loss:  -1.1653 | PDE Loss:  -1.9528 | Function Loss:  -2.2427\n",
      "Total loss:  -1.1653 | PDE Loss:  -1.9528 | Function Loss:  -2.2427\n",
      "Total loss:  -1.1653 | PDE Loss:  -1.9529 | Function Loss:  -2.2427\n",
      "Total loss:  -1.1654 | PDE Loss:  -1.953 | Function Loss:  -2.2427\n",
      "Total loss:  -1.1654 | PDE Loss:  -1.9531 | Function Loss:  -2.2427\n",
      "Total loss:  -1.1654 | PDE Loss:  -1.9532 | Function Loss:  -2.2427\n",
      "Total loss:  -1.1654 | PDE Loss:  -1.9533 | Function Loss:  -2.2427\n",
      "Total loss:  -1.1654 | PDE Loss:  -1.9534 | Function Loss:  -2.2427\n",
      "Total loss:  -1.1655 | PDE Loss:  -1.9536 | Function Loss:  -2.2427\n",
      "Total loss:  -1.1655 | PDE Loss:  -1.9537 | Function Loss:  -2.2427\n",
      "Total loss:  -1.1655 | PDE Loss:  -1.9538 | Function Loss:  -2.2427\n",
      "Total loss:  -1.1655 | PDE Loss:  -1.9539 | Function Loss:  -2.2427\n",
      "Total loss:  -1.1655 | PDE Loss:  -1.954 | Function Loss:  -2.2427\n",
      "Total loss:  -1.1656 | PDE Loss:  -1.9541 | Function Loss:  -2.2427\n",
      "Total loss:  -1.1656 | PDE Loss:  -1.9542 | Function Loss:  -2.2427\n",
      "Total loss:  -1.1656 | PDE Loss:  -1.9543 | Function Loss:  -2.2427\n",
      "Total loss:  -1.1656 | PDE Loss:  -1.9547 | Function Loss:  -2.2426\n",
      "Total loss:  -1.1656 | PDE Loss:  -1.9547 | Function Loss:  -2.2426\n",
      "Total loss:  -1.1656 | PDE Loss:  -1.9548 | Function Loss:  -2.2426\n",
      "Total loss:  -1.1657 | PDE Loss:  -1.955 | Function Loss:  -2.2426\n",
      "Total loss:  -1.1657 | PDE Loss:  -1.9552 | Function Loss:  -2.2426\n",
      "Total loss:  -1.1657 | PDE Loss:  -1.9555 | Function Loss:  -2.2426\n",
      "Total loss:  -1.1657 | PDE Loss:  -1.9557 | Function Loss:  -2.2426\n",
      "Total loss:  -1.1658 | PDE Loss:  -1.9559 | Function Loss:  -2.2426\n",
      "Total loss:  -1.1658 | PDE Loss:  -1.9562 | Function Loss:  -2.2426\n",
      "Total loss:  -1.1659 | PDE Loss:  -1.9564 | Function Loss:  -2.2426\n",
      "Total loss:  -1.1659 | PDE Loss:  -1.9565 | Function Loss:  -2.2426\n",
      "Total loss:  -1.1659 | PDE Loss:  -1.9565 | Function Loss:  -2.2426\n",
      "Total loss:  -1.1659 | PDE Loss:  -1.9566 | Function Loss:  -2.2427\n",
      "Total loss:  -1.1659 | PDE Loss:  -1.9566 | Function Loss:  -2.2427\n",
      "Total loss:  -1.1659 | PDE Loss:  -1.9561 | Function Loss:  -2.2427\n",
      "Total loss:  -1.1659 | PDE Loss:  -1.9565 | Function Loss:  -2.2427\n",
      "Total loss:  -1.166 | PDE Loss:  -1.9565 | Function Loss:  -2.2427\n",
      "Total loss:  -1.166 | PDE Loss:  -1.9565 | Function Loss:  -2.2427\n",
      "Total loss:  -1.166 | PDE Loss:  -1.9565 | Function Loss:  -2.2428\n",
      "Total loss:  -1.166 | PDE Loss:  -1.9564 | Function Loss:  -2.2428\n",
      "Total loss:  -1.166 | PDE Loss:  -1.9564 | Function Loss:  -2.2428\n",
      "Total loss:  -1.1661 | PDE Loss:  -1.9563 | Function Loss:  -2.2429\n",
      "Total loss:  -1.1661 | PDE Loss:  -1.9562 | Function Loss:  -2.2429\n",
      "Total loss:  -1.1661 | PDE Loss:  -1.9561 | Function Loss:  -2.243\n",
      "Total loss:  -1.1661 | PDE Loss:  -1.9559 | Function Loss:  -2.243\n",
      "Total loss:  -1.1661 | PDE Loss:  -1.9558 | Function Loss:  -2.2431\n",
      "Total loss:  -1.1662 | PDE Loss:  -1.9554 | Function Loss:  -2.2432\n",
      "Total loss:  -1.1662 | PDE Loss:  -1.9554 | Function Loss:  -2.2432\n",
      "Total loss:  -1.1662 | PDE Loss:  -1.9554 | Function Loss:  -2.2432\n",
      "Total loss:  -1.1662 | PDE Loss:  -1.9553 | Function Loss:  -2.2433\n",
      "Total loss:  -1.1663 | PDE Loss:  -1.9552 | Function Loss:  -2.2433\n",
      "Total loss:  -1.1663 | PDE Loss:  -1.9545 | Function Loss:  -2.2435\n",
      "Total loss:  -1.1663 | PDE Loss:  -1.9547 | Function Loss:  -2.2435\n",
      "Total loss:  -1.1664 | PDE Loss:  -1.9548 | Function Loss:  -2.2435\n",
      "Total loss:  -1.1664 | PDE Loss:  -1.9549 | Function Loss:  -2.2435\n",
      "Total loss:  -1.1664 | PDE Loss:  -1.9548 | Function Loss:  -2.2436\n",
      "Total loss:  -1.1664 | PDE Loss:  -1.9548 | Function Loss:  -2.2436\n",
      "Total loss:  -1.1664 | PDE Loss:  -1.9547 | Function Loss:  -2.2436\n",
      "Total loss:  -1.1664 | PDE Loss:  -1.9546 | Function Loss:  -2.2437\n",
      "Total loss:  -1.1665 | PDE Loss:  -1.9545 | Function Loss:  -2.2437\n",
      "Total loss:  -1.1665 | PDE Loss:  -1.9545 | Function Loss:  -2.2437\n",
      "Total loss:  -1.1665 | PDE Loss:  -1.9545 | Function Loss:  -2.2437\n",
      "Total loss:  -1.1665 | PDE Loss:  -1.9545 | Function Loss:  -2.2438\n",
      "Total loss:  -1.1665 | PDE Loss:  -1.9545 | Function Loss:  -2.2438\n",
      "Total loss:  -1.1665 | PDE Loss:  -1.9545 | Function Loss:  -2.2438\n",
      "Total loss:  -1.1666 | PDE Loss:  -1.9546 | Function Loss:  -2.2438\n",
      "Total loss:  -1.1666 | PDE Loss:  -1.9547 | Function Loss:  -2.2438\n",
      "Total loss:  -1.1666 | PDE Loss:  -1.9548 | Function Loss:  -2.2438\n",
      "Total loss:  -1.1666 | PDE Loss:  -1.9549 | Function Loss:  -2.2438\n",
      "Total loss:  -1.1666 | PDE Loss:  -1.9549 | Function Loss:  -2.2438\n",
      "Total loss:  -1.1666 | PDE Loss:  -1.9549 | Function Loss:  -2.2438\n",
      "Total loss:  -1.1666 | PDE Loss:  -1.9548 | Function Loss:  -2.2438\n",
      "Total loss:  -1.1667 | PDE Loss:  -1.9548 | Function Loss:  -2.2439\n",
      "Total loss:  -1.1667 | PDE Loss:  -1.9547 | Function Loss:  -2.2439\n",
      "Total loss:  -1.1667 | PDE Loss:  -1.9544 | Function Loss:  -2.244\n",
      "Total loss:  -1.1667 | PDE Loss:  -1.9544 | Function Loss:  -2.244\n",
      "Total loss:  -1.1667 | PDE Loss:  -1.9545 | Function Loss:  -2.244\n",
      "Total loss:  -1.1667 | PDE Loss:  -1.9545 | Function Loss:  -2.244\n",
      "Total loss:  -1.1667 | PDE Loss:  -1.9545 | Function Loss:  -2.244\n",
      "Total loss:  -1.1668 | PDE Loss:  -1.9545 | Function Loss:  -2.2441\n",
      "Total loss:  -1.1668 | PDE Loss:  -1.9544 | Function Loss:  -2.2441\n",
      "Total loss:  -1.1668 | PDE Loss:  -1.9542 | Function Loss:  -2.2442\n",
      "Total loss:  -1.1668 | PDE Loss:  -1.9542 | Function Loss:  -2.2442\n",
      "Total loss:  -1.1668 | PDE Loss:  -1.9538 | Function Loss:  -2.2443\n",
      "Total loss:  -1.1669 | PDE Loss:  -1.9539 | Function Loss:  -2.2443\n",
      "Total loss:  -1.1669 | PDE Loss:  -1.9539 | Function Loss:  -2.2443\n",
      "Total loss:  -1.1669 | PDE Loss:  -1.9539 | Function Loss:  -2.2443\n",
      "Total loss:  -1.1669 | PDE Loss:  -1.9538 | Function Loss:  -2.2443\n",
      "Total loss:  -1.1669 | PDE Loss:  -1.9538 | Function Loss:  -2.2444\n",
      "Total loss:  -1.1669 | PDE Loss:  -1.9537 | Function Loss:  -2.2444\n",
      "Total loss:  -1.1669 | PDE Loss:  -1.9536 | Function Loss:  -2.2444\n",
      "Total loss:  -1.167 | PDE Loss:  -1.9536 | Function Loss:  -2.2445\n",
      "Total loss:  -1.167 | PDE Loss:  -1.9536 | Function Loss:  -2.2445\n",
      "Total loss:  -1.167 | PDE Loss:  -1.9535 | Function Loss:  -2.2445\n",
      "Total loss:  -1.167 | PDE Loss:  -1.9535 | Function Loss:  -2.2446\n",
      "Total loss:  -1.167 | PDE Loss:  -1.9536 | Function Loss:  -2.2446\n",
      "Total loss:  -1.167 | PDE Loss:  -1.9536 | Function Loss:  -2.2445\n",
      "Total loss:  -1.1671 | PDE Loss:  -1.9537 | Function Loss:  -2.2446\n",
      "Total loss:  -1.1671 | PDE Loss:  -1.9538 | Function Loss:  -2.2446\n",
      "Total loss:  -1.1671 | PDE Loss:  -1.9538 | Function Loss:  -2.2446\n",
      "Total loss:  -1.1671 | PDE Loss:  -1.9538 | Function Loss:  -2.2446\n",
      "Total loss:  -1.167 | PDE Loss:  -1.9532 | Function Loss:  -2.2447\n",
      "Total loss:  -1.1671 | PDE Loss:  -1.9537 | Function Loss:  -2.2446\n",
      "Total loss:  -1.1671 | PDE Loss:  -1.9538 | Function Loss:  -2.2446\n",
      "Total loss:  -1.1672 | PDE Loss:  -1.9538 | Function Loss:  -2.2447\n",
      "Total loss:  -1.1672 | PDE Loss:  -1.9537 | Function Loss:  -2.2447\n",
      "Total loss:  -1.1672 | PDE Loss:  -1.9537 | Function Loss:  -2.2447\n",
      "Total loss:  -1.1672 | PDE Loss:  -1.9537 | Function Loss:  -2.2448\n",
      "Total loss:  -1.1672 | PDE Loss:  -1.9536 | Function Loss:  -2.2448\n",
      "Total loss:  -1.1672 | PDE Loss:  -1.9535 | Function Loss:  -2.2448\n",
      "Total loss:  -1.1673 | PDE Loss:  -1.9535 | Function Loss:  -2.2448\n",
      "Total loss:  -1.1673 | PDE Loss:  -1.9534 | Function Loss:  -2.2449\n",
      "Total loss:  -1.1673 | PDE Loss:  -1.9534 | Function Loss:  -2.2449\n",
      "Total loss:  -1.1673 | PDE Loss:  -1.9534 | Function Loss:  -2.2449\n",
      "Total loss:  -1.1673 | PDE Loss:  -1.9535 | Function Loss:  -2.2449\n",
      "Total loss:  -1.1673 | PDE Loss:  -1.9535 | Function Loss:  -2.245\n",
      "Total loss:  -1.1673 | PDE Loss:  -1.9534 | Function Loss:  -2.2449\n",
      "Total loss:  -1.1674 | PDE Loss:  -1.9535 | Function Loss:  -2.245\n",
      "Total loss:  -1.1674 | PDE Loss:  -1.9535 | Function Loss:  -2.245\n",
      "Total loss:  -1.1674 | PDE Loss:  -1.9536 | Function Loss:  -2.245\n",
      "Total loss:  -1.1674 | PDE Loss:  -1.9536 | Function Loss:  -2.245\n",
      "Total loss:  -1.1674 | PDE Loss:  -1.9537 | Function Loss:  -2.245\n",
      "Total loss:  -1.1675 | PDE Loss:  -1.9538 | Function Loss:  -2.245\n",
      "Total loss:  -1.1675 | PDE Loss:  -1.9538 | Function Loss:  -2.245\n",
      "Total loss:  -1.1675 | PDE Loss:  -1.9539 | Function Loss:  -2.245\n",
      "Total loss:  -1.1675 | PDE Loss:  -1.954 | Function Loss:  -2.2451\n",
      "Total loss:  -1.1675 | PDE Loss:  -1.9543 | Function Loss:  -2.245\n",
      "Total loss:  -1.1676 | PDE Loss:  -1.9543 | Function Loss:  -2.2451\n",
      "Total loss:  -1.1676 | PDE Loss:  -1.9543 | Function Loss:  -2.2451\n",
      "Total loss:  -1.1676 | PDE Loss:  -1.9542 | Function Loss:  -2.2451\n",
      "Total loss:  -1.1676 | PDE Loss:  -1.9542 | Function Loss:  -2.2451\n",
      "Total loss:  -1.1676 | PDE Loss:  -1.9543 | Function Loss:  -2.2452\n",
      "Total loss:  -1.1677 | PDE Loss:  -1.9543 | Function Loss:  -2.2452\n",
      "Total loss:  -1.1677 | PDE Loss:  -1.9544 | Function Loss:  -2.2452\n",
      "Total loss:  -1.1674 | PDE Loss:  -1.9546 | Function Loss:  -2.2448\n",
      "Total loss:  -1.1677 | PDE Loss:  -1.9546 | Function Loss:  -2.2452\n",
      "Total loss:  -1.1677 | PDE Loss:  -1.9547 | Function Loss:  -2.2452\n",
      "Total loss:  -1.1678 | PDE Loss:  -1.9547 | Function Loss:  -2.2452\n",
      "Total loss:  -1.1678 | PDE Loss:  -1.9547 | Function Loss:  -2.2453\n",
      "Total loss:  -1.1678 | PDE Loss:  -1.9547 | Function Loss:  -2.2453\n",
      "Total loss:  -1.1678 | PDE Loss:  -1.9546 | Function Loss:  -2.2453\n",
      "Total loss:  -1.1679 | PDE Loss:  -1.9545 | Function Loss:  -2.2454\n",
      "Total loss:  -1.1679 | PDE Loss:  -1.9544 | Function Loss:  -2.2454\n",
      "Total loss:  -1.1679 | PDE Loss:  -1.9544 | Function Loss:  -2.2455\n",
      "Total loss:  -1.1679 | PDE Loss:  -1.9541 | Function Loss:  -2.2455\n",
      "Total loss:  -1.168 | PDE Loss:  -1.954 | Function Loss:  -2.2456\n",
      "Total loss:  -1.168 | PDE Loss:  -1.9539 | Function Loss:  -2.2456\n",
      "Total loss:  -1.168 | PDE Loss:  -1.9539 | Function Loss:  -2.2457\n",
      "Total loss:  -1.168 | PDE Loss:  -1.9539 | Function Loss:  -2.2457\n",
      "Total loss:  -1.1681 | PDE Loss:  -1.9539 | Function Loss:  -2.2457\n",
      "Total loss:  -1.1681 | PDE Loss:  -1.9539 | Function Loss:  -2.2457\n",
      "Total loss:  -1.1681 | PDE Loss:  -1.954 | Function Loss:  -2.2457\n",
      "Total loss:  -1.1681 | PDE Loss:  -1.954 | Function Loss:  -2.2458\n",
      "Total loss:  -1.1681 | PDE Loss:  -1.954 | Function Loss:  -2.2458\n",
      "Total loss:  -1.1682 | PDE Loss:  -1.9541 | Function Loss:  -2.2458\n",
      "Total loss:  -1.1682 | PDE Loss:  -1.9542 | Function Loss:  -2.2458\n",
      "Total loss:  -1.1682 | PDE Loss:  -1.9544 | Function Loss:  -2.2458\n",
      "Total loss:  -1.1682 | PDE Loss:  -1.9544 | Function Loss:  -2.2458\n",
      "Total loss:  -1.1682 | PDE Loss:  -1.9547 | Function Loss:  -2.2457\n",
      "Total loss:  -1.1682 | PDE Loss:  -1.9546 | Function Loss:  -2.2458\n",
      "Total loss:  -1.1683 | PDE Loss:  -1.9546 | Function Loss:  -2.2458\n",
      "Total loss:  -1.1683 | PDE Loss:  -1.9547 | Function Loss:  -2.2458\n",
      "Total loss:  -1.1683 | PDE Loss:  -1.9548 | Function Loss:  -2.2458\n",
      "Total loss:  -1.1683 | PDE Loss:  -1.9549 | Function Loss:  -2.2459\n",
      "Total loss:  -1.1684 | PDE Loss:  -1.9551 | Function Loss:  -2.2459\n",
      "Total loss:  -1.1685 | PDE Loss:  -1.9549 | Function Loss:  -2.246\n",
      "Total loss:  -1.1685 | PDE Loss:  -1.955 | Function Loss:  -2.246\n",
      "Total loss:  -1.1685 | PDE Loss:  -1.955 | Function Loss:  -2.2461\n",
      "Total loss:  -1.1686 | PDE Loss:  -1.9549 | Function Loss:  -2.2461\n",
      "Total loss:  -1.1686 | PDE Loss:  -1.9548 | Function Loss:  -2.2462\n",
      "Total loss:  -1.1686 | PDE Loss:  -1.9548 | Function Loss:  -2.2462\n",
      "Total loss:  -1.1686 | PDE Loss:  -1.9548 | Function Loss:  -2.2462\n",
      "Total loss:  -1.1686 | PDE Loss:  -1.9548 | Function Loss:  -2.2462\n",
      "Total loss:  -1.1687 | PDE Loss:  -1.9548 | Function Loss:  -2.2463\n",
      "Total loss:  -1.1687 | PDE Loss:  -1.9547 | Function Loss:  -2.2463\n",
      "Total loss:  -1.1687 | PDE Loss:  -1.9546 | Function Loss:  -2.2464\n",
      "Total loss:  -1.1688 | PDE Loss:  -1.9546 | Function Loss:  -2.2464\n",
      "Total loss:  -1.1688 | PDE Loss:  -1.9545 | Function Loss:  -2.2465\n",
      "Total loss:  -1.1688 | PDE Loss:  -1.9544 | Function Loss:  -2.2465\n",
      "Total loss:  -1.1689 | PDE Loss:  -1.9543 | Function Loss:  -2.2466\n",
      "Total loss:  -1.1689 | PDE Loss:  -1.9542 | Function Loss:  -2.2466\n",
      "Total loss:  -1.1689 | PDE Loss:  -1.9542 | Function Loss:  -2.2467\n",
      "Total loss:  -1.1689 | PDE Loss:  -1.9541 | Function Loss:  -2.2467\n",
      "Total loss:  -1.1689 | PDE Loss:  -1.954 | Function Loss:  -2.2467\n",
      "Total loss:  -1.169 | PDE Loss:  -1.9541 | Function Loss:  -2.2467\n",
      "Total loss:  -1.169 | PDE Loss:  -1.954 | Function Loss:  -2.2468\n",
      "Total loss:  -1.169 | PDE Loss:  -1.9539 | Function Loss:  -2.2468\n",
      "Total loss:  -1.169 | PDE Loss:  -1.9538 | Function Loss:  -2.2469\n",
      "Total loss:  -1.169 | PDE Loss:  -1.9538 | Function Loss:  -2.2469\n",
      "Total loss:  -1.1691 | PDE Loss:  -1.9537 | Function Loss:  -2.2469\n",
      "Total loss:  -1.1691 | PDE Loss:  -1.9537 | Function Loss:  -2.247\n",
      "Total loss:  -1.1691 | PDE Loss:  -1.9532 | Function Loss:  -2.247\n",
      "Total loss:  -1.1691 | PDE Loss:  -1.9536 | Function Loss:  -2.247\n",
      "Total loss:  -1.1691 | PDE Loss:  -1.9538 | Function Loss:  -2.247\n",
      "Total loss:  -1.1692 | PDE Loss:  -1.9538 | Function Loss:  -2.2471\n",
      "Total loss:  -1.1692 | PDE Loss:  -1.9539 | Function Loss:  -2.2471\n",
      "Total loss:  -1.1692 | PDE Loss:  -1.9539 | Function Loss:  -2.2471\n",
      "Total loss:  -1.1693 | PDE Loss:  -1.954 | Function Loss:  -2.2471\n",
      "Total loss:  -1.1693 | PDE Loss:  -1.954 | Function Loss:  -2.2472\n",
      "Total loss:  -1.1693 | PDE Loss:  -1.954 | Function Loss:  -2.2472\n",
      "Total loss:  -1.1693 | PDE Loss:  -1.9534 | Function Loss:  -2.2473\n",
      "Total loss:  -1.1693 | PDE Loss:  -1.9535 | Function Loss:  -2.2473\n",
      "Total loss:  -1.1694 | PDE Loss:  -1.9536 | Function Loss:  -2.2473\n",
      "Total loss:  -1.1694 | PDE Loss:  -1.9537 | Function Loss:  -2.2474\n",
      "Total loss:  -1.1694 | PDE Loss:  -1.9536 | Function Loss:  -2.2474\n",
      "Total loss:  -1.1695 | PDE Loss:  -1.9535 | Function Loss:  -2.2475\n",
      "Total loss:  -1.1695 | PDE Loss:  -1.9534 | Function Loss:  -2.2475\n",
      "Total loss:  -1.1695 | PDE Loss:  -1.9534 | Function Loss:  -2.2476\n",
      "Total loss:  -1.1695 | PDE Loss:  -1.9521 | Function Loss:  -2.2477\n",
      "Total loss:  -1.1695 | PDE Loss:  -1.9531 | Function Loss:  -2.2476\n",
      "Total loss:  -1.1696 | PDE Loss:  -1.9531 | Function Loss:  -2.2477\n",
      "Total loss:  -1.1696 | PDE Loss:  -1.9531 | Function Loss:  -2.2477\n",
      "Total loss:  -1.1696 | PDE Loss:  -1.9532 | Function Loss:  -2.2477\n",
      "Total loss:  -1.1696 | PDE Loss:  -1.9533 | Function Loss:  -2.2477\n",
      "Total loss:  -1.1697 | PDE Loss:  -1.9534 | Function Loss:  -2.2477\n",
      "Total loss:  -1.1696 | PDE Loss:  -1.9537 | Function Loss:  -2.2477\n",
      "Total loss:  -1.1697 | PDE Loss:  -1.9535 | Function Loss:  -2.2477\n",
      "Total loss:  -1.1697 | PDE Loss:  -1.9536 | Function Loss:  -2.2477\n",
      "Total loss:  -1.1697 | PDE Loss:  -1.9537 | Function Loss:  -2.2477\n",
      "Total loss:  -1.1697 | PDE Loss:  -1.9538 | Function Loss:  -2.2477\n",
      "Total loss:  -1.1697 | PDE Loss:  -1.954 | Function Loss:  -2.2477\n",
      "Total loss:  -1.1697 | PDE Loss:  -1.9541 | Function Loss:  -2.2477\n",
      "Total loss:  -1.1698 | PDE Loss:  -1.9541 | Function Loss:  -2.2477\n",
      "Total loss:  -1.1698 | PDE Loss:  -1.9541 | Function Loss:  -2.2477\n",
      "Total loss:  -1.1698 | PDE Loss:  -1.9541 | Function Loss:  -2.2478\n",
      "Total loss:  -1.1698 | PDE Loss:  -1.954 | Function Loss:  -2.2478\n",
      "Total loss:  -1.1698 | PDE Loss:  -1.954 | Function Loss:  -2.2478\n",
      "Total loss:  -1.1698 | PDE Loss:  -1.954 | Function Loss:  -2.2478\n",
      "Total loss:  -1.1698 | PDE Loss:  -1.954 | Function Loss:  -2.2478\n",
      "Total loss:  -1.1699 | PDE Loss:  -1.954 | Function Loss:  -2.2479\n",
      "Total loss:  -1.1699 | PDE Loss:  -1.954 | Function Loss:  -2.2479\n",
      "Total loss:  -1.1699 | PDE Loss:  -1.954 | Function Loss:  -2.2479\n",
      "Total loss:  -1.1699 | PDE Loss:  -1.954 | Function Loss:  -2.2479\n",
      "Total loss:  -1.1699 | PDE Loss:  -1.954 | Function Loss:  -2.2479\n",
      "Total loss:  -1.1699 | PDE Loss:  -1.9537 | Function Loss:  -2.2479\n",
      "Total loss:  -1.1699 | PDE Loss:  -1.9539 | Function Loss:  -2.2479\n",
      "Total loss:  -1.1699 | PDE Loss:  -1.954 | Function Loss:  -2.2479\n",
      "Total loss:  -1.1699 | PDE Loss:  -1.9539 | Function Loss:  -2.248\n",
      "Total loss:  -1.17 | PDE Loss:  -1.9539 | Function Loss:  -2.248\n",
      "Total loss:  -1.17 | PDE Loss:  -1.9538 | Function Loss:  -2.248\n",
      "Total loss:  -1.17 | PDE Loss:  -1.9537 | Function Loss:  -2.2481\n",
      "Total loss:  -1.17 | PDE Loss:  -1.9535 | Function Loss:  -2.2481\n",
      "Total loss:  -1.17 | PDE Loss:  -1.9533 | Function Loss:  -2.2482\n",
      "Total loss:  -1.17 | PDE Loss:  -1.9528 | Function Loss:  -2.2483\n",
      "Total loss:  -1.1701 | PDE Loss:  -1.9526 | Function Loss:  -2.2484\n",
      "Total loss:  -1.1701 | PDE Loss:  -1.9526 | Function Loss:  -2.2484\n",
      "Total loss:  -1.1701 | PDE Loss:  -1.9525 | Function Loss:  -2.2484\n",
      "Total loss:  -1.1701 | PDE Loss:  -1.9525 | Function Loss:  -2.2485\n",
      "Total loss:  -1.1701 | PDE Loss:  -1.9525 | Function Loss:  -2.2485\n",
      "Total loss:  -1.1702 | PDE Loss:  -1.9525 | Function Loss:  -2.2485\n",
      "Total loss:  -1.1702 | PDE Loss:  -1.9526 | Function Loss:  -2.2485\n",
      "Total loss:  -1.1702 | PDE Loss:  -1.9526 | Function Loss:  -2.2485\n",
      "Total loss:  -1.1702 | PDE Loss:  -1.9527 | Function Loss:  -2.2485\n",
      "Total loss:  -1.1702 | PDE Loss:  -1.9527 | Function Loss:  -2.2485\n",
      "Total loss:  -1.1702 | PDE Loss:  -1.9527 | Function Loss:  -2.2485\n",
      "Total loss:  -1.1702 | PDE Loss:  -1.9527 | Function Loss:  -2.2486\n",
      "Total loss:  -1.1703 | PDE Loss:  -1.9527 | Function Loss:  -2.2486\n",
      "Total loss:  -1.1703 | PDE Loss:  -1.9526 | Function Loss:  -2.2486\n",
      "Total loss:  -1.1703 | PDE Loss:  -1.9525 | Function Loss:  -2.2487\n",
      "Total loss:  -1.1703 | PDE Loss:  -1.9523 | Function Loss:  -2.2487\n",
      "Total loss:  -1.1703 | PDE Loss:  -1.9521 | Function Loss:  -2.2488\n",
      "Total loss:  -1.1704 | PDE Loss:  -1.9519 | Function Loss:  -2.2489\n",
      "Total loss:  -1.1704 | PDE Loss:  -1.9513 | Function Loss:  -2.2491\n",
      "Total loss:  -1.1704 | PDE Loss:  -1.9512 | Function Loss:  -2.2491\n",
      "Total loss:  -1.1705 | PDE Loss:  -1.9512 | Function Loss:  -2.2492\n",
      "Total loss:  -1.1705 | PDE Loss:  -1.9512 | Function Loss:  -2.2492\n",
      "Total loss:  -1.1705 | PDE Loss:  -1.9512 | Function Loss:  -2.2492\n",
      "Total loss:  -1.1706 | PDE Loss:  -1.9513 | Function Loss:  -2.2492\n",
      "Total loss:  -1.1706 | PDE Loss:  -1.9513 | Function Loss:  -2.2492\n",
      "Total loss:  -1.1706 | PDE Loss:  -1.9514 | Function Loss:  -2.2493\n",
      "Total loss:  -1.1706 | PDE Loss:  -1.9514 | Function Loss:  -2.2493\n",
      "Total loss:  -1.1706 | PDE Loss:  -1.9509 | Function Loss:  -2.2494\n",
      "Total loss:  -1.1706 | PDE Loss:  -1.9513 | Function Loss:  -2.2493\n",
      "Total loss:  -1.1707 | PDE Loss:  -1.9513 | Function Loss:  -2.2494\n",
      "Total loss:  -1.1707 | PDE Loss:  -1.9512 | Function Loss:  -2.2494\n",
      "Total loss:  -1.1707 | PDE Loss:  -1.9512 | Function Loss:  -2.2494\n",
      "Total loss:  -1.1707 | PDE Loss:  -1.9511 | Function Loss:  -2.2495\n",
      "Total loss:  -1.1707 | PDE Loss:  -1.951 | Function Loss:  -2.2495\n",
      "Total loss:  -1.1707 | PDE Loss:  -1.9509 | Function Loss:  -2.2495\n",
      "Total loss:  -1.1708 | PDE Loss:  -1.9508 | Function Loss:  -2.2496\n",
      "Total loss:  -1.1708 | PDE Loss:  -1.9508 | Function Loss:  -2.2496\n",
      "Total loss:  -1.1708 | PDE Loss:  -1.9508 | Function Loss:  -2.2496\n",
      "Total loss:  -1.1708 | PDE Loss:  -1.9509 | Function Loss:  -2.2496\n",
      "Total loss:  -1.1708 | PDE Loss:  -1.951 | Function Loss:  -2.2496\n",
      "Total loss:  -1.1708 | PDE Loss:  -1.9511 | Function Loss:  -2.2496\n",
      "Total loss:  -1.1709 | PDE Loss:  -1.9512 | Function Loss:  -2.2496\n",
      "Total loss:  -1.1709 | PDE Loss:  -1.9513 | Function Loss:  -2.2496\n",
      "Total loss:  -1.1709 | PDE Loss:  -1.9514 | Function Loss:  -2.2496\n",
      "Total loss:  -1.1709 | PDE Loss:  -1.9513 | Function Loss:  -2.2496\n",
      "Total loss:  -1.171 | PDE Loss:  -1.9514 | Function Loss:  -2.2497\n",
      "Total loss:  -1.171 | PDE Loss:  -1.9514 | Function Loss:  -2.2497\n",
      "Total loss:  -1.171 | PDE Loss:  -1.9513 | Function Loss:  -2.2498\n",
      "Total loss:  -1.171 | PDE Loss:  -1.9512 | Function Loss:  -2.2498\n",
      "Total loss:  -1.1711 | PDE Loss:  -1.9511 | Function Loss:  -2.2499\n",
      "Total loss:  -1.1711 | PDE Loss:  -1.9509 | Function Loss:  -2.2499\n",
      "Total loss:  -1.1711 | PDE Loss:  -1.9508 | Function Loss:  -2.25\n",
      "Total loss:  -1.1712 | PDE Loss:  -1.9507 | Function Loss:  -2.2501\n",
      "Total loss:  -1.1712 | PDE Loss:  -1.9507 | Function Loss:  -2.2501\n",
      "Total loss:  -1.1712 | PDE Loss:  -1.9508 | Function Loss:  -2.2501\n",
      "Total loss:  -1.1713 | PDE Loss:  -1.9509 | Function Loss:  -2.2501\n",
      "Total loss:  -1.1713 | PDE Loss:  -1.9512 | Function Loss:  -2.2501\n",
      "Total loss:  -1.1713 | PDE Loss:  -1.9513 | Function Loss:  -2.2502\n",
      "Total loss:  -1.1714 | PDE Loss:  -1.9515 | Function Loss:  -2.2502\n",
      "Total loss:  -1.1714 | PDE Loss:  -1.9518 | Function Loss:  -2.2502\n",
      "Total loss:  -1.1715 | PDE Loss:  -1.9518 | Function Loss:  -2.2502\n",
      "Total loss:  -1.1715 | PDE Loss:  -1.9518 | Function Loss:  -2.2503\n",
      "Total loss:  -1.1715 | PDE Loss:  -1.9517 | Function Loss:  -2.2503\n",
      "Total loss:  -1.1716 | PDE Loss:  -1.9516 | Function Loss:  -2.2504\n",
      "Total loss:  -1.1716 | PDE Loss:  -1.9513 | Function Loss:  -2.2505\n",
      "Total loss:  -1.1717 | PDE Loss:  -1.9509 | Function Loss:  -2.2506\n",
      "Total loss:  -1.1717 | PDE Loss:  -1.9504 | Function Loss:  -2.2508\n",
      "Total loss:  -1.1718 | PDE Loss:  -1.9501 | Function Loss:  -2.2509\n",
      "Total loss:  -1.1718 | PDE Loss:  -1.9497 | Function Loss:  -2.2511\n",
      "Total loss:  -1.1719 | PDE Loss:  -1.9495 | Function Loss:  -2.2512\n",
      "Total loss:  -1.172 | PDE Loss:  -1.9492 | Function Loss:  -2.2513\n",
      "Total loss:  -1.172 | PDE Loss:  -1.9493 | Function Loss:  -2.2513\n",
      "Total loss:  -1.172 | PDE Loss:  -1.9494 | Function Loss:  -2.2514\n",
      "Total loss:  -1.1721 | PDE Loss:  -1.9495 | Function Loss:  -2.2514\n",
      "Total loss:  -1.1721 | PDE Loss:  -1.9496 | Function Loss:  -2.2514\n",
      "Total loss:  -1.1721 | PDE Loss:  -1.9494 | Function Loss:  -2.2515\n",
      "Total loss:  -1.1721 | PDE Loss:  -1.9494 | Function Loss:  -2.2515\n",
      "Total loss:  -1.1722 | PDE Loss:  -1.9493 | Function Loss:  -2.2515\n",
      "Total loss:  -1.1722 | PDE Loss:  -1.9493 | Function Loss:  -2.2516\n",
      "Total loss:  -1.1722 | PDE Loss:  -1.9493 | Function Loss:  -2.2516\n",
      "Total loss:  -1.1722 | PDE Loss:  -1.9491 | Function Loss:  -2.2516\n",
      "Total loss:  -1.1722 | PDE Loss:  -1.9492 | Function Loss:  -2.2516\n",
      "Total loss:  -1.1722 | PDE Loss:  -1.9493 | Function Loss:  -2.2516\n",
      "Total loss:  -1.1723 | PDE Loss:  -1.9495 | Function Loss:  -2.2516\n",
      "Total loss:  -1.1723 | PDE Loss:  -1.9496 | Function Loss:  -2.2516\n",
      "Total loss:  -1.1723 | PDE Loss:  -1.95 | Function Loss:  -2.2516\n",
      "Total loss:  -1.1723 | PDE Loss:  -1.9501 | Function Loss:  -2.2516\n",
      "Total loss:  -1.1723 | PDE Loss:  -1.9503 | Function Loss:  -2.2516\n",
      "Total loss:  -1.1724 | PDE Loss:  -1.9502 | Function Loss:  -2.2516\n",
      "Total loss:  -1.1724 | PDE Loss:  -1.9502 | Function Loss:  -2.2517\n",
      "Total loss:  -1.1724 | PDE Loss:  -1.9501 | Function Loss:  -2.2517\n",
      "Total loss:  -1.1725 | PDE Loss:  -1.95 | Function Loss:  -2.2518\n",
      "Total loss:  -1.1725 | PDE Loss:  -1.9499 | Function Loss:  -2.2518\n",
      "Total loss:  -1.1725 | PDE Loss:  -1.9498 | Function Loss:  -2.2519\n",
      "Total loss:  -1.1726 | PDE Loss:  -1.9497 | Function Loss:  -2.2519\n",
      "Total loss:  -1.1726 | PDE Loss:  -1.9497 | Function Loss:  -2.252\n",
      "Total loss:  -1.1726 | PDE Loss:  -1.9498 | Function Loss:  -2.252\n",
      "Total loss:  -1.1726 | PDE Loss:  -1.9501 | Function Loss:  -2.252\n",
      "Total loss:  -1.1727 | PDE Loss:  -1.9503 | Function Loss:  -2.2519\n",
      "Total loss:  -1.1727 | PDE Loss:  -1.9506 | Function Loss:  -2.2519\n",
      "Total loss:  -1.1727 | PDE Loss:  -1.9511 | Function Loss:  -2.2519\n",
      "Total loss:  -1.1727 | PDE Loss:  -1.9514 | Function Loss:  -2.2518\n",
      "Total loss:  -1.1728 | PDE Loss:  -1.9517 | Function Loss:  -2.2518\n",
      "Total loss:  -1.1728 | PDE Loss:  -1.9519 | Function Loss:  -2.2518\n",
      "Total loss:  -1.1728 | PDE Loss:  -1.9521 | Function Loss:  -2.2518\n",
      "Total loss:  -1.1728 | PDE Loss:  -1.9521 | Function Loss:  -2.2518\n",
      "Total loss:  -1.1728 | PDE Loss:  -1.9519 | Function Loss:  -2.2518\n",
      "Total loss:  -1.1729 | PDE Loss:  -1.9517 | Function Loss:  -2.2519\n",
      "Total loss:  -1.1729 | PDE Loss:  -1.9515 | Function Loss:  -2.2519\n",
      "Total loss:  -1.1729 | PDE Loss:  -1.9514 | Function Loss:  -2.252\n",
      "Total loss:  -1.1729 | PDE Loss:  -1.9512 | Function Loss:  -2.2521\n",
      "Total loss:  -1.1729 | PDE Loss:  -1.9513 | Function Loss:  -2.2521\n",
      "Total loss:  -1.1729 | PDE Loss:  -1.9511 | Function Loss:  -2.2521\n",
      "Total loss:  -1.173 | PDE Loss:  -1.9511 | Function Loss:  -2.2522\n",
      "Total loss:  -1.173 | PDE Loss:  -1.9512 | Function Loss:  -2.2522\n",
      "Total loss:  -1.1731 | PDE Loss:  -1.9516 | Function Loss:  -2.2522\n",
      "Total loss:  -1.1731 | PDE Loss:  -1.952 | Function Loss:  -2.2521\n",
      "Total loss:  -1.1731 | PDE Loss:  -1.9524 | Function Loss:  -2.2521\n",
      "Total loss:  -1.1731 | PDE Loss:  -1.9535 | Function Loss:  -2.2518\n",
      "Total loss:  -1.1732 | PDE Loss:  -1.953 | Function Loss:  -2.252\n",
      "Total loss:  -1.1732 | PDE Loss:  -1.953 | Function Loss:  -2.252\n",
      "Total loss:  -1.1732 | PDE Loss:  -1.953 | Function Loss:  -2.2521\n",
      "Total loss:  -1.1732 | PDE Loss:  -1.953 | Function Loss:  -2.2521\n",
      "Total loss:  -1.1733 | PDE Loss:  -1.9528 | Function Loss:  -2.2522\n",
      "Total loss:  -1.1733 | PDE Loss:  -1.9527 | Function Loss:  -2.2522\n",
      "Total loss:  -1.1733 | PDE Loss:  -1.9526 | Function Loss:  -2.2522\n",
      "Total loss:  -1.1733 | PDE Loss:  -1.9523 | Function Loss:  -2.2523\n",
      "Total loss:  -1.1734 | PDE Loss:  -1.9521 | Function Loss:  -2.2524\n",
      "Total loss:  -1.1734 | PDE Loss:  -1.952 | Function Loss:  -2.2525\n",
      "Total loss:  -1.1734 | PDE Loss:  -1.9519 | Function Loss:  -2.2525\n",
      "Total loss:  -1.1734 | PDE Loss:  -1.9519 | Function Loss:  -2.2526\n",
      "Total loss:  -1.1735 | PDE Loss:  -1.9516 | Function Loss:  -2.2526\n",
      "Total loss:  -1.1735 | PDE Loss:  -1.9519 | Function Loss:  -2.2526\n",
      "Total loss:  -1.1735 | PDE Loss:  -1.952 | Function Loss:  -2.2526\n",
      "Total loss:  -1.1736 | PDE Loss:  -1.9523 | Function Loss:  -2.2526\n",
      "Total loss:  -1.1736 | PDE Loss:  -1.9523 | Function Loss:  -2.2527\n",
      "Total loss:  -1.1736 | PDE Loss:  -1.9524 | Function Loss:  -2.2527\n",
      "Total loss:  -1.1737 | PDE Loss:  -1.9523 | Function Loss:  -2.2527\n",
      "Total loss:  -1.1737 | PDE Loss:  -1.9521 | Function Loss:  -2.2528\n",
      "Total loss:  -1.1737 | PDE Loss:  -1.9519 | Function Loss:  -2.2529\n",
      "Total loss:  -1.1737 | PDE Loss:  -1.9518 | Function Loss:  -2.2529\n",
      "Total loss:  -1.1737 | PDE Loss:  -1.9515 | Function Loss:  -2.253\n",
      "Total loss:  -1.1738 | PDE Loss:  -1.9513 | Function Loss:  -2.2531\n",
      "Total loss:  -1.1738 | PDE Loss:  -1.9511 | Function Loss:  -2.2531\n",
      "Total loss:  -1.1738 | PDE Loss:  -1.951 | Function Loss:  -2.2532\n",
      "Total loss:  -1.1738 | PDE Loss:  -1.9509 | Function Loss:  -2.2532\n",
      "Total loss:  -1.1738 | PDE Loss:  -1.9508 | Function Loss:  -2.2533\n",
      "Total loss:  -1.1739 | PDE Loss:  -1.9507 | Function Loss:  -2.2533\n",
      "Total loss:  -1.1739 | PDE Loss:  -1.9507 | Function Loss:  -2.2533\n",
      "Total loss:  -1.1739 | PDE Loss:  -1.9509 | Function Loss:  -2.2533\n",
      "Total loss:  -1.1739 | PDE Loss:  -1.9509 | Function Loss:  -2.2533\n",
      "Total loss:  -1.174 | PDE Loss:  -1.9511 | Function Loss:  -2.2533\n",
      "Total loss:  -1.174 | PDE Loss:  -1.951 | Function Loss:  -2.2534\n",
      "Total loss:  -1.174 | PDE Loss:  -1.951 | Function Loss:  -2.2534\n",
      "Total loss:  -1.174 | PDE Loss:  -1.9509 | Function Loss:  -2.2535\n",
      "Total loss:  -1.1741 | PDE Loss:  -1.9505 | Function Loss:  -2.2536\n",
      "Total loss:  -1.1741 | PDE Loss:  -1.9503 | Function Loss:  -2.2537\n",
      "Total loss:  -1.1741 | PDE Loss:  -1.9499 | Function Loss:  -2.2538\n",
      "Total loss:  -1.1741 | PDE Loss:  -1.9496 | Function Loss:  -2.2539\n",
      "Total loss:  -1.1742 | PDE Loss:  -1.9492 | Function Loss:  -2.254\n",
      "Total loss:  -1.1742 | PDE Loss:  -1.949 | Function Loss:  -2.254\n",
      "Total loss:  -1.1742 | PDE Loss:  -1.949 | Function Loss:  -2.2541\n",
      "Total loss:  -1.1743 | PDE Loss:  -1.9491 | Function Loss:  -2.2541\n",
      "Total loss:  -1.1743 | PDE Loss:  -1.9493 | Function Loss:  -2.2541\n",
      "Total loss:  -1.1743 | PDE Loss:  -1.9496 | Function Loss:  -2.2541\n",
      "Total loss:  -1.1744 | PDE Loss:  -1.9498 | Function Loss:  -2.2541\n",
      "Total loss:  -1.1744 | PDE Loss:  -1.95 | Function Loss:  -2.2541\n",
      "Total loss:  -1.1744 | PDE Loss:  -1.9503 | Function Loss:  -2.2541\n",
      "Total loss:  -1.1744 | PDE Loss:  -1.9505 | Function Loss:  -2.254\n",
      "Total loss:  -1.1745 | PDE Loss:  -1.9507 | Function Loss:  -2.254\n",
      "Total loss:  -1.1745 | PDE Loss:  -1.9508 | Function Loss:  -2.254\n",
      "Total loss:  -1.1745 | PDE Loss:  -1.951 | Function Loss:  -2.254\n",
      "Total loss:  -1.1745 | PDE Loss:  -1.9509 | Function Loss:  -2.254\n",
      "Total loss:  -1.1745 | PDE Loss:  -1.951 | Function Loss:  -2.254\n",
      "Total loss:  -1.1745 | PDE Loss:  -1.9509 | Function Loss:  -2.2541\n",
      "Total loss:  -1.1745 | PDE Loss:  -1.9506 | Function Loss:  -2.2541\n",
      "Total loss:  -1.1746 | PDE Loss:  -1.9506 | Function Loss:  -2.2542\n",
      "Total loss:  -1.1746 | PDE Loss:  -1.9506 | Function Loss:  -2.2542\n",
      "Total loss:  -1.1746 | PDE Loss:  -1.9506 | Function Loss:  -2.2542\n",
      "Total loss:  -1.1747 | PDE Loss:  -1.9505 | Function Loss:  -2.2543\n",
      "Total loss:  -1.1747 | PDE Loss:  -1.9506 | Function Loss:  -2.2543\n",
      "Total loss:  -1.1747 | PDE Loss:  -1.9505 | Function Loss:  -2.2544\n",
      "Total loss:  -1.1747 | PDE Loss:  -1.9507 | Function Loss:  -2.2544\n",
      "Total loss:  -1.1748 | PDE Loss:  -1.9508 | Function Loss:  -2.2544\n",
      "Total loss:  -1.1748 | PDE Loss:  -1.9509 | Function Loss:  -2.2544\n",
      "Total loss:  -1.1748 | PDE Loss:  -1.9511 | Function Loss:  -2.2544\n",
      "Total loss:  -1.1748 | PDE Loss:  -1.9512 | Function Loss:  -2.2544\n",
      "Total loss:  -1.1749 | PDE Loss:  -1.9515 | Function Loss:  -2.2543\n",
      "Total loss:  -1.1749 | PDE Loss:  -1.9516 | Function Loss:  -2.2543\n",
      "Total loss:  -1.1749 | PDE Loss:  -1.9523 | Function Loss:  -2.2542\n",
      "Total loss:  -1.1749 | PDE Loss:  -1.9523 | Function Loss:  -2.2543\n",
      "Total loss:  -1.175 | PDE Loss:  -1.9523 | Function Loss:  -2.2543\n",
      "Total loss:  -1.175 | PDE Loss:  -1.9523 | Function Loss:  -2.2543\n",
      "Total loss:  -1.175 | PDE Loss:  -1.9523 | Function Loss:  -2.2544\n",
      "Total loss:  -1.175 | PDE Loss:  -1.9522 | Function Loss:  -2.2544\n",
      "Total loss:  -1.175 | PDE Loss:  -1.9523 | Function Loss:  -2.2544\n",
      "Total loss:  -1.1751 | PDE Loss:  -1.9522 | Function Loss:  -2.2544\n",
      "Total loss:  -1.1751 | PDE Loss:  -1.9523 | Function Loss:  -2.2545\n",
      "Total loss:  -1.1751 | PDE Loss:  -1.9526 | Function Loss:  -2.2544\n",
      "Total loss:  -1.1752 | PDE Loss:  -1.9527 | Function Loss:  -2.2545\n",
      "Total loss:  -1.1752 | PDE Loss:  -1.9529 | Function Loss:  -2.2544\n",
      "Total loss:  -1.1752 | PDE Loss:  -1.953 | Function Loss:  -2.2545\n",
      "Total loss:  -1.1752 | PDE Loss:  -1.9532 | Function Loss:  -2.2544\n",
      "Total loss:  -1.1752 | PDE Loss:  -1.9533 | Function Loss:  -2.2544\n",
      "Total loss:  -1.1753 | PDE Loss:  -1.9534 | Function Loss:  -2.2544\n",
      "Total loss:  -1.1753 | PDE Loss:  -1.9536 | Function Loss:  -2.2545\n",
      "Total loss:  -1.1753 | PDE Loss:  -1.9537 | Function Loss:  -2.2545\n",
      "Total loss:  -1.1754 | PDE Loss:  -1.9538 | Function Loss:  -2.2545\n",
      "Total loss:  -1.1754 | PDE Loss:  -1.9538 | Function Loss:  -2.2545\n",
      "Total loss:  -1.1754 | PDE Loss:  -1.9538 | Function Loss:  -2.2546\n",
      "Total loss:  -1.1754 | PDE Loss:  -1.9535 | Function Loss:  -2.2546\n",
      "Total loss:  -1.1755 | PDE Loss:  -1.9537 | Function Loss:  -2.2546\n",
      "Total loss:  -1.1755 | PDE Loss:  -1.9535 | Function Loss:  -2.2547\n",
      "Total loss:  -1.1755 | PDE Loss:  -1.9533 | Function Loss:  -2.2548\n",
      "Total loss:  -1.1756 | PDE Loss:  -1.953 | Function Loss:  -2.2549\n",
      "Total loss:  -1.1756 | PDE Loss:  -1.9526 | Function Loss:  -2.255\n",
      "Total loss:  -1.1756 | PDE Loss:  -1.9523 | Function Loss:  -2.2551\n",
      "Total loss:  -1.1757 | PDE Loss:  -1.9521 | Function Loss:  -2.2552\n",
      "Total loss:  -1.1757 | PDE Loss:  -1.952 | Function Loss:  -2.2552\n",
      "Total loss:  -1.1757 | PDE Loss:  -1.9519 | Function Loss:  -2.2553\n",
      "Total loss:  -1.1758 | PDE Loss:  -1.9519 | Function Loss:  -2.2553\n",
      "Total loss:  -1.1758 | PDE Loss:  -1.9521 | Function Loss:  -2.2553\n",
      "Total loss:  -1.1758 | PDE Loss:  -1.9524 | Function Loss:  -2.2553\n",
      "Total loss:  -1.1758 | PDE Loss:  -1.9528 | Function Loss:  -2.2553\n",
      "Total loss:  -1.1759 | PDE Loss:  -1.9532 | Function Loss:  -2.2552\n",
      "Total loss:  -1.1759 | PDE Loss:  -1.9535 | Function Loss:  -2.2552\n",
      "Total loss:  -1.1759 | PDE Loss:  -1.954 | Function Loss:  -2.2551\n",
      "Total loss:  -1.1759 | PDE Loss:  -1.9542 | Function Loss:  -2.2551\n",
      "Total loss:  -1.1759 | PDE Loss:  -1.9543 | Function Loss:  -2.2551\n",
      "Total loss:  -1.176 | PDE Loss:  -1.9544 | Function Loss:  -2.2551\n",
      "Total loss:  -1.176 | PDE Loss:  -1.9544 | Function Loss:  -2.2551\n",
      "Total loss:  -1.176 | PDE Loss:  -1.9544 | Function Loss:  -2.2551\n",
      "Total loss:  -1.176 | PDE Loss:  -1.9544 | Function Loss:  -2.2551\n",
      "Total loss:  -1.176 | PDE Loss:  -1.9544 | Function Loss:  -2.2551\n",
      "Total loss:  -1.176 | PDE Loss:  -1.9544 | Function Loss:  -2.2551\n",
      "Total loss:  -1.176 | PDE Loss:  -1.9547 | Function Loss:  -2.2551\n",
      "Total loss:  -1.176 | PDE Loss:  -1.9545 | Function Loss:  -2.2551\n",
      "Total loss:  -1.176 | PDE Loss:  -1.9545 | Function Loss:  -2.2552\n",
      "Total loss:  -1.1761 | PDE Loss:  -1.9544 | Function Loss:  -2.2552\n",
      "Total loss:  -1.1761 | PDE Loss:  -1.9544 | Function Loss:  -2.2552\n",
      "Total loss:  -1.1761 | PDE Loss:  -1.9545 | Function Loss:  -2.2553\n",
      "Total loss:  -1.1762 | PDE Loss:  -1.9546 | Function Loss:  -2.2553\n",
      "Total loss:  -1.1762 | PDE Loss:  -1.9548 | Function Loss:  -2.2553\n",
      "Total loss:  -1.1762 | PDE Loss:  -1.9549 | Function Loss:  -2.2553\n",
      "Total loss:  -1.1763 | PDE Loss:  -1.9551 | Function Loss:  -2.2553\n",
      "Total loss:  -1.1763 | PDE Loss:  -1.9551 | Function Loss:  -2.2554\n",
      "Total loss:  -1.1763 | PDE Loss:  -1.9552 | Function Loss:  -2.2554\n",
      "Total loss:  -1.1764 | PDE Loss:  -1.9552 | Function Loss:  -2.2554\n",
      "Total loss:  -1.1764 | PDE Loss:  -1.9551 | Function Loss:  -2.2555\n",
      "Total loss:  -1.1764 | PDE Loss:  -1.955 | Function Loss:  -2.2555\n",
      "Total loss:  -1.1765 | PDE Loss:  -1.955 | Function Loss:  -2.2556\n",
      "Total loss:  -1.1765 | PDE Loss:  -1.9549 | Function Loss:  -2.2557\n",
      "Total loss:  -1.1766 | PDE Loss:  -1.9549 | Function Loss:  -2.2557\n",
      "Total loss:  -1.1766 | PDE Loss:  -1.9549 | Function Loss:  -2.2557\n",
      "Total loss:  -1.1766 | PDE Loss:  -1.955 | Function Loss:  -2.2558\n",
      "Total loss:  -1.1767 | PDE Loss:  -1.9551 | Function Loss:  -2.2558\n",
      "Total loss:  -1.1767 | PDE Loss:  -1.9553 | Function Loss:  -2.2558\n",
      "Total loss:  -1.1767 | PDE Loss:  -1.9555 | Function Loss:  -2.2557\n",
      "Total loss:  -1.1767 | PDE Loss:  -1.9557 | Function Loss:  -2.2557\n",
      "Total loss:  -1.1767 | PDE Loss:  -1.9558 | Function Loss:  -2.2557\n",
      "Total loss:  -1.1768 | PDE Loss:  -1.956 | Function Loss:  -2.2557\n",
      "Total loss:  -1.1768 | PDE Loss:  -1.9563 | Function Loss:  -2.2557\n",
      "Total loss:  -1.1768 | PDE Loss:  -1.9562 | Function Loss:  -2.2557\n",
      "Total loss:  -1.1768 | PDE Loss:  -1.9562 | Function Loss:  -2.2557\n",
      "Total loss:  -1.1768 | PDE Loss:  -1.9562 | Function Loss:  -2.2558\n",
      "Total loss:  -1.1768 | PDE Loss:  -1.956 | Function Loss:  -2.2558\n",
      "Total loss:  -1.1768 | PDE Loss:  -1.9559 | Function Loss:  -2.2558\n",
      "Total loss:  -1.1769 | PDE Loss:  -1.9558 | Function Loss:  -2.2559\n",
      "Total loss:  -1.1769 | PDE Loss:  -1.9556 | Function Loss:  -2.2559\n",
      "Total loss:  -1.1769 | PDE Loss:  -1.9554 | Function Loss:  -2.256\n",
      "Total loss:  -1.1769 | PDE Loss:  -1.9553 | Function Loss:  -2.256\n",
      "Total loss:  -1.1769 | PDE Loss:  -1.9551 | Function Loss:  -2.2561\n",
      "Total loss:  -1.177 | PDE Loss:  -1.9551 | Function Loss:  -2.2562\n",
      "Total loss:  -1.177 | PDE Loss:  -1.9552 | Function Loss:  -2.2562\n",
      "Total loss:  -1.177 | PDE Loss:  -1.9554 | Function Loss:  -2.2562\n",
      "Total loss:  -1.1771 | PDE Loss:  -1.9556 | Function Loss:  -2.2562\n",
      "Total loss:  -1.1771 | PDE Loss:  -1.956 | Function Loss:  -2.2561\n",
      "Total loss:  -1.1771 | PDE Loss:  -1.9561 | Function Loss:  -2.2561\n",
      "Total loss:  -1.1771 | PDE Loss:  -1.9562 | Function Loss:  -2.2561\n",
      "Total loss:  -1.1771 | PDE Loss:  -1.9562 | Function Loss:  -2.2561\n",
      "Total loss:  -1.1772 | PDE Loss:  -1.9562 | Function Loss:  -2.2562\n",
      "Total loss:  -1.1772 | PDE Loss:  -1.9561 | Function Loss:  -2.2562\n",
      "Total loss:  -1.1772 | PDE Loss:  -1.956 | Function Loss:  -2.2563\n",
      "Total loss:  -1.1772 | PDE Loss:  -1.9558 | Function Loss:  -2.2563\n",
      "Total loss:  -1.1773 | PDE Loss:  -1.9556 | Function Loss:  -2.2564\n",
      "Total loss:  -1.1773 | PDE Loss:  -1.9555 | Function Loss:  -2.2565\n",
      "Total loss:  -1.1773 | PDE Loss:  -1.9553 | Function Loss:  -2.2565\n",
      "Total loss:  -1.1773 | PDE Loss:  -1.9551 | Function Loss:  -2.2566\n",
      "Total loss:  -1.1774 | PDE Loss:  -1.9551 | Function Loss:  -2.2566\n",
      "Total loss:  -1.1774 | PDE Loss:  -1.9552 | Function Loss:  -2.2566\n",
      "Total loss:  -1.1774 | PDE Loss:  -1.9554 | Function Loss:  -2.2566\n",
      "Total loss:  -1.1774 | PDE Loss:  -1.9557 | Function Loss:  -2.2566\n",
      "Total loss:  -1.1775 | PDE Loss:  -1.9559 | Function Loss:  -2.2566\n",
      "Total loss:  -1.1775 | PDE Loss:  -1.9562 | Function Loss:  -2.2565\n",
      "Total loss:  -1.1775 | PDE Loss:  -1.9562 | Function Loss:  -2.2565\n",
      "Total loss:  -1.1775 | PDE Loss:  -1.9562 | Function Loss:  -2.2565\n",
      "Total loss:  -1.1775 | PDE Loss:  -1.9564 | Function Loss:  -2.2565\n",
      "Total loss:  -1.1775 | PDE Loss:  -1.9565 | Function Loss:  -2.2565\n",
      "Total loss:  -1.1775 | PDE Loss:  -1.9565 | Function Loss:  -2.2566\n",
      "Total loss:  -1.1776 | PDE Loss:  -1.9565 | Function Loss:  -2.2566\n",
      "Total loss:  -1.1776 | PDE Loss:  -1.9564 | Function Loss:  -2.2566\n",
      "Total loss:  -1.1776 | PDE Loss:  -1.9562 | Function Loss:  -2.2567\n",
      "Total loss:  -1.1776 | PDE Loss:  -1.9561 | Function Loss:  -2.2567\n",
      "Total loss:  -1.1776 | PDE Loss:  -1.9559 | Function Loss:  -2.2568\n",
      "Total loss:  -1.1777 | PDE Loss:  -1.9556 | Function Loss:  -2.2569\n",
      "Total loss:  -1.1777 | PDE Loss:  -1.9555 | Function Loss:  -2.2569\n",
      "Total loss:  -1.1777 | PDE Loss:  -1.9554 | Function Loss:  -2.257\n",
      "Total loss:  -1.1777 | PDE Loss:  -1.9554 | Function Loss:  -2.257\n",
      "Total loss:  -1.1778 | PDE Loss:  -1.9555 | Function Loss:  -2.257\n",
      "Total loss:  -1.1778 | PDE Loss:  -1.9557 | Function Loss:  -2.257\n",
      "Total loss:  -1.1779 | PDE Loss:  -1.9559 | Function Loss:  -2.2571\n",
      "Total loss:  -1.1779 | PDE Loss:  -1.9562 | Function Loss:  -2.2571\n",
      "Total loss:  -1.178 | PDE Loss:  -1.9564 | Function Loss:  -2.2571\n",
      "Total loss:  -1.178 | PDE Loss:  -1.9565 | Function Loss:  -2.2571\n",
      "Total loss:  -1.178 | PDE Loss:  -1.9565 | Function Loss:  -2.2572\n",
      "Total loss:  -1.1781 | PDE Loss:  -1.9565 | Function Loss:  -2.2572\n",
      "Total loss:  -1.1782 | PDE Loss:  -1.9562 | Function Loss:  -2.2574\n",
      "Total loss:  -1.1781 | PDE Loss:  -1.955 | Function Loss:  -2.2575\n",
      "Total loss:  -1.1782 | PDE Loss:  -1.956 | Function Loss:  -2.2575\n",
      "Total loss:  -1.1782 | PDE Loss:  -1.9557 | Function Loss:  -2.2576\n",
      "Total loss:  -1.1783 | PDE Loss:  -1.9553 | Function Loss:  -2.2577\n",
      "Total loss:  -1.1783 | PDE Loss:  -1.9549 | Function Loss:  -2.2578\n",
      "Total loss:  -1.1784 | PDE Loss:  -1.9545 | Function Loss:  -2.2579\n",
      "Total loss:  -1.1784 | PDE Loss:  -1.9544 | Function Loss:  -2.258\n",
      "Total loss:  -1.1784 | PDE Loss:  -1.9543 | Function Loss:  -2.258\n",
      "Total loss:  -1.1784 | PDE Loss:  -1.9542 | Function Loss:  -2.2581\n",
      "Total loss:  -1.1784 | PDE Loss:  -1.9543 | Function Loss:  -2.2581\n",
      "Total loss:  -1.1784 | PDE Loss:  -1.9543 | Function Loss:  -2.2581\n",
      "Total loss:  -1.1785 | PDE Loss:  -1.9546 | Function Loss:  -2.2581\n",
      "Total loss:  -1.1785 | PDE Loss:  -1.9548 | Function Loss:  -2.258\n",
      "Total loss:  -1.1785 | PDE Loss:  -1.9551 | Function Loss:  -2.258\n",
      "Total loss:  -1.1785 | PDE Loss:  -1.9553 | Function Loss:  -2.258\n",
      "Total loss:  -1.1785 | PDE Loss:  -1.9554 | Function Loss:  -2.258\n",
      "Total loss:  -1.1786 | PDE Loss:  -1.9556 | Function Loss:  -2.258\n",
      "Total loss:  -1.1786 | PDE Loss:  -1.9556 | Function Loss:  -2.258\n",
      "Total loss:  -1.1786 | PDE Loss:  -1.9556 | Function Loss:  -2.258\n",
      "Total loss:  -1.1786 | PDE Loss:  -1.9556 | Function Loss:  -2.2581\n",
      "Total loss:  -1.1787 | PDE Loss:  -1.9555 | Function Loss:  -2.2581\n",
      "Total loss:  -1.1787 | PDE Loss:  -1.9554 | Function Loss:  -2.2581\n",
      "Total loss:  -1.1787 | PDE Loss:  -1.9554 | Function Loss:  -2.2582\n",
      "Total loss:  -1.1787 | PDE Loss:  -1.9554 | Function Loss:  -2.2582\n",
      "Total loss:  -1.1787 | PDE Loss:  -1.9554 | Function Loss:  -2.2582\n",
      "Total loss:  -1.1787 | PDE Loss:  -1.9556 | Function Loss:  -2.2582\n",
      "Total loss:  -1.1788 | PDE Loss:  -1.9557 | Function Loss:  -2.2582\n",
      "Total loss:  -1.1788 | PDE Loss:  -1.9559 | Function Loss:  -2.2582\n",
      "Total loss:  -1.1788 | PDE Loss:  -1.9561 | Function Loss:  -2.2581\n",
      "Total loss:  -1.1788 | PDE Loss:  -1.9566 | Function Loss:  -2.2581\n",
      "Total loss:  -1.1788 | PDE Loss:  -1.9568 | Function Loss:  -2.2581\n",
      "Total loss:  -1.1789 | PDE Loss:  -1.957 | Function Loss:  -2.2581\n",
      "Total loss:  -1.1789 | PDE Loss:  -1.9572 | Function Loss:  -2.2581\n",
      "Total loss:  -1.1789 | PDE Loss:  -1.9574 | Function Loss:  -2.2581\n",
      "Total loss:  -1.179 | PDE Loss:  -1.9576 | Function Loss:  -2.2581\n",
      "Total loss:  -1.179 | PDE Loss:  -1.9576 | Function Loss:  -2.2581\n",
      "Total loss:  -1.179 | PDE Loss:  -1.9576 | Function Loss:  -2.2581\n",
      "Total loss:  -1.179 | PDE Loss:  -1.9577 | Function Loss:  -2.2581\n",
      "Total loss:  -1.1791 | PDE Loss:  -1.9577 | Function Loss:  -2.2581\n",
      "Total loss:  -1.1791 | PDE Loss:  -1.9578 | Function Loss:  -2.2582\n",
      "Total loss:  -1.1791 | PDE Loss:  -1.9579 | Function Loss:  -2.2582\n",
      "Total loss:  -1.1791 | PDE Loss:  -1.958 | Function Loss:  -2.2582\n",
      "Total loss:  -1.1791 | PDE Loss:  -1.9581 | Function Loss:  -2.2582\n",
      "Total loss:  -1.1792 | PDE Loss:  -1.9583 | Function Loss:  -2.2582\n",
      "Total loss:  -1.1792 | PDE Loss:  -1.9583 | Function Loss:  -2.2582\n",
      "Total loss:  -1.1792 | PDE Loss:  -1.9584 | Function Loss:  -2.2582\n",
      "Total loss:  -1.1792 | PDE Loss:  -1.9585 | Function Loss:  -2.2582\n",
      "Total loss:  -1.1793 | PDE Loss:  -1.9587 | Function Loss:  -2.2582\n",
      "Total loss:  -1.1793 | PDE Loss:  -1.9587 | Function Loss:  -2.2582\n",
      "Total loss:  -1.1793 | PDE Loss:  -1.9588 | Function Loss:  -2.2583\n",
      "Total loss:  -1.1794 | PDE Loss:  -1.9588 | Function Loss:  -2.2583\n",
      "Total loss:  -1.1794 | PDE Loss:  -1.9585 | Function Loss:  -2.2584\n",
      "Total loss:  -1.1794 | PDE Loss:  -1.9585 | Function Loss:  -2.2584\n",
      "Total loss:  -1.1795 | PDE Loss:  -1.9585 | Function Loss:  -2.2585\n",
      "Total loss:  -1.1795 | PDE Loss:  -1.9584 | Function Loss:  -2.2585\n",
      "Total loss:  -1.1795 | PDE Loss:  -1.9583 | Function Loss:  -2.2585\n",
      "Total loss:  -1.1795 | PDE Loss:  -1.9582 | Function Loss:  -2.2586\n",
      "Total loss:  -1.1796 | PDE Loss:  -1.958 | Function Loss:  -2.2587\n",
      "Total loss:  -1.1795 | PDE Loss:  -1.9572 | Function Loss:  -2.2588\n",
      "Total loss:  -1.1796 | PDE Loss:  -1.9577 | Function Loss:  -2.2588\n",
      "Total loss:  -1.1796 | PDE Loss:  -1.9576 | Function Loss:  -2.2588\n",
      "Total loss:  -1.1796 | PDE Loss:  -1.9575 | Function Loss:  -2.2589\n",
      "Total loss:  -1.1797 | PDE Loss:  -1.9573 | Function Loss:  -2.259\n",
      "Total loss:  -1.1797 | PDE Loss:  -1.9572 | Function Loss:  -2.259\n",
      "Total loss:  -1.1797 | PDE Loss:  -1.9572 | Function Loss:  -2.2591\n",
      "Total loss:  -1.1798 | PDE Loss:  -1.9572 | Function Loss:  -2.2591\n",
      "Total loss:  -1.1796 | PDE Loss:  -1.9555 | Function Loss:  -2.2593\n",
      "Total loss:  -1.1798 | PDE Loss:  -1.9569 | Function Loss:  -2.2592\n",
      "Total loss:  -1.1798 | PDE Loss:  -1.957 | Function Loss:  -2.2592\n",
      "Total loss:  -1.1798 | PDE Loss:  -1.957 | Function Loss:  -2.2592\n",
      "Total loss:  -1.1798 | PDE Loss:  -1.9572 | Function Loss:  -2.2592\n",
      "Total loss:  -1.1799 | PDE Loss:  -1.9575 | Function Loss:  -2.2591\n",
      "Total loss:  -1.1799 | PDE Loss:  -1.9576 | Function Loss:  -2.2592\n",
      "Total loss:  -1.1799 | PDE Loss:  -1.9577 | Function Loss:  -2.2592\n",
      "Total loss:  -1.18 | PDE Loss:  -1.958 | Function Loss:  -2.2592\n",
      "Total loss:  -1.18 | PDE Loss:  -1.9582 | Function Loss:  -2.2592\n",
      "Total loss:  -1.18 | PDE Loss:  -1.9584 | Function Loss:  -2.2592\n",
      "Total loss:  -1.1801 | PDE Loss:  -1.9586 | Function Loss:  -2.2592\n",
      "Total loss:  -1.1801 | PDE Loss:  -1.9591 | Function Loss:  -2.2591\n",
      "Total loss:  -1.1801 | PDE Loss:  -1.9591 | Function Loss:  -2.2591\n",
      "Total loss:  -1.1801 | PDE Loss:  -1.9592 | Function Loss:  -2.2591\n",
      "Total loss:  -1.1802 | PDE Loss:  -1.9592 | Function Loss:  -2.2592\n",
      "Total loss:  -1.1802 | PDE Loss:  -1.9593 | Function Loss:  -2.2592\n",
      "Total loss:  -1.1802 | PDE Loss:  -1.9594 | Function Loss:  -2.2592\n",
      "Total loss:  -1.1802 | PDE Loss:  -1.9596 | Function Loss:  -2.2592\n",
      "Total loss:  -1.1803 | PDE Loss:  -1.9596 | Function Loss:  -2.2592\n",
      "Total loss:  -1.1803 | PDE Loss:  -1.9596 | Function Loss:  -2.2592\n",
      "Total loss:  -1.1803 | PDE Loss:  -1.9596 | Function Loss:  -2.2593\n",
      "Total loss:  -1.1803 | PDE Loss:  -1.9596 | Function Loss:  -2.2593\n",
      "Total loss:  -1.1803 | PDE Loss:  -1.9596 | Function Loss:  -2.2593\n",
      "Total loss:  -1.1803 | PDE Loss:  -1.9597 | Function Loss:  -2.2593\n",
      "Total loss:  -1.1804 | PDE Loss:  -1.9597 | Function Loss:  -2.2593\n",
      "Total loss:  -1.1804 | PDE Loss:  -1.9597 | Function Loss:  -2.2593\n",
      "Total loss:  -1.1804 | PDE Loss:  -1.9597 | Function Loss:  -2.2593\n",
      "Total loss:  -1.1804 | PDE Loss:  -1.9597 | Function Loss:  -2.2594\n",
      "Total loss:  -1.1804 | PDE Loss:  -1.9597 | Function Loss:  -2.2594\n",
      "Total loss:  -1.1805 | PDE Loss:  -1.9597 | Function Loss:  -2.2594\n",
      "Total loss:  -1.1805 | PDE Loss:  -1.9597 | Function Loss:  -2.2595\n",
      "Total loss:  -1.1805 | PDE Loss:  -1.9596 | Function Loss:  -2.2595\n",
      "Total loss:  -1.1805 | PDE Loss:  -1.9596 | Function Loss:  -2.2595\n",
      "Total loss:  -1.1805 | PDE Loss:  -1.959 | Function Loss:  -2.2596\n",
      "Total loss:  -1.1805 | PDE Loss:  -1.9594 | Function Loss:  -2.2596\n",
      "Total loss:  -1.1805 | PDE Loss:  -1.9595 | Function Loss:  -2.2596\n",
      "Total loss:  -1.1806 | PDE Loss:  -1.9595 | Function Loss:  -2.2596\n",
      "Total loss:  -1.1806 | PDE Loss:  -1.9595 | Function Loss:  -2.2596\n",
      "Total loss:  -1.1806 | PDE Loss:  -1.9594 | Function Loss:  -2.2597\n",
      "Total loss:  -1.1807 | PDE Loss:  -1.9593 | Function Loss:  -2.2598\n",
      "Total loss:  -1.1807 | PDE Loss:  -1.959 | Function Loss:  -2.2598\n",
      "Total loss:  -1.1807 | PDE Loss:  -1.9588 | Function Loss:  -2.2599\n",
      "Total loss:  -1.1807 | PDE Loss:  -1.9588 | Function Loss:  -2.2599\n",
      "Total loss:  -1.1808 | PDE Loss:  -1.9588 | Function Loss:  -2.26\n",
      "Total loss:  -1.1808 | PDE Loss:  -1.9589 | Function Loss:  -2.26\n",
      "Total loss:  -1.1808 | PDE Loss:  -1.959 | Function Loss:  -2.26\n",
      "Total loss:  -1.1809 | PDE Loss:  -1.9591 | Function Loss:  -2.26\n",
      "Total loss:  -1.1809 | PDE Loss:  -1.9594 | Function Loss:  -2.26\n",
      "Total loss:  -1.1809 | PDE Loss:  -1.9597 | Function Loss:  -2.26\n",
      "Total loss:  -1.1809 | PDE Loss:  -1.9599 | Function Loss:  -2.2599\n",
      "Total loss:  -1.1809 | PDE Loss:  -1.9602 | Function Loss:  -2.2599\n",
      "Total loss:  -1.181 | PDE Loss:  -1.9604 | Function Loss:  -2.2599\n",
      "Total loss:  -1.181 | PDE Loss:  -1.9607 | Function Loss:  -2.2599\n",
      "Total loss:  -1.181 | PDE Loss:  -1.9608 | Function Loss:  -2.2599\n",
      "Total loss:  -1.1811 | PDE Loss:  -1.961 | Function Loss:  -2.2599\n",
      "Total loss:  -1.1811 | PDE Loss:  -1.9611 | Function Loss:  -2.2599\n",
      "Total loss:  -1.1812 | PDE Loss:  -1.9609 | Function Loss:  -2.26\n",
      "Total loss:  -1.1812 | PDE Loss:  -1.9609 | Function Loss:  -2.2601\n",
      "Total loss:  -1.1812 | PDE Loss:  -1.9608 | Function Loss:  -2.2601\n",
      "Total loss:  -1.1813 | PDE Loss:  -1.9607 | Function Loss:  -2.2602\n",
      "Total loss:  -1.1813 | PDE Loss:  -1.9607 | Function Loss:  -2.2603\n",
      "Total loss:  -1.1813 | PDE Loss:  -1.9606 | Function Loss:  -2.2603\n",
      "Total loss:  -1.1813 | PDE Loss:  -1.9599 | Function Loss:  -2.2604\n",
      "Total loss:  -1.1814 | PDE Loss:  -1.9605 | Function Loss:  -2.2604\n",
      "Total loss:  -1.1814 | PDE Loss:  -1.9605 | Function Loss:  -2.2604\n",
      "Total loss:  -1.1814 | PDE Loss:  -1.9606 | Function Loss:  -2.2604\n",
      "Total loss:  -1.1815 | PDE Loss:  -1.9606 | Function Loss:  -2.2604\n",
      "Total loss:  -1.1815 | PDE Loss:  -1.9608 | Function Loss:  -2.2605\n",
      "Total loss:  -1.1815 | PDE Loss:  -1.9608 | Function Loss:  -2.2605\n",
      "Total loss:  -1.1815 | PDE Loss:  -1.961 | Function Loss:  -2.2605\n",
      "Total loss:  -1.1816 | PDE Loss:  -1.9611 | Function Loss:  -2.2605\n",
      "Total loss:  -1.1816 | PDE Loss:  -1.9612 | Function Loss:  -2.2605\n",
      "Total loss:  -1.1816 | PDE Loss:  -1.9613 | Function Loss:  -2.2605\n",
      "Total loss:  -1.1816 | PDE Loss:  -1.9614 | Function Loss:  -2.2605\n",
      "Total loss:  -1.1816 | PDE Loss:  -1.9615 | Function Loss:  -2.2605\n",
      "Total loss:  -1.1817 | PDE Loss:  -1.9616 | Function Loss:  -2.2605\n",
      "Total loss:  -1.1817 | PDE Loss:  -1.9617 | Function Loss:  -2.2605\n",
      "Total loss:  -1.1817 | PDE Loss:  -1.9616 | Function Loss:  -2.2605\n",
      "Total loss:  -1.1817 | PDE Loss:  -1.9617 | Function Loss:  -2.2605\n",
      "Total loss:  -1.1817 | PDE Loss:  -1.9617 | Function Loss:  -2.2606\n",
      "Total loss:  -1.1818 | PDE Loss:  -1.9615 | Function Loss:  -2.2606\n",
      "Total loss:  -1.1818 | PDE Loss:  -1.9616 | Function Loss:  -2.2607\n",
      "Total loss:  -1.1818 | PDE Loss:  -1.9616 | Function Loss:  -2.2607\n",
      "Total loss:  -1.1819 | PDE Loss:  -1.9615 | Function Loss:  -2.2608\n",
      "Total loss:  -1.1819 | PDE Loss:  -1.9616 | Function Loss:  -2.2608\n",
      "Total loss:  -1.182 | PDE Loss:  -1.9615 | Function Loss:  -2.2609\n",
      "Total loss:  -1.182 | PDE Loss:  -1.9615 | Function Loss:  -2.2609\n",
      "Total loss:  -1.182 | PDE Loss:  -1.9616 | Function Loss:  -2.2609\n",
      "Total loss:  -1.1821 | PDE Loss:  -1.9617 | Function Loss:  -2.261\n",
      "Total loss:  -1.1821 | PDE Loss:  -1.9619 | Function Loss:  -2.261\n",
      "Total loss:  -1.1821 | PDE Loss:  -1.9622 | Function Loss:  -2.261\n",
      "Total loss:  -1.1822 | PDE Loss:  -1.9624 | Function Loss:  -2.2609\n",
      "Total loss:  -1.1822 | PDE Loss:  -1.9627 | Function Loss:  -2.2609\n",
      "Total loss:  -1.1822 | PDE Loss:  -1.9629 | Function Loss:  -2.2609\n",
      "Total loss:  -1.1822 | PDE Loss:  -1.9632 | Function Loss:  -2.2609\n",
      "Total loss:  -1.1823 | PDE Loss:  -1.9634 | Function Loss:  -2.2608\n",
      "Total loss:  -1.1823 | PDE Loss:  -1.9637 | Function Loss:  -2.2608\n",
      "Total loss:  -1.1823 | PDE Loss:  -1.9639 | Function Loss:  -2.2608\n",
      "Total loss:  -1.1823 | PDE Loss:  -1.9641 | Function Loss:  -2.2608\n",
      "Total loss:  -1.1823 | PDE Loss:  -1.9643 | Function Loss:  -2.2608\n",
      "Total loss:  -1.1824 | PDE Loss:  -1.9644 | Function Loss:  -2.2608\n",
      "Total loss:  -1.1824 | PDE Loss:  -1.9645 | Function Loss:  -2.2608\n",
      "Total loss:  -1.1824 | PDE Loss:  -1.9647 | Function Loss:  -2.2608\n",
      "Total loss:  -1.1825 | PDE Loss:  -1.9647 | Function Loss:  -2.2608\n",
      "Total loss:  -1.1825 | PDE Loss:  -1.9646 | Function Loss:  -2.2609\n",
      "Total loss:  -1.1825 | PDE Loss:  -1.9645 | Function Loss:  -2.2609\n",
      "Total loss:  -1.1825 | PDE Loss:  -1.9645 | Function Loss:  -2.261\n",
      "Total loss:  -1.1826 | PDE Loss:  -1.9645 | Function Loss:  -2.261\n",
      "Total loss:  -1.1826 | PDE Loss:  -1.9644 | Function Loss:  -2.261\n",
      "Total loss:  -1.1826 | PDE Loss:  -1.9644 | Function Loss:  -2.261\n",
      "Total loss:  -1.1826 | PDE Loss:  -1.9644 | Function Loss:  -2.261\n",
      "Total loss:  -1.1826 | PDE Loss:  -1.9645 | Function Loss:  -2.2611\n",
      "Total loss:  -1.1826 | PDE Loss:  -1.9646 | Function Loss:  -2.261\n",
      "Total loss:  -1.1826 | PDE Loss:  -1.9646 | Function Loss:  -2.261\n",
      "Total loss:  -1.1826 | PDE Loss:  -1.9646 | Function Loss:  -2.2611\n",
      "Total loss:  -1.1826 | PDE Loss:  -1.9647 | Function Loss:  -2.2611\n",
      "Total loss:  -1.1827 | PDE Loss:  -1.9647 | Function Loss:  -2.2611\n",
      "Total loss:  -1.1827 | PDE Loss:  -1.9647 | Function Loss:  -2.2611\n",
      "Total loss:  -1.1827 | PDE Loss:  -1.9648 | Function Loss:  -2.2611\n",
      "Total loss:  -1.1827 | PDE Loss:  -1.9648 | Function Loss:  -2.2611\n",
      "Total loss:  -1.1827 | PDE Loss:  -1.9648 | Function Loss:  -2.2611\n",
      "Total loss:  -1.1827 | PDE Loss:  -1.965 | Function Loss:  -2.2611\n",
      "Total loss:  -1.1827 | PDE Loss:  -1.9649 | Function Loss:  -2.2611\n",
      "Total loss:  -1.1828 | PDE Loss:  -1.9649 | Function Loss:  -2.2611\n",
      "Total loss:  -1.1828 | PDE Loss:  -1.9648 | Function Loss:  -2.2612\n",
      "Total loss:  -1.1828 | PDE Loss:  -1.9647 | Function Loss:  -2.2613\n",
      "Total loss:  -1.1828 | PDE Loss:  -1.9647 | Function Loss:  -2.2613\n",
      "Total loss:  -1.1829 | PDE Loss:  -1.9647 | Function Loss:  -2.2613\n",
      "Total loss:  -1.1829 | PDE Loss:  -1.9646 | Function Loss:  -2.2614\n",
      "Total loss:  -1.1829 | PDE Loss:  -1.9644 | Function Loss:  -2.2614\n",
      "Total loss:  -1.1829 | PDE Loss:  -1.9644 | Function Loss:  -2.2614\n",
      "Total loss:  -1.183 | PDE Loss:  -1.9645 | Function Loss:  -2.2615\n",
      "Total loss:  -1.183 | PDE Loss:  -1.9646 | Function Loss:  -2.2615\n",
      "Total loss:  -1.183 | PDE Loss:  -1.9647 | Function Loss:  -2.2615\n",
      "Total loss:  -1.183 | PDE Loss:  -1.9648 | Function Loss:  -2.2615\n",
      "Total loss:  -1.183 | PDE Loss:  -1.9649 | Function Loss:  -2.2615\n",
      "Total loss:  -1.1831 | PDE Loss:  -1.9652 | Function Loss:  -2.2615\n",
      "Total loss:  -1.1831 | PDE Loss:  -1.9654 | Function Loss:  -2.2615\n",
      "Total loss:  -1.1832 | PDE Loss:  -1.9656 | Function Loss:  -2.2615\n",
      "Total loss:  -1.1832 | PDE Loss:  -1.9657 | Function Loss:  -2.2615\n",
      "Total loss:  -1.1832 | PDE Loss:  -1.9658 | Function Loss:  -2.2615\n",
      "Total loss:  -1.1832 | PDE Loss:  -1.9659 | Function Loss:  -2.2615\n",
      "Total loss:  -1.1832 | PDE Loss:  -1.966 | Function Loss:  -2.2615\n",
      "Total loss:  -1.1832 | PDE Loss:  -1.966 | Function Loss:  -2.2615\n",
      "Total loss:  -1.1833 | PDE Loss:  -1.966 | Function Loss:  -2.2615\n",
      "Total loss:  -1.1833 | PDE Loss:  -1.9659 | Function Loss:  -2.2616\n",
      "Total loss:  -1.1833 | PDE Loss:  -1.9658 | Function Loss:  -2.2616\n",
      "Total loss:  -1.1833 | PDE Loss:  -1.9658 | Function Loss:  -2.2617\n",
      "Total loss:  -1.1834 | PDE Loss:  -1.9657 | Function Loss:  -2.2617\n",
      "Total loss:  -1.1834 | PDE Loss:  -1.9658 | Function Loss:  -2.2617\n",
      "Total loss:  -1.1834 | PDE Loss:  -1.9657 | Function Loss:  -2.2618\n",
      "Total loss:  -1.1835 | PDE Loss:  -1.9656 | Function Loss:  -2.2619\n",
      "Total loss:  -1.1835 | PDE Loss:  -1.9656 | Function Loss:  -2.2619\n",
      "Total loss:  -1.1835 | PDE Loss:  -1.9657 | Function Loss:  -2.2619\n",
      "Total loss:  -1.1835 | PDE Loss:  -1.9657 | Function Loss:  -2.2619\n",
      "Total loss:  -1.1836 | PDE Loss:  -1.9662 | Function Loss:  -2.2619\n",
      "Total loss:  -1.1836 | PDE Loss:  -1.9662 | Function Loss:  -2.2619\n",
      "Total loss:  -1.1836 | PDE Loss:  -1.9664 | Function Loss:  -2.2618\n",
      "Total loss:  -1.1836 | PDE Loss:  -1.9666 | Function Loss:  -2.2618\n",
      "Total loss:  -1.1836 | PDE Loss:  -1.9669 | Function Loss:  -2.2618\n",
      "Total loss:  -1.1837 | PDE Loss:  -1.967 | Function Loss:  -2.2618\n",
      "Total loss:  -1.1837 | PDE Loss:  -1.9673 | Function Loss:  -2.2618\n",
      "Total loss:  -1.1837 | PDE Loss:  -1.9674 | Function Loss:  -2.2618\n",
      "Total loss:  -1.1837 | PDE Loss:  -1.9674 | Function Loss:  -2.2618\n",
      "Total loss:  -1.1837 | PDE Loss:  -1.9675 | Function Loss:  -2.2618\n",
      "Total loss:  -1.1838 | PDE Loss:  -1.9675 | Function Loss:  -2.2618\n",
      "Total loss:  -1.1838 | PDE Loss:  -1.9677 | Function Loss:  -2.2618\n",
      "Total loss:  -1.1838 | PDE Loss:  -1.9676 | Function Loss:  -2.2618\n",
      "Total loss:  -1.1838 | PDE Loss:  -1.9676 | Function Loss:  -2.2619\n",
      "Total loss:  -1.1838 | PDE Loss:  -1.9676 | Function Loss:  -2.2619\n",
      "Total loss:  -1.1838 | PDE Loss:  -1.9677 | Function Loss:  -2.2619\n",
      "Total loss:  -1.1838 | PDE Loss:  -1.9678 | Function Loss:  -2.2619\n",
      "Total loss:  -1.1839 | PDE Loss:  -1.9679 | Function Loss:  -2.2619\n",
      "Total loss:  -1.1839 | PDE Loss:  -1.968 | Function Loss:  -2.2619\n",
      "Total loss:  -1.1839 | PDE Loss:  -1.9681 | Function Loss:  -2.2619\n",
      "Total loss:  -1.1839 | PDE Loss:  -1.9682 | Function Loss:  -2.2619\n",
      "Total loss:  -1.1839 | PDE Loss:  -1.9683 | Function Loss:  -2.2619\n",
      "Total loss:  -1.1839 | PDE Loss:  -1.9684 | Function Loss:  -2.2619\n",
      "Total loss:  -1.184 | PDE Loss:  -1.9684 | Function Loss:  -2.2619\n",
      "Total loss:  -1.184 | PDE Loss:  -1.9683 | Function Loss:  -2.2619\n",
      "Total loss:  -1.184 | PDE Loss:  -1.9683 | Function Loss:  -2.2619\n",
      "Total loss:  -1.184 | PDE Loss:  -1.9683 | Function Loss:  -2.262\n",
      "Total loss:  -1.184 | PDE Loss:  -1.9682 | Function Loss:  -2.262\n",
      "Total loss:  -1.184 | PDE Loss:  -1.9682 | Function Loss:  -2.262\n",
      "Total loss:  -1.184 | PDE Loss:  -1.9683 | Function Loss:  -2.262\n",
      "Total loss:  -1.184 | PDE Loss:  -1.9685 | Function Loss:  -2.262\n",
      "Total loss:  -1.1841 | PDE Loss:  -1.9686 | Function Loss:  -2.262\n",
      "Total loss:  -1.1841 | PDE Loss:  -1.9688 | Function Loss:  -2.2619\n",
      "Total loss:  -1.1841 | PDE Loss:  -1.9689 | Function Loss:  -2.2619\n",
      "Total loss:  -1.1841 | PDE Loss:  -1.9691 | Function Loss:  -2.2619\n",
      "Total loss:  -1.1841 | PDE Loss:  -1.9693 | Function Loss:  -2.2619\n",
      "Total loss:  -1.1841 | PDE Loss:  -1.9696 | Function Loss:  -2.2618\n",
      "Total loss:  -1.1841 | PDE Loss:  -1.9698 | Function Loss:  -2.2618\n",
      "Total loss:  -1.1842 | PDE Loss:  -1.97 | Function Loss:  -2.2618\n",
      "Total loss:  -1.1842 | PDE Loss:  -1.9701 | Function Loss:  -2.2618\n",
      "Total loss:  -1.1842 | PDE Loss:  -1.9701 | Function Loss:  -2.2619\n",
      "Total loss:  -1.1842 | PDE Loss:  -1.9701 | Function Loss:  -2.2619\n",
      "Total loss:  -1.1843 | PDE Loss:  -1.9702 | Function Loss:  -2.2619\n",
      "Total loss:  -1.1843 | PDE Loss:  -1.9702 | Function Loss:  -2.262\n",
      "Total loss:  -1.1843 | PDE Loss:  -1.9703 | Function Loss:  -2.262\n",
      "Total loss:  -1.1844 | PDE Loss:  -1.9705 | Function Loss:  -2.262\n",
      "Total loss:  -1.1844 | PDE Loss:  -1.9706 | Function Loss:  -2.262\n",
      "Total loss:  -1.1844 | PDE Loss:  -1.9707 | Function Loss:  -2.262\n",
      "Total loss:  -1.1844 | PDE Loss:  -1.9708 | Function Loss:  -2.262\n",
      "Total loss:  -1.1845 | PDE Loss:  -1.9707 | Function Loss:  -2.262\n",
      "Total loss:  -1.1845 | PDE Loss:  -1.9706 | Function Loss:  -2.2621\n",
      "Total loss:  -1.1845 | PDE Loss:  -1.9705 | Function Loss:  -2.2622\n",
      "Total loss:  -1.1846 | PDE Loss:  -1.9704 | Function Loss:  -2.2622\n",
      "Total loss:  -1.1846 | PDE Loss:  -1.9703 | Function Loss:  -2.2623\n",
      "Total loss:  -1.1846 | PDE Loss:  -1.9702 | Function Loss:  -2.2623\n",
      "Total loss:  -1.1846 | PDE Loss:  -1.9703 | Function Loss:  -2.2623\n",
      "Total loss:  -1.1847 | PDE Loss:  -1.9701 | Function Loss:  -2.2624\n",
      "Total loss:  -1.1847 | PDE Loss:  -1.9702 | Function Loss:  -2.2624\n",
      "Total loss:  -1.1847 | PDE Loss:  -1.9702 | Function Loss:  -2.2624\n",
      "Total loss:  -1.1847 | PDE Loss:  -1.9703 | Function Loss:  -2.2625\n",
      "Total loss:  -1.1848 | PDE Loss:  -1.9704 | Function Loss:  -2.2625\n",
      "Total loss:  -1.1848 | PDE Loss:  -1.9706 | Function Loss:  -2.2625\n",
      "Total loss:  -1.1848 | PDE Loss:  -1.9709 | Function Loss:  -2.2624\n",
      "Total loss:  -1.1848 | PDE Loss:  -1.9709 | Function Loss:  -2.2624\n",
      "Total loss:  -1.1849 | PDE Loss:  -1.971 | Function Loss:  -2.2624\n",
      "Total loss:  -1.1849 | PDE Loss:  -1.9712 | Function Loss:  -2.2624\n",
      "Total loss:  -1.1849 | PDE Loss:  -1.9714 | Function Loss:  -2.2624\n",
      "Total loss:  -1.1849 | PDE Loss:  -1.9715 | Function Loss:  -2.2625\n",
      "Total loss:  -1.185 | PDE Loss:  -1.9716 | Function Loss:  -2.2625\n",
      "Total loss:  -1.1849 | PDE Loss:  -1.9716 | Function Loss:  -2.2624\n",
      "Total loss:  -1.185 | PDE Loss:  -1.9716 | Function Loss:  -2.2625\n",
      "Total loss:  -1.185 | PDE Loss:  -1.9717 | Function Loss:  -2.2625\n",
      "Total loss:  -1.185 | PDE Loss:  -1.9718 | Function Loss:  -2.2625\n",
      "Total loss:  -1.185 | PDE Loss:  -1.9719 | Function Loss:  -2.2625\n",
      "Total loss:  -1.1851 | PDE Loss:  -1.9719 | Function Loss:  -2.2625\n",
      "Total loss:  -1.1851 | PDE Loss:  -1.9719 | Function Loss:  -2.2626\n",
      "Total loss:  -1.1851 | PDE Loss:  -1.9717 | Function Loss:  -2.2626\n",
      "Total loss:  -1.1851 | PDE Loss:  -1.9717 | Function Loss:  -2.2627\n",
      "Total loss:  -1.1852 | PDE Loss:  -1.9717 | Function Loss:  -2.2627\n",
      "Total loss:  -1.1852 | PDE Loss:  -1.9716 | Function Loss:  -2.2627\n",
      "Total loss:  -1.1852 | PDE Loss:  -1.9716 | Function Loss:  -2.2628\n",
      "Total loss:  -1.1853 | PDE Loss:  -1.9715 | Function Loss:  -2.2628\n",
      "Total loss:  -1.1853 | PDE Loss:  -1.9715 | Function Loss:  -2.2629\n",
      "Total loss:  -1.1853 | PDE Loss:  -1.9711 | Function Loss:  -2.263\n",
      "Total loss:  -1.1853 | PDE Loss:  -1.9712 | Function Loss:  -2.263\n",
      "Total loss:  -1.1854 | PDE Loss:  -1.9713 | Function Loss:  -2.263\n",
      "Total loss:  -1.1854 | PDE Loss:  -1.9714 | Function Loss:  -2.263\n",
      "Total loss:  -1.1854 | PDE Loss:  -1.9715 | Function Loss:  -2.263\n",
      "Total loss:  -1.1854 | PDE Loss:  -1.9716 | Function Loss:  -2.263\n",
      "Total loss:  -1.1855 | PDE Loss:  -1.9717 | Function Loss:  -2.2631\n",
      "Total loss:  -1.1855 | PDE Loss:  -1.9718 | Function Loss:  -2.2631\n",
      "Total loss:  -1.1856 | PDE Loss:  -1.9721 | Function Loss:  -2.2631\n",
      "Total loss:  -1.1856 | PDE Loss:  -1.9722 | Function Loss:  -2.2631\n",
      "Total loss:  -1.1856 | PDE Loss:  -1.9721 | Function Loss:  -2.2632\n",
      "Total loss:  -1.1857 | PDE Loss:  -1.972 | Function Loss:  -2.2632\n",
      "Total loss:  -1.1857 | PDE Loss:  -1.9721 | Function Loss:  -2.2632\n",
      "Total loss:  -1.1857 | PDE Loss:  -1.9723 | Function Loss:  -2.2632\n",
      "Total loss:  -1.1857 | PDE Loss:  -1.9724 | Function Loss:  -2.2632\n",
      "Total loss:  -1.1857 | PDE Loss:  -1.9724 | Function Loss:  -2.2632\n",
      "Total loss:  -1.1857 | PDE Loss:  -1.9725 | Function Loss:  -2.2632\n",
      "Total loss:  -1.1858 | PDE Loss:  -1.9725 | Function Loss:  -2.2632\n",
      "Total loss:  -1.1858 | PDE Loss:  -1.9726 | Function Loss:  -2.2632\n",
      "Total loss:  -1.1858 | PDE Loss:  -1.9726 | Function Loss:  -2.2633\n",
      "Total loss:  -1.1858 | PDE Loss:  -1.9726 | Function Loss:  -2.2633\n",
      "Total loss:  -1.1858 | PDE Loss:  -1.9726 | Function Loss:  -2.2633\n",
      "Total loss:  -1.1859 | PDE Loss:  -1.9727 | Function Loss:  -2.2633\n",
      "Total loss:  -1.1859 | PDE Loss:  -1.9728 | Function Loss:  -2.2633\n",
      "Total loss:  -1.1859 | PDE Loss:  -1.973 | Function Loss:  -2.2634\n",
      "Total loss:  -1.186 | PDE Loss:  -1.9731 | Function Loss:  -2.2634\n",
      "Total loss:  -1.186 | PDE Loss:  -1.9733 | Function Loss:  -2.2634\n",
      "Total loss:  -1.186 | PDE Loss:  -1.9736 | Function Loss:  -2.2634\n",
      "Total loss:  -1.1861 | PDE Loss:  -1.9739 | Function Loss:  -2.2633\n",
      "Total loss:  -1.1861 | PDE Loss:  -1.9741 | Function Loss:  -2.2633\n",
      "Total loss:  -1.1861 | PDE Loss:  -1.9743 | Function Loss:  -2.2633\n",
      "Total loss:  -1.1862 | PDE Loss:  -1.9745 | Function Loss:  -2.2633\n",
      "Total loss:  -1.1862 | PDE Loss:  -1.9747 | Function Loss:  -2.2633\n",
      "Total loss:  -1.1862 | PDE Loss:  -1.9749 | Function Loss:  -2.2633\n",
      "Total loss:  -1.1863 | PDE Loss:  -1.975 | Function Loss:  -2.2633\n",
      "Total loss:  -1.1863 | PDE Loss:  -1.9752 | Function Loss:  -2.2634\n",
      "Total loss:  -1.1863 | PDE Loss:  -1.9752 | Function Loss:  -2.2634\n",
      "Total loss:  -1.1863 | PDE Loss:  -1.9757 | Function Loss:  -2.2633\n",
      "Total loss:  -1.1863 | PDE Loss:  -1.9755 | Function Loss:  -2.2634\n",
      "Total loss:  -1.1864 | PDE Loss:  -1.9755 | Function Loss:  -2.2634\n",
      "Total loss:  -1.1864 | PDE Loss:  -1.9754 | Function Loss:  -2.2634\n",
      "Total loss:  -1.1864 | PDE Loss:  -1.9754 | Function Loss:  -2.2634\n",
      "Total loss:  -1.1864 | PDE Loss:  -1.9754 | Function Loss:  -2.2635\n",
      "Total loss:  -1.1864 | PDE Loss:  -1.9755 | Function Loss:  -2.2635\n",
      "Total loss:  -1.1864 | PDE Loss:  -1.9755 | Function Loss:  -2.2635\n",
      "Total loss:  -1.1865 | PDE Loss:  -1.9755 | Function Loss:  -2.2635\n",
      "Total loss:  -1.1863 | PDE Loss:  -1.9752 | Function Loss:  -2.2634\n",
      "Total loss:  -1.1865 | PDE Loss:  -1.9756 | Function Loss:  -2.2635\n",
      "Total loss:  -1.1865 | PDE Loss:  -1.9756 | Function Loss:  -2.2635\n",
      "Total loss:  -1.1865 | PDE Loss:  -1.9756 | Function Loss:  -2.2635\n",
      "Total loss:  -1.1865 | PDE Loss:  -1.9756 | Function Loss:  -2.2636\n",
      "Total loss:  -1.1866 | PDE Loss:  -1.9756 | Function Loss:  -2.2636\n",
      "Total loss:  -1.1866 | PDE Loss:  -1.9755 | Function Loss:  -2.2636\n",
      "Total loss:  -1.1866 | PDE Loss:  -1.9754 | Function Loss:  -2.2637\n",
      "Total loss:  -1.1866 | PDE Loss:  -1.9753 | Function Loss:  -2.2637\n",
      "Total loss:  -1.1867 | PDE Loss:  -1.9751 | Function Loss:  -2.2638\n",
      "Total loss:  -1.1867 | PDE Loss:  -1.975 | Function Loss:  -2.2639\n",
      "Total loss:  -1.1867 | PDE Loss:  -1.9749 | Function Loss:  -2.2639\n",
      "Total loss:  -1.1867 | PDE Loss:  -1.9748 | Function Loss:  -2.264\n",
      "Total loss:  -1.1868 | PDE Loss:  -1.9747 | Function Loss:  -2.264\n",
      "Total loss:  -1.1868 | PDE Loss:  -1.9747 | Function Loss:  -2.264\n",
      "Total loss:  -1.1868 | PDE Loss:  -1.9747 | Function Loss:  -2.264\n",
      "Total loss:  -1.1868 | PDE Loss:  -1.9747 | Function Loss:  -2.264\n",
      "Total loss:  -1.1868 | PDE Loss:  -1.9747 | Function Loss:  -2.2641\n",
      "Total loss:  -1.1868 | PDE Loss:  -1.9745 | Function Loss:  -2.2641\n",
      "Total loss:  -1.1868 | PDE Loss:  -1.9744 | Function Loss:  -2.2642\n",
      "Total loss:  -1.1869 | PDE Loss:  -1.9744 | Function Loss:  -2.2642\n",
      "Total loss:  -1.1869 | PDE Loss:  -1.9743 | Function Loss:  -2.2642\n",
      "Total loss:  -1.1869 | PDE Loss:  -1.9742 | Function Loss:  -2.2643\n",
      "Total loss:  -1.1869 | PDE Loss:  -1.9741 | Function Loss:  -2.2643\n",
      "Total loss:  -1.1869 | PDE Loss:  -1.9739 | Function Loss:  -2.2643\n",
      "Total loss:  -1.1869 | PDE Loss:  -1.9738 | Function Loss:  -2.2644\n",
      "Total loss:  -1.187 | PDE Loss:  -1.9736 | Function Loss:  -2.2645\n",
      "Total loss:  -1.187 | PDE Loss:  -1.9731 | Function Loss:  -2.2646\n",
      "Total loss:  -1.187 | PDE Loss:  -1.973 | Function Loss:  -2.2646\n",
      "Total loss:  -1.187 | PDE Loss:  -1.9731 | Function Loss:  -2.2647\n",
      "Total loss:  -1.1871 | PDE Loss:  -1.9731 | Function Loss:  -2.2647\n",
      "Total loss:  -1.1871 | PDE Loss:  -1.9732 | Function Loss:  -2.2647\n",
      "Total loss:  -1.1871 | PDE Loss:  -1.973 | Function Loss:  -2.2647\n",
      "Total loss:  -1.1871 | PDE Loss:  -1.9731 | Function Loss:  -2.2648\n",
      "Total loss:  -1.1871 | PDE Loss:  -1.973 | Function Loss:  -2.2648\n",
      "Total loss:  -1.1871 | PDE Loss:  -1.973 | Function Loss:  -2.2648\n",
      "Total loss:  -1.1872 | PDE Loss:  -1.9729 | Function Loss:  -2.2648\n",
      "Total loss:  -1.1872 | PDE Loss:  -1.9729 | Function Loss:  -2.2648\n",
      "Total loss:  -1.1872 | PDE Loss:  -1.9729 | Function Loss:  -2.2649\n",
      "Total loss:  -1.1872 | PDE Loss:  -1.9729 | Function Loss:  -2.2649\n",
      "Total loss:  -1.1872 | PDE Loss:  -1.9729 | Function Loss:  -2.2649\n",
      "Total loss:  -1.1872 | PDE Loss:  -1.9729 | Function Loss:  -2.2649\n",
      "Total loss:  -1.1872 | PDE Loss:  -1.973 | Function Loss:  -2.2649\n",
      "Total loss:  -1.1872 | PDE Loss:  -1.9731 | Function Loss:  -2.2649\n",
      "Total loss:  -1.1872 | PDE Loss:  -1.9732 | Function Loss:  -2.2649\n",
      "Total loss:  -1.1873 | PDE Loss:  -1.9733 | Function Loss:  -2.2649\n",
      "Total loss:  -1.1873 | PDE Loss:  -1.9735 | Function Loss:  -2.2649\n",
      "Total loss:  -1.1873 | PDE Loss:  -1.9735 | Function Loss:  -2.2648\n",
      "Total loss:  -1.1873 | PDE Loss:  -1.9736 | Function Loss:  -2.2648\n",
      "Total loss:  -1.1873 | PDE Loss:  -1.9736 | Function Loss:  -2.2648\n",
      "Total loss:  -1.1873 | PDE Loss:  -1.9736 | Function Loss:  -2.2649\n",
      "Total loss:  -1.1873 | PDE Loss:  -1.9737 | Function Loss:  -2.2648\n",
      "Total loss:  -1.1873 | PDE Loss:  -1.9737 | Function Loss:  -2.2649\n",
      "Total loss:  -1.1873 | PDE Loss:  -1.9737 | Function Loss:  -2.2649\n",
      "Total loss:  -1.1873 | PDE Loss:  -1.9737 | Function Loss:  -2.2649\n",
      "Total loss:  -1.1873 | PDE Loss:  -1.9736 | Function Loss:  -2.2649\n",
      "Total loss:  -1.1874 | PDE Loss:  -1.9735 | Function Loss:  -2.265\n",
      "Total loss:  -1.1874 | PDE Loss:  -1.9734 | Function Loss:  -2.265\n",
      "Total loss:  -1.1874 | PDE Loss:  -1.9733 | Function Loss:  -2.265\n",
      "Total loss:  -1.1874 | PDE Loss:  -1.9731 | Function Loss:  -2.2651\n",
      "Total loss:  -1.1874 | PDE Loss:  -1.9728 | Function Loss:  -2.2652\n",
      "Total loss:  -1.1874 | PDE Loss:  -1.9728 | Function Loss:  -2.2652\n",
      "Total loss:  -1.1875 | PDE Loss:  -1.9728 | Function Loss:  -2.2652\n",
      "Total loss:  -1.1875 | PDE Loss:  -1.9729 | Function Loss:  -2.2653\n",
      "Total loss:  -1.1875 | PDE Loss:  -1.9729 | Function Loss:  -2.2653\n",
      "Total loss:  -1.1875 | PDE Loss:  -1.9729 | Function Loss:  -2.2653\n",
      "Total loss:  -1.1875 | PDE Loss:  -1.9733 | Function Loss:  -2.2652\n",
      "Total loss:  -1.1876 | PDE Loss:  -1.9732 | Function Loss:  -2.2653\n",
      "Total loss:  -1.1876 | PDE Loss:  -1.9731 | Function Loss:  -2.2653\n",
      "Total loss:  -1.1876 | PDE Loss:  -1.973 | Function Loss:  -2.2653\n",
      "Total loss:  -1.1876 | PDE Loss:  -1.973 | Function Loss:  -2.2654\n",
      "Total loss:  -1.1876 | PDE Loss:  -1.973 | Function Loss:  -2.2654\n",
      "Total loss:  -1.1876 | PDE Loss:  -1.973 | Function Loss:  -2.2654\n",
      "Total loss:  -1.1877 | PDE Loss:  -1.973 | Function Loss:  -2.2654\n",
      "Total loss:  -1.1877 | PDE Loss:  -1.973 | Function Loss:  -2.2655\n",
      "Total loss:  -1.1877 | PDE Loss:  -1.9728 | Function Loss:  -2.2655\n",
      "Total loss:  -1.1877 | PDE Loss:  -1.9728 | Function Loss:  -2.2656\n",
      "Total loss:  -1.1878 | PDE Loss:  -1.9729 | Function Loss:  -2.2656\n",
      "Total loss:  -1.1878 | PDE Loss:  -1.9729 | Function Loss:  -2.2656\n",
      "Total loss:  -1.1878 | PDE Loss:  -1.973 | Function Loss:  -2.2656\n",
      "Total loss:  -1.1878 | PDE Loss:  -1.9731 | Function Loss:  -2.2656\n",
      "Total loss:  -1.1879 | PDE Loss:  -1.9731 | Function Loss:  -2.2656\n",
      "Total loss:  -1.1879 | PDE Loss:  -1.9731 | Function Loss:  -2.2657\n",
      "Total loss:  -1.1879 | PDE Loss:  -1.9731 | Function Loss:  -2.2657\n",
      "Total loss:  -1.1879 | PDE Loss:  -1.9731 | Function Loss:  -2.2657\n",
      "Total loss:  -1.1879 | PDE Loss:  -1.9731 | Function Loss:  -2.2657\n",
      "Total loss:  -1.188 | PDE Loss:  -1.973 | Function Loss:  -2.2658\n",
      "Total loss:  -1.188 | PDE Loss:  -1.9729 | Function Loss:  -2.2658\n",
      "Total loss:  -1.188 | PDE Loss:  -1.9727 | Function Loss:  -2.2659\n",
      "Total loss:  -1.188 | PDE Loss:  -1.9726 | Function Loss:  -2.2659\n",
      "Total loss:  -1.188 | PDE Loss:  -1.9724 | Function Loss:  -2.266\n",
      "Total loss:  -1.1881 | PDE Loss:  -1.9722 | Function Loss:  -2.2661\n",
      "Total loss:  -1.1881 | PDE Loss:  -1.9719 | Function Loss:  -2.2662\n",
      "Total loss:  -1.1882 | PDE Loss:  -1.9717 | Function Loss:  -2.2663\n",
      "Total loss:  -1.1882 | PDE Loss:  -1.9716 | Function Loss:  -2.2663\n",
      "Total loss:  -1.1882 | PDE Loss:  -1.9715 | Function Loss:  -2.2663\n",
      "Total loss:  -1.1882 | PDE Loss:  -1.9717 | Function Loss:  -2.2663\n",
      "Total loss:  -1.1882 | PDE Loss:  -1.9717 | Function Loss:  -2.2663\n",
      "Total loss:  -1.1882 | PDE Loss:  -1.9719 | Function Loss:  -2.2663\n",
      "Total loss:  -1.1883 | PDE Loss:  -1.9721 | Function Loss:  -2.2663\n",
      "Total loss:  -1.1883 | PDE Loss:  -1.9723 | Function Loss:  -2.2663\n",
      "Total loss:  -1.1883 | PDE Loss:  -1.9725 | Function Loss:  -2.2663\n",
      "Total loss:  -1.1884 | PDE Loss:  -1.9726 | Function Loss:  -2.2663\n",
      "Total loss:  -1.1884 | PDE Loss:  -1.9729 | Function Loss:  -2.2663\n",
      "Total loss:  -1.1884 | PDE Loss:  -1.9727 | Function Loss:  -2.2664\n",
      "Total loss:  -1.1884 | PDE Loss:  -1.9729 | Function Loss:  -2.2664\n",
      "Total loss:  -1.1884 | PDE Loss:  -1.9729 | Function Loss:  -2.2664\n",
      "Total loss:  -1.1885 | PDE Loss:  -1.9729 | Function Loss:  -2.2664\n",
      "Total loss:  -1.1885 | PDE Loss:  -1.9729 | Function Loss:  -2.2664\n",
      "Total loss:  -1.1885 | PDE Loss:  -1.9728 | Function Loss:  -2.2665\n",
      "Total loss:  -1.1885 | PDE Loss:  -1.9727 | Function Loss:  -2.2665\n",
      "Total loss:  -1.1885 | PDE Loss:  -1.9727 | Function Loss:  -2.2665\n",
      "Total loss:  -1.1886 | PDE Loss:  -1.9724 | Function Loss:  -2.2666\n",
      "Total loss:  -1.1886 | PDE Loss:  -1.9725 | Function Loss:  -2.2666\n",
      "Total loss:  -1.1886 | PDE Loss:  -1.9726 | Function Loss:  -2.2666\n",
      "Total loss:  -1.1886 | PDE Loss:  -1.9727 | Function Loss:  -2.2667\n",
      "Total loss:  -1.1887 | PDE Loss:  -1.9727 | Function Loss:  -2.2667\n",
      "Total loss:  -1.1887 | PDE Loss:  -1.9728 | Function Loss:  -2.2667\n",
      "Total loss:  -1.1887 | PDE Loss:  -1.9728 | Function Loss:  -2.2667\n",
      "Total loss:  -1.1887 | PDE Loss:  -1.9729 | Function Loss:  -2.2667\n",
      "Total loss:  -1.1888 | PDE Loss:  -1.9728 | Function Loss:  -2.2668\n",
      "Total loss:  -1.1888 | PDE Loss:  -1.9727 | Function Loss:  -2.2668\n",
      "Total loss:  -1.1888 | PDE Loss:  -1.9728 | Function Loss:  -2.2668\n",
      "Total loss:  -1.1888 | PDE Loss:  -1.9728 | Function Loss:  -2.2668\n",
      "Total loss:  -1.1888 | PDE Loss:  -1.9729 | Function Loss:  -2.2668\n",
      "Total loss:  -1.1889 | PDE Loss:  -1.9729 | Function Loss:  -2.2669\n",
      "Total loss:  -1.1889 | PDE Loss:  -1.973 | Function Loss:  -2.2669\n",
      "Total loss:  -1.1889 | PDE Loss:  -1.9731 | Function Loss:  -2.2669\n",
      "Total loss:  -1.1889 | PDE Loss:  -1.9732 | Function Loss:  -2.2669\n",
      "Total loss:  -1.189 | PDE Loss:  -1.9735 | Function Loss:  -2.2669\n",
      "Total loss:  -1.189 | PDE Loss:  -1.9737 | Function Loss:  -2.2668\n",
      "Total loss:  -1.189 | PDE Loss:  -1.9737 | Function Loss:  -2.2669\n",
      "Total loss:  -1.1886 | PDE Loss:  -1.9721 | Function Loss:  -2.2667\n",
      "Total loss:  -1.189 | PDE Loss:  -1.9737 | Function Loss:  -2.2669\n",
      "Total loss:  -1.189 | PDE Loss:  -1.9739 | Function Loss:  -2.2669\n",
      "Total loss:  -1.1891 | PDE Loss:  -1.9741 | Function Loss:  -2.2669\n",
      "Total loss:  -1.1891 | PDE Loss:  -1.9742 | Function Loss:  -2.2669\n",
      "Total loss:  -1.1891 | PDE Loss:  -1.9743 | Function Loss:  -2.2669\n",
      "Total loss:  -1.1891 | PDE Loss:  -1.9744 | Function Loss:  -2.2669\n",
      "Total loss:  -1.1891 | PDE Loss:  -1.9743 | Function Loss:  -2.2669\n",
      "Total loss:  -1.1892 | PDE Loss:  -1.9743 | Function Loss:  -2.2669\n",
      "Total loss:  -1.1892 | PDE Loss:  -1.9744 | Function Loss:  -2.2669\n",
      "Total loss:  -1.1892 | PDE Loss:  -1.9743 | Function Loss:  -2.267\n",
      "Total loss:  -1.1892 | PDE Loss:  -1.9743 | Function Loss:  -2.267\n",
      "Total loss:  -1.1892 | PDE Loss:  -1.9742 | Function Loss:  -2.267\n",
      "Total loss:  -1.1892 | PDE Loss:  -1.9742 | Function Loss:  -2.267\n",
      "Total loss:  -1.1892 | PDE Loss:  -1.9742 | Function Loss:  -2.2671\n",
      "Total loss:  -1.1892 | PDE Loss:  -1.9741 | Function Loss:  -2.2671\n",
      "Total loss:  -1.1893 | PDE Loss:  -1.974 | Function Loss:  -2.2671\n",
      "Total loss:  -1.1893 | PDE Loss:  -1.974 | Function Loss:  -2.2672\n",
      "Total loss:  -1.1893 | PDE Loss:  -1.9739 | Function Loss:  -2.2672\n",
      "Total loss:  -1.1893 | PDE Loss:  -1.9738 | Function Loss:  -2.2673\n",
      "Total loss:  -1.1894 | PDE Loss:  -1.9738 | Function Loss:  -2.2673\n",
      "Total loss:  -1.1894 | PDE Loss:  -1.9738 | Function Loss:  -2.2673\n",
      "Total loss:  -1.1894 | PDE Loss:  -1.9739 | Function Loss:  -2.2673\n",
      "Total loss:  -1.1894 | PDE Loss:  -1.974 | Function Loss:  -2.2673\n",
      "Total loss:  -1.1894 | PDE Loss:  -1.9742 | Function Loss:  -2.2673\n",
      "Total loss:  -1.1893 | PDE Loss:  -1.9738 | Function Loss:  -2.2672\n",
      "Total loss:  -1.1894 | PDE Loss:  -1.9742 | Function Loss:  -2.2673\n",
      "Total loss:  -1.1895 | PDE Loss:  -1.9744 | Function Loss:  -2.2673\n",
      "Total loss:  -1.1895 | PDE Loss:  -1.9746 | Function Loss:  -2.2673\n",
      "Total loss:  -1.1895 | PDE Loss:  -1.9747 | Function Loss:  -2.2673\n",
      "Total loss:  -1.1895 | PDE Loss:  -1.9749 | Function Loss:  -2.2673\n",
      "Total loss:  -1.1895 | PDE Loss:  -1.9751 | Function Loss:  -2.2673\n",
      "Total loss:  -1.1895 | PDE Loss:  -1.9751 | Function Loss:  -2.2673\n",
      "Total loss:  -1.1896 | PDE Loss:  -1.9751 | Function Loss:  -2.2673\n",
      "Total loss:  -1.1896 | PDE Loss:  -1.9754 | Function Loss:  -2.2672\n",
      "Total loss:  -1.1896 | PDE Loss:  -1.9753 | Function Loss:  -2.2673\n",
      "Total loss:  -1.1896 | PDE Loss:  -1.9752 | Function Loss:  -2.2673\n",
      "Total loss:  -1.1896 | PDE Loss:  -1.9752 | Function Loss:  -2.2673\n",
      "Total loss:  -1.1896 | PDE Loss:  -1.975 | Function Loss:  -2.2674\n",
      "Total loss:  -1.1896 | PDE Loss:  -1.9751 | Function Loss:  -2.2674\n",
      "Total loss:  -1.1896 | PDE Loss:  -1.9751 | Function Loss:  -2.2674\n",
      "Total loss:  -1.1897 | PDE Loss:  -1.9753 | Function Loss:  -2.2674\n",
      "Total loss:  -1.1897 | PDE Loss:  -1.9754 | Function Loss:  -2.2674\n",
      "Total loss:  -1.1897 | PDE Loss:  -1.9756 | Function Loss:  -2.2673\n",
      "Total loss:  -1.1897 | PDE Loss:  -1.9757 | Function Loss:  -2.2673\n",
      "Total loss:  -1.1897 | PDE Loss:  -1.9758 | Function Loss:  -2.2673\n",
      "Total loss:  -1.1897 | PDE Loss:  -1.976 | Function Loss:  -2.2673\n",
      "Total loss:  -1.1897 | PDE Loss:  -1.9762 | Function Loss:  -2.2673\n",
      "Total loss:  -1.1898 | PDE Loss:  -1.9763 | Function Loss:  -2.2673\n",
      "Total loss:  -1.1898 | PDE Loss:  -1.9762 | Function Loss:  -2.2673\n",
      "Total loss:  -1.1898 | PDE Loss:  -1.9762 | Function Loss:  -2.2674\n",
      "Total loss:  -1.1898 | PDE Loss:  -1.9761 | Function Loss:  -2.2674\n",
      "Total loss:  -1.1899 | PDE Loss:  -1.976 | Function Loss:  -2.2675\n",
      "Total loss:  -1.1899 | PDE Loss:  -1.9758 | Function Loss:  -2.2675\n",
      "Total loss:  -1.1898 | PDE Loss:  -1.9757 | Function Loss:  -2.2675\n",
      "Total loss:  -1.1899 | PDE Loss:  -1.9758 | Function Loss:  -2.2675\n",
      "Total loss:  -1.1899 | PDE Loss:  -1.9757 | Function Loss:  -2.2676\n",
      "Total loss:  -1.1899 | PDE Loss:  -1.9756 | Function Loss:  -2.2676\n",
      "Total loss:  -1.1899 | PDE Loss:  -1.9756 | Function Loss:  -2.2676\n",
      "Total loss:  -1.19 | PDE Loss:  -1.9752 | Function Loss:  -2.2677\n",
      "Total loss:  -1.19 | PDE Loss:  -1.9752 | Function Loss:  -2.2678\n",
      "Total loss:  -1.19 | PDE Loss:  -1.9752 | Function Loss:  -2.2678\n",
      "Total loss:  -1.19 | PDE Loss:  -1.9752 | Function Loss:  -2.2678\n",
      "Total loss:  -1.1901 | PDE Loss:  -1.9752 | Function Loss:  -2.2678\n",
      "Total loss:  -1.1901 | PDE Loss:  -1.9752 | Function Loss:  -2.2679\n",
      "Total loss:  -1.1901 | PDE Loss:  -1.9751 | Function Loss:  -2.2679\n",
      "Total loss:  -1.1901 | PDE Loss:  -1.9751 | Function Loss:  -2.268\n",
      "Total loss:  -1.1901 | PDE Loss:  -1.975 | Function Loss:  -2.268\n",
      "Total loss:  -1.1902 | PDE Loss:  -1.975 | Function Loss:  -2.2681\n",
      "Total loss:  -1.1902 | PDE Loss:  -1.9745 | Function Loss:  -2.2681\n",
      "Total loss:  -1.1902 | PDE Loss:  -1.9748 | Function Loss:  -2.2681\n",
      "Total loss:  -1.1902 | PDE Loss:  -1.9748 | Function Loss:  -2.2681\n",
      "Total loss:  -1.1903 | PDE Loss:  -1.9748 | Function Loss:  -2.2682\n",
      "Total loss:  -1.1903 | PDE Loss:  -1.9749 | Function Loss:  -2.2682\n",
      "Total loss:  -1.1903 | PDE Loss:  -1.9749 | Function Loss:  -2.2682\n",
      "Total loss:  -1.1903 | PDE Loss:  -1.9749 | Function Loss:  -2.2682\n",
      "Total loss:  -1.1903 | PDE Loss:  -1.975 | Function Loss:  -2.2682\n",
      "Total loss:  -1.1903 | PDE Loss:  -1.975 | Function Loss:  -2.2682\n",
      "Total loss:  -1.1903 | PDE Loss:  -1.975 | Function Loss:  -2.2682\n",
      "Total loss:  -1.1904 | PDE Loss:  -1.9749 | Function Loss:  -2.2683\n",
      "Total loss:  -1.1904 | PDE Loss:  -1.9748 | Function Loss:  -2.2683\n",
      "Total loss:  -1.1904 | PDE Loss:  -1.9747 | Function Loss:  -2.2684\n",
      "Total loss:  -1.1904 | PDE Loss:  -1.9746 | Function Loss:  -2.2684\n",
      "Total loss:  -1.1905 | PDE Loss:  -1.9745 | Function Loss:  -2.2685\n",
      "Total loss:  -1.1905 | PDE Loss:  -1.9745 | Function Loss:  -2.2685\n",
      "Total loss:  -1.1906 | PDE Loss:  -1.9744 | Function Loss:  -2.2686\n",
      "Total loss:  -1.1906 | PDE Loss:  -1.9744 | Function Loss:  -2.2687\n",
      "Total loss:  -1.1907 | PDE Loss:  -1.9744 | Function Loss:  -2.2687\n",
      "Total loss:  -1.1907 | PDE Loss:  -1.9744 | Function Loss:  -2.2688\n",
      "Total loss:  -1.1907 | PDE Loss:  -1.9745 | Function Loss:  -2.2688\n",
      "Total loss:  -1.1908 | PDE Loss:  -1.9746 | Function Loss:  -2.2688\n",
      "Total loss:  -1.1908 | PDE Loss:  -1.9747 | Function Loss:  -2.2688\n",
      "Total loss:  -1.1908 | PDE Loss:  -1.9748 | Function Loss:  -2.2688\n",
      "Total loss:  -1.1909 | PDE Loss:  -1.9748 | Function Loss:  -2.2689\n",
      "Total loss:  -1.1909 | PDE Loss:  -1.9751 | Function Loss:  -2.2689\n",
      "Total loss:  -1.1909 | PDE Loss:  -1.9749 | Function Loss:  -2.269\n",
      "Total loss:  -1.191 | PDE Loss:  -1.9749 | Function Loss:  -2.269\n",
      "Total loss:  -1.191 | PDE Loss:  -1.9748 | Function Loss:  -2.2691\n",
      "Total loss:  -1.191 | PDE Loss:  -1.9749 | Function Loss:  -2.2691\n",
      "Total loss:  -1.1911 | PDE Loss:  -1.9746 | Function Loss:  -2.2692\n",
      "Total loss:  -1.1911 | PDE Loss:  -1.9746 | Function Loss:  -2.2692\n",
      "Total loss:  -1.1911 | PDE Loss:  -1.9746 | Function Loss:  -2.2692\n",
      "Total loss:  -1.1911 | PDE Loss:  -1.9747 | Function Loss:  -2.2693\n",
      "Total loss:  -1.1912 | PDE Loss:  -1.9748 | Function Loss:  -2.2693\n",
      "Total loss:  -1.1912 | PDE Loss:  -1.975 | Function Loss:  -2.2693\n",
      "Total loss:  -1.1913 | PDE Loss:  -1.9753 | Function Loss:  -2.2693\n",
      "Total loss:  -1.1913 | PDE Loss:  -1.9757 | Function Loss:  -2.2693\n",
      "Total loss:  -1.1914 | PDE Loss:  -1.976 | Function Loss:  -2.2692\n",
      "Total loss:  -1.1914 | PDE Loss:  -1.9761 | Function Loss:  -2.2693\n",
      "Total loss:  -1.1914 | PDE Loss:  -1.9762 | Function Loss:  -2.2693\n",
      "Total loss:  -1.1914 | PDE Loss:  -1.9762 | Function Loss:  -2.2693\n",
      "Total loss:  -1.1915 | PDE Loss:  -1.9761 | Function Loss:  -2.2694\n",
      "Total loss:  -1.1915 | PDE Loss:  -1.9759 | Function Loss:  -2.2694\n",
      "Total loss:  -1.1915 | PDE Loss:  -1.9755 | Function Loss:  -2.2695\n",
      "Total loss:  -1.1915 | PDE Loss:  -1.9753 | Function Loss:  -2.2696\n",
      "Total loss:  -1.1915 | PDE Loss:  -1.975 | Function Loss:  -2.2697\n",
      "Total loss:  -1.1916 | PDE Loss:  -1.9748 | Function Loss:  -2.2697\n",
      "Total loss:  -1.1916 | PDE Loss:  -1.9745 | Function Loss:  -2.2698\n",
      "Total loss:  -1.1916 | PDE Loss:  -1.9743 | Function Loss:  -2.2699\n",
      "Total loss:  -1.1917 | PDE Loss:  -1.9743 | Function Loss:  -2.27\n",
      "Total loss:  -1.1917 | PDE Loss:  -1.9742 | Function Loss:  -2.27\n",
      "Total loss:  -1.1918 | PDE Loss:  -1.9743 | Function Loss:  -2.2701\n",
      "Total loss:  -1.1918 | PDE Loss:  -1.9742 | Function Loss:  -2.2701\n",
      "Total loss:  -1.1918 | PDE Loss:  -1.9744 | Function Loss:  -2.2701\n",
      "Total loss:  -1.1918 | PDE Loss:  -1.9745 | Function Loss:  -2.2701\n",
      "Total loss:  -1.1919 | PDE Loss:  -1.9747 | Function Loss:  -2.2701\n",
      "Total loss:  -1.1919 | PDE Loss:  -1.9748 | Function Loss:  -2.2701\n",
      "Total loss:  -1.1919 | PDE Loss:  -1.9749 | Function Loss:  -2.2701\n",
      "Total loss:  -1.1919 | PDE Loss:  -1.9749 | Function Loss:  -2.2701\n",
      "Total loss:  -1.1919 | PDE Loss:  -1.975 | Function Loss:  -2.2701\n",
      "Total loss:  -1.192 | PDE Loss:  -1.975 | Function Loss:  -2.2702\n",
      "Total loss:  -1.192 | PDE Loss:  -1.9751 | Function Loss:  -2.2702\n",
      "Total loss:  -1.1921 | PDE Loss:  -1.9752 | Function Loss:  -2.2702\n",
      "Total loss:  -1.1921 | PDE Loss:  -1.9751 | Function Loss:  -2.2703\n",
      "Total loss:  -1.1921 | PDE Loss:  -1.9751 | Function Loss:  -2.2703\n",
      "Total loss:  -1.1922 | PDE Loss:  -1.9751 | Function Loss:  -2.2704\n",
      "Total loss:  -1.1922 | PDE Loss:  -1.9752 | Function Loss:  -2.2704\n",
      "Total loss:  -1.1923 | PDE Loss:  -1.9751 | Function Loss:  -2.2705\n",
      "Total loss:  -1.1923 | PDE Loss:  -1.9753 | Function Loss:  -2.2705\n",
      "Total loss:  -1.1923 | PDE Loss:  -1.9753 | Function Loss:  -2.2705\n",
      "Total loss:  -1.1924 | PDE Loss:  -1.9754 | Function Loss:  -2.2706\n",
      "Total loss:  -1.1924 | PDE Loss:  -1.9753 | Function Loss:  -2.2707\n",
      "Total loss:  -1.1924 | PDE Loss:  -1.9752 | Function Loss:  -2.2707\n",
      "Total loss:  -1.1925 | PDE Loss:  -1.975 | Function Loss:  -2.2708\n",
      "Total loss:  -1.1925 | PDE Loss:  -1.9748 | Function Loss:  -2.2708\n",
      "Total loss:  -1.1925 | PDE Loss:  -1.9746 | Function Loss:  -2.2709\n",
      "Total loss:  -1.1925 | PDE Loss:  -1.9744 | Function Loss:  -2.271\n",
      "Total loss:  -1.1925 | PDE Loss:  -1.974 | Function Loss:  -2.271\n",
      "Total loss:  -1.1925 | PDE Loss:  -1.974 | Function Loss:  -2.2711\n",
      "Total loss:  -1.1925 | PDE Loss:  -1.974 | Function Loss:  -2.2711\n",
      "Total loss:  -1.1926 | PDE Loss:  -1.974 | Function Loss:  -2.2711\n",
      "Total loss:  -1.1926 | PDE Loss:  -1.974 | Function Loss:  -2.2711\n",
      "Total loss:  -1.1926 | PDE Loss:  -1.9739 | Function Loss:  -2.2711\n",
      "Total loss:  -1.1926 | PDE Loss:  -1.9739 | Function Loss:  -2.2711\n",
      "Total loss:  -1.1926 | PDE Loss:  -1.9739 | Function Loss:  -2.2711\n",
      "Total loss:  -1.1926 | PDE Loss:  -1.9739 | Function Loss:  -2.2712\n",
      "Total loss:  -1.1926 | PDE Loss:  -1.9738 | Function Loss:  -2.2712\n",
      "Total loss:  -1.1926 | PDE Loss:  -1.9739 | Function Loss:  -2.2712\n",
      "Total loss:  -1.1926 | PDE Loss:  -1.9739 | Function Loss:  -2.2712\n",
      "Total loss:  -1.1927 | PDE Loss:  -1.9739 | Function Loss:  -2.2712\n",
      "Total loss:  -1.1927 | PDE Loss:  -1.9739 | Function Loss:  -2.2713\n",
      "Total loss:  -1.1927 | PDE Loss:  -1.974 | Function Loss:  -2.2713\n",
      "Total loss:  -1.1927 | PDE Loss:  -1.9741 | Function Loss:  -2.2713\n",
      "Total loss:  -1.1927 | PDE Loss:  -1.9742 | Function Loss:  -2.2712\n",
      "Total loss:  -1.1927 | PDE Loss:  -1.9744 | Function Loss:  -2.2712\n",
      "Total loss:  -1.1928 | PDE Loss:  -1.9745 | Function Loss:  -2.2712\n",
      "Total loss:  -1.1927 | PDE Loss:  -1.9747 | Function Loss:  -2.2712\n",
      "Total loss:  -1.1928 | PDE Loss:  -1.9746 | Function Loss:  -2.2712\n",
      "Total loss:  -1.1928 | PDE Loss:  -1.9747 | Function Loss:  -2.2712\n",
      "Total loss:  -1.1928 | PDE Loss:  -1.9748 | Function Loss:  -2.2712\n",
      "Total loss:  -1.1928 | PDE Loss:  -1.9749 | Function Loss:  -2.2712\n",
      "Total loss:  -1.1929 | PDE Loss:  -1.975 | Function Loss:  -2.2712\n",
      "Total loss:  -1.1929 | PDE Loss:  -1.975 | Function Loss:  -2.2713\n",
      "Total loss:  -1.1929 | PDE Loss:  -1.975 | Function Loss:  -2.2713\n",
      "Total loss:  -1.1929 | PDE Loss:  -1.975 | Function Loss:  -2.2713\n",
      "Total loss:  -1.193 | PDE Loss:  -1.975 | Function Loss:  -2.2714\n",
      "Total loss:  -1.193 | PDE Loss:  -1.975 | Function Loss:  -2.2714\n",
      "Total loss:  -1.193 | PDE Loss:  -1.9752 | Function Loss:  -2.2714\n",
      "Total loss:  -1.193 | PDE Loss:  -1.9752 | Function Loss:  -2.2714\n",
      "Total loss:  -1.1931 | PDE Loss:  -1.9752 | Function Loss:  -2.2715\n",
      "Total loss:  -1.1931 | PDE Loss:  -1.9751 | Function Loss:  -2.2715\n",
      "Total loss:  -1.1931 | PDE Loss:  -1.9753 | Function Loss:  -2.2715\n",
      "Total loss:  -1.1932 | PDE Loss:  -1.9756 | Function Loss:  -2.2715\n",
      "Total loss:  -1.1932 | PDE Loss:  -1.9758 | Function Loss:  -2.2715\n",
      "Total loss:  -1.1929 | PDE Loss:  -1.9759 | Function Loss:  -2.2712\n",
      "Total loss:  -1.1932 | PDE Loss:  -1.9759 | Function Loss:  -2.2715\n",
      "Total loss:  -1.1932 | PDE Loss:  -1.9761 | Function Loss:  -2.2715\n",
      "Total loss:  -1.1932 | PDE Loss:  -1.9762 | Function Loss:  -2.2714\n",
      "Total loss:  -1.1932 | PDE Loss:  -1.9764 | Function Loss:  -2.2714\n",
      "Total loss:  -1.1933 | PDE Loss:  -1.9765 | Function Loss:  -2.2714\n",
      "Total loss:  -1.1933 | PDE Loss:  -1.9766 | Function Loss:  -2.2714\n",
      "Total loss:  -1.1933 | PDE Loss:  -1.9766 | Function Loss:  -2.2715\n",
      "Total loss:  -1.1933 | PDE Loss:  -1.9765 | Function Loss:  -2.2715\n",
      "Total loss:  -1.1933 | PDE Loss:  -1.9765 | Function Loss:  -2.2715\n",
      "Total loss:  -1.1934 | PDE Loss:  -1.9764 | Function Loss:  -2.2716\n",
      "Total loss:  -1.1934 | PDE Loss:  -1.9762 | Function Loss:  -2.2717\n",
      "Total loss:  -1.1934 | PDE Loss:  -1.9761 | Function Loss:  -2.2717\n",
      "Total loss:  -1.1935 | PDE Loss:  -1.9757 | Function Loss:  -2.2718\n",
      "Total loss:  -1.1935 | PDE Loss:  -1.9758 | Function Loss:  -2.2718\n",
      "Total loss:  -1.1935 | PDE Loss:  -1.9759 | Function Loss:  -2.2718\n",
      "Total loss:  -1.1935 | PDE Loss:  -1.9761 | Function Loss:  -2.2718\n",
      "Total loss:  -1.1935 | PDE Loss:  -1.9762 | Function Loss:  -2.2718\n",
      "Total loss:  -1.1935 | PDE Loss:  -1.9763 | Function Loss:  -2.2718\n",
      "Total loss:  -1.1936 | PDE Loss:  -1.9764 | Function Loss:  -2.2718\n",
      "Total loss:  -1.1936 | PDE Loss:  -1.9764 | Function Loss:  -2.2718\n",
      "Total loss:  -1.1936 | PDE Loss:  -1.9764 | Function Loss:  -2.2718\n",
      "Total loss:  -1.1936 | PDE Loss:  -1.9763 | Function Loss:  -2.2719\n",
      "Total loss:  -1.1936 | PDE Loss:  -1.9765 | Function Loss:  -2.2718\n",
      "Total loss:  -1.1936 | PDE Loss:  -1.9764 | Function Loss:  -2.2719\n",
      "Total loss:  -1.1936 | PDE Loss:  -1.9764 | Function Loss:  -2.2719\n",
      "Total loss:  -1.1936 | PDE Loss:  -1.9763 | Function Loss:  -2.2719\n",
      "Total loss:  -1.1936 | PDE Loss:  -1.9763 | Function Loss:  -2.2719\n",
      "Total loss:  -1.1937 | PDE Loss:  -1.9763 | Function Loss:  -2.2719\n",
      "Total loss:  -1.1937 | PDE Loss:  -1.9763 | Function Loss:  -2.272\n",
      "Total loss:  -1.1937 | PDE Loss:  -1.9764 | Function Loss:  -2.272\n",
      "Total loss:  -1.1937 | PDE Loss:  -1.9765 | Function Loss:  -2.2719\n",
      "Total loss:  -1.1937 | PDE Loss:  -1.9767 | Function Loss:  -2.2719\n",
      "Total loss:  -1.1937 | PDE Loss:  -1.9769 | Function Loss:  -2.2719\n",
      "Total loss:  -1.1938 | PDE Loss:  -1.9771 | Function Loss:  -2.2719\n",
      "Total loss:  -1.1938 | PDE Loss:  -1.9772 | Function Loss:  -2.2719\n",
      "Total loss:  -1.1938 | PDE Loss:  -1.9773 | Function Loss:  -2.2719\n",
      "Total loss:  -1.1938 | PDE Loss:  -1.9773 | Function Loss:  -2.272\n",
      "Total loss:  -1.1938 | PDE Loss:  -1.9772 | Function Loss:  -2.272\n",
      "Total loss:  -1.1939 | PDE Loss:  -1.9772 | Function Loss:  -2.272\n",
      "Total loss:  -1.1939 | PDE Loss:  -1.9771 | Function Loss:  -2.2721\n",
      "Total loss:  -1.1939 | PDE Loss:  -1.977 | Function Loss:  -2.2721\n",
      "Total loss:  -1.1939 | PDE Loss:  -1.9769 | Function Loss:  -2.2721\n",
      "Total loss:  -1.1939 | PDE Loss:  -1.977 | Function Loss:  -2.2721\n",
      "Total loss:  -1.1939 | PDE Loss:  -1.9769 | Function Loss:  -2.2722\n",
      "Total loss:  -1.194 | PDE Loss:  -1.977 | Function Loss:  -2.2722\n",
      "Total loss:  -1.194 | PDE Loss:  -1.9771 | Function Loss:  -2.2722\n",
      "Total loss:  -1.194 | PDE Loss:  -1.9773 | Function Loss:  -2.2722\n",
      "Total loss:  -1.194 | PDE Loss:  -1.9775 | Function Loss:  -2.2721\n",
      "Total loss:  -1.194 | PDE Loss:  -1.9778 | Function Loss:  -2.2721\n",
      "Total loss:  -1.1941 | PDE Loss:  -1.9781 | Function Loss:  -2.2721\n",
      "Total loss:  -1.1941 | PDE Loss:  -1.9784 | Function Loss:  -2.272\n",
      "Total loss:  -1.1941 | PDE Loss:  -1.9786 | Function Loss:  -2.272\n",
      "Total loss:  -1.1941 | PDE Loss:  -1.9787 | Function Loss:  -2.272\n",
      "Total loss:  -1.1941 | PDE Loss:  -1.9788 | Function Loss:  -2.272\n",
      "Total loss:  -1.1941 | PDE Loss:  -1.9788 | Function Loss:  -2.272\n",
      "Total loss:  -1.1942 | PDE Loss:  -1.9788 | Function Loss:  -2.272\n",
      "Total loss:  -1.1942 | PDE Loss:  -1.9788 | Function Loss:  -2.2721\n",
      "Total loss:  -1.1942 | PDE Loss:  -1.9788 | Function Loss:  -2.2721\n",
      "Total loss:  -1.1942 | PDE Loss:  -1.9788 | Function Loss:  -2.2721\n",
      "Total loss:  -1.1942 | PDE Loss:  -1.9788 | Function Loss:  -2.2721\n",
      "Total loss:  -1.1943 | PDE Loss:  -1.9789 | Function Loss:  -2.2722\n",
      "Total loss:  -1.1943 | PDE Loss:  -1.9789 | Function Loss:  -2.2722\n",
      "Total loss:  -1.1943 | PDE Loss:  -1.9791 | Function Loss:  -2.2722\n",
      "Total loss:  -1.1943 | PDE Loss:  -1.9792 | Function Loss:  -2.2722\n",
      "Total loss:  -1.1943 | PDE Loss:  -1.9792 | Function Loss:  -2.2722\n",
      "Total loss:  -1.1944 | PDE Loss:  -1.9791 | Function Loss:  -2.2722\n",
      "Total loss:  -1.1944 | PDE Loss:  -1.9792 | Function Loss:  -2.2723\n",
      "Total loss:  -1.1944 | PDE Loss:  -1.9793 | Function Loss:  -2.2723\n",
      "Total loss:  -1.1944 | PDE Loss:  -1.9793 | Function Loss:  -2.2723\n",
      "Total loss:  -1.1945 | PDE Loss:  -1.9793 | Function Loss:  -2.2723\n",
      "Total loss:  -1.1945 | PDE Loss:  -1.9794 | Function Loss:  -2.2723\n",
      "Total loss:  -1.1945 | PDE Loss:  -1.9794 | Function Loss:  -2.2724\n",
      "Total loss:  -1.1945 | PDE Loss:  -1.9794 | Function Loss:  -2.2724\n",
      "Total loss:  -1.1946 | PDE Loss:  -1.9794 | Function Loss:  -2.2724\n",
      "Total loss:  -1.1946 | PDE Loss:  -1.9794 | Function Loss:  -2.2725\n",
      "Total loss:  -1.1946 | PDE Loss:  -1.9794 | Function Loss:  -2.2725\n",
      "Total loss:  -1.1946 | PDE Loss:  -1.9794 | Function Loss:  -2.2725\n",
      "Total loss:  -1.1946 | PDE Loss:  -1.9795 | Function Loss:  -2.2725\n",
      "Total loss:  -1.1947 | PDE Loss:  -1.9795 | Function Loss:  -2.2725\n",
      "Total loss:  -1.1947 | PDE Loss:  -1.9798 | Function Loss:  -2.2725\n",
      "Total loss:  -1.1947 | PDE Loss:  -1.9798 | Function Loss:  -2.2725\n",
      "Total loss:  -1.1947 | PDE Loss:  -1.9799 | Function Loss:  -2.2725\n",
      "Total loss:  -1.1947 | PDE Loss:  -1.9801 | Function Loss:  -2.2725\n",
      "Total loss:  -1.1948 | PDE Loss:  -1.9803 | Function Loss:  -2.2725\n",
      "Total loss:  -1.1948 | PDE Loss:  -1.9805 | Function Loss:  -2.2725\n",
      "Total loss:  -1.1948 | PDE Loss:  -1.9804 | Function Loss:  -2.2725\n",
      "Total loss:  -1.1948 | PDE Loss:  -1.9804 | Function Loss:  -2.2725\n",
      "Total loss:  -1.1948 | PDE Loss:  -1.9804 | Function Loss:  -2.2726\n",
      "Total loss:  -1.1949 | PDE Loss:  -1.9802 | Function Loss:  -2.2726\n",
      "Total loss:  -1.1949 | PDE Loss:  -1.98 | Function Loss:  -2.2727\n",
      "Total loss:  -1.1949 | PDE Loss:  -1.9798 | Function Loss:  -2.2727\n",
      "Total loss:  -1.1949 | PDE Loss:  -1.9796 | Function Loss:  -2.2728\n",
      "Total loss:  -1.1949 | PDE Loss:  -1.9795 | Function Loss:  -2.2728\n",
      "Total loss:  -1.1949 | PDE Loss:  -1.9793 | Function Loss:  -2.2729\n",
      "Total loss:  -1.1949 | PDE Loss:  -1.9792 | Function Loss:  -2.2729\n",
      "Total loss:  -1.195 | PDE Loss:  -1.9791 | Function Loss:  -2.2729\n",
      "Total loss:  -1.1949 | PDE Loss:  -1.9789 | Function Loss:  -2.273\n",
      "Total loss:  -1.195 | PDE Loss:  -1.9791 | Function Loss:  -2.273\n",
      "Total loss:  -1.195 | PDE Loss:  -1.979 | Function Loss:  -2.273\n",
      "Total loss:  -1.195 | PDE Loss:  -1.979 | Function Loss:  -2.273\n",
      "Total loss:  -1.1951 | PDE Loss:  -1.9792 | Function Loss:  -2.2731\n",
      "Total loss:  -1.1951 | PDE Loss:  -1.9795 | Function Loss:  -2.2731\n",
      "Total loss:  -1.1951 | PDE Loss:  -1.9797 | Function Loss:  -2.273\n",
      "Total loss:  -1.1952 | PDE Loss:  -1.98 | Function Loss:  -2.273\n",
      "Total loss:  -1.1952 | PDE Loss:  -1.9802 | Function Loss:  -2.273\n",
      "Total loss:  -1.1953 | PDE Loss:  -1.9806 | Function Loss:  -2.273\n",
      "Total loss:  -1.1953 | PDE Loss:  -1.9807 | Function Loss:  -2.2731\n",
      "Total loss:  -1.1954 | PDE Loss:  -1.9808 | Function Loss:  -2.2731\n",
      "Total loss:  -1.1954 | PDE Loss:  -1.9807 | Function Loss:  -2.2732\n",
      "Total loss:  -1.1955 | PDE Loss:  -1.9807 | Function Loss:  -2.2732\n",
      "Total loss:  -1.1955 | PDE Loss:  -1.9806 | Function Loss:  -2.2733\n",
      "Total loss:  -1.1955 | PDE Loss:  -1.9805 | Function Loss:  -2.2733\n",
      "Total loss:  -1.1955 | PDE Loss:  -1.9803 | Function Loss:  -2.2734\n",
      "Total loss:  -1.1955 | PDE Loss:  -1.9802 | Function Loss:  -2.2734\n",
      "Total loss:  -1.1956 | PDE Loss:  -1.9801 | Function Loss:  -2.2735\n",
      "Total loss:  -1.1956 | PDE Loss:  -1.98 | Function Loss:  -2.2735\n",
      "Total loss:  -1.1956 | PDE Loss:  -1.9796 | Function Loss:  -2.2736\n",
      "Total loss:  -1.1956 | PDE Loss:  -1.9797 | Function Loss:  -2.2736\n",
      "Total loss:  -1.1956 | PDE Loss:  -1.9797 | Function Loss:  -2.2736\n",
      "Total loss:  -1.1957 | PDE Loss:  -1.9798 | Function Loss:  -2.2737\n",
      "Total loss:  -1.1957 | PDE Loss:  -1.9797 | Function Loss:  -2.2737\n",
      "Total loss:  -1.1957 | PDE Loss:  -1.9798 | Function Loss:  -2.2737\n",
      "Total loss:  -1.1957 | PDE Loss:  -1.9796 | Function Loss:  -2.2737\n",
      "Total loss:  -1.1957 | PDE Loss:  -1.9793 | Function Loss:  -2.2738\n",
      "Total loss:  -1.1957 | PDE Loss:  -1.979 | Function Loss:  -2.2739\n",
      "Total loss:  -1.1957 | PDE Loss:  -1.9787 | Function Loss:  -2.274\n",
      "Total loss:  -1.1958 | PDE Loss:  -1.9778 | Function Loss:  -2.2742\n",
      "Total loss:  -1.1958 | PDE Loss:  -1.9778 | Function Loss:  -2.2742\n",
      "Total loss:  -1.1958 | PDE Loss:  -1.9779 | Function Loss:  -2.2742\n",
      "Total loss:  -1.1958 | PDE Loss:  -1.978 | Function Loss:  -2.2742\n",
      "Total loss:  -1.1958 | PDE Loss:  -1.978 | Function Loss:  -2.2742\n",
      "Total loss:  -1.1958 | PDE Loss:  -1.9781 | Function Loss:  -2.2742\n",
      "Total loss:  -1.1958 | PDE Loss:  -1.9779 | Function Loss:  -2.2742\n",
      "Total loss:  -1.1959 | PDE Loss:  -1.978 | Function Loss:  -2.2742\n",
      "Total loss:  -1.1959 | PDE Loss:  -1.978 | Function Loss:  -2.2743\n",
      "Total loss:  -1.1959 | PDE Loss:  -1.978 | Function Loss:  -2.2743\n",
      "Total loss:  -1.1959 | PDE Loss:  -1.978 | Function Loss:  -2.2743\n",
      "Total loss:  -1.1959 | PDE Loss:  -1.978 | Function Loss:  -2.2743\n",
      "Total loss:  -1.1959 | PDE Loss:  -1.9781 | Function Loss:  -2.2743\n",
      "Total loss:  -1.1959 | PDE Loss:  -1.978 | Function Loss:  -2.2743\n",
      "Total loss:  -1.1959 | PDE Loss:  -1.9781 | Function Loss:  -2.2743\n",
      "Total loss:  -1.196 | PDE Loss:  -1.9782 | Function Loss:  -2.2743\n",
      "Total loss:  -1.196 | PDE Loss:  -1.9784 | Function Loss:  -2.2743\n",
      "Total loss:  -1.196 | PDE Loss:  -1.9785 | Function Loss:  -2.2743\n",
      "Total loss:  -1.196 | PDE Loss:  -1.9787 | Function Loss:  -2.2743\n",
      "Total loss:  -1.196 | PDE Loss:  -1.9787 | Function Loss:  -2.2743\n",
      "Total loss:  -1.1961 | PDE Loss:  -1.9789 | Function Loss:  -2.2743\n",
      "Total loss:  -1.1961 | PDE Loss:  -1.9788 | Function Loss:  -2.2743\n",
      "Total loss:  -1.1961 | PDE Loss:  -1.9788 | Function Loss:  -2.2744\n",
      "Total loss:  -1.1961 | PDE Loss:  -1.9787 | Function Loss:  -2.2744\n",
      "Total loss:  -1.1961 | PDE Loss:  -1.9787 | Function Loss:  -2.2744\n",
      "Total loss:  -1.1961 | PDE Loss:  -1.9787 | Function Loss:  -2.2744\n",
      "Total loss:  -1.1961 | PDE Loss:  -1.9788 | Function Loss:  -2.2744\n",
      "Total loss:  -1.1962 | PDE Loss:  -1.979 | Function Loss:  -2.2744\n",
      "Total loss:  -1.1962 | PDE Loss:  -1.9791 | Function Loss:  -2.2744\n",
      "Total loss:  -1.1962 | PDE Loss:  -1.9792 | Function Loss:  -2.2744\n",
      "Total loss:  -1.1962 | PDE Loss:  -1.9794 | Function Loss:  -2.2744\n",
      "Total loss:  -1.1962 | PDE Loss:  -1.9794 | Function Loss:  -2.2744\n",
      "Total loss:  -1.1963 | PDE Loss:  -1.9795 | Function Loss:  -2.2744\n",
      "Total loss:  -1.1963 | PDE Loss:  -1.9796 | Function Loss:  -2.2744\n",
      "Total loss:  -1.1963 | PDE Loss:  -1.9796 | Function Loss:  -2.2744\n",
      "Total loss:  -1.1963 | PDE Loss:  -1.9796 | Function Loss:  -2.2744\n",
      "Total loss:  -1.1963 | PDE Loss:  -1.9795 | Function Loss:  -2.2745\n",
      "Total loss:  -1.1963 | PDE Loss:  -1.9794 | Function Loss:  -2.2745\n",
      "Total loss:  -1.1963 | PDE Loss:  -1.9793 | Function Loss:  -2.2746\n",
      "Total loss:  -1.1963 | PDE Loss:  -1.9788 | Function Loss:  -2.2747\n",
      "Total loss:  -1.1964 | PDE Loss:  -1.9791 | Function Loss:  -2.2746\n",
      "Total loss:  -1.1964 | PDE Loss:  -1.9791 | Function Loss:  -2.2746\n",
      "Total loss:  -1.1964 | PDE Loss:  -1.9791 | Function Loss:  -2.2747\n",
      "Total loss:  -1.1964 | PDE Loss:  -1.9791 | Function Loss:  -2.2747\n",
      "Total loss:  -1.1964 | PDE Loss:  -1.9791 | Function Loss:  -2.2747\n",
      "Total loss:  -1.1964 | PDE Loss:  -1.9792 | Function Loss:  -2.2747\n",
      "Total loss:  -1.1964 | PDE Loss:  -1.9792 | Function Loss:  -2.2747\n",
      "Total loss:  -1.1965 | PDE Loss:  -1.9793 | Function Loss:  -2.2747\n",
      "Total loss:  -1.1965 | PDE Loss:  -1.9794 | Function Loss:  -2.2747\n",
      "Total loss:  -1.1965 | PDE Loss:  -1.9794 | Function Loss:  -2.2747\n",
      "Total loss:  -1.1965 | PDE Loss:  -1.9794 | Function Loss:  -2.2747\n",
      "Total loss:  -1.1965 | PDE Loss:  -1.9794 | Function Loss:  -2.2747\n",
      "Total loss:  -1.1965 | PDE Loss:  -1.9794 | Function Loss:  -2.2748\n",
      "Total loss:  -1.1965 | PDE Loss:  -1.9793 | Function Loss:  -2.2748\n",
      "Total loss:  -1.1965 | PDE Loss:  -1.9793 | Function Loss:  -2.2748\n",
      "Total loss:  -1.1966 | PDE Loss:  -1.9792 | Function Loss:  -2.2748\n",
      "Total loss:  -1.1966 | PDE Loss:  -1.9793 | Function Loss:  -2.2749\n",
      "Total loss:  -1.1966 | PDE Loss:  -1.9793 | Function Loss:  -2.2749\n",
      "Total loss:  -1.1966 | PDE Loss:  -1.9797 | Function Loss:  -2.2748\n",
      "Total loss:  -1.1966 | PDE Loss:  -1.9797 | Function Loss:  -2.2748\n",
      "Total loss:  -1.1966 | PDE Loss:  -1.9797 | Function Loss:  -2.2748\n",
      "Total loss:  -1.1967 | PDE Loss:  -1.9798 | Function Loss:  -2.2748\n",
      "Total loss:  -1.1967 | PDE Loss:  -1.9799 | Function Loss:  -2.2748\n",
      "Total loss:  -1.1967 | PDE Loss:  -1.98 | Function Loss:  -2.2748\n",
      "Total loss:  -1.1967 | PDE Loss:  -1.9801 | Function Loss:  -2.2748\n",
      "Total loss:  -1.1967 | PDE Loss:  -1.9802 | Function Loss:  -2.2748\n",
      "Total loss:  -1.1967 | PDE Loss:  -1.9802 | Function Loss:  -2.2748\n",
      "Total loss:  -1.1967 | PDE Loss:  -1.9804 | Function Loss:  -2.2748\n",
      "Total loss:  -1.1968 | PDE Loss:  -1.9804 | Function Loss:  -2.2749\n",
      "Total loss:  -1.1968 | PDE Loss:  -1.9803 | Function Loss:  -2.2749\n",
      "Total loss:  -1.1968 | PDE Loss:  -1.9802 | Function Loss:  -2.2749\n",
      "Total loss:  -1.1968 | PDE Loss:  -1.9801 | Function Loss:  -2.275\n",
      "Total loss:  -1.1968 | PDE Loss:  -1.98 | Function Loss:  -2.275\n",
      "Total loss:  -1.1968 | PDE Loss:  -1.9798 | Function Loss:  -2.2751\n",
      "Total loss:  -1.1968 | PDE Loss:  -1.9798 | Function Loss:  -2.2751\n",
      "Total loss:  -1.1969 | PDE Loss:  -1.9797 | Function Loss:  -2.2751\n",
      "Total loss:  -1.1969 | PDE Loss:  -1.9798 | Function Loss:  -2.2751\n",
      "Total loss:  -1.1969 | PDE Loss:  -1.9797 | Function Loss:  -2.2752\n",
      "Total loss:  -1.1969 | PDE Loss:  -1.9797 | Function Loss:  -2.2752\n",
      "Total loss:  -1.1969 | PDE Loss:  -1.9798 | Function Loss:  -2.2752\n",
      "Total loss:  -1.1969 | PDE Loss:  -1.9798 | Function Loss:  -2.2752\n",
      "Total loss:  -1.1969 | PDE Loss:  -1.9798 | Function Loss:  -2.2752\n",
      "Total loss:  -1.197 | PDE Loss:  -1.9798 | Function Loss:  -2.2752\n",
      "Total loss:  -1.197 | PDE Loss:  -1.9797 | Function Loss:  -2.2753\n",
      "Total loss:  -1.197 | PDE Loss:  -1.9796 | Function Loss:  -2.2753\n",
      "Total loss:  -1.197 | PDE Loss:  -1.9793 | Function Loss:  -2.2754\n",
      "Total loss:  -1.1971 | PDE Loss:  -1.9793 | Function Loss:  -2.2754\n",
      "Total loss:  -1.1971 | PDE Loss:  -1.9793 | Function Loss:  -2.2754\n",
      "Total loss:  -1.1971 | PDE Loss:  -1.9793 | Function Loss:  -2.2755\n",
      "Total loss:  -1.1971 | PDE Loss:  -1.9793 | Function Loss:  -2.2755\n",
      "Total loss:  -1.1971 | PDE Loss:  -1.9794 | Function Loss:  -2.2755\n",
      "Total loss:  -1.1971 | PDE Loss:  -1.9796 | Function Loss:  -2.2754\n",
      "Total loss:  -1.1971 | PDE Loss:  -1.9797 | Function Loss:  -2.2754\n",
      "Total loss:  -1.1971 | PDE Loss:  -1.9798 | Function Loss:  -2.2754\n",
      "Total loss:  -1.1972 | PDE Loss:  -1.98 | Function Loss:  -2.2754\n",
      "Total loss:  -1.1972 | PDE Loss:  -1.9801 | Function Loss:  -2.2754\n",
      "Total loss:  -1.1972 | PDE Loss:  -1.9801 | Function Loss:  -2.2754\n",
      "Total loss:  -1.1972 | PDE Loss:  -1.9801 | Function Loss:  -2.2755\n",
      "Total loss:  -1.1972 | PDE Loss:  -1.9801 | Function Loss:  -2.2755\n",
      "Total loss:  -1.1973 | PDE Loss:  -1.98 | Function Loss:  -2.2755\n",
      "Total loss:  -1.1973 | PDE Loss:  -1.98 | Function Loss:  -2.2756\n",
      "Total loss:  -1.1973 | PDE Loss:  -1.9799 | Function Loss:  -2.2756\n",
      "Total loss:  -1.1974 | PDE Loss:  -1.98 | Function Loss:  -2.2756\n",
      "Total loss:  -1.1974 | PDE Loss:  -1.9803 | Function Loss:  -2.2756\n",
      "Total loss:  -1.1974 | PDE Loss:  -1.9805 | Function Loss:  -2.2756\n",
      "Total loss:  -1.1974 | PDE Loss:  -1.9808 | Function Loss:  -2.2756\n",
      "Total loss:  -1.1975 | PDE Loss:  -1.9811 | Function Loss:  -2.2756\n",
      "Total loss:  -1.1975 | PDE Loss:  -1.9813 | Function Loss:  -2.2755\n",
      "Total loss:  -1.1975 | PDE Loss:  -1.9817 | Function Loss:  -2.2755\n",
      "Total loss:  -1.1975 | PDE Loss:  -1.9818 | Function Loss:  -2.2755\n",
      "Total loss:  -1.1975 | PDE Loss:  -1.9819 | Function Loss:  -2.2754\n",
      "Total loss:  -1.1975 | PDE Loss:  -1.9821 | Function Loss:  -2.2754\n",
      "Total loss:  -1.1975 | PDE Loss:  -1.9823 | Function Loss:  -2.2754\n",
      "Total loss:  -1.1975 | PDE Loss:  -1.9824 | Function Loss:  -2.2754\n",
      "Total loss:  -1.1975 | PDE Loss:  -1.9825 | Function Loss:  -2.2754\n",
      "Total loss:  -1.1976 | PDE Loss:  -1.9825 | Function Loss:  -2.2754\n",
      "Total loss:  -1.1976 | PDE Loss:  -1.9825 | Function Loss:  -2.2754\n",
      "Total loss:  -1.1974 | PDE Loss:  -1.9818 | Function Loss:  -2.2754\n",
      "Total loss:  -1.1976 | PDE Loss:  -1.9825 | Function Loss:  -2.2754\n",
      "Total loss:  -1.1976 | PDE Loss:  -1.9824 | Function Loss:  -2.2755\n",
      "Total loss:  -1.1976 | PDE Loss:  -1.9823 | Function Loss:  -2.2755\n",
      "Total loss:  -1.1976 | PDE Loss:  -1.9821 | Function Loss:  -2.2756\n",
      "Total loss:  -1.1977 | PDE Loss:  -1.9819 | Function Loss:  -2.2756\n",
      "Total loss:  -1.1977 | PDE Loss:  -1.9817 | Function Loss:  -2.2757\n",
      "Total loss:  -1.1977 | PDE Loss:  -1.9815 | Function Loss:  -2.2757\n",
      "Total loss:  -1.1977 | PDE Loss:  -1.9814 | Function Loss:  -2.2758\n",
      "Total loss:  -1.1977 | PDE Loss:  -1.9811 | Function Loss:  -2.2759\n",
      "Total loss:  -1.1978 | PDE Loss:  -1.9811 | Function Loss:  -2.2759\n",
      "Total loss:  -1.1978 | PDE Loss:  -1.9812 | Function Loss:  -2.2759\n",
      "Total loss:  -1.1978 | PDE Loss:  -1.9814 | Function Loss:  -2.2759\n",
      "Total loss:  -1.1978 | PDE Loss:  -1.9813 | Function Loss:  -2.2759\n",
      "Total loss:  -1.1978 | PDE Loss:  -1.9815 | Function Loss:  -2.2759\n",
      "Total loss:  -1.1978 | PDE Loss:  -1.9817 | Function Loss:  -2.2759\n",
      "Total loss:  -1.1978 | PDE Loss:  -1.9819 | Function Loss:  -2.2759\n",
      "Total loss:  -1.1979 | PDE Loss:  -1.9821 | Function Loss:  -2.2758\n",
      "Total loss:  -1.1979 | PDE Loss:  -1.9822 | Function Loss:  -2.2758\n",
      "Total loss:  -1.1979 | PDE Loss:  -1.9823 | Function Loss:  -2.2758\n",
      "Total loss:  -1.1979 | PDE Loss:  -1.9823 | Function Loss:  -2.2759\n",
      "Total loss:  -1.1979 | PDE Loss:  -1.9823 | Function Loss:  -2.2759\n",
      "Total loss:  -1.1979 | PDE Loss:  -1.9824 | Function Loss:  -2.2759\n",
      "Total loss:  -1.198 | PDE Loss:  -1.9824 | Function Loss:  -2.2759\n",
      "Total loss:  -1.198 | PDE Loss:  -1.9825 | Function Loss:  -2.2759\n",
      "Total loss:  -1.198 | PDE Loss:  -1.9826 | Function Loss:  -2.2759\n",
      "Total loss:  -1.198 | PDE Loss:  -1.9827 | Function Loss:  -2.2759\n",
      "Total loss:  -1.198 | PDE Loss:  -1.9828 | Function Loss:  -2.2759\n",
      "Total loss:  -1.1981 | PDE Loss:  -1.983 | Function Loss:  -2.2759\n",
      "Total loss:  -1.1981 | PDE Loss:  -1.9831 | Function Loss:  -2.2759\n",
      "Total loss:  -1.1981 | PDE Loss:  -1.9832 | Function Loss:  -2.2759\n",
      "Total loss:  -1.1981 | PDE Loss:  -1.9832 | Function Loss:  -2.2759\n",
      "Total loss:  -1.1981 | PDE Loss:  -1.9832 | Function Loss:  -2.2759\n",
      "Total loss:  -1.1981 | PDE Loss:  -1.9832 | Function Loss:  -2.2759\n",
      "Total loss:  -1.1981 | PDE Loss:  -1.9832 | Function Loss:  -2.276\n",
      "Total loss:  -1.1981 | PDE Loss:  -1.9831 | Function Loss:  -2.276\n",
      "Total loss:  -1.1982 | PDE Loss:  -1.9829 | Function Loss:  -2.276\n",
      "Total loss:  -1.1982 | PDE Loss:  -1.9827 | Function Loss:  -2.2761\n",
      "Total loss:  -1.1982 | PDE Loss:  -1.9828 | Function Loss:  -2.2761\n",
      "Total loss:  -1.1982 | PDE Loss:  -1.9826 | Function Loss:  -2.2762\n",
      "Total loss:  -1.1983 | PDE Loss:  -1.9827 | Function Loss:  -2.2762\n",
      "Total loss:  -1.1983 | PDE Loss:  -1.9826 | Function Loss:  -2.2762\n",
      "Total loss:  -1.1983 | PDE Loss:  -1.9827 | Function Loss:  -2.2762\n",
      "Total loss:  -1.1983 | PDE Loss:  -1.983 | Function Loss:  -2.2762\n",
      "Total loss:  -1.1984 | PDE Loss:  -1.9833 | Function Loss:  -2.2762\n",
      "Total loss:  -1.1984 | PDE Loss:  -1.9836 | Function Loss:  -2.2762\n",
      "Total loss:  -1.1984 | PDE Loss:  -1.9838 | Function Loss:  -2.2761\n",
      "Total loss:  -1.1984 | PDE Loss:  -1.984 | Function Loss:  -2.2761\n",
      "Total loss:  -1.1984 | PDE Loss:  -1.9842 | Function Loss:  -2.2761\n",
      "Total loss:  -1.1984 | PDE Loss:  -1.9843 | Function Loss:  -2.2761\n",
      "Total loss:  -1.1984 | PDE Loss:  -1.9844 | Function Loss:  -2.2761\n",
      "Total loss:  -1.1985 | PDE Loss:  -1.9845 | Function Loss:  -2.2761\n",
      "Total loss:  -1.1984 | PDE Loss:  -1.9848 | Function Loss:  -2.276\n",
      "Total loss:  -1.1985 | PDE Loss:  -1.9847 | Function Loss:  -2.2761\n",
      "Total loss:  -1.1985 | PDE Loss:  -1.9847 | Function Loss:  -2.2761\n",
      "Total loss:  -1.1985 | PDE Loss:  -1.9848 | Function Loss:  -2.2761\n",
      "Total loss:  -1.1985 | PDE Loss:  -1.985 | Function Loss:  -2.276\n",
      "Total loss:  -1.1985 | PDE Loss:  -1.9851 | Function Loss:  -2.276\n",
      "Total loss:  -1.1985 | PDE Loss:  -1.9852 | Function Loss:  -2.276\n",
      "Total loss:  -1.1985 | PDE Loss:  -1.9853 | Function Loss:  -2.276\n",
      "Total loss:  -1.1986 | PDE Loss:  -1.9853 | Function Loss:  -2.276\n",
      "Total loss:  -1.1986 | PDE Loss:  -1.9854 | Function Loss:  -2.276\n",
      "Total loss:  -1.1986 | PDE Loss:  -1.9853 | Function Loss:  -2.2761\n",
      "Total loss:  -1.1986 | PDE Loss:  -1.9853 | Function Loss:  -2.2761\n",
      "Total loss:  -1.1986 | PDE Loss:  -1.9853 | Function Loss:  -2.2761\n",
      "Total loss:  -1.1986 | PDE Loss:  -1.9852 | Function Loss:  -2.2762\n",
      "Total loss:  -1.1987 | PDE Loss:  -1.9849 | Function Loss:  -2.2762\n",
      "Total loss:  -1.1987 | PDE Loss:  -1.9851 | Function Loss:  -2.2762\n",
      "Total loss:  -1.1987 | PDE Loss:  -1.9851 | Function Loss:  -2.2762\n",
      "Total loss:  -1.1987 | PDE Loss:  -1.9851 | Function Loss:  -2.2762\n",
      "Total loss:  -1.1987 | PDE Loss:  -1.9851 | Function Loss:  -2.2762\n",
      "Total loss:  -1.1987 | PDE Loss:  -1.9851 | Function Loss:  -2.2763\n",
      "Total loss:  -1.1987 | PDE Loss:  -1.9852 | Function Loss:  -2.2763\n",
      "Total loss:  -1.1987 | PDE Loss:  -1.9852 | Function Loss:  -2.2762\n",
      "Total loss:  -1.1987 | PDE Loss:  -1.9853 | Function Loss:  -2.2762\n",
      "Total loss:  -1.1987 | PDE Loss:  -1.9855 | Function Loss:  -2.2762\n",
      "Total loss:  -1.1988 | PDE Loss:  -1.9857 | Function Loss:  -2.2762\n",
      "Total loss:  -1.1988 | PDE Loss:  -1.9859 | Function Loss:  -2.2762\n",
      "Total loss:  -1.1988 | PDE Loss:  -1.9866 | Function Loss:  -2.276\n",
      "Total loss:  -1.1988 | PDE Loss:  -1.9862 | Function Loss:  -2.2761\n",
      "Total loss:  -1.1988 | PDE Loss:  -1.9862 | Function Loss:  -2.2761\n",
      "Total loss:  -1.1988 | PDE Loss:  -1.9863 | Function Loss:  -2.2762\n",
      "Total loss:  -1.1988 | PDE Loss:  -1.9862 | Function Loss:  -2.2762\n",
      "Total loss:  -1.1988 | PDE Loss:  -1.9862 | Function Loss:  -2.2762\n",
      "Total loss:  -1.1988 | PDE Loss:  -1.9862 | Function Loss:  -2.2762\n",
      "Total loss:  -1.1989 | PDE Loss:  -1.9861 | Function Loss:  -2.2762\n",
      "Total loss:  -1.1989 | PDE Loss:  -1.986 | Function Loss:  -2.2763\n",
      "Total loss:  -1.1989 | PDE Loss:  -1.9859 | Function Loss:  -2.2763\n",
      "Total loss:  -1.1989 | PDE Loss:  -1.9858 | Function Loss:  -2.2763\n",
      "Total loss:  -1.1989 | PDE Loss:  -1.9859 | Function Loss:  -2.2764\n",
      "Total loss:  -1.1989 | PDE Loss:  -1.9859 | Function Loss:  -2.2764\n",
      "Total loss:  -1.1989 | PDE Loss:  -1.986 | Function Loss:  -2.2764\n",
      "Total loss:  -1.1989 | PDE Loss:  -1.986 | Function Loss:  -2.2764\n",
      "Total loss:  -1.199 | PDE Loss:  -1.986 | Function Loss:  -2.2764\n",
      "Total loss:  -1.199 | PDE Loss:  -1.986 | Function Loss:  -2.2764\n",
      "Total loss:  -1.199 | PDE Loss:  -1.986 | Function Loss:  -2.2764\n",
      "Total loss:  -1.199 | PDE Loss:  -1.986 | Function Loss:  -2.2765\n",
      "Total loss:  -1.199 | PDE Loss:  -1.9861 | Function Loss:  -2.2765\n",
      "Total loss:  -1.1991 | PDE Loss:  -1.986 | Function Loss:  -2.2765\n",
      "Total loss:  -1.1991 | PDE Loss:  -1.9861 | Function Loss:  -2.2765\n",
      "Total loss:  -1.1991 | PDE Loss:  -1.9861 | Function Loss:  -2.2765\n",
      "Total loss:  -1.1991 | PDE Loss:  -1.9864 | Function Loss:  -2.2765\n",
      "Total loss:  -1.1992 | PDE Loss:  -1.9866 | Function Loss:  -2.2765\n",
      "Total loss:  -1.1992 | PDE Loss:  -1.987 | Function Loss:  -2.2765\n",
      "Total loss:  -1.1992 | PDE Loss:  -1.9873 | Function Loss:  -2.2764\n",
      "Total loss:  -1.1992 | PDE Loss:  -1.9874 | Function Loss:  -2.2764\n",
      "Total loss:  -1.1992 | PDE Loss:  -1.9876 | Function Loss:  -2.2764\n",
      "Total loss:  -1.1993 | PDE Loss:  -1.9877 | Function Loss:  -2.2764\n",
      "Total loss:  -1.1993 | PDE Loss:  -1.9879 | Function Loss:  -2.2764\n",
      "Total loss:  -1.1993 | PDE Loss:  -1.9879 | Function Loss:  -2.2764\n",
      "Total loss:  -1.1993 | PDE Loss:  -1.9879 | Function Loss:  -2.2764\n",
      "Total loss:  -1.1993 | PDE Loss:  -1.9879 | Function Loss:  -2.2764\n",
      "Total loss:  -1.1993 | PDE Loss:  -1.9878 | Function Loss:  -2.2765\n",
      "Total loss:  -1.1993 | PDE Loss:  -1.9877 | Function Loss:  -2.2765\n",
      "Total loss:  -1.1993 | PDE Loss:  -1.9876 | Function Loss:  -2.2765\n",
      "Total loss:  -1.1994 | PDE Loss:  -1.9876 | Function Loss:  -2.2765\n",
      "Total loss:  -1.1994 | PDE Loss:  -1.9876 | Function Loss:  -2.2766\n",
      "Total loss:  -1.1994 | PDE Loss:  -1.9876 | Function Loss:  -2.2766\n",
      "Total loss:  -1.1994 | PDE Loss:  -1.9876 | Function Loss:  -2.2766\n",
      "Total loss:  -1.1994 | PDE Loss:  -1.9877 | Function Loss:  -2.2766\n",
      "Total loss:  -1.1994 | PDE Loss:  -1.9878 | Function Loss:  -2.2766\n",
      "Total loss:  -1.1995 | PDE Loss:  -1.9879 | Function Loss:  -2.2766\n",
      "Total loss:  -1.1995 | PDE Loss:  -1.988 | Function Loss:  -2.2766\n",
      "Total loss:  -1.1995 | PDE Loss:  -1.9882 | Function Loss:  -2.2766\n",
      "Total loss:  -1.1995 | PDE Loss:  -1.9882 | Function Loss:  -2.2766\n",
      "Total loss:  -1.1995 | PDE Loss:  -1.9884 | Function Loss:  -2.2766\n",
      "Total loss:  -1.1996 | PDE Loss:  -1.9884 | Function Loss:  -2.2766\n",
      "Total loss:  -1.1996 | PDE Loss:  -1.9885 | Function Loss:  -2.2766\n",
      "Total loss:  -1.1996 | PDE Loss:  -1.9886 | Function Loss:  -2.2766\n",
      "Total loss:  -1.1996 | PDE Loss:  -1.9887 | Function Loss:  -2.2766\n",
      "Total loss:  -1.1996 | PDE Loss:  -1.9888 | Function Loss:  -2.2766\n",
      "Total loss:  -1.1997 | PDE Loss:  -1.989 | Function Loss:  -2.2766\n",
      "Total loss:  -1.1997 | PDE Loss:  -1.9891 | Function Loss:  -2.2766\n",
      "Total loss:  -1.1997 | PDE Loss:  -1.9893 | Function Loss:  -2.2766\n",
      "Total loss:  -1.1997 | PDE Loss:  -1.9896 | Function Loss:  -2.2766\n",
      "Total loss:  -1.1997 | PDE Loss:  -1.9899 | Function Loss:  -2.2766\n",
      "Total loss:  -1.1998 | PDE Loss:  -1.99 | Function Loss:  -2.2765\n",
      "Total loss:  -1.1998 | PDE Loss:  -1.9902 | Function Loss:  -2.2765\n",
      "Total loss:  -1.1998 | PDE Loss:  -1.9903 | Function Loss:  -2.2765\n",
      "Total loss:  -1.1998 | PDE Loss:  -1.9904 | Function Loss:  -2.2765\n",
      "Total loss:  -1.1998 | PDE Loss:  -1.9904 | Function Loss:  -2.2765\n",
      "Total loss:  -1.1998 | PDE Loss:  -1.9905 | Function Loss:  -2.2765\n",
      "Total loss:  -1.1998 | PDE Loss:  -1.9906 | Function Loss:  -2.2765\n",
      "Total loss:  -1.1998 | PDE Loss:  -1.9906 | Function Loss:  -2.2765\n",
      "Total loss:  -1.1998 | PDE Loss:  -1.9907 | Function Loss:  -2.2765\n",
      "Total loss:  -1.1999 | PDE Loss:  -1.9907 | Function Loss:  -2.2766\n",
      "Total loss:  -1.1999 | PDE Loss:  -1.9906 | Function Loss:  -2.2766\n",
      "Total loss:  -1.1999 | PDE Loss:  -1.9906 | Function Loss:  -2.2766\n",
      "Total loss:  -1.1999 | PDE Loss:  -1.9905 | Function Loss:  -2.2767\n",
      "Total loss:  -1.1999 | PDE Loss:  -1.9905 | Function Loss:  -2.2767\n",
      "Total loss:  -1.1999 | PDE Loss:  -1.9905 | Function Loss:  -2.2767\n",
      "Total loss:  -1.2 | PDE Loss:  -1.9904 | Function Loss:  -2.2767\n",
      "Total loss:  -1.2 | PDE Loss:  -1.9904 | Function Loss:  -2.2767\n",
      "Total loss:  -1.2 | PDE Loss:  -1.9905 | Function Loss:  -2.2767\n",
      "Total loss:  -1.2 | PDE Loss:  -1.9905 | Function Loss:  -2.2767\n",
      "Total loss:  -1.2 | PDE Loss:  -1.9905 | Function Loss:  -2.2768\n",
      "Total loss:  -1.2 | PDE Loss:  -1.9905 | Function Loss:  -2.2768\n",
      "Total loss:  -1.2 | PDE Loss:  -1.9905 | Function Loss:  -2.2768\n",
      "Total loss:  -1.2 | PDE Loss:  -1.9905 | Function Loss:  -2.2768\n",
      "Total loss:  -1.2001 | PDE Loss:  -1.9905 | Function Loss:  -2.2768\n",
      "Total loss:  -1.2001 | PDE Loss:  -1.9904 | Function Loss:  -2.2769\n",
      "Total loss:  -1.2001 | PDE Loss:  -1.9902 | Function Loss:  -2.2769\n",
      "Total loss:  -1.2001 | PDE Loss:  -1.9901 | Function Loss:  -2.277\n",
      "Total loss:  -1.2001 | PDE Loss:  -1.99 | Function Loss:  -2.277\n",
      "Total loss:  -1.2001 | PDE Loss:  -1.9899 | Function Loss:  -2.277\n",
      "Total loss:  -1.2001 | PDE Loss:  -1.9898 | Function Loss:  -2.277\n",
      "Total loss:  -1.2002 | PDE Loss:  -1.9898 | Function Loss:  -2.2771\n",
      "Total loss:  -1.2002 | PDE Loss:  -1.99 | Function Loss:  -2.277\n",
      "Total loss:  -1.2002 | PDE Loss:  -1.9899 | Function Loss:  -2.2771\n",
      "Total loss:  -1.2002 | PDE Loss:  -1.9899 | Function Loss:  -2.2771\n",
      "Total loss:  -1.2002 | PDE Loss:  -1.99 | Function Loss:  -2.2771\n",
      "Total loss:  -1.2002 | PDE Loss:  -1.9902 | Function Loss:  -2.2771\n",
      "Total loss:  -1.2002 | PDE Loss:  -1.9904 | Function Loss:  -2.277\n",
      "Total loss:  -1.2002 | PDE Loss:  -1.9906 | Function Loss:  -2.277\n",
      "Total loss:  -1.2003 | PDE Loss:  -1.9909 | Function Loss:  -2.277\n",
      "Total loss:  -1.2003 | PDE Loss:  -1.9913 | Function Loss:  -2.2769\n",
      "Total loss:  -1.2003 | PDE Loss:  -1.9916 | Function Loss:  -2.2769\n",
      "Total loss:  -1.2003 | PDE Loss:  -1.9918 | Function Loss:  -2.2769\n",
      "Total loss:  -1.2003 | PDE Loss:  -1.992 | Function Loss:  -2.2769\n",
      "Total loss:  -1.2004 | PDE Loss:  -1.9921 | Function Loss:  -2.2769\n",
      "Total loss:  -1.2004 | PDE Loss:  -1.992 | Function Loss:  -2.2769\n",
      "Total loss:  -1.2004 | PDE Loss:  -1.9921 | Function Loss:  -2.2769\n",
      "Total loss:  -1.2004 | PDE Loss:  -1.992 | Function Loss:  -2.2769\n",
      "Total loss:  -1.2004 | PDE Loss:  -1.9919 | Function Loss:  -2.277\n",
      "Total loss:  -1.2004 | PDE Loss:  -1.992 | Function Loss:  -2.277\n",
      "Total loss:  -1.2005 | PDE Loss:  -1.9921 | Function Loss:  -2.277\n",
      "Total loss:  -1.2005 | PDE Loss:  -1.9923 | Function Loss:  -2.277\n",
      "Total loss:  -1.2005 | PDE Loss:  -1.9925 | Function Loss:  -2.277\n",
      "Total loss:  -1.2006 | PDE Loss:  -1.9927 | Function Loss:  -2.277\n",
      "Total loss:  -1.2006 | PDE Loss:  -1.9929 | Function Loss:  -2.277\n",
      "Total loss:  -1.2006 | PDE Loss:  -1.9932 | Function Loss:  -2.277\n",
      "Total loss:  -1.2006 | PDE Loss:  -1.9933 | Function Loss:  -2.277\n",
      "Total loss:  -1.2006 | PDE Loss:  -1.9933 | Function Loss:  -2.277\n",
      "Total loss:  -1.2007 | PDE Loss:  -1.9934 | Function Loss:  -2.277\n",
      "Total loss:  -1.2007 | PDE Loss:  -1.9934 | Function Loss:  -2.277\n",
      "Total loss:  -1.2007 | PDE Loss:  -1.9933 | Function Loss:  -2.277\n",
      "Total loss:  -1.2007 | PDE Loss:  -1.9932 | Function Loss:  -2.2771\n",
      "Total loss:  -1.2007 | PDE Loss:  -1.9931 | Function Loss:  -2.2771\n",
      "Total loss:  -1.2008 | PDE Loss:  -1.993 | Function Loss:  -2.2772\n",
      "Total loss:  -1.2008 | PDE Loss:  -1.993 | Function Loss:  -2.2772\n",
      "Total loss:  -1.2008 | PDE Loss:  -1.9931 | Function Loss:  -2.2772\n",
      "Total loss:  -1.2009 | PDE Loss:  -1.9933 | Function Loss:  -2.2772\n",
      "Total loss:  -1.2009 | PDE Loss:  -1.9934 | Function Loss:  -2.2773\n",
      "Total loss:  -1.2009 | PDE Loss:  -1.9938 | Function Loss:  -2.2772\n",
      "Total loss:  -1.2009 | PDE Loss:  -1.9939 | Function Loss:  -2.2772\n",
      "Total loss:  -1.2009 | PDE Loss:  -1.9942 | Function Loss:  -2.2772\n",
      "Total loss:  -1.201 | PDE Loss:  -1.9944 | Function Loss:  -2.2772\n",
      "Total loss:  -1.201 | PDE Loss:  -1.9946 | Function Loss:  -2.2772\n",
      "Total loss:  -1.201 | PDE Loss:  -1.9951 | Function Loss:  -2.2771\n",
      "Total loss:  -1.201 | PDE Loss:  -1.995 | Function Loss:  -2.2771\n",
      "Total loss:  -1.2011 | PDE Loss:  -1.995 | Function Loss:  -2.2772\n",
      "Total loss:  -1.2011 | PDE Loss:  -1.9949 | Function Loss:  -2.2772\n",
      "Total loss:  -1.2011 | PDE Loss:  -1.9949 | Function Loss:  -2.2772\n",
      "Total loss:  -1.2011 | PDE Loss:  -1.9948 | Function Loss:  -2.2773\n",
      "Total loss:  -1.2011 | PDE Loss:  -1.9947 | Function Loss:  -2.2773\n",
      "Total loss:  -1.2012 | PDE Loss:  -1.9944 | Function Loss:  -2.2774\n",
      "Total loss:  -1.2012 | PDE Loss:  -1.9945 | Function Loss:  -2.2774\n",
      "Total loss:  -1.2012 | PDE Loss:  -1.9945 | Function Loss:  -2.2774\n",
      "Total loss:  -1.2012 | PDE Loss:  -1.9945 | Function Loss:  -2.2774\n",
      "Total loss:  -1.2012 | PDE Loss:  -1.9945 | Function Loss:  -2.2774\n",
      "Total loss:  -1.2012 | PDE Loss:  -1.9944 | Function Loss:  -2.2775\n",
      "Total loss:  -1.2012 | PDE Loss:  -1.9944 | Function Loss:  -2.2775\n",
      "Total loss:  -1.2013 | PDE Loss:  -1.9944 | Function Loss:  -2.2775\n",
      "Total loss:  -1.2013 | PDE Loss:  -1.9944 | Function Loss:  -2.2775\n",
      "Total loss:  -1.2013 | PDE Loss:  -1.9944 | Function Loss:  -2.2776\n",
      "Total loss:  -1.2013 | PDE Loss:  -1.9944 | Function Loss:  -2.2776\n",
      "Total loss:  -1.2014 | PDE Loss:  -1.9945 | Function Loss:  -2.2776\n",
      "Total loss:  -1.2014 | PDE Loss:  -1.9946 | Function Loss:  -2.2776\n",
      "Total loss:  -1.2014 | PDE Loss:  -1.9947 | Function Loss:  -2.2776\n",
      "Total loss:  -1.2014 | PDE Loss:  -1.9948 | Function Loss:  -2.2776\n",
      "Total loss:  -1.2014 | PDE Loss:  -1.9951 | Function Loss:  -2.2776\n",
      "Total loss:  -1.2014 | PDE Loss:  -1.9947 | Function Loss:  -2.2777\n",
      "Total loss:  -1.2015 | PDE Loss:  -1.9949 | Function Loss:  -2.2776\n",
      "Total loss:  -1.2015 | PDE Loss:  -1.9951 | Function Loss:  -2.2776\n",
      "Total loss:  -1.2015 | PDE Loss:  -1.9953 | Function Loss:  -2.2776\n",
      "Total loss:  -1.2015 | PDE Loss:  -1.9955 | Function Loss:  -2.2776\n",
      "Total loss:  -1.2015 | PDE Loss:  -1.9956 | Function Loss:  -2.2776\n",
      "Total loss:  -1.2015 | PDE Loss:  -1.9958 | Function Loss:  -2.2776\n",
      "Total loss:  -1.2015 | PDE Loss:  -1.996 | Function Loss:  -2.2775\n",
      "Total loss:  -1.2016 | PDE Loss:  -1.9962 | Function Loss:  -2.2775\n",
      "Total loss:  -1.2016 | PDE Loss:  -1.9965 | Function Loss:  -2.2775\n",
      "Total loss:  -1.2016 | PDE Loss:  -1.9967 | Function Loss:  -2.2774\n",
      "Total loss:  -1.2016 | PDE Loss:  -1.9968 | Function Loss:  -2.2774\n",
      "Total loss:  -1.2016 | PDE Loss:  -1.9969 | Function Loss:  -2.2774\n",
      "Total loss:  -1.2016 | PDE Loss:  -1.997 | Function Loss:  -2.2774\n",
      "Total loss:  -1.2016 | PDE Loss:  -1.9971 | Function Loss:  -2.2774\n",
      "Total loss:  -1.2016 | PDE Loss:  -1.9972 | Function Loss:  -2.2774\n",
      "Total loss:  -1.2017 | PDE Loss:  -1.9972 | Function Loss:  -2.2774\n",
      "Total loss:  -1.2017 | PDE Loss:  -1.9973 | Function Loss:  -2.2774\n",
      "Total loss:  -1.2017 | PDE Loss:  -1.9974 | Function Loss:  -2.2774\n",
      "Total loss:  -1.2017 | PDE Loss:  -1.9974 | Function Loss:  -2.2774\n",
      "Total loss:  -1.2017 | PDE Loss:  -1.9974 | Function Loss:  -2.2775\n",
      "Total loss:  -1.2017 | PDE Loss:  -1.9975 | Function Loss:  -2.2775\n",
      "Total loss:  -1.2017 | PDE Loss:  -1.9976 | Function Loss:  -2.2775\n",
      "Total loss:  -1.2018 | PDE Loss:  -1.9976 | Function Loss:  -2.2775\n",
      "Total loss:  -1.2018 | PDE Loss:  -1.9977 | Function Loss:  -2.2775\n",
      "Total loss:  -1.2018 | PDE Loss:  -1.9978 | Function Loss:  -2.2775\n",
      "Total loss:  -1.2018 | PDE Loss:  -1.998 | Function Loss:  -2.2775\n",
      "Total loss:  -1.2019 | PDE Loss:  -1.9981 | Function Loss:  -2.2775\n",
      "Total loss:  -1.2019 | PDE Loss:  -1.9982 | Function Loss:  -2.2775\n",
      "Total loss:  -1.2019 | PDE Loss:  -1.9983 | Function Loss:  -2.2775\n",
      "Total loss:  -1.2019 | PDE Loss:  -1.9984 | Function Loss:  -2.2776\n",
      "Total loss:  -1.202 | PDE Loss:  -1.9985 | Function Loss:  -2.2776\n",
      "Total loss:  -1.202 | PDE Loss:  -1.9985 | Function Loss:  -2.2776\n",
      "Total loss:  -1.202 | PDE Loss:  -1.9986 | Function Loss:  -2.2776\n",
      "Total loss:  -1.202 | PDE Loss:  -1.9988 | Function Loss:  -2.2776\n",
      "Total loss:  -1.2021 | PDE Loss:  -1.9988 | Function Loss:  -2.2776\n",
      "Total loss:  -1.2021 | PDE Loss:  -1.9989 | Function Loss:  -2.2776\n",
      "Total loss:  -1.2021 | PDE Loss:  -1.9991 | Function Loss:  -2.2776\n",
      "Total loss:  -1.2021 | PDE Loss:  -1.9992 | Function Loss:  -2.2776\n",
      "Total loss:  -1.2022 | PDE Loss:  -1.9995 | Function Loss:  -2.2776\n",
      "Total loss:  -1.2022 | PDE Loss:  -1.9994 | Function Loss:  -2.2777\n",
      "Total loss:  -1.2022 | PDE Loss:  -2.0001 | Function Loss:  -2.2775\n",
      "Total loss:  -1.2022 | PDE Loss:  -1.9998 | Function Loss:  -2.2777\n",
      "Total loss:  -1.2023 | PDE Loss:  -1.9998 | Function Loss:  -2.2777\n",
      "Total loss:  -1.2023 | PDE Loss:  -1.9999 | Function Loss:  -2.2777\n",
      "Total loss:  -1.2024 | PDE Loss:  -2.0 | Function Loss:  -2.2777\n",
      "Total loss:  -1.2024 | PDE Loss:  -2.0 | Function Loss:  -2.2778\n",
      "Total loss:  -1.2024 | PDE Loss:  -2.0001 | Function Loss:  -2.2778\n",
      "Total loss:  -1.2025 | PDE Loss:  -2.0002 | Function Loss:  -2.2778\n",
      "Total loss:  -1.2025 | PDE Loss:  -2.0003 | Function Loss:  -2.2778\n",
      "Total loss:  -1.2025 | PDE Loss:  -2.0004 | Function Loss:  -2.2779\n",
      "Total loss:  -1.2026 | PDE Loss:  -2.0007 | Function Loss:  -2.2778\n",
      "Total loss:  -1.2026 | PDE Loss:  -2.0007 | Function Loss:  -2.2779\n",
      "Total loss:  -1.2026 | PDE Loss:  -2.0008 | Function Loss:  -2.2779\n",
      "Total loss:  -1.2026 | PDE Loss:  -2.0008 | Function Loss:  -2.2779\n",
      "Total loss:  -1.2027 | PDE Loss:  -2.0008 | Function Loss:  -2.2779\n",
      "Total loss:  -1.2027 | PDE Loss:  -2.0008 | Function Loss:  -2.278\n",
      "Total loss:  -1.2027 | PDE Loss:  -2.0009 | Function Loss:  -2.278\n",
      "Total loss:  -1.2027 | PDE Loss:  -2.0008 | Function Loss:  -2.278\n",
      "Total loss:  -1.2027 | PDE Loss:  -2.0009 | Function Loss:  -2.278\n",
      "Total loss:  -1.2027 | PDE Loss:  -2.0008 | Function Loss:  -2.278\n",
      "Total loss:  -1.2027 | PDE Loss:  -2.0009 | Function Loss:  -2.278\n",
      "Total loss:  -1.2028 | PDE Loss:  -2.001 | Function Loss:  -2.278\n",
      "Total loss:  -1.2028 | PDE Loss:  -2.0012 | Function Loss:  -2.278\n",
      "Total loss:  -1.2028 | PDE Loss:  -2.0013 | Function Loss:  -2.278\n",
      "Total loss:  -1.2028 | PDE Loss:  -2.0015 | Function Loss:  -2.278\n",
      "Total loss:  -1.2029 | PDE Loss:  -2.0017 | Function Loss:  -2.278\n",
      "Total loss:  -1.2029 | PDE Loss:  -2.002 | Function Loss:  -2.278\n",
      "Total loss:  -1.2029 | PDE Loss:  -2.0023 | Function Loss:  -2.278\n",
      "Total loss:  -1.2029 | PDE Loss:  -2.0028 | Function Loss:  -2.2779\n",
      "Total loss:  -1.203 | PDE Loss:  -2.0029 | Function Loss:  -2.2779\n",
      "Total loss:  -1.203 | PDE Loss:  -2.0031 | Function Loss:  -2.2779\n",
      "Total loss:  -1.203 | PDE Loss:  -2.0032 | Function Loss:  -2.2779\n",
      "Total loss:  -1.203 | PDE Loss:  -2.0033 | Function Loss:  -2.2779\n",
      "Total loss:  -1.2031 | PDE Loss:  -2.0033 | Function Loss:  -2.278\n",
      "Total loss:  -1.2031 | PDE Loss:  -2.0034 | Function Loss:  -2.278\n",
      "Total loss:  -1.203 | PDE Loss:  -2.0031 | Function Loss:  -2.2779\n",
      "Total loss:  -1.2031 | PDE Loss:  -2.0034 | Function Loss:  -2.278\n",
      "Total loss:  -1.2031 | PDE Loss:  -2.0035 | Function Loss:  -2.278\n",
      "Total loss:  -1.2031 | PDE Loss:  -2.0035 | Function Loss:  -2.278\n",
      "Total loss:  -1.2031 | PDE Loss:  -2.0036 | Function Loss:  -2.278\n",
      "Total loss:  -1.2032 | PDE Loss:  -2.0037 | Function Loss:  -2.278\n",
      "Total loss:  -1.2032 | PDE Loss:  -2.0038 | Function Loss:  -2.278\n",
      "Total loss:  -1.2032 | PDE Loss:  -2.0039 | Function Loss:  -2.278\n",
      "Total loss:  -1.2032 | PDE Loss:  -2.004 | Function Loss:  -2.278\n",
      "Total loss:  -1.2032 | PDE Loss:  -2.004 | Function Loss:  -2.278\n",
      "Total loss:  -1.2033 | PDE Loss:  -2.0039 | Function Loss:  -2.2781\n",
      "Total loss:  -1.2033 | PDE Loss:  -2.0038 | Function Loss:  -2.2781\n",
      "Total loss:  -1.2033 | PDE Loss:  -2.0037 | Function Loss:  -2.2782\n",
      "Total loss:  -1.2033 | PDE Loss:  -2.0037 | Function Loss:  -2.2782\n",
      "Total loss:  -1.2033 | PDE Loss:  -2.0036 | Function Loss:  -2.2782\n",
      "Total loss:  -1.2034 | PDE Loss:  -2.0035 | Function Loss:  -2.2783\n",
      "Total loss:  -1.2034 | PDE Loss:  -2.0034 | Function Loss:  -2.2783\n",
      "Total loss:  -1.2034 | PDE Loss:  -2.0032 | Function Loss:  -2.2784\n",
      "Total loss:  -1.2034 | PDE Loss:  -2.0031 | Function Loss:  -2.2784\n",
      "Total loss:  -1.2035 | PDE Loss:  -2.003 | Function Loss:  -2.2785\n",
      "Total loss:  -1.2035 | PDE Loss:  -2.003 | Function Loss:  -2.2785\n",
      "Total loss:  -1.2035 | PDE Loss:  -2.0031 | Function Loss:  -2.2785\n",
      "Total loss:  -1.2035 | PDE Loss:  -2.0031 | Function Loss:  -2.2786\n",
      "Total loss:  -1.2036 | PDE Loss:  -2.0034 | Function Loss:  -2.2785\n",
      "Total loss:  -1.2036 | PDE Loss:  -2.0035 | Function Loss:  -2.2785\n",
      "Total loss:  -1.2036 | PDE Loss:  -2.0035 | Function Loss:  -2.2785\n",
      "Total loss:  -1.2036 | PDE Loss:  -2.0035 | Function Loss:  -2.2786\n",
      "Total loss:  -1.2036 | PDE Loss:  -2.0035 | Function Loss:  -2.2786\n",
      "Total loss:  -1.2036 | PDE Loss:  -2.0035 | Function Loss:  -2.2786\n",
      "Total loss:  -1.2036 | PDE Loss:  -2.0035 | Function Loss:  -2.2786\n",
      "Total loss:  -1.2037 | PDE Loss:  -2.0036 | Function Loss:  -2.2786\n",
      "Total loss:  -1.2037 | PDE Loss:  -2.0036 | Function Loss:  -2.2786\n",
      "Total loss:  -1.2037 | PDE Loss:  -2.0036 | Function Loss:  -2.2786\n",
      "Total loss:  -1.2037 | PDE Loss:  -2.0036 | Function Loss:  -2.2787\n",
      "Total loss:  -1.2037 | PDE Loss:  -2.0036 | Function Loss:  -2.2787\n",
      "Total loss:  -1.2037 | PDE Loss:  -2.0036 | Function Loss:  -2.2787\n",
      "Total loss:  -1.2037 | PDE Loss:  -2.0036 | Function Loss:  -2.2787\n",
      "Total loss:  -1.2038 | PDE Loss:  -2.0035 | Function Loss:  -2.2787\n",
      "Total loss:  -1.2038 | PDE Loss:  -2.0034 | Function Loss:  -2.2788\n",
      "Total loss:  -1.2038 | PDE Loss:  -2.0034 | Function Loss:  -2.2788\n",
      "Total loss:  -1.2038 | PDE Loss:  -2.0033 | Function Loss:  -2.2789\n",
      "Total loss:  -1.2038 | PDE Loss:  -2.0032 | Function Loss:  -2.2789\n",
      "Total loss:  -1.2038 | PDE Loss:  -2.003 | Function Loss:  -2.2789\n",
      "Total loss:  -1.2039 | PDE Loss:  -2.0031 | Function Loss:  -2.279\n",
      "Total loss:  -1.2039 | PDE Loss:  -2.0031 | Function Loss:  -2.279\n",
      "Total loss:  -1.2039 | PDE Loss:  -2.0032 | Function Loss:  -2.279\n",
      "Total loss:  -1.2039 | PDE Loss:  -2.0032 | Function Loss:  -2.279\n",
      "Total loss:  -1.2039 | PDE Loss:  -2.0033 | Function Loss:  -2.279\n",
      "Total loss:  -1.2039 | PDE Loss:  -2.0033 | Function Loss:  -2.279\n",
      "Total loss:  -1.2039 | PDE Loss:  -2.0034 | Function Loss:  -2.279\n",
      "Total loss:  -1.2039 | PDE Loss:  -2.0035 | Function Loss:  -2.279\n",
      "Total loss:  -1.204 | PDE Loss:  -2.0035 | Function Loss:  -2.279\n",
      "Total loss:  -1.204 | PDE Loss:  -2.0036 | Function Loss:  -2.279\n",
      "Total loss:  -1.204 | PDE Loss:  -2.0035 | Function Loss:  -2.279\n",
      "Total loss:  -1.204 | PDE Loss:  -2.0036 | Function Loss:  -2.279\n",
      "Total loss:  -1.204 | PDE Loss:  -2.0035 | Function Loss:  -2.2791\n",
      "Total loss:  -1.2041 | PDE Loss:  -2.0036 | Function Loss:  -2.2791\n",
      "Total loss:  -1.2036 | PDE Loss:  -2.0009 | Function Loss:  -2.279\n",
      "Total loss:  -1.2041 | PDE Loss:  -2.0035 | Function Loss:  -2.2791\n",
      "Total loss:  -1.2041 | PDE Loss:  -2.0035 | Function Loss:  -2.2791\n",
      "Total loss:  -1.2041 | PDE Loss:  -2.0036 | Function Loss:  -2.2791\n",
      "Total loss:  -1.2041 | PDE Loss:  -2.0037 | Function Loss:  -2.2791\n",
      "Total loss:  -1.2041 | PDE Loss:  -2.0037 | Function Loss:  -2.2791\n",
      "Total loss:  -1.2041 | PDE Loss:  -2.0038 | Function Loss:  -2.2791\n",
      "Total loss:  -1.2042 | PDE Loss:  -2.0038 | Function Loss:  -2.2791\n",
      "Total loss:  -1.2042 | PDE Loss:  -2.0037 | Function Loss:  -2.2792\n",
      "Total loss:  -1.2042 | PDE Loss:  -2.0038 | Function Loss:  -2.2792\n",
      "Total loss:  -1.2042 | PDE Loss:  -2.0037 | Function Loss:  -2.2793\n",
      "Total loss:  -1.2043 | PDE Loss:  -2.0036 | Function Loss:  -2.2793\n",
      "Total loss:  -1.2043 | PDE Loss:  -2.0035 | Function Loss:  -2.2794\n",
      "Total loss:  -1.2043 | PDE Loss:  -2.0035 | Function Loss:  -2.2794\n",
      "Total loss:  -1.2043 | PDE Loss:  -2.0034 | Function Loss:  -2.2794\n",
      "Total loss:  -1.2043 | PDE Loss:  -2.0035 | Function Loss:  -2.2795\n",
      "Total loss:  -1.2044 | PDE Loss:  -2.0035 | Function Loss:  -2.2795\n",
      "Total loss:  -1.2044 | PDE Loss:  -2.0036 | Function Loss:  -2.2795\n",
      "Total loss:  -1.2044 | PDE Loss:  -2.0037 | Function Loss:  -2.2795\n",
      "Total loss:  -1.2044 | PDE Loss:  -2.0037 | Function Loss:  -2.2795\n",
      "Total loss:  -1.2045 | PDE Loss:  -2.0038 | Function Loss:  -2.2795\n",
      "Total loss:  -1.2045 | PDE Loss:  -2.0039 | Function Loss:  -2.2795\n",
      "Total loss:  -1.2045 | PDE Loss:  -2.0039 | Function Loss:  -2.2795\n",
      "Total loss:  -1.2045 | PDE Loss:  -2.0038 | Function Loss:  -2.2796\n",
      "Total loss:  -1.2045 | PDE Loss:  -2.0041 | Function Loss:  -2.2795\n",
      "Total loss:  -1.2045 | PDE Loss:  -2.0041 | Function Loss:  -2.2796\n",
      "Total loss:  -1.2046 | PDE Loss:  -2.0041 | Function Loss:  -2.2796\n",
      "Total loss:  -1.2046 | PDE Loss:  -2.0042 | Function Loss:  -2.2796\n",
      "Total loss:  -1.2046 | PDE Loss:  -2.0042 | Function Loss:  -2.2796\n",
      "Total loss:  -1.2047 | PDE Loss:  -2.0044 | Function Loss:  -2.2796\n",
      "Total loss:  -1.2046 | PDE Loss:  -2.0041 | Function Loss:  -2.2796\n",
      "Total loss:  -1.2047 | PDE Loss:  -2.0044 | Function Loss:  -2.2797\n",
      "Total loss:  -1.2047 | PDE Loss:  -2.0045 | Function Loss:  -2.2797\n",
      "Total loss:  -1.2047 | PDE Loss:  -2.0047 | Function Loss:  -2.2796\n",
      "Total loss:  -1.2047 | PDE Loss:  -2.0048 | Function Loss:  -2.2796\n",
      "Total loss:  -1.2047 | PDE Loss:  -2.0048 | Function Loss:  -2.2796\n",
      "Total loss:  -1.2047 | PDE Loss:  -2.0049 | Function Loss:  -2.2796\n",
      "Total loss:  -1.2047 | PDE Loss:  -2.005 | Function Loss:  -2.2796\n",
      "Total loss:  -1.2048 | PDE Loss:  -2.0052 | Function Loss:  -2.2796\n",
      "Total loss:  -1.2048 | PDE Loss:  -2.0052 | Function Loss:  -2.2797\n",
      "Total loss:  -1.2048 | PDE Loss:  -2.0053 | Function Loss:  -2.2797\n",
      "Total loss:  -1.2047 | PDE Loss:  -2.0033 | Function Loss:  -2.2799\n",
      "Total loss:  -1.2049 | PDE Loss:  -2.005 | Function Loss:  -2.2798\n",
      "Total loss:  -1.2049 | PDE Loss:  -2.0051 | Function Loss:  -2.2798\n",
      "Total loss:  -1.2049 | PDE Loss:  -2.005 | Function Loss:  -2.2798\n",
      "Total loss:  -1.2049 | PDE Loss:  -2.005 | Function Loss:  -2.2799\n",
      "Total loss:  -1.2049 | PDE Loss:  -2.0049 | Function Loss:  -2.2799\n",
      "Total loss:  -1.205 | PDE Loss:  -2.0048 | Function Loss:  -2.2799\n",
      "Total loss:  -1.205 | PDE Loss:  -2.0047 | Function Loss:  -2.28\n",
      "Total loss:  -1.205 | PDE Loss:  -2.0048 | Function Loss:  -2.28\n",
      "Total loss:  -1.205 | PDE Loss:  -2.0048 | Function Loss:  -2.28\n",
      "Total loss:  -1.205 | PDE Loss:  -2.0048 | Function Loss:  -2.28\n",
      "Total loss:  -1.205 | PDE Loss:  -2.0049 | Function Loss:  -2.28\n",
      "Total loss:  -1.2051 | PDE Loss:  -2.005 | Function Loss:  -2.28\n",
      "Total loss:  -1.2051 | PDE Loss:  -2.0052 | Function Loss:  -2.28\n",
      "Total loss:  -1.2051 | PDE Loss:  -2.0054 | Function Loss:  -2.28\n",
      "Total loss:  -1.2051 | PDE Loss:  -2.0057 | Function Loss:  -2.28\n",
      "Total loss:  -1.2052 | PDE Loss:  -2.0061 | Function Loss:  -2.2799\n",
      "Total loss:  -1.2052 | PDE Loss:  -2.0064 | Function Loss:  -2.2799\n",
      "Total loss:  -1.2052 | PDE Loss:  -2.0066 | Function Loss:  -2.2799\n",
      "Total loss:  -1.2052 | PDE Loss:  -2.007 | Function Loss:  -2.2798\n",
      "Total loss:  -1.2052 | PDE Loss:  -2.0071 | Function Loss:  -2.2798\n",
      "Total loss:  -1.2052 | PDE Loss:  -2.0072 | Function Loss:  -2.2798\n",
      "Total loss:  -1.2053 | PDE Loss:  -2.0074 | Function Loss:  -2.2798\n",
      "Total loss:  -1.2053 | PDE Loss:  -2.0076 | Function Loss:  -2.2798\n",
      "Total loss:  -1.2053 | PDE Loss:  -2.0078 | Function Loss:  -2.2798\n",
      "Total loss:  -1.2054 | PDE Loss:  -2.008 | Function Loss:  -2.2798\n",
      "Total loss:  -1.2054 | PDE Loss:  -2.0081 | Function Loss:  -2.2798\n",
      "Total loss:  -1.2054 | PDE Loss:  -2.0083 | Function Loss:  -2.2798\n",
      "Total loss:  -1.2054 | PDE Loss:  -2.0084 | Function Loss:  -2.2798\n",
      "Total loss:  -1.2054 | PDE Loss:  -2.0084 | Function Loss:  -2.2798\n",
      "Total loss:  -1.2055 | PDE Loss:  -2.0085 | Function Loss:  -2.2798\n",
      "Total loss:  -1.2055 | PDE Loss:  -2.0086 | Function Loss:  -2.2798\n",
      "Total loss:  -1.2055 | PDE Loss:  -2.0087 | Function Loss:  -2.2798\n",
      "Total loss:  -1.2055 | PDE Loss:  -2.0086 | Function Loss:  -2.2799\n",
      "Total loss:  -1.2055 | PDE Loss:  -2.0086 | Function Loss:  -2.2799\n",
      "Total loss:  -1.2055 | PDE Loss:  -2.0086 | Function Loss:  -2.2799\n",
      "Total loss:  -1.2056 | PDE Loss:  -2.0087 | Function Loss:  -2.2799\n",
      "Total loss:  -1.2056 | PDE Loss:  -2.0087 | Function Loss:  -2.2799\n",
      "Total loss:  -1.2056 | PDE Loss:  -2.0088 | Function Loss:  -2.2799\n",
      "Total loss:  -1.2056 | PDE Loss:  -2.0089 | Function Loss:  -2.2799\n",
      "Total loss:  -1.2056 | PDE Loss:  -2.0089 | Function Loss:  -2.28\n",
      "Total loss:  -1.2057 | PDE Loss:  -2.0092 | Function Loss:  -2.2799\n",
      "Total loss:  -1.2057 | PDE Loss:  -2.0092 | Function Loss:  -2.2799\n",
      "Total loss:  -1.2057 | PDE Loss:  -2.0095 | Function Loss:  -2.2799\n",
      "Total loss:  -1.2057 | PDE Loss:  -2.0097 | Function Loss:  -2.2799\n",
      "Total loss:  -1.2057 | PDE Loss:  -2.0098 | Function Loss:  -2.2799\n",
      "Total loss:  -1.2057 | PDE Loss:  -2.0099 | Function Loss:  -2.2798\n",
      "Total loss:  -1.2057 | PDE Loss:  -2.0101 | Function Loss:  -2.2798\n",
      "Total loss:  -1.2057 | PDE Loss:  -2.0102 | Function Loss:  -2.2798\n",
      "Total loss:  -1.2057 | PDE Loss:  -2.0104 | Function Loss:  -2.2798\n",
      "Total loss:  -1.2057 | PDE Loss:  -2.0103 | Function Loss:  -2.2798\n",
      "Total loss:  -1.2057 | PDE Loss:  -2.0104 | Function Loss:  -2.2798\n",
      "Total loss:  -1.2057 | PDE Loss:  -2.0105 | Function Loss:  -2.2798\n",
      "Total loss:  -1.2058 | PDE Loss:  -2.0106 | Function Loss:  -2.2798\n",
      "Total loss:  -1.2058 | PDE Loss:  -2.0107 | Function Loss:  -2.2798\n",
      "Total loss:  -1.2058 | PDE Loss:  -2.0108 | Function Loss:  -2.2798\n",
      "Total loss:  -1.2058 | PDE Loss:  -2.0109 | Function Loss:  -2.2798\n",
      "Total loss:  -1.2058 | PDE Loss:  -2.011 | Function Loss:  -2.2798\n",
      "Total loss:  -1.2058 | PDE Loss:  -2.0111 | Function Loss:  -2.2798\n",
      "Total loss:  -1.2058 | PDE Loss:  -2.0113 | Function Loss:  -2.2798\n",
      "Total loss:  -1.2058 | PDE Loss:  -2.0114 | Function Loss:  -2.2797\n",
      "Total loss:  -1.2059 | PDE Loss:  -2.0115 | Function Loss:  -2.2797\n",
      "Total loss:  -1.2059 | PDE Loss:  -2.0117 | Function Loss:  -2.2797\n",
      "Total loss:  -1.2059 | PDE Loss:  -2.012 | Function Loss:  -2.2797\n",
      "Total loss:  -1.2059 | PDE Loss:  -2.0121 | Function Loss:  -2.2797\n",
      "Total loss:  -1.2059 | PDE Loss:  -2.0123 | Function Loss:  -2.2797\n",
      "Total loss:  -1.2059 | PDE Loss:  -2.0124 | Function Loss:  -2.2797\n",
      "Total loss:  -1.206 | PDE Loss:  -2.0125 | Function Loss:  -2.2797\n",
      "Total loss:  -1.206 | PDE Loss:  -2.0125 | Function Loss:  -2.2797\n",
      "Total loss:  -1.206 | PDE Loss:  -2.0125 | Function Loss:  -2.2797\n",
      "Total loss:  -1.206 | PDE Loss:  -2.0126 | Function Loss:  -2.2797\n",
      "Total loss:  -1.206 | PDE Loss:  -2.0126 | Function Loss:  -2.2797\n",
      "Total loss:  -1.206 | PDE Loss:  -2.0129 | Function Loss:  -2.2797\n",
      "Total loss:  -1.206 | PDE Loss:  -2.0128 | Function Loss:  -2.2797\n",
      "Total loss:  -1.2061 | PDE Loss:  -2.0128 | Function Loss:  -2.2797\n",
      "Total loss:  -1.2061 | PDE Loss:  -2.0129 | Function Loss:  -2.2797\n",
      "Total loss:  -1.2061 | PDE Loss:  -2.0129 | Function Loss:  -2.2798\n",
      "Total loss:  -1.2061 | PDE Loss:  -2.0129 | Function Loss:  -2.2798\n",
      "Total loss:  -1.2061 | PDE Loss:  -2.013 | Function Loss:  -2.2798\n",
      "Total loss:  -1.2061 | PDE Loss:  -2.013 | Function Loss:  -2.2798\n",
      "Total loss:  -1.2061 | PDE Loss:  -2.0129 | Function Loss:  -2.2798\n",
      "Total loss:  -1.2062 | PDE Loss:  -2.0128 | Function Loss:  -2.2799\n",
      "Total loss:  -1.2062 | PDE Loss:  -2.0128 | Function Loss:  -2.2799\n",
      "Total loss:  -1.2062 | PDE Loss:  -2.0127 | Function Loss:  -2.2799\n",
      "Total loss:  -1.2062 | PDE Loss:  -2.0126 | Function Loss:  -2.28\n",
      "Total loss:  -1.2062 | PDE Loss:  -2.0125 | Function Loss:  -2.28\n",
      "Total loss:  -1.2063 | PDE Loss:  -2.0124 | Function Loss:  -2.28\n",
      "Total loss:  -1.2063 | PDE Loss:  -2.0123 | Function Loss:  -2.2801\n",
      "Total loss:  -1.2063 | PDE Loss:  -2.0122 | Function Loss:  -2.2801\n",
      "Total loss:  -1.2063 | PDE Loss:  -2.0122 | Function Loss:  -2.2801\n",
      "Total loss:  -1.2063 | PDE Loss:  -2.0121 | Function Loss:  -2.2802\n",
      "Total loss:  -1.2063 | PDE Loss:  -2.0121 | Function Loss:  -2.2802\n",
      "Total loss:  -1.2063 | PDE Loss:  -2.0122 | Function Loss:  -2.2802\n",
      "Total loss:  -1.2063 | PDE Loss:  -2.0122 | Function Loss:  -2.2802\n",
      "Total loss:  -1.2063 | PDE Loss:  -2.0123 | Function Loss:  -2.2802\n",
      "Total loss:  -1.2063 | PDE Loss:  -2.0123 | Function Loss:  -2.2802\n",
      "Total loss:  -1.2064 | PDE Loss:  -2.0123 | Function Loss:  -2.2802\n",
      "Total loss:  -1.2064 | PDE Loss:  -2.0123 | Function Loss:  -2.2802\n",
      "Total loss:  -1.2064 | PDE Loss:  -2.0124 | Function Loss:  -2.2802\n",
      "Total loss:  -1.2064 | PDE Loss:  -2.0124 | Function Loss:  -2.2802\n",
      "Total loss:  -1.2064 | PDE Loss:  -2.0124 | Function Loss:  -2.2802\n",
      "Total loss:  -1.2064 | PDE Loss:  -2.0124 | Function Loss:  -2.2802\n",
      "Total loss:  -1.2064 | PDE Loss:  -2.0123 | Function Loss:  -2.2802\n",
      "Total loss:  -1.2064 | PDE Loss:  -2.0123 | Function Loss:  -2.2803\n",
      "Total loss:  -1.2064 | PDE Loss:  -2.0123 | Function Loss:  -2.2803\n",
      "Total loss:  -1.2064 | PDE Loss:  -2.0122 | Function Loss:  -2.2803\n",
      "Total loss:  -1.2065 | PDE Loss:  -2.0122 | Function Loss:  -2.2803\n",
      "Total loss:  -1.2065 | PDE Loss:  -2.0121 | Function Loss:  -2.2803\n",
      "Total loss:  -1.2065 | PDE Loss:  -2.012 | Function Loss:  -2.2804\n",
      "Total loss:  -1.2065 | PDE Loss:  -2.012 | Function Loss:  -2.2804\n",
      "Total loss:  -1.2065 | PDE Loss:  -2.0119 | Function Loss:  -2.2804\n",
      "Total loss:  -1.2065 | PDE Loss:  -2.012 | Function Loss:  -2.2804\n",
      "Total loss:  -1.2065 | PDE Loss:  -2.0119 | Function Loss:  -2.2804\n",
      "Total loss:  -1.2065 | PDE Loss:  -2.0119 | Function Loss:  -2.2804\n",
      "Total loss:  -1.2065 | PDE Loss:  -2.0118 | Function Loss:  -2.2805\n",
      "Total loss:  -1.2065 | PDE Loss:  -2.0118 | Function Loss:  -2.2805\n",
      "Total loss:  -1.2065 | PDE Loss:  -2.0118 | Function Loss:  -2.2805\n",
      "Total loss:  -1.2065 | PDE Loss:  -2.012 | Function Loss:  -2.2805\n",
      "Total loss:  -1.2065 | PDE Loss:  -2.0119 | Function Loss:  -2.2805\n",
      "Total loss:  -1.2065 | PDE Loss:  -2.0119 | Function Loss:  -2.2805\n",
      "Total loss:  -1.2066 | PDE Loss:  -2.0119 | Function Loss:  -2.2805\n",
      "Total loss:  -1.2066 | PDE Loss:  -2.012 | Function Loss:  -2.2805\n",
      "Total loss:  -1.2066 | PDE Loss:  -2.012 | Function Loss:  -2.2805\n",
      "Total loss:  -1.2066 | PDE Loss:  -2.0121 | Function Loss:  -2.2805\n",
      "Total loss:  -1.2066 | PDE Loss:  -2.0121 | Function Loss:  -2.2805\n",
      "Total loss:  -1.2066 | PDE Loss:  -2.0122 | Function Loss:  -2.2805\n",
      "Total loss:  -1.2066 | PDE Loss:  -2.0125 | Function Loss:  -2.2805\n",
      "Total loss:  -1.2066 | PDE Loss:  -2.0126 | Function Loss:  -2.2805\n",
      "Total loss:  -1.2067 | PDE Loss:  -2.0128 | Function Loss:  -2.2804\n",
      "Total loss:  -1.2067 | PDE Loss:  -2.0129 | Function Loss:  -2.2805\n",
      "Total loss:  -1.2067 | PDE Loss:  -2.0129 | Function Loss:  -2.2805\n",
      "Total loss:  -1.2067 | PDE Loss:  -2.0129 | Function Loss:  -2.2805\n",
      "Total loss:  -1.2067 | PDE Loss:  -2.013 | Function Loss:  -2.2805\n",
      "Total loss:  -1.2067 | PDE Loss:  -2.013 | Function Loss:  -2.2805\n",
      "Total loss:  -1.2067 | PDE Loss:  -2.013 | Function Loss:  -2.2805\n",
      "Total loss:  -1.2067 | PDE Loss:  -2.013 | Function Loss:  -2.2805\n",
      "Total loss:  -1.2067 | PDE Loss:  -2.0131 | Function Loss:  -2.2805\n",
      "Total loss:  -1.2067 | PDE Loss:  -2.0132 | Function Loss:  -2.2805\n",
      "Total loss:  -1.2068 | PDE Loss:  -2.0133 | Function Loss:  -2.2805\n",
      "Total loss:  -1.2068 | PDE Loss:  -2.0133 | Function Loss:  -2.2805\n",
      "Total loss:  -1.2068 | PDE Loss:  -2.0133 | Function Loss:  -2.2805\n",
      "Total loss:  -1.2068 | PDE Loss:  -2.0134 | Function Loss:  -2.2805\n",
      "Total loss:  -1.2068 | PDE Loss:  -2.0134 | Function Loss:  -2.2805\n",
      "Total loss:  -1.2068 | PDE Loss:  -2.0133 | Function Loss:  -2.2805\n",
      "Total loss:  -1.2068 | PDE Loss:  -2.0134 | Function Loss:  -2.2805\n",
      "Total loss:  -1.2068 | PDE Loss:  -2.0135 | Function Loss:  -2.2805\n",
      "Total loss:  -1.2068 | PDE Loss:  -2.0135 | Function Loss:  -2.2805\n",
      "Total loss:  -1.2068 | PDE Loss:  -2.0135 | Function Loss:  -2.2805\n",
      "Total loss:  -1.2068 | PDE Loss:  -2.0135 | Function Loss:  -2.2805\n",
      "Total loss:  -1.2068 | PDE Loss:  -2.0135 | Function Loss:  -2.2805\n",
      "Total loss:  -1.2069 | PDE Loss:  -2.0134 | Function Loss:  -2.2806\n",
      "Total loss:  -1.2069 | PDE Loss:  -2.0134 | Function Loss:  -2.2806\n",
      "Total loss:  -1.2069 | PDE Loss:  -2.0133 | Function Loss:  -2.2806\n",
      "Total loss:  -1.2069 | PDE Loss:  -2.0133 | Function Loss:  -2.2807\n",
      "Total loss:  -1.2069 | PDE Loss:  -2.0133 | Function Loss:  -2.2807\n",
      "Total loss:  -1.2069 | PDE Loss:  -2.0134 | Function Loss:  -2.2807\n",
      "Total loss:  -1.207 | PDE Loss:  -2.0133 | Function Loss:  -2.2807\n",
      "Total loss:  -1.207 | PDE Loss:  -2.0133 | Function Loss:  -2.2807\n",
      "Total loss:  -1.207 | PDE Loss:  -2.0133 | Function Loss:  -2.2807\n",
      "Total loss:  -1.207 | PDE Loss:  -2.0133 | Function Loss:  -2.2807\n",
      "Total loss:  -1.207 | PDE Loss:  -2.0133 | Function Loss:  -2.2808\n",
      "Total loss:  -1.207 | PDE Loss:  -2.0133 | Function Loss:  -2.2808\n",
      "Total loss:  -1.207 | PDE Loss:  -2.0133 | Function Loss:  -2.2808\n",
      "Total loss:  -1.207 | PDE Loss:  -2.0131 | Function Loss:  -2.2809\n",
      "Total loss:  -1.2071 | PDE Loss:  -2.0131 | Function Loss:  -2.2809\n",
      "Total loss:  -1.2071 | PDE Loss:  -2.0131 | Function Loss:  -2.2809\n",
      "Total loss:  -1.2071 | PDE Loss:  -2.0132 | Function Loss:  -2.2809\n",
      "Total loss:  -1.2071 | PDE Loss:  -2.0132 | Function Loss:  -2.2809\n",
      "Total loss:  -1.2071 | PDE Loss:  -2.0132 | Function Loss:  -2.281\n",
      "Total loss:  -1.2072 | PDE Loss:  -2.0132 | Function Loss:  -2.281\n",
      "Total loss:  -1.2072 | PDE Loss:  -2.0131 | Function Loss:  -2.281\n",
      "Total loss:  -1.2072 | PDE Loss:  -2.0132 | Function Loss:  -2.2811\n",
      "Total loss:  -1.2073 | PDE Loss:  -2.0132 | Function Loss:  -2.2811\n",
      "Total loss:  -1.2073 | PDE Loss:  -2.0132 | Function Loss:  -2.2811\n",
      "Total loss:  -1.2073 | PDE Loss:  -2.0132 | Function Loss:  -2.2812\n",
      "Total loss:  -1.2074 | PDE Loss:  -2.0132 | Function Loss:  -2.2812\n",
      "Total loss:  -1.2074 | PDE Loss:  -2.0131 | Function Loss:  -2.2813\n",
      "Total loss:  -1.2074 | PDE Loss:  -2.0131 | Function Loss:  -2.2813\n",
      "Total loss:  -1.2074 | PDE Loss:  -2.0132 | Function Loss:  -2.2813\n",
      "Total loss:  -1.2075 | PDE Loss:  -2.0134 | Function Loss:  -2.2813\n",
      "Total loss:  -1.2075 | PDE Loss:  -2.0134 | Function Loss:  -2.2813\n",
      "Total loss:  -1.2075 | PDE Loss:  -2.0136 | Function Loss:  -2.2813\n",
      "Total loss:  -1.2075 | PDE Loss:  -2.0137 | Function Loss:  -2.2813\n",
      "Total loss:  -1.2075 | PDE Loss:  -2.0137 | Function Loss:  -2.2813\n",
      "Total loss:  -1.2075 | PDE Loss:  -2.014 | Function Loss:  -2.2813\n",
      "Total loss:  -1.2076 | PDE Loss:  -2.0139 | Function Loss:  -2.2813\n",
      "Total loss:  -1.2076 | PDE Loss:  -2.0138 | Function Loss:  -2.2814\n",
      "Total loss:  -1.2076 | PDE Loss:  -2.0137 | Function Loss:  -2.2814\n",
      "Total loss:  -1.2076 | PDE Loss:  -2.0137 | Function Loss:  -2.2814\n",
      "Total loss:  -1.2076 | PDE Loss:  -2.0136 | Function Loss:  -2.2815\n",
      "Total loss:  -1.2077 | PDE Loss:  -2.0136 | Function Loss:  -2.2815\n",
      "Total loss:  -1.2077 | PDE Loss:  -2.0136 | Function Loss:  -2.2815\n",
      "Total loss:  -1.2077 | PDE Loss:  -2.0136 | Function Loss:  -2.2815\n",
      "Total loss:  -1.2077 | PDE Loss:  -2.0137 | Function Loss:  -2.2815\n",
      "Total loss:  -1.2077 | PDE Loss:  -2.0138 | Function Loss:  -2.2815\n",
      "Total loss:  -1.2077 | PDE Loss:  -2.0139 | Function Loss:  -2.2815\n",
      "Total loss:  -1.2078 | PDE Loss:  -2.0141 | Function Loss:  -2.2815\n",
      "Total loss:  -1.2078 | PDE Loss:  -2.0142 | Function Loss:  -2.2815\n",
      "Total loss:  -1.2078 | PDE Loss:  -2.0143 | Function Loss:  -2.2815\n",
      "Total loss:  -1.2078 | PDE Loss:  -2.0143 | Function Loss:  -2.2815\n",
      "Total loss:  -1.2078 | PDE Loss:  -2.0153 | Function Loss:  -2.2814\n",
      "Total loss:  -1.2078 | PDE Loss:  -2.015 | Function Loss:  -2.2814\n",
      "Total loss:  -1.2079 | PDE Loss:  -2.0149 | Function Loss:  -2.2815\n",
      "Total loss:  -1.2079 | PDE Loss:  -2.0147 | Function Loss:  -2.2815\n",
      "Total loss:  -1.2079 | PDE Loss:  -2.0147 | Function Loss:  -2.2816\n",
      "Total loss:  -1.2079 | PDE Loss:  -2.0147 | Function Loss:  -2.2816\n",
      "Total loss:  -1.2079 | PDE Loss:  -2.0148 | Function Loss:  -2.2816\n",
      "Total loss:  -1.2079 | PDE Loss:  -2.0148 | Function Loss:  -2.2816\n",
      "Total loss:  -1.208 | PDE Loss:  -2.015 | Function Loss:  -2.2816\n",
      "Total loss:  -1.208 | PDE Loss:  -2.0152 | Function Loss:  -2.2816\n",
      "Total loss:  -1.208 | PDE Loss:  -2.0154 | Function Loss:  -2.2816\n",
      "Total loss:  -1.208 | PDE Loss:  -2.0156 | Function Loss:  -2.2816\n",
      "Total loss:  -1.2081 | PDE Loss:  -2.0158 | Function Loss:  -2.2816\n",
      "Total loss:  -1.2081 | PDE Loss:  -2.016 | Function Loss:  -2.2815\n",
      "Total loss:  -1.2081 | PDE Loss:  -2.0161 | Function Loss:  -2.2816\n",
      "Total loss:  -1.2081 | PDE Loss:  -2.0163 | Function Loss:  -2.2816\n",
      "Total loss:  -1.2081 | PDE Loss:  -2.0163 | Function Loss:  -2.2816\n",
      "Total loss:  -1.2082 | PDE Loss:  -2.0162 | Function Loss:  -2.2816\n",
      "Total loss:  -1.2082 | PDE Loss:  -2.0162 | Function Loss:  -2.2816\n",
      "Total loss:  -1.2082 | PDE Loss:  -2.0162 | Function Loss:  -2.2817\n",
      "Total loss:  -1.2082 | PDE Loss:  -2.0161 | Function Loss:  -2.2817\n",
      "Total loss:  -1.2082 | PDE Loss:  -2.0162 | Function Loss:  -2.2817\n",
      "Total loss:  -1.2083 | PDE Loss:  -2.0162 | Function Loss:  -2.2817\n",
      "Total loss:  -1.2083 | PDE Loss:  -2.0164 | Function Loss:  -2.2817\n",
      "Total loss:  -1.2083 | PDE Loss:  -2.0165 | Function Loss:  -2.2817\n",
      "Total loss:  -1.2083 | PDE Loss:  -2.0166 | Function Loss:  -2.2817\n",
      "Total loss:  -1.2083 | PDE Loss:  -2.0167 | Function Loss:  -2.2817\n",
      "Total loss:  -1.2083 | PDE Loss:  -2.0168 | Function Loss:  -2.2817\n",
      "Total loss:  -1.2083 | PDE Loss:  -2.0169 | Function Loss:  -2.2817\n",
      "Total loss:  -1.2084 | PDE Loss:  -2.0169 | Function Loss:  -2.2817\n",
      "Total loss:  -1.2084 | PDE Loss:  -2.017 | Function Loss:  -2.2817\n",
      "Total loss:  -1.2084 | PDE Loss:  -2.0169 | Function Loss:  -2.2818\n",
      "Total loss:  -1.2084 | PDE Loss:  -2.0168 | Function Loss:  -2.2818\n",
      "Total loss:  -1.2085 | PDE Loss:  -2.0166 | Function Loss:  -2.2819\n",
      "Total loss:  -1.2085 | PDE Loss:  -2.0165 | Function Loss:  -2.2819\n",
      "Total loss:  -1.2085 | PDE Loss:  -2.0164 | Function Loss:  -2.282\n",
      "Total loss:  -1.2084 | PDE Loss:  -2.0164 | Function Loss:  -2.2819\n",
      "Total loss:  -1.2085 | PDE Loss:  -2.0165 | Function Loss:  -2.282\n",
      "Total loss:  -1.2085 | PDE Loss:  -2.0165 | Function Loss:  -2.282\n",
      "Total loss:  -1.2085 | PDE Loss:  -2.0166 | Function Loss:  -2.282\n",
      "Total loss:  -1.2086 | PDE Loss:  -2.0167 | Function Loss:  -2.282\n",
      "Total loss:  -1.2086 | PDE Loss:  -2.0169 | Function Loss:  -2.282\n",
      "Total loss:  -1.2086 | PDE Loss:  -2.0171 | Function Loss:  -2.282\n",
      "Total loss:  -1.2086 | PDE Loss:  -2.0173 | Function Loss:  -2.282\n",
      "Total loss:  -1.2087 | PDE Loss:  -2.0174 | Function Loss:  -2.282\n",
      "Total loss:  -1.2087 | PDE Loss:  -2.0175 | Function Loss:  -2.282\n",
      "Total loss:  -1.2087 | PDE Loss:  -2.0175 | Function Loss:  -2.282\n",
      "Total loss:  -1.2087 | PDE Loss:  -2.0174 | Function Loss:  -2.2821\n",
      "Total loss:  -1.2087 | PDE Loss:  -2.0172 | Function Loss:  -2.2821\n",
      "Total loss:  -1.2088 | PDE Loss:  -2.017 | Function Loss:  -2.2822\n",
      "Total loss:  -1.2088 | PDE Loss:  -2.0168 | Function Loss:  -2.2822\n",
      "Total loss:  -1.2088 | PDE Loss:  -2.0166 | Function Loss:  -2.2823\n",
      "Total loss:  -1.2088 | PDE Loss:  -2.0164 | Function Loss:  -2.2823\n",
      "Total loss:  -1.2088 | PDE Loss:  -2.0162 | Function Loss:  -2.2824\n",
      "Total loss:  -1.2089 | PDE Loss:  -2.016 | Function Loss:  -2.2825\n",
      "Total loss:  -1.2089 | PDE Loss:  -2.016 | Function Loss:  -2.2825\n",
      "Total loss:  -1.2089 | PDE Loss:  -2.016 | Function Loss:  -2.2825\n",
      "Total loss:  -1.2089 | PDE Loss:  -2.0162 | Function Loss:  -2.2825\n",
      "Total loss:  -1.209 | PDE Loss:  -2.0165 | Function Loss:  -2.2825\n",
      "Total loss:  -1.2089 | PDE Loss:  -2.0167 | Function Loss:  -2.2824\n",
      "Total loss:  -1.209 | PDE Loss:  -2.0166 | Function Loss:  -2.2825\n",
      "Total loss:  -1.209 | PDE Loss:  -2.0169 | Function Loss:  -2.2825\n",
      "Total loss:  -1.209 | PDE Loss:  -2.0172 | Function Loss:  -2.2825\n",
      "Total loss:  -1.209 | PDE Loss:  -2.0174 | Function Loss:  -2.2824\n",
      "Total loss:  -1.2091 | PDE Loss:  -2.0175 | Function Loss:  -2.2824\n",
      "Total loss:  -1.2091 | PDE Loss:  -2.0176 | Function Loss:  -2.2825\n",
      "Total loss:  -1.2091 | PDE Loss:  -2.0175 | Function Loss:  -2.2825\n",
      "Total loss:  -1.2091 | PDE Loss:  -2.0174 | Function Loss:  -2.2825\n",
      "Total loss:  -1.2091 | PDE Loss:  -2.0172 | Function Loss:  -2.2826\n",
      "Total loss:  -1.2092 | PDE Loss:  -2.0169 | Function Loss:  -2.2827\n",
      "Total loss:  -1.2092 | PDE Loss:  -2.0167 | Function Loss:  -2.2827\n",
      "Total loss:  -1.2092 | PDE Loss:  -2.0163 | Function Loss:  -2.2828\n",
      "Total loss:  -1.2092 | PDE Loss:  -2.0161 | Function Loss:  -2.2829\n",
      "Total loss:  -1.2092 | PDE Loss:  -2.0162 | Function Loss:  -2.2829\n",
      "Total loss:  -1.2093 | PDE Loss:  -2.0164 | Function Loss:  -2.2829\n",
      "Total loss:  -1.2093 | PDE Loss:  -2.0165 | Function Loss:  -2.2829\n",
      "Total loss:  -1.2093 | PDE Loss:  -2.0167 | Function Loss:  -2.2829\n",
      "Total loss:  -1.2093 | PDE Loss:  -2.0167 | Function Loss:  -2.2829\n",
      "Total loss:  -1.2093 | PDE Loss:  -2.0169 | Function Loss:  -2.2829\n",
      "Total loss:  -1.2093 | PDE Loss:  -2.017 | Function Loss:  -2.2829\n",
      "Total loss:  -1.2094 | PDE Loss:  -2.0171 | Function Loss:  -2.2829\n",
      "Total loss:  -1.2094 | PDE Loss:  -2.0171 | Function Loss:  -2.2829\n",
      "Total loss:  -1.2094 | PDE Loss:  -2.0171 | Function Loss:  -2.2829\n",
      "Total loss:  -1.2094 | PDE Loss:  -2.0171 | Function Loss:  -2.2829\n",
      "Total loss:  -1.2094 | PDE Loss:  -2.0171 | Function Loss:  -2.2829\n",
      "Total loss:  -1.2095 | PDE Loss:  -2.0171 | Function Loss:  -2.283\n",
      "Total loss:  -1.2095 | PDE Loss:  -2.0171 | Function Loss:  -2.283\n",
      "Total loss:  -1.2095 | PDE Loss:  -2.0172 | Function Loss:  -2.2831\n",
      "Total loss:  -1.2096 | PDE Loss:  -2.0171 | Function Loss:  -2.2831\n",
      "Total loss:  -1.2096 | PDE Loss:  -2.0172 | Function Loss:  -2.2831\n",
      "Total loss:  -1.2096 | PDE Loss:  -2.0173 | Function Loss:  -2.2831\n",
      "Total loss:  -1.2096 | PDE Loss:  -2.0173 | Function Loss:  -2.2832\n",
      "Total loss:  -1.2097 | PDE Loss:  -2.0174 | Function Loss:  -2.2832\n",
      "Total loss:  -1.2097 | PDE Loss:  -2.0174 | Function Loss:  -2.2832\n",
      "Total loss:  -1.2097 | PDE Loss:  -2.0174 | Function Loss:  -2.2832\n",
      "Total loss:  -1.2096 | PDE Loss:  -2.0173 | Function Loss:  -2.2831\n",
      "Total loss:  -1.2097 | PDE Loss:  -2.0175 | Function Loss:  -2.2832\n",
      "Total loss:  -1.2097 | PDE Loss:  -2.0175 | Function Loss:  -2.2832\n",
      "Total loss:  -1.2097 | PDE Loss:  -2.0175 | Function Loss:  -2.2832\n",
      "Total loss:  -1.2098 | PDE Loss:  -2.0175 | Function Loss:  -2.2833\n",
      "Total loss:  -1.2098 | PDE Loss:  -2.0176 | Function Loss:  -2.2833\n",
      "Total loss:  -1.2098 | PDE Loss:  -2.0176 | Function Loss:  -2.2833\n",
      "Total loss:  -1.2098 | PDE Loss:  -2.0178 | Function Loss:  -2.2833\n",
      "Total loss:  -1.2099 | PDE Loss:  -2.018 | Function Loss:  -2.2833\n",
      "Total loss:  -1.2099 | PDE Loss:  -2.0182 | Function Loss:  -2.2833\n",
      "Total loss:  -1.2099 | PDE Loss:  -2.0184 | Function Loss:  -2.2832\n",
      "Total loss:  -1.2099 | PDE Loss:  -2.0188 | Function Loss:  -2.2832\n",
      "Total loss:  -1.2099 | PDE Loss:  -2.019 | Function Loss:  -2.2832\n",
      "Total loss:  -1.2099 | PDE Loss:  -2.0192 | Function Loss:  -2.2832\n",
      "Total loss:  -1.21 | PDE Loss:  -2.0196 | Function Loss:  -2.2831\n",
      "Total loss:  -1.21 | PDE Loss:  -2.0195 | Function Loss:  -2.2831\n",
      "Total loss:  -1.21 | PDE Loss:  -2.0196 | Function Loss:  -2.2831\n",
      "Total loss:  -1.21 | PDE Loss:  -2.0197 | Function Loss:  -2.2832\n",
      "Total loss:  -1.21 | PDE Loss:  -2.0198 | Function Loss:  -2.2832\n",
      "Total loss:  -1.2101 | PDE Loss:  -2.0198 | Function Loss:  -2.2832\n",
      "Total loss:  -1.2101 | PDE Loss:  -2.0199 | Function Loss:  -2.2832\n",
      "Total loss:  -1.2101 | PDE Loss:  -2.0199 | Function Loss:  -2.2832\n",
      "Total loss:  -1.2101 | PDE Loss:  -2.02 | Function Loss:  -2.2832\n",
      "Total loss:  -1.2101 | PDE Loss:  -2.02 | Function Loss:  -2.2832\n",
      "Total loss:  -1.2101 | PDE Loss:  -2.0201 | Function Loss:  -2.2832\n",
      "Total loss:  -1.2101 | PDE Loss:  -2.0202 | Function Loss:  -2.2832\n",
      "Total loss:  -1.2101 | PDE Loss:  -2.0203 | Function Loss:  -2.2832\n",
      "Total loss:  -1.2102 | PDE Loss:  -2.0204 | Function Loss:  -2.2832\n",
      "Total loss:  -1.2102 | PDE Loss:  -2.0205 | Function Loss:  -2.2832\n",
      "Total loss:  -1.2102 | PDE Loss:  -2.0206 | Function Loss:  -2.2832\n",
      "Total loss:  -1.2102 | PDE Loss:  -2.0206 | Function Loss:  -2.2832\n",
      "Total loss:  -1.2102 | PDE Loss:  -2.0208 | Function Loss:  -2.2832\n",
      "Total loss:  -1.2103 | PDE Loss:  -2.0207 | Function Loss:  -2.2833\n",
      "Total loss:  -1.2103 | PDE Loss:  -2.0207 | Function Loss:  -2.2833\n",
      "Total loss:  -1.2103 | PDE Loss:  -2.0207 | Function Loss:  -2.2833\n",
      "Total loss:  -1.2104 | PDE Loss:  -2.0208 | Function Loss:  -2.2834\n",
      "Total loss:  -1.2104 | PDE Loss:  -2.0209 | Function Loss:  -2.2834\n",
      "Total loss:  -1.2104 | PDE Loss:  -2.021 | Function Loss:  -2.2834\n",
      "Total loss:  -1.2104 | PDE Loss:  -2.0213 | Function Loss:  -2.2833\n",
      "Total loss:  -1.2105 | PDE Loss:  -2.0218 | Function Loss:  -2.2833\n",
      "Total loss:  -1.2105 | PDE Loss:  -2.0221 | Function Loss:  -2.2833\n",
      "Total loss:  -1.2105 | PDE Loss:  -2.0224 | Function Loss:  -2.2832\n",
      "Total loss:  -1.2105 | PDE Loss:  -2.0227 | Function Loss:  -2.2832\n",
      "Total loss:  -1.2106 | PDE Loss:  -2.0228 | Function Loss:  -2.2832\n",
      "Total loss:  -1.2106 | PDE Loss:  -2.0229 | Function Loss:  -2.2832\n",
      "Total loss:  -1.2106 | PDE Loss:  -2.0229 | Function Loss:  -2.2833\n",
      "Total loss:  -1.2106 | PDE Loss:  -2.0229 | Function Loss:  -2.2833\n",
      "Total loss:  -1.2106 | PDE Loss:  -2.0228 | Function Loss:  -2.2833\n",
      "Total loss:  -1.2107 | PDE Loss:  -2.0229 | Function Loss:  -2.2834\n",
      "Total loss:  -1.2107 | PDE Loss:  -2.0228 | Function Loss:  -2.2834\n",
      "Total loss:  -1.2107 | PDE Loss:  -2.0228 | Function Loss:  -2.2834\n",
      "Total loss:  -1.2108 | PDE Loss:  -2.0227 | Function Loss:  -2.2835\n",
      "Total loss:  -1.2108 | PDE Loss:  -2.0228 | Function Loss:  -2.2835\n",
      "Total loss:  -1.2108 | PDE Loss:  -2.0228 | Function Loss:  -2.2835\n",
      "Total loss:  -1.2108 | PDE Loss:  -2.023 | Function Loss:  -2.2835\n",
      "Total loss:  -1.2108 | PDE Loss:  -2.023 | Function Loss:  -2.2835\n",
      "Total loss:  -1.2109 | PDE Loss:  -2.0231 | Function Loss:  -2.2835\n",
      "Total loss:  -1.2109 | PDE Loss:  -2.0232 | Function Loss:  -2.2835\n",
      "Total loss:  -1.2109 | PDE Loss:  -2.0233 | Function Loss:  -2.2835\n",
      "Total loss:  -1.2109 | PDE Loss:  -2.0234 | Function Loss:  -2.2835\n",
      "Total loss:  -1.2109 | PDE Loss:  -2.0236 | Function Loss:  -2.2835\n",
      "Total loss:  -1.2109 | PDE Loss:  -2.0237 | Function Loss:  -2.2835\n",
      "Total loss:  -1.2109 | PDE Loss:  -2.024 | Function Loss:  -2.2835\n",
      "Total loss:  -1.211 | PDE Loss:  -2.0243 | Function Loss:  -2.2834\n",
      "Total loss:  -1.211 | PDE Loss:  -2.0245 | Function Loss:  -2.2834\n",
      "Total loss:  -1.211 | PDE Loss:  -2.0248 | Function Loss:  -2.2834\n",
      "Total loss:  -1.211 | PDE Loss:  -2.0249 | Function Loss:  -2.2834\n",
      "Total loss:  -1.2111 | PDE Loss:  -2.0252 | Function Loss:  -2.2834\n",
      "Total loss:  -1.2111 | PDE Loss:  -2.0252 | Function Loss:  -2.2834\n",
      "Total loss:  -1.2111 | PDE Loss:  -2.0253 | Function Loss:  -2.2834\n",
      "Total loss:  -1.2111 | PDE Loss:  -2.0253 | Function Loss:  -2.2834\n",
      "Total loss:  -1.2111 | PDE Loss:  -2.0253 | Function Loss:  -2.2834\n",
      "Total loss:  -1.2111 | PDE Loss:  -2.0252 | Function Loss:  -2.2835\n",
      "Total loss:  -1.2112 | PDE Loss:  -2.0251 | Function Loss:  -2.2835\n",
      "Total loss:  -1.2112 | PDE Loss:  -2.0251 | Function Loss:  -2.2835\n",
      "Total loss:  -1.2112 | PDE Loss:  -2.0251 | Function Loss:  -2.2835\n",
      "Total loss:  -1.2112 | PDE Loss:  -2.0251 | Function Loss:  -2.2836\n",
      "Total loss:  -1.2112 | PDE Loss:  -2.0251 | Function Loss:  -2.2836\n",
      "Total loss:  -1.2112 | PDE Loss:  -2.0248 | Function Loss:  -2.2836\n",
      "Total loss:  -1.2112 | PDE Loss:  -2.0251 | Function Loss:  -2.2836\n",
      "Total loss:  -1.2112 | PDE Loss:  -2.0251 | Function Loss:  -2.2836\n",
      "Total loss:  -1.2112 | PDE Loss:  -2.0252 | Function Loss:  -2.2836\n",
      "Total loss:  -1.2112 | PDE Loss:  -2.0253 | Function Loss:  -2.2836\n",
      "Total loss:  -1.2112 | PDE Loss:  -2.0254 | Function Loss:  -2.2836\n",
      "Total loss:  -1.2113 | PDE Loss:  -2.0254 | Function Loss:  -2.2836\n",
      "Total loss:  -1.2113 | PDE Loss:  -2.0254 | Function Loss:  -2.2836\n",
      "Total loss:  -1.2113 | PDE Loss:  -2.0255 | Function Loss:  -2.2836\n",
      "Total loss:  -1.2113 | PDE Loss:  -2.0255 | Function Loss:  -2.2836\n",
      "Total loss:  -1.2113 | PDE Loss:  -2.0254 | Function Loss:  -2.2836\n",
      "Total loss:  -1.2113 | PDE Loss:  -2.0252 | Function Loss:  -2.2837\n",
      "Total loss:  -1.2113 | PDE Loss:  -2.0251 | Function Loss:  -2.2837\n",
      "Total loss:  -1.2113 | PDE Loss:  -2.0249 | Function Loss:  -2.2838\n",
      "Total loss:  -1.2114 | PDE Loss:  -2.0247 | Function Loss:  -2.2838\n",
      "Total loss:  -1.2114 | PDE Loss:  -2.0246 | Function Loss:  -2.2839\n",
      "Total loss:  -1.2114 | PDE Loss:  -2.0245 | Function Loss:  -2.2839\n",
      "Total loss:  -1.2114 | PDE Loss:  -2.0245 | Function Loss:  -2.284\n",
      "Total loss:  -1.2113 | PDE Loss:  -2.0242 | Function Loss:  -2.2839\n",
      "Total loss:  -1.2114 | PDE Loss:  -2.0245 | Function Loss:  -2.284\n",
      "Total loss:  -1.2115 | PDE Loss:  -2.0244 | Function Loss:  -2.284\n",
      "Total loss:  -1.2115 | PDE Loss:  -2.0246 | Function Loss:  -2.284\n",
      "Total loss:  -1.2115 | PDE Loss:  -2.0246 | Function Loss:  -2.2841\n",
      "Total loss:  -1.2115 | PDE Loss:  -2.0246 | Function Loss:  -2.2841\n",
      "Total loss:  -1.2116 | PDE Loss:  -2.0247 | Function Loss:  -2.2841\n",
      "Total loss:  -1.2116 | PDE Loss:  -2.0248 | Function Loss:  -2.2841\n",
      "Total loss:  -1.2116 | PDE Loss:  -2.0248 | Function Loss:  -2.2841\n",
      "Total loss:  -1.2117 | PDE Loss:  -2.0248 | Function Loss:  -2.2842\n",
      "Total loss:  -1.2117 | PDE Loss:  -2.0248 | Function Loss:  -2.2842\n",
      "Total loss:  -1.2117 | PDE Loss:  -2.0248 | Function Loss:  -2.2842\n",
      "Total loss:  -1.2117 | PDE Loss:  -2.0248 | Function Loss:  -2.2843\n",
      "Total loss:  -1.2117 | PDE Loss:  -2.0248 | Function Loss:  -2.2843\n",
      "Total loss:  -1.2118 | PDE Loss:  -2.0249 | Function Loss:  -2.2843\n",
      "Total loss:  -1.2118 | PDE Loss:  -2.025 | Function Loss:  -2.2843\n",
      "Total loss:  -1.2118 | PDE Loss:  -2.0251 | Function Loss:  -2.2843\n",
      "Total loss:  -1.2118 | PDE Loss:  -2.0251 | Function Loss:  -2.2843\n",
      "Total loss:  -1.2118 | PDE Loss:  -2.0252 | Function Loss:  -2.2843\n",
      "Total loss:  -1.2118 | PDE Loss:  -2.0252 | Function Loss:  -2.2843\n",
      "Total loss:  -1.2118 | PDE Loss:  -2.0255 | Function Loss:  -2.2843\n",
      "Total loss:  -1.2119 | PDE Loss:  -2.0255 | Function Loss:  -2.2843\n",
      "Total loss:  -1.2119 | PDE Loss:  -2.0256 | Function Loss:  -2.2843\n",
      "Total loss:  -1.2119 | PDE Loss:  -2.0258 | Function Loss:  -2.2843\n",
      "Total loss:  -1.212 | PDE Loss:  -2.0259 | Function Loss:  -2.2843\n",
      "Total loss:  -1.212 | PDE Loss:  -2.0258 | Function Loss:  -2.2844\n",
      "Total loss:  -1.2119 | PDE Loss:  -2.0257 | Function Loss:  -2.2844\n",
      "Total loss:  -1.212 | PDE Loss:  -2.0258 | Function Loss:  -2.2844\n",
      "Total loss:  -1.212 | PDE Loss:  -2.0258 | Function Loss:  -2.2844\n",
      "Total loss:  -1.212 | PDE Loss:  -2.0257 | Function Loss:  -2.2844\n",
      "Total loss:  -1.212 | PDE Loss:  -2.0257 | Function Loss:  -2.2845\n",
      "Total loss:  -1.212 | PDE Loss:  -2.0256 | Function Loss:  -2.2845\n",
      "Total loss:  -1.2121 | PDE Loss:  -2.0256 | Function Loss:  -2.2845\n",
      "Total loss:  -1.2121 | PDE Loss:  -2.0256 | Function Loss:  -2.2845\n",
      "Total loss:  -1.2121 | PDE Loss:  -2.0256 | Function Loss:  -2.2845\n",
      "Total loss:  -1.2121 | PDE Loss:  -2.0256 | Function Loss:  -2.2845\n",
      "Total loss:  -1.2121 | PDE Loss:  -2.0256 | Function Loss:  -2.2846\n",
      "Total loss:  -1.2121 | PDE Loss:  -2.0256 | Function Loss:  -2.2846\n",
      "Total loss:  -1.2121 | PDE Loss:  -2.0256 | Function Loss:  -2.2846\n",
      "Total loss:  -1.2121 | PDE Loss:  -2.0256 | Function Loss:  -2.2846\n",
      "Total loss:  -1.2121 | PDE Loss:  -2.0255 | Function Loss:  -2.2846\n",
      "Total loss:  -1.2121 | PDE Loss:  -2.0255 | Function Loss:  -2.2846\n",
      "Total loss:  -1.2122 | PDE Loss:  -2.0254 | Function Loss:  -2.2846\n",
      "Total loss:  -1.2122 | PDE Loss:  -2.0249 | Function Loss:  -2.2847\n",
      "Total loss:  -1.2122 | PDE Loss:  -2.0253 | Function Loss:  -2.2847\n",
      "Total loss:  -1.2122 | PDE Loss:  -2.0252 | Function Loss:  -2.2847\n",
      "Total loss:  -1.2122 | PDE Loss:  -2.0253 | Function Loss:  -2.2847\n",
      "Total loss:  -1.2122 | PDE Loss:  -2.0253 | Function Loss:  -2.2847\n",
      "Total loss:  -1.2122 | PDE Loss:  -2.0253 | Function Loss:  -2.2847\n",
      "Total loss:  -1.2122 | PDE Loss:  -2.0254 | Function Loss:  -2.2847\n",
      "Total loss:  -1.2123 | PDE Loss:  -2.0255 | Function Loss:  -2.2847\n",
      "Total loss:  -1.2123 | PDE Loss:  -2.0257 | Function Loss:  -2.2847\n",
      "Total loss:  -1.2123 | PDE Loss:  -2.0258 | Function Loss:  -2.2847\n",
      "Total loss:  -1.2123 | PDE Loss:  -2.026 | Function Loss:  -2.2847\n",
      "Total loss:  -1.2122 | PDE Loss:  -2.0254 | Function Loss:  -2.2847\n",
      "Total loss:  -1.2123 | PDE Loss:  -2.026 | Function Loss:  -2.2847\n",
      "Total loss:  -1.2123 | PDE Loss:  -2.0261 | Function Loss:  -2.2847\n",
      "Total loss:  -1.2123 | PDE Loss:  -2.0262 | Function Loss:  -2.2847\n",
      "Total loss:  -1.2124 | PDE Loss:  -2.0263 | Function Loss:  -2.2847\n",
      "Total loss:  -1.2124 | PDE Loss:  -2.0263 | Function Loss:  -2.2847\n",
      "Total loss:  -1.2124 | PDE Loss:  -2.0263 | Function Loss:  -2.2847\n",
      "Total loss:  -1.2124 | PDE Loss:  -2.0264 | Function Loss:  -2.2848\n",
      "Total loss:  -1.2124 | PDE Loss:  -2.0265 | Function Loss:  -2.2847\n",
      "Total loss:  -1.2124 | PDE Loss:  -2.0265 | Function Loss:  -2.2847\n",
      "Total loss:  -1.2124 | PDE Loss:  -2.0265 | Function Loss:  -2.2848\n",
      "Total loss:  -1.2124 | PDE Loss:  -2.0265 | Function Loss:  -2.2848\n",
      "Total loss:  -1.2125 | PDE Loss:  -2.0266 | Function Loss:  -2.2848\n",
      "Total loss:  -1.2125 | PDE Loss:  -2.0267 | Function Loss:  -2.2848\n",
      "Total loss:  -1.2125 | PDE Loss:  -2.0267 | Function Loss:  -2.2848\n",
      "Total loss:  -1.2125 | PDE Loss:  -2.0268 | Function Loss:  -2.2848\n",
      "Total loss:  -1.2125 | PDE Loss:  -2.0269 | Function Loss:  -2.2848\n",
      "Total loss:  -1.2125 | PDE Loss:  -2.0269 | Function Loss:  -2.2848\n",
      "Total loss:  -1.2125 | PDE Loss:  -2.027 | Function Loss:  -2.2848\n",
      "Total loss:  -1.2126 | PDE Loss:  -2.0268 | Function Loss:  -2.2849\n",
      "Total loss:  -1.2126 | PDE Loss:  -2.0269 | Function Loss:  -2.2849\n",
      "Total loss:  -1.2126 | PDE Loss:  -2.0269 | Function Loss:  -2.2849\n",
      "Total loss:  -1.2126 | PDE Loss:  -2.0268 | Function Loss:  -2.2849\n",
      "Total loss:  -1.2126 | PDE Loss:  -2.0266 | Function Loss:  -2.285\n",
      "Total loss:  -1.2126 | PDE Loss:  -2.0265 | Function Loss:  -2.285\n",
      "Total loss:  -1.2127 | PDE Loss:  -2.0263 | Function Loss:  -2.2851\n",
      "Total loss:  -1.2127 | PDE Loss:  -2.0262 | Function Loss:  -2.2851\n",
      "Total loss:  -1.2127 | PDE Loss:  -2.0262 | Function Loss:  -2.2851\n",
      "Total loss:  -1.2127 | PDE Loss:  -2.0262 | Function Loss:  -2.2851\n",
      "Total loss:  -1.2127 | PDE Loss:  -2.0263 | Function Loss:  -2.2852\n",
      "Total loss:  -1.2127 | PDE Loss:  -2.0263 | Function Loss:  -2.2852\n",
      "Total loss:  -1.2127 | PDE Loss:  -2.0264 | Function Loss:  -2.2852\n",
      "Total loss:  -1.2128 | PDE Loss:  -2.0266 | Function Loss:  -2.2851\n",
      "Total loss:  -1.2128 | PDE Loss:  -2.0266 | Function Loss:  -2.2851\n",
      "Total loss:  -1.2128 | PDE Loss:  -2.0267 | Function Loss:  -2.2852\n",
      "Total loss:  -1.2128 | PDE Loss:  -2.0268 | Function Loss:  -2.2852\n",
      "Total loss:  -1.2128 | PDE Loss:  -2.0268 | Function Loss:  -2.2852\n",
      "Total loss:  -1.2129 | PDE Loss:  -2.0267 | Function Loss:  -2.2853\n",
      "Total loss:  -1.2129 | PDE Loss:  -2.0266 | Function Loss:  -2.2853\n",
      "Total loss:  -1.2129 | PDE Loss:  -2.0265 | Function Loss:  -2.2854\n",
      "Total loss:  -1.213 | PDE Loss:  -2.0264 | Function Loss:  -2.2854\n",
      "Total loss:  -1.213 | PDE Loss:  -2.0262 | Function Loss:  -2.2855\n",
      "Total loss:  -1.213 | PDE Loss:  -2.0262 | Function Loss:  -2.2855\n",
      "Total loss:  -1.213 | PDE Loss:  -2.0262 | Function Loss:  -2.2855\n",
      "Total loss:  -1.213 | PDE Loss:  -2.0262 | Function Loss:  -2.2855\n",
      "Total loss:  -1.213 | PDE Loss:  -2.0263 | Function Loss:  -2.2855\n",
      "Total loss:  -1.213 | PDE Loss:  -2.0263 | Function Loss:  -2.2855\n",
      "Total loss:  -1.2131 | PDE Loss:  -2.0263 | Function Loss:  -2.2856\n",
      "Total loss:  -1.2131 | PDE Loss:  -2.0263 | Function Loss:  -2.2856\n",
      "Total loss:  -1.2131 | PDE Loss:  -2.0259 | Function Loss:  -2.2856\n",
      "Total loss:  -1.2131 | PDE Loss:  -2.0262 | Function Loss:  -2.2856\n",
      "Total loss:  -1.2131 | PDE Loss:  -2.0262 | Function Loss:  -2.2856\n",
      "Total loss:  -1.2131 | PDE Loss:  -2.0262 | Function Loss:  -2.2857\n",
      "Total loss:  -1.2132 | PDE Loss:  -2.0262 | Function Loss:  -2.2857\n",
      "Total loss:  -1.2132 | PDE Loss:  -2.0263 | Function Loss:  -2.2857\n",
      "Total loss:  -1.2132 | PDE Loss:  -2.0263 | Function Loss:  -2.2857\n",
      "Total loss:  -1.2132 | PDE Loss:  -2.0264 | Function Loss:  -2.2857\n",
      "Total loss:  -1.2133 | PDE Loss:  -2.0265 | Function Loss:  -2.2857\n",
      "Total loss:  -1.2133 | PDE Loss:  -2.0267 | Function Loss:  -2.2857\n",
      "Total loss:  -1.2133 | PDE Loss:  -2.0268 | Function Loss:  -2.2857\n",
      "Total loss:  -1.2133 | PDE Loss:  -2.0273 | Function Loss:  -2.2857\n",
      "Total loss:  -1.2134 | PDE Loss:  -2.0274 | Function Loss:  -2.2857\n",
      "Total loss:  -1.2134 | PDE Loss:  -2.0276 | Function Loss:  -2.2857\n",
      "Total loss:  -1.2134 | PDE Loss:  -2.0277 | Function Loss:  -2.2857\n",
      "Total loss:  -1.2135 | PDE Loss:  -2.0278 | Function Loss:  -2.2858\n",
      "Total loss:  -1.2135 | PDE Loss:  -2.0277 | Function Loss:  -2.2858\n",
      "Total loss:  -1.2135 | PDE Loss:  -2.0278 | Function Loss:  -2.2858\n",
      "Total loss:  -1.2135 | PDE Loss:  -2.0278 | Function Loss:  -2.2858\n",
      "Total loss:  -1.2135 | PDE Loss:  -2.0277 | Function Loss:  -2.2858\n",
      "Total loss:  -1.2135 | PDE Loss:  -2.0278 | Function Loss:  -2.2858\n",
      "Total loss:  -1.2135 | PDE Loss:  -2.0278 | Function Loss:  -2.2858\n",
      "Total loss:  -1.2135 | PDE Loss:  -2.0278 | Function Loss:  -2.2859\n",
      "Total loss:  -1.2136 | PDE Loss:  -2.0279 | Function Loss:  -2.2859\n",
      "Total loss:  -1.2136 | PDE Loss:  -2.028 | Function Loss:  -2.2859\n",
      "Total loss:  -1.2136 | PDE Loss:  -2.0283 | Function Loss:  -2.2858\n",
      "Total loss:  -1.2136 | PDE Loss:  -2.0284 | Function Loss:  -2.2858\n",
      "Total loss:  -1.2136 | PDE Loss:  -2.0285 | Function Loss:  -2.2858\n",
      "Total loss:  -1.2137 | PDE Loss:  -2.0287 | Function Loss:  -2.2858\n",
      "Total loss:  -1.2137 | PDE Loss:  -2.0289 | Function Loss:  -2.2858\n",
      "Total loss:  -1.2137 | PDE Loss:  -2.0289 | Function Loss:  -2.2858\n",
      "Total loss:  -1.2137 | PDE Loss:  -2.029 | Function Loss:  -2.2858\n",
      "Total loss:  -1.2137 | PDE Loss:  -2.0294 | Function Loss:  -2.2858\n",
      "Total loss:  -1.2137 | PDE Loss:  -2.0293 | Function Loss:  -2.2858\n",
      "Total loss:  -1.2137 | PDE Loss:  -2.0292 | Function Loss:  -2.2858\n",
      "Total loss:  -1.2137 | PDE Loss:  -2.0292 | Function Loss:  -2.2858\n",
      "Total loss:  -1.2138 | PDE Loss:  -2.0291 | Function Loss:  -2.2859\n",
      "Total loss:  -1.2138 | PDE Loss:  -2.0291 | Function Loss:  -2.2859\n",
      "Total loss:  -1.2138 | PDE Loss:  -2.0291 | Function Loss:  -2.2859\n",
      "Total loss:  -1.2138 | PDE Loss:  -2.0291 | Function Loss:  -2.2859\n",
      "Total loss:  -1.2138 | PDE Loss:  -2.0292 | Function Loss:  -2.2859\n",
      "Total loss:  -1.2138 | PDE Loss:  -2.0292 | Function Loss:  -2.2859\n",
      "Total loss:  -1.2138 | PDE Loss:  -2.0292 | Function Loss:  -2.2859\n",
      "Total loss:  -1.2138 | PDE Loss:  -2.0292 | Function Loss:  -2.2859\n",
      "Total loss:  -1.2138 | PDE Loss:  -2.0292 | Function Loss:  -2.2859\n",
      "Total loss:  -1.2138 | PDE Loss:  -2.0293 | Function Loss:  -2.2859\n",
      "Total loss:  -1.2138 | PDE Loss:  -2.0293 | Function Loss:  -2.2859\n",
      "Total loss:  -1.2139 | PDE Loss:  -2.0294 | Function Loss:  -2.2859\n",
      "Total loss:  -1.2139 | PDE Loss:  -2.0293 | Function Loss:  -2.286\n",
      "Total loss:  -1.2139 | PDE Loss:  -2.0294 | Function Loss:  -2.286\n",
      "Total loss:  -1.2139 | PDE Loss:  -2.0295 | Function Loss:  -2.286\n",
      "Total loss:  -1.2139 | PDE Loss:  -2.0296 | Function Loss:  -2.286\n",
      "Total loss:  -1.2139 | PDE Loss:  -2.0296 | Function Loss:  -2.286\n",
      "Total loss:  -1.2139 | PDE Loss:  -2.0297 | Function Loss:  -2.286\n",
      "Total loss:  -1.2139 | PDE Loss:  -2.0297 | Function Loss:  -2.286\n",
      "Total loss:  -1.2139 | PDE Loss:  -2.0296 | Function Loss:  -2.286\n",
      "Final Loss:  -1.2139256000518799\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x164e895b0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 960x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hist = pinn.fit(num_epochs=n_epochs,\n",
    "                optimizer=optimizer_LBFGS,\n",
    "                verbose=True)\n",
    "\n",
    "plt.figure(dpi=150)\n",
    "plt.grid(True, which=\"both\", ls=\":\")\n",
    "plt.plot(np.arange(1, len(hist) + 1), hist, label=\"Train Loss\")\n",
    "plt.xscale(\"log\")\n",
    "plt.legend()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(pinn.approximate_solution.state_dict(), 'approximate_solution_parameters.pth')\n",
    "torch.save(pinn.approximate_coefficient.state_dict(), 'approximate_coefficient_parameters.pth')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporal Boundary Conditions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The initial conditions for the fluid temperature are:\n",
    "\n",
    "$$\n",
    "T_f(x, t=0) = T_0 \\quad \\forall x \\in [0, 1]\n",
    "$$\n",
    "\n",
    "Where for us: $\\quad T_0 = 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "Parametes of the error for the Temporal Boundary Conditions\n",
      "------------------------------------------------------------\n",
      "mean of the relative error:\t 0.1246023178100586 %\n",
      "mean error:\t\t\t 0.0012460232246667147\n",
      "std error:\t\t\t 0.0015384427970275283\n"
     ]
    }
   ],
   "source": [
    "# Create the input points where to evaluate the predioctions\n",
    "input_test_tb = pinn.soboleng.draw(10)\n",
    "input_test_tb[:, 0] = torch.full(input_test_tb[:, 0].shape, 0)    # assigne t=0 to all points\n",
    "\n",
    "Tf = pinn.approximate_solution(input_test_tb)\n",
    "exact_solution = torch.full(Tf.shape, pinn.T_0)\n",
    "\n",
    "print('------------------------------------------------------------')\n",
    "print('Parametes of the error for the Temporal Boundary Conditions')\n",
    "print('------------------------------------------------------------')\n",
    "print('mean of the relative error:\\t', torch.mean(torch.abs((exact_solution-Tf))/torch.abs(exact_solution)*100).item(), '%' )\n",
    "print('mean error:\\t\\t\\t', torch.mean(torch.abs( (exact_solution-Tf)) ).item())\n",
    "print('std error:\\t\\t\\t', torch.std( (exact_solution-Tf) ).item())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spatial Boundary Conditions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Charing Phase"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The boundary conditions for the Charing Phase are:\n",
    "\n",
    "$$\n",
    "T_f(x=0, t)=T_{hot}, \\qquad \\frac{\\partial{T_f(x,t)}}{\\partial{x}}\\Bigr|_{x=1} = 0, \\quad \\forall t \\in [0,1]\\vee[4,5]\n",
    "$$\n",
    "\n",
    "Where for us: $\\quad T_{hot}=4$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "Parametes of the error for the Spatial Boundary Conditions -> Charging Phase\n",
      "---------------------------------------------------------------------------------\n",
      "Cycle 1\t\t\t\t|\tx=x0\t\t|\tx=xL\t\t|\n",
      "mean of the relative error:\t|\t 0.3504 %\t|\t inf %\t|\n",
      "\n",
      "Cycle 2\t\t\t\t|\tx=x0\t\t|\tx=xL\t\t|\n",
      "mean of the relative error:\t|\t 0.2515 %\t|\t inf %\t|\n"
     ]
    }
   ],
   "source": [
    "x0 = pinn.domain_extrema[1, 0]\n",
    "xL = pinn.domain_extrema[1, 1]\n",
    "\n",
    "# Create the input points where to evaluate the predioctions\n",
    "input_test_sb_charing = pinn.soboleng.draw(10)\n",
    "delta_time = torch.zeros_like(input_test_sb_charing)\n",
    "\n",
    "# Exact solution\n",
    "exact_solution_x0 = torch.full(delta_time[:,0].shape, pinn.T_hot)\n",
    "exact_solution_xL = torch.full(delta_time[:,0].shape, 0)\n",
    "\n",
    "\"\"\"Cycle 1\"\"\"\n",
    "delta_time[:, 0] = torch.full(delta_time[:, 0].shape, 0)\n",
    "\n",
    "input_test_sb_charing_x0_1 = torch.clone(input_test_sb_charing)\n",
    "input_test_sb_charing_x0_1[:, 1] = torch.full(input_test_sb_charing[:, 1].shape, x0)    # assigne x=x0 to all points\n",
    "input_test_sb_charing_x0_1 = input_test_sb_charing_x0_1 + delta_time    # add the delta_time\n",
    "\n",
    "input_test_sb_charing_xL_1 = torch.clone(input_test_sb_charing)\n",
    "input_test_sb_charing_xL_1[:, 1] = torch.full(input_test_sb_charing[:, 1].shape, xL)    # assigne x=x0 to all points\n",
    "input_test_sb_charing_xL_1 = input_test_sb_charing_xL_1 + delta_time    # add the delta_time\n",
    "\n",
    "# Make predictions\n",
    "Tf_x0_1 = pinn.approximate_solution(input_test_sb_charing_x0_1)\n",
    "\n",
    "input_test_sb_charing_xL_1.requires_grad = True\n",
    "Tf_xL_1 = pinn.approximate_solution(input_test_sb_charing_xL_1)\n",
    "dTf_xL_1 = torch.autograd.grad(Tf_xL_1.sum(), input_test_sb_charing_xL_1)[0][:, 1] # take dx\n",
    "\n",
    "# Compute the relative error\n",
    "rel_err_x0_1 = torch.mean(torch.abs((exact_solution_x0-Tf_x0_1))/torch.abs(exact_solution_x0)*100)\n",
    "rel_err_xL_1 = torch.mean(torch.abs((exact_solution_xL-dTf_xL_1))/torch.abs(exact_solution_xL)*100)\n",
    "\n",
    "\"\"\"Cycle 2\"\"\"\n",
    "delta_time[:, 0] = torch.full(delta_time[:, 0].shape, 4)\n",
    "\n",
    "input_test_sb_charing_x0_2 = torch.clone(input_test_sb_charing)\n",
    "input_test_sb_charing_x0_2[:, 1] = torch.full(input_test_sb_charing[:, 1].shape, x0)    # assigne x=x0 to all points\n",
    "input_test_sb_charing_x0_2 = input_test_sb_charing_x0_2 + delta_time    # add the delta_time\n",
    "\n",
    "input_test_sb_charing_xL_2 = torch.clone(input_test_sb_charing)\n",
    "input_test_sb_charing_xL_2[:, 1] = torch.full(input_test_sb_charing[:, 1].shape, xL)    # assigne x=x0 to all points\n",
    "input_test_sb_charing_xL_2 = input_test_sb_charing_xL_2 + delta_time    # add the delta_time\n",
    "\n",
    "# Make predictions\n",
    "Tf_x0_2 = pinn.approximate_solution(input_test_sb_charing_x0_2)\n",
    "\n",
    "input_test_sb_charing_xL_2.requires_grad = True\n",
    "Tf_xL_2 = pinn.approximate_solution(input_test_sb_charing_xL_2)\n",
    "dTf_xL_2 = torch.autograd.grad(Tf_xL_2.sum(), input_test_sb_charing_xL_2)[0][:, 1] # take dx\n",
    "\n",
    "# Compute the relative error\n",
    "rel_err_x0_2 = torch.mean(torch.abs((exact_solution_x0-Tf_x0_2))/torch.abs(exact_solution_x0)*100)\n",
    "rel_err_xL_2 = torch.mean(torch.abs((exact_solution_xL-dTf_xL_2))/torch.abs(exact_solution_xL)*100)\n",
    "\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('Parametes of the error for the Spatial Boundary Conditions -> Charging Phase')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('Cycle 1\\t\\t\\t\\t|\\tx=x0\\t\\t|\\tx=xL\\t\\t|')\n",
    "print('mean of the relative error:\\t|\\t', round(rel_err_x0_1.item(), 4) , '%\\t|\\t', round(rel_err_xL_1.item(), 4) , '%\\t|' )\n",
    "print()\n",
    "print('Cycle 2\\t\\t\\t\\t|\\tx=x0\\t\\t|\\tx=xL\\t\\t|')\n",
    "print('mean of the relative error:\\t|\\t', round(rel_err_x0_2.item(), 4) , '%\\t|\\t', round(rel_err_xL_2.item(), 4) , '%\\t|' )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discharging Phase"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The boundary conditions for the Discharging Phase are:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial{T_f(x,t)}}{\\partial{x}}\\Bigr|_{x=0} = 0, \\qquad T_f(x=1, t)=T_{0}, \\quad \\forall t \\in [2,3]\\vee[6,7]\n",
    "$$\n",
    "\n",
    "Where for us: $\\quad T_{0}=0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "Parametes of the error for the Spatial Boundary Conditions -> Discharging Phase\n",
      "---------------------------------------------------------------------------------\n",
      "Cycle 1\t\t\t\t|\tx=x0\t\t|\tx=xL\t\t|\n",
      "mean of the relative error:\t|\t inf %\t|\t 0.9034 %\t|\n",
      "\n",
      "Cycle 2\t\t\t\t|\tx=x0\t\t|\tx=xL\t\t|\n",
      "mean of the relative error:\t|\t inf %\t|\t 1.4785 %\t|\n"
     ]
    }
   ],
   "source": [
    "x0 = pinn.domain_extrema[1, 0]\n",
    "xL = pinn.domain_extrema[1, 1]\n",
    "\n",
    "# Create the input points where to evaluate the predioctions\n",
    "input_test_sb_discharging = pinn.soboleng.draw(10)\n",
    "delta_time = torch.zeros_like(input_test_sb_discharging)\n",
    "\n",
    "# Exact solution\n",
    "exact_solution_x0 = torch.full(delta_time[:,0].shape, 0)\n",
    "exact_solution_xL = torch.full(delta_time[:,0].shape, pinn.T_0)\n",
    "\n",
    "\"\"\"Cycle 1\"\"\"\n",
    "delta_time[:, 0] = torch.full(delta_time[:, 0].shape, 2)\n",
    "\n",
    "input_test_sb_discharging_x0_1 = torch.clone(input_test_sb_discharging)\n",
    "input_test_sb_discharging_x0_1[:, 1] = torch.full(input_test_sb_discharging[:, 1].shape, x0)    # assigne x=x0 to all points\n",
    "input_test_sb_discharging_x0_1 = input_test_sb_discharging_x0_1 + delta_time    # add the delta_time\n",
    "\n",
    "input_test_sb_discharging_xL_1 = torch.clone(input_test_sb_discharging)\n",
    "input_test_sb_discharging_xL_1[:, 1] = torch.full(input_test_sb_discharging[:, 1].shape, xL)    # assigne x=x0 to all points\n",
    "input_test_sb_discharging_xL_1 = input_test_sb_discharging_xL_1 + delta_time    # add the delta_time\n",
    "\n",
    "# Make predictions\n",
    "\n",
    "input_test_sb_discharging_x0_1.requires_grad = True\n",
    "Tf_x0_1 = pinn.approximate_solution(input_test_sb_discharging_x0_1)\n",
    "dTf_x0_1 = torch.autograd.grad(Tf_x0_1.sum(), input_test_sb_discharging_x0_1)[0][:, 1] # take dx\n",
    "\n",
    "Tf_xL_1 = pinn.approximate_solution(input_test_sb_discharging_xL_1)\n",
    "\n",
    "# Compute the relative error\n",
    "rel_err_x0_1 = torch.mean(torch.abs((exact_solution_x0-dTf_x0_1))/torch.abs(exact_solution_x0)*100)\n",
    "rel_err_xL_1 = torch.mean(torch.abs((exact_solution_xL-Tf_xL_1))/torch.abs(exact_solution_xL)*100)\n",
    "\n",
    "\"\"\"Cycle 2\"\"\"\n",
    "delta_time[:, 0] = torch.full(delta_time[:, 0].shape, 6)\n",
    "\n",
    "input_test_sb_discharging_x0_2 = torch.clone(input_test_sb_discharging)\n",
    "input_test_sb_discharging_x0_2[:, 1] = torch.full(input_test_sb_discharging[:, 1].shape, x0)    # assigne x=x0 to all points\n",
    "input_test_sb_discharging_x0_2 = input_test_sb_discharging_x0_2 + delta_time    # add the delta_time\n",
    "\n",
    "input_test_sb_discharging_xL_2 = torch.clone(input_test_sb_discharging)\n",
    "input_test_sb_discharging_xL_2[:, 1] = torch.full(input_test_sb_discharging[:, 1].shape, xL)    # assigne x=x0 to all points\n",
    "input_test_sb_discharging_xL_2 = input_test_sb_discharging_xL_2 + delta_time    # add the delta_time\n",
    "\n",
    "# Make predictions\n",
    "input_test_sb_discharging_x0_2.requires_grad = True\n",
    "Tf_x0_2 = pinn.approximate_solution(input_test_sb_discharging_x0_2)\n",
    "dTf_x0_2 = torch.autograd.grad(Tf_x0_2.sum(), input_test_sb_discharging_x0_2)[0][:, 1] # take dx\n",
    "\n",
    "\n",
    "Tf_xL_2 = pinn.approximate_solution(input_test_sb_discharging_xL_2)\n",
    "\n",
    "# Compute the relative error\n",
    "rel_err_x0_2 = torch.mean(torch.abs((exact_solution_x0-dTf_x0_2))/torch.abs(exact_solution_x0)*100)\n",
    "rel_err_xL_2 = torch.mean(torch.abs((exact_solution_xL-Tf_xL_2))/torch.abs(exact_solution_xL)*100)\n",
    "\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('Parametes of the error for the Spatial Boundary Conditions -> Discharging Phase')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('Cycle 1\\t\\t\\t\\t|\\tx=x0\\t\\t|\\tx=xL\\t\\t|')\n",
    "print('mean of the relative error:\\t|\\t', round(rel_err_x0_1.item(), 4) , '%\\t|\\t', round(rel_err_xL_1.item(), 4) , '%\\t|' )\n",
    "print()\n",
    "print('Cycle 2\\t\\t\\t\\t|\\tx=x0\\t\\t|\\tx=xL\\t\\t|')\n",
    "print('mean of the relative error:\\t|\\t', round(rel_err_x0_2.item(), 4) , '%\\t|\\t', round(rel_err_xL_2.item(), 4) , '%\\t|' )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Idle Phase"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The boundary conditions for the Idle Phase are:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial{T_f(x,t)}}{\\partial{x}}\\Bigr|_{x=0} = 0, \\qquad \\frac{\\partial{T_f(x,t)}}{\\partial{x}}\\Bigr|_{x=1} = 0, \\quad \\forall t \\in [1,2]\\vee[3,4]\\vee[5,6]\\vee[7,8]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "Parametes of the error for the Spatial Boundary Conditions -> Idle Phase\n",
      "---------------------------------------------------------------------------------\n",
      "Cycle 1\t\t\t\t|\tx=x0\t\t|\tx=xL\t\t|\n",
      "mean of the relative error:\t|\t inf %\t|\t inf %\t|\n",
      "\n",
      "Cycle 2\t\t\t\t|\tx=x0\t\t|\tx=xL\t\t|\n",
      "mean of the relative error:\t|\t inf %\t|\t inf %\t|\n",
      "\n",
      "Cycle 3\t\t\t\t|\tx=x0\t\t|\tx=xL\t\t|\n",
      "mean of the relative error:\t|\t inf %\t|\t inf %\t|\n",
      "\n",
      "Cycle 3\t\t\t\t|\tx=x0\t\t|\tx=xL\t\t|\n",
      "mean of the relative error:\t|\t inf %\t|\t inf %\t|\n"
     ]
    }
   ],
   "source": [
    "x0 = pinn.domain_extrema[1, 0]\n",
    "xL = pinn.domain_extrema[1, 1]\n",
    "\n",
    "# Create the input points where to evaluate the predioctions\n",
    "input_test_sb_idle = pinn.soboleng.draw(10)\n",
    "delta_time = torch.zeros_like(input_test_sb_idle)\n",
    "\n",
    "# Exact solution\n",
    "exact_solution_x0 = torch.full(delta_time[:,0].shape, 0)\n",
    "exact_solution_xL = torch.full(delta_time[:,0].shape, 0)\n",
    "\n",
    "\"\"\"Cycle 1\"\"\"\n",
    "delta_time[:, 0] = torch.full(delta_time[:, 0].shape, 1)\n",
    "\n",
    "input_test_sb_idle_x0_1 = torch.clone(input_test_sb_idle)\n",
    "input_test_sb_idle_x0_1[:, 1] = torch.full(input_test_sb_idle[:, 1].shape, x0)    # assigne x=x0 to all points\n",
    "input_test_sb_idle_x0_1 = input_test_sb_idle_x0_1 + delta_time    # add the delta_time\n",
    "\n",
    "input_test_sb_idle_xL_1 = torch.clone(input_test_sb_idle)\n",
    "input_test_sb_idle_xL_1[:, 1] = torch.full(input_test_sb_idle[:, 1].shape, xL)    # assigne x=x0 to all points\n",
    "input_test_sb_idle_xL_1 = input_test_sb_idle_xL_1 + delta_time    # add the delta_time\n",
    "\n",
    "# Make predictions\n",
    "\n",
    "input_test_sb_idle_x0_1.requires_grad = True\n",
    "Tf_x0_1 = pinn.approximate_solution(input_test_sb_idle_x0_1)\n",
    "dTf_x0_1 = torch.autograd.grad(Tf_x0_1.sum(), input_test_sb_idle_x0_1)[0][:, 1] # take dx\n",
    "\n",
    "input_test_sb_idle_xL_1.requires_grad = True\n",
    "Tf_xL_1 = pinn.approximate_solution(input_test_sb_idle_xL_1)\n",
    "dTf_xL_1 = torch.autograd.grad(Tf_xL_1.sum(), input_test_sb_idle_xL_1)[0][:, 1] # take dx\n",
    "\n",
    "# Compute the relative error\n",
    "rel_err_x0_1 = torch.mean(torch.abs((exact_solution_x0-dTf_x0_1))/torch.abs(exact_solution_x0)*100)\n",
    "rel_err_xL_1 = torch.mean(torch.abs((exact_solution_xL-dTf_xL_1))/torch.abs(exact_solution_xL)*100)\n",
    "\n",
    "\"\"\"Cycle 2\"\"\"\n",
    "delta_time[:, 0] = torch.full(delta_time[:, 0].shape, 3)\n",
    "\n",
    "input_test_sb_idle_x0_2 = torch.clone(input_test_sb_idle)\n",
    "input_test_sb_idle_x0_2[:, 1] = torch.full(input_test_sb_idle[:, 1].shape, x0)    # assigne x=x0 to all points\n",
    "input_test_sb_idle_x0_2 = input_test_sb_idle_x0_2 + delta_time    # add the delta_time\n",
    "\n",
    "input_test_sb_idle_xL_2 = torch.clone(input_test_sb_idle)\n",
    "input_test_sb_idle_xL_2[:, 1] = torch.full(input_test_sb_idle[:, 1].shape, xL)    # assigne x=x0 to all points\n",
    "input_test_sb_idle_xL_2 = input_test_sb_idle_xL_2 + delta_time    # add the delta_time\n",
    "\n",
    "# Make predictions\n",
    "input_test_sb_idle_x0_2.requires_grad = True\n",
    "Tf_x0_2 = pinn.approximate_solution(input_test_sb_idle_x0_2)\n",
    "dTf_x0_2 = torch.autograd.grad(Tf_x0_2.sum(), input_test_sb_idle_x0_2)[0][:, 1] # take dx\n",
    "\n",
    "input_test_sb_idle_xL_2.requires_grad = True\n",
    "Tf_xL_2 = pinn.approximate_solution(input_test_sb_idle_xL_2)\n",
    "dTf_xL_2 = torch.autograd.grad(Tf_xL_2.sum(), input_test_sb_idle_xL_2)[0][:, 1] # take dx\n",
    "\n",
    "# Compute the relative error\n",
    "rel_err_x0_2 = torch.mean(torch.abs((exact_solution_x0-dTf_x0_2))/torch.abs(exact_solution_x0)*100)\n",
    "rel_err_xL_2 = torch.mean(torch.abs((exact_solution_xL-dTf_xL_2))/torch.abs(exact_solution_xL)*100)\n",
    "\n",
    "\"\"\"Cycle 3\"\"\"\n",
    "delta_time[:, 0] = torch.full(delta_time[:, 0].shape, 5)\n",
    "\n",
    "input_test_sb_idle_x0_3 = torch.clone(input_test_sb_idle)\n",
    "input_test_sb_idle_x0_3[:, 1] = torch.full(input_test_sb_idle[:, 1].shape, x0)    # assigne x=x0 to all points\n",
    "input_test_sb_idle_x0_3 = input_test_sb_idle_x0_3 + delta_time    # add the delta_time\n",
    "\n",
    "input_test_sb_idle_xL_3 = torch.clone(input_test_sb_idle)\n",
    "input_test_sb_idle_xL_3[:, 1] = torch.full(input_test_sb_idle[:, 1].shape, xL)    # assigne x=x0 to all points\n",
    "input_test_sb_idle_xL_3 = input_test_sb_idle_xL_3 + delta_time    # add the delta_time\n",
    "\n",
    "# Make predictions\n",
    "input_test_sb_idle_x0_3.requires_grad = True\n",
    "Tf_x0_3 = pinn.approximate_solution(input_test_sb_idle_x0_3)\n",
    "dTf_x0_3 = torch.autograd.grad(Tf_x0_3.sum(), input_test_sb_idle_x0_3)[0][:, 1] # take dx\n",
    "\n",
    "input_test_sb_idle_xL_3.requires_grad = True\n",
    "Tf_xL_3 = pinn.approximate_solution(input_test_sb_idle_xL_3)\n",
    "dTf_xL_3 = torch.autograd.grad(Tf_xL_3.sum(), input_test_sb_idle_xL_3)[0][:, 1] # take dx\n",
    "\n",
    "# Compute the relative error\n",
    "rel_err_x0_3 = torch.mean(torch.abs((exact_solution_x0-dTf_x0_3))/torch.abs(exact_solution_x0)*100)\n",
    "rel_err_xL_3 = torch.mean(torch.abs((exact_solution_xL-dTf_xL_3))/torch.abs(exact_solution_xL)*100)\n",
    "\n",
    "\"\"\"Cycle 4\"\"\"\n",
    "delta_time[:, 0] = torch.full(delta_time[:, 0].shape, 7)\n",
    "\n",
    "input_test_sb_idle_x0_4 = torch.clone(input_test_sb_idle)\n",
    "input_test_sb_idle_x0_4[:, 1] = torch.full(input_test_sb_idle[:, 1].shape, x0)    # assigne x=x0 to all points\n",
    "input_test_sb_idle_x0_4 = input_test_sb_idle_x0_4 + delta_time    # add the delta_time\n",
    "\n",
    "input_test_sb_idle_xL_4 = torch.clone(input_test_sb_idle)\n",
    "input_test_sb_idle_xL_4[:, 1] = torch.full(input_test_sb_idle[:, 1].shape, xL)    # assigne x=x0 to all points\n",
    "input_test_sb_idle_xL_4 = input_test_sb_idle_xL_4 + delta_time    # add the delta_time\n",
    "\n",
    "# Make predictions\n",
    "input_test_sb_idle_x0_4.requires_grad = True\n",
    "Tf_x0_4 = pinn.approximate_solution(input_test_sb_idle_x0_4)\n",
    "dTf_x0_4 = torch.autograd.grad(Tf_x0_4.sum(), input_test_sb_idle_x0_4)[0][:, 1] # take dx\n",
    "\n",
    "input_test_sb_idle_xL_4.requires_grad = True\n",
    "Tf_xL_4 = pinn.approximate_solution(input_test_sb_idle_xL_4)\n",
    "dTf_xL_4 = torch.autograd.grad(Tf_xL_4.sum(), input_test_sb_idle_xL_4)[0][:, 1] # take dx\n",
    "\n",
    "# Compute the relative error\n",
    "rel_err_x0_4 = torch.mean(torch.abs((exact_solution_x0-dTf_x0_4))/torch.abs(exact_solution_x0)*100)\n",
    "rel_err_xL_4 = torch.mean(torch.abs((exact_solution_xL-dTf_xL_4))/torch.abs(exact_solution_xL)*100)\n",
    "\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('Parametes of the error for the Spatial Boundary Conditions -> Idle Phase')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('Cycle 1\\t\\t\\t\\t|\\tx=x0\\t\\t|\\tx=xL\\t\\t|')\n",
    "print('mean of the relative error:\\t|\\t', round(rel_err_x0_1.item(), 4) , '%\\t|\\t', round(rel_err_xL_1.item(), 4) , '%\\t|' )\n",
    "print()\n",
    "print('Cycle 2\\t\\t\\t\\t|\\tx=x0\\t\\t|\\tx=xL\\t\\t|')\n",
    "print('mean of the relative error:\\t|\\t', round(rel_err_x0_2.item(), 4) , '%\\t|\\t', round(rel_err_xL_2.item(), 4) , '%\\t|' )\n",
    "print()\n",
    "print('Cycle 3\\t\\t\\t\\t|\\tx=x0\\t\\t|\\tx=xL\\t\\t|')\n",
    "print('mean of the relative error:\\t|\\t', round(rel_err_x0_3.item(), 4) , '%\\t|\\t', round(rel_err_xL_3.item(), 4) , '%\\t|' )\n",
    "print()\n",
    "print('Cycle 3\\t\\t\\t\\t|\\tx=x0\\t\\t|\\tx=xL\\t\\t|')\n",
    "print('mean of the relative error:\\t|\\t', round(rel_err_x0_4.item(), 4) , '%\\t|\\t', round(rel_err_xL_4.item(), 4) , '%\\t|' )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measured data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------\n",
      "Parametes of the error for the Measured Data\n",
      "----------------------------------------------------\n",
      "mean of the relative error:\t 4.4561333656311035 %\n",
      "mean error:\t\t\t 0.047417815774679184\n",
      "std error:\t\t\t 0.06892482936382294\n"
     ]
    }
   ],
   "source": [
    "input_meas, exact_meas = pinn.get_measurement_data()\n",
    "\n",
    "prediction_meas = pinn.approximate_solution(input_meas)\n",
    "\n",
    "print('----------------------------------------------------')\n",
    "print('Parametes of the error for the Measured Data')\n",
    "print('----------------------------------------------------')\n",
    "print('mean of the relative error:\\t', torch.mean(torch.abs((exact_meas-prediction_meas))/torch.abs(exact_meas)*100).item(), '%' )\n",
    "print('mean error:\\t\\t\\t', torch.mean(torch.abs( (exact_meas-prediction_meas)) ).item())\n",
    "print('std error:\\t\\t\\t', torch.std( (exact_meas-prediction_meas) ).item())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fluid velocity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate some points for the time in [0, 8]\n",
    "input_time = torch.linspace(0, 8, 1000, dtype=torch.float).reshape(-1, 1)\n",
    "\n",
    "Uf = pinn.fluid_velocity(input_time)\n",
    "\n",
    "plt.figure(figsize=(8, 4), dpi=150)\n",
    "plt.grid(True, which=\"both\", ls=\":\")\n",
    "plt.plot(input_time.detach().numpy(), Uf.detach().numpy())\n",
    "plt.xlabel(\"time\")\n",
    "plt.ylabel(\"Uf\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MasterThesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
